a:5:{s:5:"title";N;s:6:"status";i:1;s:6:"number";i:80;s:8:"comments";a:80:{s:32:"7809898bd1d544419c47d78fff56bec5";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"kboockvar";s:4:"name";s:16:"Kenneth Boockvar";s:4:"mail";s:25:"kenneth.boockvar@mssm.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1534878617;}s:3:"raw";s:5696:"Boockvar—Assignment #1a

Draft a specific aims page of your proposed study. Consistent with most well-written D&I-focused specific aims pages, it should generally address the following 5 elements (and questions)

Health problem:  Why is this health issue a problem, how bad or burdensome is the problem, and why should we be concerned about it?
•	Among older veterans who also have Medicare coverage, 42.6% have been reported to use both VA and Medicare services, representing approximately 1 million veterans. Cross-site and cross-system service utilization results in test duplication, medication prescribing errors, and adverse events that lead to worse health outcomes and excess hospital utilization.  
  
Evidence-based practice, program, intervention or guideline:  What do we know works? How effective is this intervention or program? What is the evidence base for it? If additional evidence is needed regarding the effectiveness of the innovation, describe the types of effectiveness data needed (e.g., different populations, settings).
•	Electronic health information exchange (HIE) holds promise to improve quality of care for patients receiving care in multiple sites, and to improve outcomes associated with care handoffs and transfers.  A recent systematic review of high quality research studies demonstrated benefits from HIE on fewer duplicated procedures, reduced imaging, lower costs, and improved patient safety.  Studies evaluating community HIEs were more likely to find benefits than studies that evaluated vendor or other types of HIEs.  Data are still needed on the effectiveness of different ways of delivering and responding to health information received from an outside system.

Dissemination or implementation gap:  Summarize the gap between the existing evidence base and what is being done in practice. What evidence indicates that the intervention, policy, practice, or guideline is not being disseminated or implemented? What are some reasons why it is not being optimally (or at all) disseminated or implemented? What is known about potential barriers or facilitators to successful implementation? Consider whether there is evidence about differential effectiveness across populations, settings, and other contextual factors. If relevant, discuss whether the proposed implementation will consider any potential de-implementation of an existing program or practice.
•	The VA has initiated several programs to implement HIE between VA and non-VA providers, including exchange of care summaries and direct messaging between VA and non-VA providers. The latter has the potential to impact care through messages that enable VA providers to respond in a timely fashion to events they might otherwise be unaware of.  However, uptake of HIE inside and outside the VA has been slow.  In one study providers accessed information from an HIE in less than 5% of clinical situations that it was available.   Many geographic areas in the U.S. do not have meaningful HIE capabilities and financial losses has resulted in closure of some HIEs.  Barriers to HIE development and sustainability include:  technological (e.g., clinical data compatibility and gaps), regulatory (e.g., state laws about consent), legal (e.g., agreements between systems), uptake by end-users (e.g., provider workflow and use), and costs.
  
Specific Aims (~2-3 aims depending on size and scope of proposed study and funding mechanism):  State each objective of your proposed study, followed by 2-3 sentences summarizing how you will accomplish the objective, and answer any stated research questions or hypotheses.  Your specific aims should include, if possible, information about the population being targeted, context/setting of the proposed study, scope (e.g., number of sites) and timeframe.

The overarching objective of this study is to understand the barriers to implementation of HIE in the VA system and to identify interventions to improve it, with a focus on ways to implement HIE to enhance provider decision-making.  The specific aims of this study are to:  
•	Describe the barriers to HIE implementation in the VA:  We proposed to conduct a qualitative study with VA system stakeholders.  We will conduct open-ended one-to-one interviews with providers and administrators at multiple VA medical centers that have varying degrees of HIE implementation.  We will also observe VA providers interacting with HIE systems while “thinking aloud.” Interviews will be recorded, transcribed and coded using standard qualitative methods to identify barriers to HIE uptake and implementation.  
•	Propose strategies for overcoming these barriers to optimize implementation of HIE in the VA:  We will utilize the RE-AIM framework to improve implementation of HIE, focusing on uptake by providers who are likely end-users of HIE.  We hypothesize that results of Aim 1 will inform selection of activities designed to improve adoption and implementation of HIE.

Importance to field:  Briefly summarize why the proposed study is important to the field, advances our knowledge, and fills a gap in the D&I literature.
•	The overall objective of this study is to apply methods from implementation science to inform improving use of HIE by VA providers.  Results from this study will yield strategies to help integrate use of HIE into routine VA health care.  We hypothesize to find modifiable barriers that can be addressed by interventions.  Project findings, when complete, will be submitted in a manuscript to an implementation journal.  In addition, follow-up research will be proposed to study implementation of HIE at multiple VA and non-VA sites in multi-center studies.  

.  

";s:5:"xhtml";s:5810:"Boockvar—Assignment #1a<br /><br />Draft a specific aims page of your proposed study. Consistent with most well-written D&amp;I-focused specific aims pages, it should generally address the following 5 elements (and questions)<br /><br />Health problem:  Why is this health issue a problem, how bad or burdensome is the problem, and why should we be concerned about it?<br />•	Among older veterans who also have Medicare coverage, 42.6% have been reported to use both VA and Medicare services, representing approximately 1 million veterans. Cross-site and cross-system service utilization results in test duplication, medication prescribing errors, and adverse events that lead to worse health outcomes and excess hospital utilization.  <br />  <br />Evidence-based practice, program, intervention or guideline:  What do we know works? How effective is this intervention or program? What is the evidence base for it? If additional evidence is needed regarding the effectiveness of the innovation, describe the types of effectiveness data needed (e.g., different populations, settings).<br />•	Electronic health information exchange (HIE) holds promise to improve quality of care for patients receiving care in multiple sites, and to improve outcomes associated with care handoffs and transfers.  A recent systematic review of high quality research studies demonstrated benefits from HIE on fewer duplicated procedures, reduced imaging, lower costs, and improved patient safety.  Studies evaluating community HIEs were more likely to find benefits than studies that evaluated vendor or other types of HIEs.  Data are still needed on the effectiveness of different ways of delivering and responding to health information received from an outside system.<br /><br />Dissemination or implementation gap:  Summarize the gap between the existing evidence base and what is being done in practice. What evidence indicates that the intervention, policy, practice, or guideline is not being disseminated or implemented? What are some reasons why it is not being optimally (or at all) disseminated or implemented? What is known about potential barriers or facilitators to successful implementation? Consider whether there is evidence about differential effectiveness across populations, settings, and other contextual factors. If relevant, discuss whether the proposed implementation will consider any potential de-implementation of an existing program or practice.<br />•	The VA has initiated several programs to implement HIE between VA and non-VA providers, including exchange of care summaries and direct messaging between VA and non-VA providers. The latter has the potential to impact care through messages that enable VA providers to respond in a timely fashion to events they might otherwise be unaware of.  However, uptake of HIE inside and outside the VA has been slow.  In one study providers accessed information from an HIE in less than 5% of clinical situations that it was available.   Many geographic areas in the U.S. do not have meaningful HIE capabilities and financial losses has resulted in closure of some HIEs.  Barriers to HIE development and sustainability include:  technological (e.g., clinical data compatibility and gaps), regulatory (e.g., state laws about consent), legal (e.g., agreements between systems), uptake by end-users (e.g., provider workflow and use), and costs.<br />  <br />Specific Aims (~2-3 aims depending on size and scope of proposed study and funding mechanism):  State each objective of your proposed study, followed by 2-3 sentences summarizing how you will accomplish the objective, and answer any stated research questions or hypotheses.  Your specific aims should include, if possible, information about the population being targeted, context/setting of the proposed study, scope (e.g., number of sites) and timeframe.<br /><br />The overarching objective of this study is to understand the barriers to implementation of HIE in the VA system and to identify interventions to improve it, with a focus on ways to implement HIE to enhance provider decision-making.  The specific aims of this study are to:  <br />•	Describe the barriers to HIE implementation in the VA:  We proposed to conduct a qualitative study with VA system stakeholders.  We will conduct open-ended one-to-one interviews with providers and administrators at multiple VA medical centers that have varying degrees of HIE implementation.  We will also observe VA providers interacting with HIE systems while “thinking aloud.” Interviews will be recorded, transcribed and coded using standard qualitative methods to identify barriers to HIE uptake and implementation.  <br />•	Propose strategies for overcoming these barriers to optimize implementation of HIE in the VA:  We will utilize the RE-AIM framework to improve implementation of HIE, focusing on uptake by providers who are likely end-users of HIE.  We hypothesize that results of Aim 1 will inform selection of activities designed to improve adoption and implementation of HIE.<br /><br />Importance to field:  Briefly summarize why the proposed study is important to the field, advances our knowledge, and fills a gap in the D&amp;I literature.<br />•	The overall objective of this study is to apply methods from implementation science to inform improving use of HIE by VA providers.  Results from this study will yield strategies to help integrate use of HIE into routine VA health care.  We hypothesize to find modifiable barriers that can be addressed by interventions.  Project findings, when complete, will be submitted in a manuscript to an implementation journal.  In addition, follow-up research will be proposed to study implementation of HIE at multiple VA and non-VA sites in multi-center studies.  <br /><br />.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"9ac6bf1db50d7cd16b20f3b00ea8a6e0";}s:4:"show";b:1;s:3:"cid";s:32:"7809898bd1d544419c47d78fff56bec5";}s:32:"2c42d3b994d5a348b0101c87935bd032";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"sshohat";s:4:"name";s:20:"Sivan Spitzer-Shohat";s:4:"mail";s:30:"sivan.spitzer-shohat@biu.ac.il";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1534949915;}s:3:"raw";s:7094:"Spitzer-Shohat - Assignment #1a
1.	Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&I-focused specific aims pages.

Working Title: Overcoming the Translation Challenge of Implementing Equity

Health care organizations have aimed to reduce inequities in access and quality of care for over 30 years (1). While some efforts have shown success in minimizing disparities (2,3) others have documented stagnation or even exacerbation (4,5). Different conceptual frameworks guide how an organization can change its policies and practices to make care and outcomes more equitable for patients, and how the organization itself can become more equitable(6–10). Nonetheless, health care organizations often struggle with implementing these frameworks (11–14). 
Change is complex because comprehensive equity efforts require health care providers to tailor access and processes of care to different population groups, while also changing internal processes to promote diversity and inclusion within the organization, such as through employee hiring and retention(15,16). Interventions require changing policies, processes and practices throughout multiple levels of the organization. Major obstacles include the difficulty of changing the organizational culture and environment, carving out a new course when the organization seems to be functioning well, and planning and executing implementation(17–19). Additionally, whereas equity as a value laden concept is understood, its translation into organizational processes is not always clear(12,13,20). Often, executive leadership outline the overarching equity aims, leaving the process of intra-organizational translation, adaptation, and implementation across different department and staff levels a black box to decipher. Middle managers entrusted with driving change and implementation often lack the knowledge and skills to effectively translate change processes (21,22). This often leads to organizational ‘sensemaking’, a process of social construction, in which individuals attempt to interpret and explain a set of cues, such as change initiatives, to create a plan of action for dealing with uncertainty or ambiguity (23,24). 
The proposed research aims to address the organizational translation process of equity through the development and evaluation of a training program and its effect on implementation of the organization-wide equity initiative currently implemented by University of Chicago’s Medical Center (UChicago Medicine). In 2013, UChicago Medicine’s management put forth a plan to adopt and implement equity with the goal of transforming the organization and attaining equitable patient outcomes. The first phase, focused on formation of a committee-based governance structure to adjust organizational policies, create cross-departmental linkages and consult on implementation processes. An in-house cultural competence training course based on Relational Cultural Theory, Intersectionality and Critical Consciousness was developed with the aim of not only building awareness to the needs of different population groups, but also creating change agents who will assist in implementing equity throughout the organization. Interviews and surveys we conducted, with both committee members and mid-level managers, revealed that the organization was successful in creating a shared value on the importance of equity and a willingness to act, but encountered a major barrier in the ability of employees to understand: “What do I do?”, i.e. how to implement equity into their everyday work(25). 
Following these findings, UCM has decided in phase two, currently underway, to focus on the translation of equity into organizational practices, including a re-design of the governing committee’s role from policy-based to a proactive implementation body. 
The Aims of the proposed study include:

Aim 1 Pre-Implementation phase: Develop equity translation training focusing on the knowledge and skills needed for applying an equity lens to existing organizational processes.
•	Develop together with stakeholders from the organization, such as Director of Diversity and Equity, Director of Organization Change, Culture & Engagement, Director of Operational Excellence, an action-oriented training module, contextualized to UChicago Medicine, focusing on organizational equity translation and institutionalization of the sensemaking process(26).

Aim 2 Implementation phase: Recruit, train, and observe inter-departmental implementation pilot teams in their design and implementation of equity in three of the organization’s eight cross-cutting departments. 
•	Recruit, in a mixed top-down bottom-up approach, members from the Diversity and Equity governing committee representing different departments and different organizational positions, such as middle management as well as front line staff.
•	Train pilot team members on equity translation and implementation strategies.
•	Observe Pilot teams’ 12-month implementation of the organizational equity lens in their selected department focusing on: (a) their identification of key organizational departmental processes; (b) their translation of equity as it relates to the key processes identified, and (c) their implementation strategies and efforts. 

Aim 3 Evaluation phase: Determine to what extent translation training affected the work of implementation teams and assisted in achieving implementation of equity focused organizational processes. Employing a between-method triangulation research design, composed of qualitative and quantitative methods we will:
•	Assess implementation teams’ social network to ascertain not only intra-team relations but gain contextual insight to the differential actors’ advice network by mapping their ties to other people across the organization(27). 
•	Assess teams’ perceived effectiveness through an adapted tool evaluating their knowledge and skills for implementing an equity focus to daily work-processes, perceived organizational support, and agreement with the organizational goal(28) 
•	Conduct semi-structured interviews with implementation team members, department personnel, and committee members to understand implementation barriers and facilitators and perceived relevance of equity to daily work procedures
•	Assess, through interviews conducted with implementation team members, the extent training assisted in mitigating organizational sensemaking.

The proposed research will provide a comprehensive investigation of an area currently understudied—the translation process of equity into organizational processes. It will shed light on practical skills and training needed, assessing whether a general approach to equity translation is adequate or if there is a need to tailor training to specific groups/frontline staff, considering their area of care. Other deliverables may give insight to what organizational resources and support mechanisms will incentivize employees to adopt equitable organizational processes of care.  ";s:5:"xhtml";s:7225:"Spitzer-Shohat - Assignment #1a<br />1.	Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&amp;I-focused specific aims pages.<br /><br />Working Title: Overcoming the Translation Challenge of Implementing Equity<br /><br />Health care organizations have aimed to reduce inequities in access and quality of care for over 30 years (1). While some efforts have shown success in minimizing disparities (2,3) others have documented stagnation or even exacerbation (4,5). Different conceptual frameworks guide how an organization can change its policies and practices to make care and outcomes more equitable for patients, and how the organization itself can become more equitable(6–10). Nonetheless, health care organizations often struggle with implementing these frameworks (11–14). <br />Change is complex because comprehensive equity efforts require health care providers to tailor access and processes of care to different population groups, while also changing internal processes to promote diversity and inclusion within the organization, such as through employee hiring and retention(15,16). Interventions require changing policies, processes and practices throughout multiple levels of the organization. Major obstacles include the difficulty of changing the organizational culture and environment, carving out a new course when the organization seems to be functioning well, and planning and executing implementation(17–19). Additionally, whereas equity as a value laden concept is understood, its translation into organizational processes is not always clear(12,13,20). Often, executive leadership outline the overarching equity aims, leaving the process of intra-organizational translation, adaptation, and implementation across different department and staff levels a black box to decipher. Middle managers entrusted with driving change and implementation often lack the knowledge and skills to effectively translate change processes (21,22). This often leads to organizational ‘sensemaking’, a process of social construction, in which individuals attempt to interpret and explain a set of cues, such as change initiatives, to create a plan of action for dealing with uncertainty or ambiguity (23,24). <br />The proposed research aims to address the organizational translation process of equity through the development and evaluation of a training program and its effect on implementation of the organization-wide equity initiative currently implemented by University of Chicago’s Medical Center (UChicago Medicine). In 2013, UChicago Medicine’s management put forth a plan to adopt and implement equity with the goal of transforming the organization and attaining equitable patient outcomes. The first phase, focused on formation of a committee-based governance structure to adjust organizational policies, create cross-departmental linkages and consult on implementation processes. An in-house cultural competence training course based on Relational Cultural Theory, Intersectionality and Critical Consciousness was developed with the aim of not only building awareness to the needs of different population groups, but also creating change agents who will assist in implementing equity throughout the organization. Interviews and surveys we conducted, with both committee members and mid-level managers, revealed that the organization was successful in creating a shared value on the importance of equity and a willingness to act, but encountered a major barrier in the ability of employees to understand: “What do I do?”, i.e. how to implement equity into their everyday work(25). <br />Following these findings, UCM has decided in phase two, currently underway, to focus on the translation of equity into organizational practices, including a re-design of the governing committee’s role from policy-based to a proactive implementation body. <br />The Aims of the proposed study include:<br /><br />Aim 1 Pre-Implementation phase: Develop equity translation training focusing on the knowledge and skills needed for applying an equity lens to existing organizational processes.<br />•	Develop together with stakeholders from the organization, such as Director of Diversity and Equity, Director of Organization Change, Culture &amp; Engagement, Director of Operational Excellence, an action-oriented training module, contextualized to UChicago Medicine, focusing on organizational equity translation and institutionalization of the sensemaking process(26).<br /><br />Aim 2 Implementation phase: Recruit, train, and observe inter-departmental implementation pilot teams in their design and implementation of equity in three of the organization’s eight cross-cutting departments. <br />•	Recruit, in a mixed top-down bottom-up approach, members from the Diversity and Equity governing committee representing different departments and different organizational positions, such as middle management as well as front line staff.<br />•	Train pilot team members on equity translation and implementation strategies.<br />•	Observe Pilot teams’ 12-month implementation of the organizational equity lens in their selected department focusing on: (a) their identification of key organizational departmental processes; (b) their translation of equity as it relates to the key processes identified, and (c) their implementation strategies and efforts. <br /><br />Aim 3 Evaluation phase: Determine to what extent translation training affected the work of implementation teams and assisted in achieving implementation of equity focused organizational processes. Employing a between-method triangulation research design, composed of qualitative and quantitative methods we will:<br />•	Assess implementation teams’ social network to ascertain not only intra-team relations but gain contextual insight to the differential actors’ advice network by mapping their ties to other people across the organization(27). <br />•	Assess teams’ perceived effectiveness through an adapted tool evaluating their knowledge and skills for implementing an equity focus to daily work-processes, perceived organizational support, and agreement with the organizational goal(28) <br />•	Conduct semi-structured interviews with implementation team members, department personnel, and committee members to understand implementation barriers and facilitators and perceived relevance of equity to daily work procedures<br />•	Assess, through interviews conducted with implementation team members, the extent training assisted in mitigating organizational sensemaking.<br /><br />The proposed research will provide a comprehensive investigation of an area currently understudied—the translation process of equity into organizational processes. It will shed light on practical skills and training needed, assessing whether a general approach to equity translation is adequate or if there is a need to tailor training to specific groups/frontline staff, considering their area of care. Other deliverables may give insight to what organizational resources and support mechanisms will incentivize employees to adopt equitable organizational processes of care.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"b09f88b26b64d950c7d7354efffb56b0";}s:4:"show";b:1;s:3:"cid";s:32:"2c42d3b994d5a348b0101c87935bd032";}s:32:"e6ee7e8294d995ac6a4d247bed6a71a2";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"tlagu";s:4:"name";s:9:"Tara Lagu";s:4:"mail";s:28:"Tara.Lagu@baystatehealth.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1534956931;}s:3:"raw";s:5937:"Lagu—Assignment #1a

Draft a specific aims page of your proposed study. Consistent with most well-written D&I-focused specific aims pages, it should generally address the following 5 elements (and questions)

Health problem and Evidence-based practice, program, intervention or guideline:

Heart failure (HF) affects 6.5 million Americans and is the leading cause of hospitalization in patients 65 years and older. Patients admitted with a diagnosis of heart failure have a 30-day mortality rate of 10%, and a 30-day readmission rate of 22%. Cardiac rehabilitation (CR), a secondary prevention program that includes exercise training and risk factor education, improves outcomes in patient with HF. For patients with reduced ejection fraction (HFrEF), exercise training reduces mortality and hospitalizations. For patients with both preserved (HFpEF) and HFrEF, CR improves exercise tolerance and quality of life. Small studies of HFpEF patients also suggest reductions in rehospitalization. On the basis of this evidence, the American College of Cardiology and the American Heart Association recommend exercise training and HF-related self-care counseling (the two major components of CR) at the Class I level, and the Centers for Medicare and Medicaid Services (CMS) initiated coverage in 2014 for CR for Medicare beneficiaries with HFrEF.

Dissemination or implementation gap: 

Because HF is a chronic illness, patients are eligible for CR after an exacerbation resulting in hospitalization. To ensure that patients with HF are clinically stable, CMS mandates a 6-week waiting period between hospital discharge and CR initiation. This stands in contrast to the approach used for patients with acute coronary syndromes (ACS) and those undergoing coronary bypass (CABG), in whom the goal is to start CR immediately. Hospital-based recruitment strategies that are effective for conditions that do not include a waiting period may therefore be less successful at increasing CR participation among patients with HF. More importantly, the HF population is clinically different than patients with acute coronary events. Patients with HF are older, have greater comorbidity burden, and more likely to have impaired baseline functional status. Thus, the barriers to CR participation and the strategies that can be used to overcome these barriers are likely to be quite different for patients with HF compared to patients with ACS or CABG. However, there has been little research on the strategies that are most influential in increasing CR participation for HF patients.
Prior estimates report that only 2-17% of patients with HF attend CR after a hospitalization, and our preliminary studies suggest that there is variation, across hospitals, in participation in CR for patients with HF. There have been no analyses of outpatient rates of CR participation since the 2014 coverage change, however, and no estimates of participation at the level of hospital referral regions (HRR). Computing HRR-specific rates of CR participation has two advantages over hospital-based estimates: 1.) HRRs reflect the full ecosystem of factors that influence CR participation (patient, hospital, community and outpatient); and 2.) Because of the mandatory waiting period following hospitalization, outpatient referrals may play a more prominent role in recruitment strategies28; HRR-level estimates would allow us to better explore this possibility. 

Specific Aims:

The objective of this proposal is to identify implementation strategies that increase participation in CR among patients with HF and then prioritize those strategies that are the most acceptable, feasible, and responsive to the needs of stakeholders. In Aim 1, we will analyze Medicare claims to identify HRRs that are most and least successfully recruiting recently hospitalized patients with HF to CR (“high and low performers”). Beginning with these “deviant” CR programs, we will use the Consolidated Framework for Implementation Research (CFIR) to guide our qualitative methods, identifying facilitators and barriers to CR participation that map to specific CFIR constructs. We will present our findings to a panel of stakeholders who will prioritize strategies, and we will then recruit a subset of practices to pilot the strategies, in order to develop a final set of recommendations that will inform patients, practice, policy, and future research. 
1. Among recently hospitalized Medicare beneficiaries with HF, calculate HRR-specific risk-standardized rates of CR participation, identifying HRRs that have the highest and lowest rates. 
2. Refine hypotheses about the contextual actors and strategies that lead to effective implementation of CR and to identify barriers to participation.
3. Convene a panel of stakeholders that includes a.) patients and advocates; b.) clinicians and health system leaders (including representatives from the American Association of Cardiovascular and Pulmonary Rehabilitation [AACVPR]); and c.) policymakers and payers. Using a modified Delphi method, prioritize strategies that are the most acceptable, feasible, and responsive to community needs while also accounting for cultural and organizational factors.
4. In collaboration with the AACVPR, pilot the identified strategies among a subset of CR programs, obtain further iterative feedback, and refine a final set of recommendations that inform practice (e.g., outpatient and inpatient clinicians, CR programs, patients), policy (e.g., clinical practice guidelines and reimbursement strategies), and future research (e.g., implementation trials). 

Importance to field: 
The proposed set of investigations will identify implementation strategies that improve CR participation among patients with HF and create a roadmap for future development, including practice guidelines, national initiatives to improve CR participation among patients with HF, and future implementation trials.
";s:5:"xhtml";s:6050:"Lagu—Assignment #1a<br /><br />Draft a specific aims page of your proposed study. Consistent with most well-written D&amp;I-focused specific aims pages, it should generally address the following 5 elements (and questions)<br /><br />Health problem and Evidence-based practice, program, intervention or guideline:<br /><br />Heart failure (HF) affects 6.5 million Americans and is the leading cause of hospitalization in patients 65 years and older. Patients admitted with a diagnosis of heart failure have a 30-day mortality rate of 10%, and a 30-day readmission rate of 22%. Cardiac rehabilitation (CR), a secondary prevention program that includes exercise training and risk factor education, improves outcomes in patient with HF. For patients with reduced ejection fraction (HFrEF), exercise training reduces mortality and hospitalizations. For patients with both preserved (HFpEF) and HFrEF, CR improves exercise tolerance and quality of life. Small studies of HFpEF patients also suggest reductions in rehospitalization. On the basis of this evidence, the American College of Cardiology and the American Heart Association recommend exercise training and HF-related self-care counseling (the two major components of CR) at the Class I level, and the Centers for Medicare and Medicaid Services (CMS) initiated coverage in 2014 for CR for Medicare beneficiaries with HFrEF.<br /><br />Dissemination or implementation gap: <br /><br />Because HF is a chronic illness, patients are eligible for CR after an exacerbation resulting in hospitalization. To ensure that patients with HF are clinically stable, CMS mandates a 6-week waiting period between hospital discharge and CR initiation. This stands in contrast to the approach used for patients with acute coronary syndromes (ACS) and those undergoing coronary bypass (CABG), in whom the goal is to start CR immediately. Hospital-based recruitment strategies that are effective for conditions that do not include a waiting period may therefore be less successful at increasing CR participation among patients with HF. More importantly, the HF population is clinically different than patients with acute coronary events. Patients with HF are older, have greater comorbidity burden, and more likely to have impaired baseline functional status. Thus, the barriers to CR participation and the strategies that can be used to overcome these barriers are likely to be quite different for patients with HF compared to patients with ACS or CABG. However, there has been little research on the strategies that are most influential in increasing CR participation for HF patients.<br />Prior estimates report that only 2-17% of patients with HF attend CR after a hospitalization, and our preliminary studies suggest that there is variation, across hospitals, in participation in CR for patients with HF. There have been no analyses of outpatient rates of CR participation since the 2014 coverage change, however, and no estimates of participation at the level of hospital referral regions (HRR). Computing HRR-specific rates of CR participation has two advantages over hospital-based estimates: 1.) HRRs reflect the full ecosystem of factors that influence CR participation (patient, hospital, community and outpatient); and 2.) Because of the mandatory waiting period following hospitalization, outpatient referrals may play a more prominent role in recruitment strategies28; HRR-level estimates would allow us to better explore this possibility. <br /><br />Specific Aims:<br /><br />The objective of this proposal is to identify implementation strategies that increase participation in CR among patients with HF and then prioritize those strategies that are the most acceptable, feasible, and responsive to the needs of stakeholders. In Aim 1, we will analyze Medicare claims to identify HRRs that are most and least successfully recruiting recently hospitalized patients with HF to CR (“high and low performers”). Beginning with these “deviant” CR programs, we will use the Consolidated Framework for Implementation Research (CFIR) to guide our qualitative methods, identifying facilitators and barriers to CR participation that map to specific CFIR constructs. We will present our findings to a panel of stakeholders who will prioritize strategies, and we will then recruit a subset of practices to pilot the strategies, in order to develop a final set of recommendations that will inform patients, practice, policy, and future research. <br />1. Among recently hospitalized Medicare beneficiaries with HF, calculate HRR-specific risk-standardized rates of CR participation, identifying HRRs that have the highest and lowest rates. <br />2. Refine hypotheses about the contextual actors and strategies that lead to effective implementation of CR and to identify barriers to participation.<br />3. Convene a panel of stakeholders that includes a.) patients and advocates; b.) clinicians and health system leaders (including representatives from the American Association of Cardiovascular and Pulmonary Rehabilitation [AACVPR]); and c.) policymakers and payers. Using a modified Delphi method, prioritize strategies that are the most acceptable, feasible, and responsive to community needs while also accounting for cultural and organizational factors.<br />4. In collaboration with the AACVPR, pilot the identified strategies among a subset of CR programs, obtain further iterative feedback, and refine a final set of recommendations that inform practice (e.g., outpatient and inpatient clinicians, CR programs, patients), policy (e.g., clinical practice guidelines and reimbursement strategies), and future research (e.g., implementation trials). <br /><br />Importance to field: <br />The proposed set of investigations will identify implementation strategies that improve CR participation among patients with HF and create a roadmap for future development, including practice guidelines, national initiatives to improve CR participation among patients with HF, and future implementation trials.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"d1c1b0eeb8fbe92c2b90ccdc95c9240a";}s:4:"show";b:1;s:3:"cid";s:32:"e6ee7e8294d995ac6a4d247bed6a71a2";}s:32:"9a718b0b5a506fbdea1ecea21c84b63d";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"kfisher";s:4:"name";s:15:"Kimberly Fisher";s:4:"mail";s:33:"Kimberly.Fisher@umassmemorial.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535110681;}s:3:"raw";s:5244:"Fisher- Assignment #1a 

Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&I-focused specific aims pages, it should generally address the following 5 elements (and questions). 

Health problem:
Patients are extremely vulnerable to breakdowns in care during the transition from the hospital to other care settings, such as home.  These breakdowns have significant negative consequences including the need for additional medical care (unplanned doctor and emergency room visits, re-admission to the hospital), prolonged symptoms, emotional distress, and life disruption for patients and their families.  Improving the care provided during the high-risk transition from the hospital to home (‘care transitions’) is therefore a major goal of the healthcare system.  Indeed, the Centers for Medicare and Medicaid Services (CMS) and federal funding agencies such as the Agency for Healthcare Research and Quality (AHRQ) have made significant investments in initiatives to improve care transitions. 
	
Evidence-based practice, program, intervention or guideline:
There has been a proliferation of interventions to reduce readmissions.  However, a systematic review found that despite individually successful studies, no intervention or bundle of interventions that consistently reduces readmissions has been identified.  Interventions targeting multiple care transitions domains (e.g., symptom management, discharge planning) are associated with more reliable reductions in readmissions, but these interventions are often resource intensive, requiring additional training and staff.   As a result, many hospitals may not have the resources required to adopt these interventions. 

Implementation gap:
	Despite the many interventions to improve care transitions, the quality of care transition across hospitals in the U.S. remains low and highly variable between hospitals.  Only half of U.S. hospitals achieved a care transition star rating of > 3 (scale 1-5) in 2015, demonstrating significant room for improvement in care transitions.  The exact reason why care transition quality remains low is unknown.  One possible explanation is that care transition interventions have not been adopted widely owing to their resource intensive nature.  Additionally, specific interventions may not be effective across all hospital settings.  Addressing these gaps requires identification of care transition interventions that are feasible for hospitals to adopt and a greater understanding of whether there is differential effectiveness across different hospital settings.   
Regardless of the reason many hospitals have not achieved high quality care transitions, some hospitals already consistently achieve excellent care transitions.  

This project leverages the existing variability in hospital performance to identify factors and strategies used by high-performing hospitals, with an overall goal of identifying effective and feasible care transition strategies using a positive deviance approach.  Positive deviance is a research method based on the premise that practical solutions to problems can be discovered through the study of high performers, or “positive deviants”.  In addition to discovering effective strategies to address a particular problem, positive deviance can identify influential contextual factors, such as institutional culture, which is a critical determinant of whether an intervention is successfully implemented. Second, by identifying strategies already in use by members of the target population, positive deviance discovers more practical and sustainable approaches than externally developed interventions, increasing the likelihood that these interventions will be adopted by other (non-high-performing) members of the target population.

Specific aims of this project are to:
1. Generate and refine hypotheses about contextual factors and strategies that result in excellent care transitions through qualitative semi-structured interviews (n = 40) with key informants (e.g., quality officers, medical and nursing directors, frontline staff, discharge coordinators) at a sample (n = 10) of high-performing acute-care hospitals.  High performing hospitals will be identified through analysis of the publicly available results of the Care Transition Measure-3 (CTM-3), after exclusion of specialty hospitals.  They will be selected to be diverse with regards to size, geographic location, and teaching status.   
2. Determine which factors and strategies identified in Aim 1 are associated with care transition performance through a multi-item close-ended survey of a large, representative sample of U.S. hospitals.  Subset analysis will be conducted to assess for differential effectiveness based on hospital setting or contextual factors.  

Importance to field:
The proposed study will lead to the discovery of effective strategies for achieving high-quality care transitions that are in use by high-performing hospitals. This project will lead to the development of a care transition intervention (or suite of interventions) based on practices of high-performing hospitals, for dissemination to hospitals seeking to improve their care transition performance. ";s:5:"xhtml";s:5355:"Fisher- Assignment #1a <br /><br />Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&amp;I-focused specific aims pages, it should generally address the following 5 elements (and questions). <br /><br />Health problem:<br />Patients are extremely vulnerable to breakdowns in care during the transition from the hospital to other care settings, such as home.  These breakdowns have significant negative consequences including the need for additional medical care (unplanned doctor and emergency room visits, re-admission to the hospital), prolonged symptoms, emotional distress, and life disruption for patients and their families.  Improving the care provided during the high-risk transition from the hospital to home (‘care transitions’) is therefore a major goal of the healthcare system.  Indeed, the Centers for Medicare and Medicaid Services (CMS) and federal funding agencies such as the Agency for Healthcare Research and Quality (AHRQ) have made significant investments in initiatives to improve care transitions. <br />	<br />Evidence-based practice, program, intervention or guideline:<br />There has been a proliferation of interventions to reduce readmissions.  However, a systematic review found that despite individually successful studies, no intervention or bundle of interventions that consistently reduces readmissions has been identified.  Interventions targeting multiple care transitions domains (e.g., symptom management, discharge planning) are associated with more reliable reductions in readmissions, but these interventions are often resource intensive, requiring additional training and staff.   As a result, many hospitals may not have the resources required to adopt these interventions. <br /><br />Implementation gap:<br />	Despite the many interventions to improve care transitions, the quality of care transition across hospitals in the U.S. remains low and highly variable between hospitals.  Only half of U.S. hospitals achieved a care transition star rating of &gt; 3 (scale 1-5) in 2015, demonstrating significant room for improvement in care transitions.  The exact reason why care transition quality remains low is unknown.  One possible explanation is that care transition interventions have not been adopted widely owing to their resource intensive nature.  Additionally, specific interventions may not be effective across all hospital settings.  Addressing these gaps requires identification of care transition interventions that are feasible for hospitals to adopt and a greater understanding of whether there is differential effectiveness across different hospital settings.   <br />Regardless of the reason many hospitals have not achieved high quality care transitions, some hospitals already consistently achieve excellent care transitions.  <br /><br />This project leverages the existing variability in hospital performance to identify factors and strategies used by high-performing hospitals, with an overall goal of identifying effective and feasible care transition strategies using a positive deviance approach.  Positive deviance is a research method based on the premise that practical solutions to problems can be discovered through the study of high performers, or “positive deviants”.  In addition to discovering effective strategies to address a particular problem, positive deviance can identify influential contextual factors, such as institutional culture, which is a critical determinant of whether an intervention is successfully implemented. Second, by identifying strategies already in use by members of the target population, positive deviance discovers more practical and sustainable approaches than externally developed interventions, increasing the likelihood that these interventions will be adopted by other (non-high-performing) members of the target population.<br /><br />Specific aims of this project are to:<br />1. Generate and refine hypotheses about contextual factors and strategies that result in excellent care transitions through qualitative semi-structured interviews (n = 40) with key informants (e.g., quality officers, medical and nursing directors, frontline staff, discharge coordinators) at a sample (n = 10) of high-performing acute-care hospitals.  High performing hospitals will be identified through analysis of the publicly available results of the Care Transition Measure-3 (CTM-3), after exclusion of specialty hospitals.  They will be selected to be diverse with regards to size, geographic location, and teaching status.   <br />2. Determine which factors and strategies identified in Aim 1 are associated with care transition performance through a multi-item close-ended survey of a large, representative sample of U.S. hospitals.  Subset analysis will be conducted to assess for differential effectiveness based on hospital setting or contextual factors.  <br /><br />Importance to field:<br />The proposed study will lead to the discovery of effective strategies for achieving high-quality care transitions that are in use by high-performing hospitals. This project will lead to the development of a care transition intervention (or suite of interventions) based on practices of high-performing hospitals, for dissemination to hospitals seeking to improve their care transition performance.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"dedbd315bf2155e9b96c0748618c4b7e";}s:4:"show";b:1;s:3:"cid";s:32:"9a718b0b5a506fbdea1ecea21c84b63d";}s:32:"b44dfdf4b47c0e775b908d9e1dd10c44";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jpittman";s:4:"name";s:13:"James Pittman";s:4:"mail";s:17:"jpittman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535119470;}s:3:"raw";s:7029:"Pittman - Assignment #1a 

1a. Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&I-focused specific aims pages, it should generally address the following 5 elements (and questions). 

Health Problem: The Veterans Health Administration (VHA) serves over 9 million Veterans each year with finite resources and overwhelming demand. To improve quality and access to healthcare several initiatives are underway. One example is the prioritization of technology and innovation to aid in the provision of health care. There is a growing body of literature supporting the feasibility of technology to automate patient self-report health screening in a variety of health settings. Technology supported collection of screening data has also been shown to yield benefits for patients, providers, and health systems. Automated self-report screening can help with concurrent mental health initiatives such as the measurement-based care initiative, which supports collecting information from consumers at regular intervals during office visits to assist in making shared decisions about care.

Evidence Based Program: eScreening is a web-based application developed by the Center of Excellence for Stress and Mental Health (CESAMH) in conjunction with VHA Center of Innovation and with input from clinical, research, informatics, and technology subject matter experts at VA San Diego. eScreening uses evidence-based screening tools and provides customized and automated self-report mental and physical health screening via mobile tablet for Veterans seen in VA healthcare settings. The system has a robust forms editor that allows content to be tailored to fit the needs of facilities and clinics to assist in meeting clinical demands. eScreening reads from and writes to the VHA electronic medical record (EMR) and provides clinicians with real-time scoring of screens and an individualized editable EMR note. In addition, the program generates printable, personalized summaries for Veterans, produces safety and risk alerts for clinicians, and can be used to monitor health symptoms over-time.

The eScreening interface was developed iteratively and refined based on input from two rounds of focus groups to enhance usability. It has been utilized in over 6,000 Veterans to support measurement-based care, shared decision making, and early identification initiatives. In a pilot study comparing eScreening to paper-based screening in 1,372 newly enrolling Veterans, accessibility, rate of screening completion, and clinical processes were significantly better with eScreening than paper screening. Further, eScreening increased the rate of suicide risk assessment by 15% in the initial pilot. Larger evaluation of eScreening in other VHA healthcare systems and programs is needed to establish the full impact of eScreening on VHA. Nevertheless, eScreening was named a Gold Standard Practice for diffusion throughout VA by the Under-Secretary for Health and is aligned with the organizational priorities. 

Implementation Gap: The eScreening program has been disseminated to 6 other VHA systems with an additional 50 on the waitlist for rollout. eScreening was allocated FY19 technology funding to develop a new enterprise version of the software that will be centrally deployed to the 170 VHA Healthcare systems while supporting further dissemination of the current version. The VHA Office of Information Technology will provide technical support for each implementation site for the enterprise version of eScreening as it does for other centralized VHA informatics tools. We have already garnered VA Central Office executive sponsorship from both the Office of Mental Health and the Office of Care Management and Social Work Services to move eScreening forward from a programmatic level. Because technological assistance will be in place for eScreening implementation, the focus of this study will be on the evaluation of the process of implementation at the clinic level. 

Each VHA Healthcare system has a systems redesign program that helps programs within the system become more efficient and effective using Lean Thinking and Six Sigma methodology. To prepare for broader scale-up of eScreening, we used an innovative implementation approach, Rapid Process Improvement Workshop (RPIW) guided by Lean Thinking and Six Sigma methodology to develop and pilot test an implementation playbook for eScreening. The playbook outlines steps for how an internal systems redesign facilitator can work with each program to conduct a modified RPIW process customizing a plan for integration of eScreening into clinical processes. Results from a recent pilot test using this strategy suggest that facilitation and full use of the RPIW process are important for successful implementation of eScreening. 

Specific Aims: In order to build an adaptive implementation strategy for national roll-out of eScreening, this study will utilize a sequential multiple assignment randomized trial (SMART) approach. The unit of analysis will be specific clinical programs implementing eScreening. The focus will be on mental health outpatient teams, transition care management teams, primary care, and specialty programs. Eighty clinical teams will be provided the eScreening playbook for implementation of eScreening. Programs that have not progressed in implementation of eScreening after 6 weeks, as defined by initiating eScreening with greater than 50% of the programs’ patients, will be randomized to either internal facilitation via the Systems Redesign program or internal facilitation plus external facilitation via the CESAMH eScreening team. 

This study will use the Practical, Robust, Implementation, and Sustainability model (PRISM) in this 4-year hybrid type 2 SMART trial to evaluate the effectiveness of eScreening and the implementation strategy of RPIW with or without facilitation. Aims are to:

1)Evaluate the use of facilitation and RPIW strategies for implementing eScreening in multiple VHA sites in mental health outpatient teams, transition care management teams, primary care, and specialty programs.  
2)Examine the effectiveness of eScreening compared to care as usual on rapid identification of mental health concerns in transition care management, primary care, and specialty programs. 
3)Examine the effectiveness of eScreening compared to care as usual on the use of measurement-based treatment monitoring and on facilitating shared decision making. 

Importance to the field: Results of this study will benefit implementation science as a field because it will identify conditions that influence the implementation strategy. The strategy could then be used in other initiatives in large health organizations preparing for scale-up of other interventions or programs. This study is of particular value to VHA because it leverages existing infrastructure at VHA facilities to facilitate implementation using familiar language and processes, and the systems redesign program.
";s:5:"xhtml";s:7142:"Pittman - Assignment #1a <br /><br />1a. Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&amp;I-focused specific aims pages, it should generally address the following 5 elements (and questions). <br /><br />Health Problem: The Veterans Health Administration (VHA) serves over 9 million Veterans each year with finite resources and overwhelming demand. To improve quality and access to healthcare several initiatives are underway. One example is the prioritization of technology and innovation to aid in the provision of health care. There is a growing body of literature supporting the feasibility of technology to automate patient self-report health screening in a variety of health settings. Technology supported collection of screening data has also been shown to yield benefits for patients, providers, and health systems. Automated self-report screening can help with concurrent mental health initiatives such as the measurement-based care initiative, which supports collecting information from consumers at regular intervals during office visits to assist in making shared decisions about care.<br /><br />Evidence Based Program: eScreening is a web-based application developed by the Center of Excellence for Stress and Mental Health (CESAMH) in conjunction with VHA Center of Innovation and with input from clinical, research, informatics, and technology subject matter experts at VA San Diego. eScreening uses evidence-based screening tools and provides customized and automated self-report mental and physical health screening via mobile tablet for Veterans seen in VA healthcare settings. The system has a robust forms editor that allows content to be tailored to fit the needs of facilities and clinics to assist in meeting clinical demands. eScreening reads from and writes to the VHA electronic medical record (EMR) and provides clinicians with real-time scoring of screens and an individualized editable EMR note. In addition, the program generates printable, personalized summaries for Veterans, produces safety and risk alerts for clinicians, and can be used to monitor health symptoms over-time.<br /><br />The eScreening interface was developed iteratively and refined based on input from two rounds of focus groups to enhance usability. It has been utilized in over 6,000 Veterans to support measurement-based care, shared decision making, and early identification initiatives. In a pilot study comparing eScreening to paper-based screening in 1,372 newly enrolling Veterans, accessibility, rate of screening completion, and clinical processes were significantly better with eScreening than paper screening. Further, eScreening increased the rate of suicide risk assessment by 15% in the initial pilot. Larger evaluation of eScreening in other VHA healthcare systems and programs is needed to establish the full impact of eScreening on VHA. Nevertheless, eScreening was named a Gold Standard Practice for diffusion throughout VA by the Under-Secretary for Health and is aligned with the organizational priorities. <br /><br />Implementation Gap: The eScreening program has been disseminated to 6 other VHA systems with an additional 50 on the waitlist for rollout. eScreening was allocated FY19 technology funding to develop a new enterprise version of the software that will be centrally deployed to the 170 VHA Healthcare systems while supporting further dissemination of the current version. The VHA Office of Information Technology will provide technical support for each implementation site for the enterprise version of eScreening as it does for other centralized VHA informatics tools. We have already garnered VA Central Office executive sponsorship from both the Office of Mental Health and the Office of Care Management and Social Work Services to move eScreening forward from a programmatic level. Because technological assistance will be in place for eScreening implementation, the focus of this study will be on the evaluation of the process of implementation at the clinic level. <br /><br />Each VHA Healthcare system has a systems redesign program that helps programs within the system become more efficient and effective using Lean Thinking and Six Sigma methodology. To prepare for broader scale-up of eScreening, we used an innovative implementation approach, Rapid Process Improvement Workshop (RPIW) guided by Lean Thinking and Six Sigma methodology to develop and pilot test an implementation playbook for eScreening. The playbook outlines steps for how an internal systems redesign facilitator can work with each program to conduct a modified RPIW process customizing a plan for integration of eScreening into clinical processes. Results from a recent pilot test using this strategy suggest that facilitation and full use of the RPIW process are important for successful implementation of eScreening. <br /><br />Specific Aims: In order to build an adaptive implementation strategy for national roll-out of eScreening, this study will utilize a sequential multiple assignment randomized trial (SMART) approach. The unit of analysis will be specific clinical programs implementing eScreening. The focus will be on mental health outpatient teams, transition care management teams, primary care, and specialty programs. Eighty clinical teams will be provided the eScreening playbook for implementation of eScreening. Programs that have not progressed in implementation of eScreening after 6 weeks, as defined by initiating eScreening with greater than 50% of the programs’ patients, will be randomized to either internal facilitation via the Systems Redesign program or internal facilitation plus external facilitation via the CESAMH eScreening team. <br /><br />This study will use the Practical, Robust, Implementation, and Sustainability model (PRISM) in this 4-year hybrid type 2 SMART trial to evaluate the effectiveness of eScreening and the implementation strategy of RPIW with or without facilitation. Aims are to:<br /><br />1)Evaluate the use of facilitation and RPIW strategies for implementing eScreening in multiple VHA sites in mental health outpatient teams, transition care management teams, primary care, and specialty programs.  <br />2)Examine the effectiveness of eScreening compared to care as usual on rapid identification of mental health concerns in transition care management, primary care, and specialty programs. <br />3)Examine the effectiveness of eScreening compared to care as usual on the use of measurement-based treatment monitoring and on facilitating shared decision making. <br /><br />Importance to the field: Results of this study will benefit implementation science as a field because it will identify conditions that influence the implementation strategy. The strategy could then be used in other initiatives in large health organizations preparing for scale-up of other interventions or programs. This study is of particular value to VHA because it leverages existing infrastructure at VHA facilities to facilitate implementation using familiar language and processes, and the systems redesign program.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"513ae4a492c994356a55fc59c593bba4";}s:4:"show";b:1;s:3:"cid";s:32:"b44dfdf4b47c0e775b908d9e1dd10c44";}s:32:"2eee2bfc8c72178b255c810bbe07107c";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"bjones";s:4:"name";s:13:"Barbara Jones";s:4:"mail";s:26:"Barbara.Jones@hsc.utah.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535133954;}s:3:"raw";s:4698:"Jones - Assignment #1a: Draft Specific Aims Page (Due to Small Group August 24, 2018)
Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&I-focused specific aims pages, it should generally address the following 5 elements (and questions):
Pneumonia is the most common infectious cause of death in the United States, with an estimated 50,000 deaths per year. Accurate diagnosis, site-of-care, and antibiotic selection decisions are crucial to appropriate management. The adoption of consensus practice guidelines has been associated with improved outcomes and reduction of unnecessary utilization, and local adaptation and implementation of guidelines has been listed as a grade 1a recommendation. However, management practices in pneumonia still vary substantially across facilities and providers, with widespread hospital-wide differences in hospital care, antimicrobial use, and mortality reported. While this variation may be viewed as a failure of clinicians to apply standard practice to their patients, it may also be viewed as a failure of that standard practice to be applicable to many patients and settings. 
The limitations of pneumonia practice guidelines are increasingly recognized: some components of the guidelines lack evidence, may fail to provide guidance for complex patients or settings, and may also lack relevance for patients and settings that are underrepresented in the evidence and clinical experience from which they were derived. While pneumonia is a common disease, each clinical scenario requires a substantial amount of clinical judgment beyond the guidelines to provide the right care. Strict implementation of guidelines, or emphasizing performance that is defined strictly by guideline adherence, may overlook this complexity and discourage providers from deliberatively individualizing care to their patients. 
The VA is a large health system that serves a diverse population of patients across a variety of settings. The growth of information technology and computerized clinical decision support across the VA also offers unprecedented opportunities to sustainably embed standardized care processes that can be adapted to diverse settings, and can also feed back information about clinical experience to a learning healthcare system. Rather than adopting a one-size-fits-all approach, we aim to leverage information technology to develop ways to learn from our own population and collective clinical experience, generate evidence that includes previously underrepresented patients and settings, and improve its applicability to real practice.
The purpose of this proposal is to design, implement, and test a computerized clinical decision support tool for pneumonia that simultaneously supports and learns from decision-making across diverse settings. The specific aims of the proposal are to:
1)	Characterize the process of clinical decision-making for pneumonia among emergency department (ED) providers across the VA healthcare system, with a focus on identifying setting-level differences in mental models of disease management, provider self-efficacy/motivation, information and clinical needs, and implementation constructs particularly surrounding informatics tool usability.
a.	Site visits will be conducted across 16 sites across the VA over a 2-year time period and will include qualitative physician intereviews with cognitive task analysis and CFIR pre-implementation evaluations, interviews with clinical leadership, and interviews and evaluations of informatics technology staff at each facility.
2)	Design and test pilot informatics tools for ED pneumonia management using an iterative user-centered design approach.
a.	A prototype will be introduced and adapted to each setting with collaboration of local clinicians, clinical leadership and informatics staff (clinical applications coordinators).
3)	Conduct a controlled implementation trial of an adaptive informatics tool for ED pneumonia management across multiple settings in the VA healthcare system.
a.	Stepped wedge design, or sequential multiple assignment randomized trial format over a 2-year period?
By designing, implementing, and evaluating informatics technology that engages with providers in a deliberative integration of standard care processes with patient and setting factors, we aim to simultaneously promote appropriate, individualized care and examine the process of clinical decision-making, interaction with informatics technology, and adaptation of guidelines and technology to each patient. This work will thus advance the fields of informatics, EHR usability, medical decision-making, and implementation science.
";s:5:"xhtml";s:4761:"Jones - Assignment #1a: Draft Specific Aims Page (Due to Small Group August 24, 2018)<br />Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&amp;I-focused specific aims pages, it should generally address the following 5 elements (and questions):<br />Pneumonia is the most common infectious cause of death in the United States, with an estimated 50,000 deaths per year. Accurate diagnosis, site-of-care, and antibiotic selection decisions are crucial to appropriate management. The adoption of consensus practice guidelines has been associated with improved outcomes and reduction of unnecessary utilization, and local adaptation and implementation of guidelines has been listed as a grade 1a recommendation. However, management practices in pneumonia still vary substantially across facilities and providers, with widespread hospital-wide differences in hospital care, antimicrobial use, and mortality reported. While this variation may be viewed as a failure of clinicians to apply standard practice to their patients, it may also be viewed as a failure of that standard practice to be applicable to many patients and settings. <br />The limitations of pneumonia practice guidelines are increasingly recognized: some components of the guidelines lack evidence, may fail to provide guidance for complex patients or settings, and may also lack relevance for patients and settings that are underrepresented in the evidence and clinical experience from which they were derived. While pneumonia is a common disease, each clinical scenario requires a substantial amount of clinical judgment beyond the guidelines to provide the right care. Strict implementation of guidelines, or emphasizing performance that is defined strictly by guideline adherence, may overlook this complexity and discourage providers from deliberatively individualizing care to their patients. <br />The VA is a large health system that serves a diverse population of patients across a variety of settings. The growth of information technology and computerized clinical decision support across the VA also offers unprecedented opportunities to sustainably embed standardized care processes that can be adapted to diverse settings, and can also feed back information about clinical experience to a learning healthcare system. Rather than adopting a one-size-fits-all approach, we aim to leverage information technology to develop ways to learn from our own population and collective clinical experience, generate evidence that includes previously underrepresented patients and settings, and improve its applicability to real practice.<br />The purpose of this proposal is to design, implement, and test a computerized clinical decision support tool for pneumonia that simultaneously supports and learns from decision-making across diverse settings. The specific aims of the proposal are to:<br />1)	Characterize the process of clinical decision-making for pneumonia among emergency department (ED) providers across the VA healthcare system, with a focus on identifying setting-level differences in mental models of disease management, provider self-efficacy/motivation, information and clinical needs, and implementation constructs particularly surrounding informatics tool usability.<br />a.	Site visits will be conducted across 16 sites across the VA over a 2-year time period and will include qualitative physician intereviews with cognitive task analysis and CFIR pre-implementation evaluations, interviews with clinical leadership, and interviews and evaluations of informatics technology staff at each facility.<br />2)	Design and test pilot informatics tools for ED pneumonia management using an iterative user-centered design approach.<br />a.	A prototype will be introduced and adapted to each setting with collaboration of local clinicians, clinical leadership and informatics staff (clinical applications coordinators).<br />3)	Conduct a controlled implementation trial of an adaptive informatics tool for ED pneumonia management across multiple settings in the VA healthcare system.<br />a.	Stepped wedge design, or sequential multiple assignment randomized trial format over a 2-year period?<br />By designing, implementing, and evaluating informatics technology that engages with providers in a deliberative integration of standard care processes with patient and setting factors, we aim to simultaneously promote appropriate, individualized care and examine the process of clinical decision-making, interaction with informatics technology, and adaptation of guidelines and technology to each patient. This work will thus advance the fields of informatics, EHR usability, medical decision-making, and implementation science.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"2eee2bfc8c72178b255c810bbe07107c";}s:32:"d1c1b0eeb8fbe92c2b90ccdc95c9240a";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lducharme";s:4:"name";s:13:"Lori Ducharme";s:4:"mail";s:21:"lori.ducharme@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535662403;}s:3:"raw";s:1425:"Hi Tara – this is an interesting concept and I could see it having broader appeal to any number of areas where we have difficulties linking patients with ongoing post-hospital recovery services.  I also like the idea of utilizing “positive deviance” to identify high performing organizations and then re-engineer your way to potential solutions.  (You and Kimberly might want to talk offline, as you're using similar approaches.)

I have a couple of questions for you to think about.  Your initial step is to analyze claims data to identify regions with the best CR recruitment approaches, but it’s unclear exactly how your qualitative methods would then be used to identify the actual strategies that are being used by each of the high performers.  How would you account for contextual factors (variations across sites) in the CR recruitment strategies they’ve chosen – i.e., is their success context-dependent?  And how does that factor into the stakeholder prioritization?  Finally, you’ll want to specify some actual implementation strategies for Step 4, in which you propose to pilot some of the CR recruitment activities – how would you go about putting these CR strategies into practice, and how might you take advantage of an experimental approach at that stage?

Just some food for thought at this point.  Look forward to talking on Tuesday’s call, and working with you throughout TIDIRH. – Lori
";s:5:"xhtml";s:1449:"Hi Tara – this is an interesting concept and I could see it having broader appeal to any number of areas where we have difficulties linking patients with ongoing post-hospital recovery services.  I also like the idea of utilizing “positive deviance” to identify high performing organizations and then re-engineer your way to potential solutions.  (You and Kimberly might want to talk offline, as you&#039;re using similar approaches.)<br /><br />I have a couple of questions for you to think about.  Your initial step is to analyze claims data to identify regions with the best CR recruitment approaches, but it’s unclear exactly how your qualitative methods would then be used to identify the actual strategies that are being used by each of the high performers.  How would you account for contextual factors (variations across sites) in the CR recruitment strategies they’ve chosen – i.e., is their success context-dependent?  And how does that factor into the stakeholder prioritization?  Finally, you’ll want to specify some actual implementation strategies for Step 4, in which you propose to pilot some of the CR recruitment activities – how would you go about putting these CR strategies into practice, and how might you take advantage of an experimental approach at that stage?<br /><br />Just some food for thought at this point.  Look forward to talking on Tuesday’s call, and working with you throughout TIDIRH. – Lori";s:6:"parent";s:32:"e6ee7e8294d995ac6a4d247bed6a71a2";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"d1c1b0eeb8fbe92c2b90ccdc95c9240a";}s:32:"dedbd315bf2155e9b96c0748618c4b7e";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lducharme";s:4:"name";s:13:"Lori Ducharme";s:4:"mail";s:21:"lori.ducharme@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535662724;}s:3:"raw";s:1392:"Hi Kimberly – it strikes me that there are some conceptual and methodological similarities between your proposal and Tara’s – you both may want to talk offline about some of these overlaps and how you’ve each been thinking them through.

I’d like to see you push your concept a little further in two ways.  First, for Aim 2, is there a way to also capture information on the acceptability, feasibility, and cost of the identified strategies?  Beyond that, if I’m looking at this as the beginnings of a 5-year NIH R01 proposal (bear with me!), then reviewers are going to want to see an Aim 3 that incorporates actual implementation of the identified effective strategies – how would hospitals learn *how* to implement these approaches in routine practice, and how might you arrive at that answer?  Can you unpack some of that?  For example, I could imagine a project that includes the development of an implementation toolkit that hospitals could use to identify their care transition gaps (how?), select from a menu of strategy options (how?), and follow a series of steps (like what?) to implement that option?  The foundational work you've proposed is important, but I'd like to see you at least imagine this out to an actual implementation phase and what that might look like as a research project.

Looking forward to seeing this develop over the course of TIDIRH! -- Lori
";s:5:"xhtml";s:1421:"Hi Kimberly – it strikes me that there are some conceptual and methodological similarities between your proposal and Tara’s – you both may want to talk offline about some of these overlaps and how you’ve each been thinking them through.<br /><br />I’d like to see you push your concept a little further in two ways.  First, for Aim 2, is there a way to also capture information on the acceptability, feasibility, and cost of the identified strategies?  Beyond that, if I’m looking at this as the beginnings of a 5-year NIH R01 proposal (bear with me!), then reviewers are going to want to see an Aim 3 that incorporates actual implementation of the identified effective strategies – how would hospitals learn *how* to implement these approaches in routine practice, and how might you arrive at that answer?  Can you unpack some of that?  For example, I could imagine a project that includes the development of an implementation toolkit that hospitals could use to identify their care transition gaps (how?), select from a menu of strategy options (how?), and follow a series of steps (like what?) to implement that option?  The foundational work you&#039;ve proposed is important, but I&#039;d like to see you at least imagine this out to an actual implementation phase and what that might look like as a research project.<br /><br />Looking forward to seeing this develop over the course of TIDIRH! -- Lori";s:6:"parent";s:32:"9a718b0b5a506fbdea1ecea21c84b63d";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"dedbd315bf2155e9b96c0748618c4b7e";}s:32:"9ac6bf1db50d7cd16b20f3b00ea8a6e0";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dgoodrich";s:4:"name";s:14:"David Goodrich";s:4:"mail";s:22:"David.Goodrich2@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535675000;}s:3:"raw";s:2433:"Hi Kenneth, I read your proposed study with great interest because care coordination between VHA and community providers is a national clinical priority, particularly for local hospital leaders who feel the pressure of recent legislation such as the VA MISSION ACT of 2018. My primary question is whether you feel that it is feasible to dial back the scale of your proposal? Your aims are very broad and I am not sure that looking at HIEs from a broad perspective will result in actionable findings that could benefit a particular population of veterans or type of VA service provider. Rather, could the focus of your project be limited to a particularly type of specialty service/medication prescribing commonly referred out to community providers within a limited number of VA healthcare systems (e.g., in one VISN) and/or requiring close care coordination? 

While Health Information Exchanges have the potential to create virtual Health Neighborhoods, the reality has not caught up with the well-entrenched barriers you identified. I suspect there is tremendous variation in care coordination capabilities of community providers, depending on their specialty and locale but contextual variation is not addressed. If your goal is to develop a clear set of implementation strategies to foster more effective care coordination, focusing your efforts on one area of care may make your findings more generalizable and impactful. Another way to think about this is: who would you pitch your idea to in Central Office if you were seeking operational support to implement a change? How would you explain to a policy maker or VA leader the logical progression of your research? Is it possible to craft your proposal that leads to testing your hypotheses and findings of your formative efforts in Aim 1? So if Aim 2 identifies strategies for overcoming barriers to HIE for a specific service, could Aim 3 test their effectiveness? How would you conceptualize effectiveness (from a theoretical perspective) vs a pragmatic perspective (e.g., usability, feasibility, cost, etc.) This is a really important topic and I look forward to seeing it evolve over the coming weeks and months! Check out the feedback on other projects and reflect on common themes that transcend HIEs. Please feel free to follow-up with questions and I encourage you to complete the survey on my feedback to let me know if my comments are helpful. Best regards, David ";s:5:"xhtml";s:2442:"Hi Kenneth, I read your proposed study with great interest because care coordination between VHA and community providers is a national clinical priority, particularly for local hospital leaders who feel the pressure of recent legislation such as the VA MISSION ACT of 2018. My primary question is whether you feel that it is feasible to dial back the scale of your proposal? Your aims are very broad and I am not sure that looking at HIEs from a broad perspective will result in actionable findings that could benefit a particular population of veterans or type of VA service provider. Rather, could the focus of your project be limited to a particularly type of specialty service/medication prescribing commonly referred out to community providers within a limited number of VA healthcare systems (e.g., in one VISN) and/or requiring close care coordination? <br /><br />While Health Information Exchanges have the potential to create virtual Health Neighborhoods, the reality has not caught up with the well-entrenched barriers you identified. I suspect there is tremendous variation in care coordination capabilities of community providers, depending on their specialty and locale but contextual variation is not addressed. If your goal is to develop a clear set of implementation strategies to foster more effective care coordination, focusing your efforts on one area of care may make your findings more generalizable and impactful. Another way to think about this is: who would you pitch your idea to in Central Office if you were seeking operational support to implement a change? How would you explain to a policy maker or VA leader the logical progression of your research? Is it possible to craft your proposal that leads to testing your hypotheses and findings of your formative efforts in Aim 1? So if Aim 2 identifies strategies for overcoming barriers to HIE for a specific service, could Aim 3 test their effectiveness? How would you conceptualize effectiveness (from a theoretical perspective) vs a pragmatic perspective (e.g., usability, feasibility, cost, etc.) This is a really important topic and I look forward to seeing it evolve over the coming weeks and months! Check out the feedback on other projects and reflect on common themes that transcend HIEs. Please feel free to follow-up with questions and I encourage you to complete the survey on my feedback to let me know if my comments are helpful. Best regards, David";s:6:"parent";s:32:"7809898bd1d544419c47d78fff56bec5";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"9ac6bf1db50d7cd16b20f3b00ea8a6e0";}s:32:"ae4c20ac42f560bedb4e4046ccbb1013";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dgoodrich";s:4:"name";s:14:"David Goodrich";s:4:"mail";s:22:"David.Goodrich2@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535677464;}s:3:"raw";s:2680:"Hi Barbara, I love your topic (my sister is an ID physician)! I really like the focus on user centered design, focusing on the ER, and progressing from formative research and tool development to testing your clinical innovation with a multisite implementation trial design. I had three general topics that I wanted to know more about after reading your proposal. First, why will the focus of your first two Aims focus solely on the ER? It seems like these patients would be referred on to other providers if they require hospitalization. This raises sub-questions like who are the likely veterans showing up in the ER with pneumonia? What is the variation in presenting cases that can be anticipated in the guidelines or based on content experts? Are physicians the only providers who might be involved in the decision-making process or could other members of the care team be involved and if so who would they be? Relatedly, how would information from the tool be transmitted to other non-VA providers and would the tool be usable for these other care teams? The bottom line is, who are the clinical stakeholders or "end-users" of the tool and how would they likely "adapt" the guidelines for VA settings and patients? 

A second question is, how is development of the tool informed by other best practices in infection control in other VHA services? VA has been a leader in infection control and it seems like your proposed clinical innovation would require the ER to coordinate veteran care planning with other specialists and care teams. Who would those be and how would your tool development be informed by their role, perspectives, and interest in this topic?

Finally, my third question comes down to what are the implementation strategies that you suspect will be needed to not only implement the tool but to prepare ER teams for adopting it and integrating it as part of usual care? This not something you have to answer today. You are essentially proposing to develop a clinical intervention to improve care but HOW you implement this tool from an implementation science perspective is not totally apparent in your current aims.

I hope that I did not overwhelm you with my many questions. You have developed a really strong proposal that will be a good candidate for an HSR&D grant proposal. My questions are only meant to get you thinking as you engage in the readings and content in the coming weeks. Please feel free to use the wiki to ask follow up questions of me or the other facilitators. I also encourage you to use the wiki tools (e.g., survey on homework assignment feedback) to get the most out of TIDIRH. Look forward to talking with you next week! - David
";s:5:"xhtml";s:2733:"Hi Barbara, I love your topic (my sister is an ID physician)! I really like the focus on user centered design, focusing on the ER, and progressing from formative research and tool development to testing your clinical innovation with a multisite implementation trial design. I had three general topics that I wanted to know more about after reading your proposal. First, why will the focus of your first two Aims focus solely on the ER? It seems like these patients would be referred on to other providers if they require hospitalization. This raises sub-questions like who are the likely veterans showing up in the ER with pneumonia? What is the variation in presenting cases that can be anticipated in the guidelines or based on content experts? Are physicians the only providers who might be involved in the decision-making process or could other members of the care team be involved and if so who would they be? Relatedly, how would information from the tool be transmitted to other non-VA providers and would the tool be usable for these other care teams? The bottom line is, who are the clinical stakeholders or &quot;end-users&quot; of the tool and how would they likely &quot;adapt&quot; the guidelines for VA settings and patients? <br /><br />A second question is, how is development of the tool informed by other best practices in infection control in other VHA services? VA has been a leader in infection control and it seems like your proposed clinical innovation would require the ER to coordinate veteran care planning with other specialists and care teams. Who would those be and how would your tool development be informed by their role, perspectives, and interest in this topic?<br /><br />Finally, my third question comes down to what are the implementation strategies that you suspect will be needed to not only implement the tool but to prepare ER teams for adopting it and integrating it as part of usual care? This not something you have to answer today. You are essentially proposing to develop a clinical intervention to improve care but HOW you implement this tool from an implementation science perspective is not totally apparent in your current aims.<br /><br />I hope that I did not overwhelm you with my many questions. You have developed a really strong proposal that will be a good candidate for an HSR&amp;D grant proposal. My questions are only meant to get you thinking as you engage in the readings and content in the coming weeks. Please feel free to use the wiki to ask follow up questions of me or the other facilitators. I also encourage you to use the wiki tools (e.g., survey on homework assignment feedback) to get the most out of TIDIRH. Look forward to talking with you next week! - David";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"ae4c20ac42f560bedb4e4046ccbb1013";}s:32:"6d5421a1982050b5627250139b046ae6";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"bjones";s:4:"name";s:13:"Barbara Jones";s:4:"mail";s:26:"Barbara.Jones@hsc.utah.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535831196;}s:3:"raw";s:1854:"These are wonderful questions - thank you very much for stimulating some deeper thoughts into some of the aspects of my project! A few responses (not sure if it is OK to use the reply for this? is the wiki you speak of?) to continue the conversation... the rationale behind the ED-physician focus is that the site of care, resuscitation and initial antibiotic selection decisions lie largely on their shoulders, so the focus is to design and implement a tool that is supportive of their work - however yes there are many other stakeholders. The 2 other people directly involved in the ED are the patient (which brings up some shared decision-making issues, especially around hospitalization) and the triage nurse (who performs the first assessment of the patient); the other immediate stakeholders include the receiving physicians (ward hospitalists and ICU intensivits), clinical leadership/operations, antimicrobial stewardship teams. We are piloting this project locally, and have consulted with all of these stakeholders to identify goals and needs from their perspectives, but these consultations were not performed in as rigorous a format as ED physician interviews - these are the only actual users of the tool. We did learn a lot from the other stakeholders, and we actually have designed and implemented CDS for pneumonia for the ICU and medicine teams as well - perhaps it would not be too broad to include these users and tools in a multi-site project? I would love input on how to conduct interviews with the non-user stakeholders in a way that follows a good implementation framework.
The multi-site implementation is still a bit fuzzy in my mind as well. We hope to identify sites that are engaged through our site visits, and then approach them for the ultimate implementation study. 
Thank you again for your stimulating response! - barb ";s:5:"xhtml";s:1863:"These are wonderful questions - thank you very much for stimulating some deeper thoughts into some of the aspects of my project! A few responses (not sure if it is OK to use the reply for this? is the wiki you speak of?) to continue the conversation... the rationale behind the ED-physician focus is that the site of care, resuscitation and initial antibiotic selection decisions lie largely on their shoulders, so the focus is to design and implement a tool that is supportive of their work - however yes there are many other stakeholders. The 2 other people directly involved in the ED are the patient (which brings up some shared decision-making issues, especially around hospitalization) and the triage nurse (who performs the first assessment of the patient); the other immediate stakeholders include the receiving physicians (ward hospitalists and ICU intensivits), clinical leadership/operations, antimicrobial stewardship teams. We are piloting this project locally, and have consulted with all of these stakeholders to identify goals and needs from their perspectives, but these consultations were not performed in as rigorous a format as ED physician interviews - these are the only actual users of the tool. We did learn a lot from the other stakeholders, and we actually have designed and implemented CDS for pneumonia for the ICU and medicine teams as well - perhaps it would not be too broad to include these users and tools in a multi-site project? I would love input on how to conduct interviews with the non-user stakeholders in a way that follows a good implementation framework.<br />The multi-site implementation is still a bit fuzzy in my mind as well. We hope to identify sites that are engaged through our site visits, and then approach them for the ultimate implementation study. <br />Thank you again for your stimulating response! - barb";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"6d5421a1982050b5627250139b046ae6";}s:32:"513ae4a492c994356a55fc59c593bba4";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"bjpowell";s:4:"name";s:12:"Byron Powell";s:4:"mail";s:16:"bjpowell@unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535855019;}s:3:"raw";s:2641:"Hi James,

This is an interesting and really ambitious project! It seems like you're potentially already far along w/ the logistics of this (perhaps it has already been submitted). I'll look forward to hearing more about it, and also learning more about the type of feedback that will be most helpful to you in the coming months of TIDIRH. I had a few thoughts in reviewing your first assignment: 

First, I thought you did a nice job of describing the need for eScreening and the demand for it within the VA; however, I think it would be helpful to describe the barriers that you face when attempting to implement eScreening system-wide. Describing these challenges and how they vary across different VAs would help create a stronger justification for using the RPIW process and also potentially the two facilitation approaches (internal and external). 

Second, it will be important to be very clear about the strategies that you are proposing. The overlap between the RPIW, the implementation playbook, and  internal and external implementation was not entirely clear to me. 

Third, given the tailored and adaptive approaches, I wondered how you would track and describe the implementation strategies that were used by different clinics (i.e., it seems like both RPIW and facilitation approaches could lead to the use of a wide range of strategies). Again, I liked the tailored and adaptive nature of the approaches you're using, and I think that could be emphasized as a real strength. We have written about the importance of developing more systematic methods for tailoring implementation strategies, and your approach would be a good example of that.

Fourth, I wondered if/how you might position this as advancing our understanding of facilitation by operationalizing the approach through the RPIW process. Being very explicit about how you might advance the literature on facilitation seems valuable to me. 

Fifth, being more explicit about your rationale for using a SMART trial (connecting it to resource constraints as you do early on) might be useful. 

Sixth, it wasn't clear to me why you selected the PRISM framework or how it would be used in the context of this study. To be clear, I'm not saying it doesn't make sense, but just that it isn't clear how it will be used. 

Finally, in reading your last section, it wasn't entirely clear how you will identify conditions that influence the implementation strategy. More detail about your use of the PRISM framework (or others) may help make this more clear. 

I'm really looking forward to learning more about your work and this study in particular. Talk to you Tuesday!

BP";s:5:"xhtml";s:2786:"Hi James,<br /><br />This is an interesting and really ambitious project! It seems like you&#039;re potentially already far along w/ the logistics of this (perhaps it has already been submitted). I&#039;ll look forward to hearing more about it, and also learning more about the type of feedback that will be most helpful to you in the coming months of TIDIRH. I had a few thoughts in reviewing your first assignment: <br /><br />First, I thought you did a nice job of describing the need for eScreening and the demand for it within the VA; however, I think it would be helpful to describe the barriers that you face when attempting to implement eScreening system-wide. Describing these challenges and how they vary across different VAs would help create a stronger justification for using the RPIW process and also potentially the two facilitation approaches (internal and external). <br /><br />Second, it will be important to be very clear about the strategies that you are proposing. The overlap between the RPIW, the implementation playbook, and  internal and external implementation was not entirely clear to me. <br /><br />Third, given the tailored and adaptive approaches, I wondered how you would track and describe the implementation strategies that were used by different clinics (i.e., it seems like both RPIW and facilitation approaches could lead to the use of a wide range of strategies). Again, I liked the tailored and adaptive nature of the approaches you&#039;re using, and I think that could be emphasized as a real strength. We have written about the importance of developing more systematic methods for tailoring implementation strategies, and your approach would be a good example of that.<br /><br />Fourth, I wondered if/how you might position this as advancing our understanding of facilitation by operationalizing the approach through the RPIW process. Being very explicit about how you might advance the literature on facilitation seems valuable to me. <br /><br />Fifth, being more explicit about your rationale for using a SMART trial (connecting it to resource constraints as you do early on) might be useful. <br /><br />Sixth, it wasn&#039;t clear to me why you selected the PRISM framework or how it would be used in the context of this study. To be clear, I&#039;m not saying it doesn&#039;t make sense, but just that it isn&#039;t clear how it will be used. <br /><br />Finally, in reading your last section, it wasn&#039;t entirely clear how you will identify conditions that influence the implementation strategy. More detail about your use of the PRISM framework (or others) may help make this more clear. <br /><br />I&#039;m really looking forward to learning more about your work and this study in particular. Talk to you Tuesday!<br /><br />BP";s:6:"parent";s:32:"b44dfdf4b47c0e775b908d9e1dd10c44";s:7:"replies";a:1:{i:0;s:32:"8606dc649d85232a48a52f91e4816067";}s:4:"show";b:1;s:3:"cid";s:32:"513ae4a492c994356a55fc59c593bba4";}s:32:"b09f88b26b64d950c7d7354efffb56b0";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"bjpowell";s:4:"name";s:12:"Byron Powell";s:4:"mail";s:16:"bjpowell@unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535858782;}s:3:"raw";s:1632:"Hi Sivan,

It is great to see your commitment to health equity and advancing that goal within implementation research. Likewise, it is neat to see UChicago's commitment to this area. I will look forward to learning more about your work in the coming months. Here are some initial thoughts and reactions from what you've written so far:

First, I think one of the challenges is that it sounds like you're in the process of developing a curriculum/training to translate an equity lens into the UChicago system. Am I correct in assuming that this is developmental at this point, and that there is not yet a training or an intervention that has demonstrated effectiveness in changing organizational behavior in some way? If this is the case, you may need to focus your effort on developing evidence for the intervention/curriculum itself. Overall, I am not entirely clear what you are implementing.

Second, it wasn't particularly clear to me what outcomes you would be assessing. It may be helpful to clearly document the types of outcomes you will be looking to change with respect to the "equity lens" as well as what relevant implementation outcomes (using Proctor et al., 2011 or RE-AIM) might be most relevant.

Third, I really liked the multiple methods of assessing outcomes and processes, and I will look forward to hearing more about the rationale/justification for each method. It also made me wonder if there may be an opportunity to use an explicitly mixed methods (rather that simply multiple methods) design.

Thank you for sharing this work! I'm really looking forward to learning more about this effort on Tuesday.

BP
";s:5:"xhtml";s:1726:"Hi Sivan,<br /><br />It is great to see your commitment to health equity and advancing that goal within implementation research. Likewise, it is neat to see UChicago&#039;s commitment to this area. I will look forward to learning more about your work in the coming months. Here are some initial thoughts and reactions from what you&#039;ve written so far:<br /><br />First, I think one of the challenges is that it sounds like you&#039;re in the process of developing a curriculum/training to translate an equity lens into the UChicago system. Am I correct in assuming that this is developmental at this point, and that there is not yet a training or an intervention that has demonstrated effectiveness in changing organizational behavior in some way? If this is the case, you may need to focus your effort on developing evidence for the intervention/curriculum itself. Overall, I am not entirely clear what you are implementing.<br /><br />Second, it wasn&#039;t particularly clear to me what outcomes you would be assessing. It may be helpful to clearly document the types of outcomes you will be looking to change with respect to the &quot;equity lens&quot; as well as what relevant implementation outcomes (using Proctor et al., 2011 or RE-AIM) might be most relevant.<br /><br />Third, I really liked the multiple methods of assessing outcomes and processes, and I will look forward to hearing more about the rationale/justification for each method. It also made me wonder if there may be an opportunity to use an explicitly mixed methods (rather that simply multiple methods) design.<br /><br />Thank you for sharing this work! I&#039;m really looking forward to learning more about this effort on Tuesday.<br /><br />BP";s:6:"parent";s:32:"2c42d3b994d5a348b0101c87935bd032";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"b09f88b26b64d950c7d7354efffb56b0";}s:32:"ef1eee39318916242ebed92436a5d36c";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"bjones";s:4:"name";s:13:"Barbara Jones";s:4:"mail";s:26:"Barbara.Jones@hsc.utah.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536312093;}s:3:"raw";s:5024:"Jones - Assignment #2: 
Q: Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?
A: Yes! Fidelity and adaptation can be measured in several ways. The goal of the project is to implement and adapt CDS for pneumonia across multiple settings in the VA that demonstrate variation in pneumonia care. The evidence-based intervention will be our prototype clinical decision support tool (CDS) that encompasses a standard care pathway for pneumonia; however, this intervention will be heavily tailored to each setting through a user-centered design process. In aim 1, we will explore differences across settings in mental models, goals, information needs, and implementation constructs that may lead to anticipated adaptations. In Aim 2, we will track the user-centered design process at each setting which involves stakeholder in iterations (design prototype, conduct formative evaluations in use, incorporate feedback in re-design, repeat). Real-time tracking of the adaptations during the design process can follow the format suggested by Rabin (Table 3). In Aim 3, we will monitor fidelity by tracking individual provider interactions with the CDS and adaptation of the care pathway to individual patients at the point of care. (I’m particularly excited about this approach.) That last question regarding the level of fidelity is a tough one. There is some literature suggesting that adherence to certain components of the guidelines (such as first-line antibiotics and objective severity assessment) leads to improved outcomes and reduced cost; however, other components of the guidelines have less evidence. Previous pneumonia care pathway implementation studies have reported guideline adherence, but have not measured specific modifications to the intervention.
Q: Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?
A: Yes! Adaptations to the initial EBI are crucial to our design. We anticipate and will measure several contextual as well as content-specific modifications (Table). I am not sure what process to use to guide these adaptations, although the core elements of the CDS are the promotion of situational awareness, deliberative thought, engagement, satisfaction, and enhancement of a mental model of pneumonia among using providers. Our goal is to increase adherence to standard care when it is appropriate, and to examine adaptations/deviations when it is not deemed to be.
Contextual Modifications-
Format: how CDS integrates into cognitive workflow for user; timing of CDS, location within the EHR, visual display preferences; must map to mental models of both pneumonia care processes and EHR interface (which differ by setting)
Setting: may be used in primary care clinic as well as EDs, wards and ICUS 		
Personnel: providers differ by setting	
Content Modifications-
Setting/unit level: Tailoring, adding/removing elements, shortening/lengthening, substituting elemence, re-ordering to fit with mental models, and integrating with sepsis protocols can/will all occur during the CDS user-centered design.
Provider level: individualization of care can/will happen at the point of care – adopting, rejecting, substituting different components of the CDS for each patient, will be encouraged/supported and tracked.
Population: different setting resources (ie, availability of specialty care, home servies, PCP follow-up) could change what recommendations are appropriate	

Revised specific aims: 
1) Identify differences in mental models of pneumonia diagnosis and management, information needs, clinical goals, self-efficacy/motivation, and implementation constructs surrounding standard care processes for pneumonia and informatics tool usability among stakeholders across multiple VA facilities demonstrating variation in pneumonia care processes.
	- CFIR pre-implementation interview of all stakeholders (clinical leadership, abx stewardship, information technology staff, and physicians who are end-users of the CDS)
	- cognitive workflow analysis of end-users of CDS (physicians)
2) Characterize the adaptation of a pneumonia standard care pathway to multiple settings through user-centered design of clinical decision support (CDS) for pneumonia across multiple VA emergency departments. 
	- start with pilot “original” EBI
        - measure fidelity/adaptation throughout the iterative design process with stakeholders 
3) Examine the adaptation and clinical impact of CDS for pneumonia across VA emergency departments through a stepped-wedge multi-site intervention.
	- formative evaluations during implementation
	- tracking of use, provider-level, and patient-level adaptation of CDS 
";s:5:"xhtml";s:5137:"Jones - Assignment #2: <br />Q: Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br />A: Yes! Fidelity and adaptation can be measured in several ways. The goal of the project is to implement and adapt CDS for pneumonia across multiple settings in the VA that demonstrate variation in pneumonia care. The evidence-based intervention will be our prototype clinical decision support tool (CDS) that encompasses a standard care pathway for pneumonia; however, this intervention will be heavily tailored to each setting through a user-centered design process. In aim 1, we will explore differences across settings in mental models, goals, information needs, and implementation constructs that may lead to anticipated adaptations. In Aim 2, we will track the user-centered design process at each setting which involves stakeholder in iterations (design prototype, conduct formative evaluations in use, incorporate feedback in re-design, repeat). Real-time tracking of the adaptations during the design process can follow the format suggested by Rabin (Table 3). In Aim 3, we will monitor fidelity by tracking individual provider interactions with the CDS and adaptation of the care pathway to individual patients at the point of care. (I’m particularly excited about this approach.) That last question regarding the level of fidelity is a tough one. There is some literature suggesting that adherence to certain components of the guidelines (such as first-line antibiotics and objective severity assessment) leads to improved outcomes and reduced cost; however, other components of the guidelines have less evidence. Previous pneumonia care pathway implementation studies have reported guideline adherence, but have not measured specific modifications to the intervention.<br />Q: Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br />A: Yes! Adaptations to the initial EBI are crucial to our design. We anticipate and will measure several contextual as well as content-specific modifications (Table). I am not sure what process to use to guide these adaptations, although the core elements of the CDS are the promotion of situational awareness, deliberative thought, engagement, satisfaction, and enhancement of a mental model of pneumonia among using providers. Our goal is to increase adherence to standard care when it is appropriate, and to examine adaptations/deviations when it is not deemed to be.<br />Contextual Modifications-<br />Format: how CDS integrates into cognitive workflow for user; timing of CDS, location within the EHR, visual display preferences; must map to mental models of both pneumonia care processes and EHR interface (which differ by setting)<br />Setting: may be used in primary care clinic as well as EDs, wards and ICUS 		<br />Personnel: providers differ by setting	<br />Content Modifications-<br />Setting/unit level: Tailoring, adding/removing elements, shortening/lengthening, substituting elemence, re-ordering to fit with mental models, and integrating with sepsis protocols can/will all occur during the CDS user-centered design.<br />Provider level: individualization of care can/will happen at the point of care – adopting, rejecting, substituting different components of the CDS for each patient, will be encouraged/supported and tracked.<br />Population: different setting resources (ie, availability of specialty care, home servies, PCP follow-up) could change what recommendations are appropriate	<br /><br />Revised specific aims: <br />1) Identify differences in mental models of pneumonia diagnosis and management, information needs, clinical goals, self-efficacy/motivation, and implementation constructs surrounding standard care processes for pneumonia and informatics tool usability among stakeholders across multiple VA facilities demonstrating variation in pneumonia care processes.<br />	- CFIR pre-implementation interview of all stakeholders (clinical leadership, abx stewardship, information technology staff, and physicians who are end-users of the CDS)<br />	- cognitive workflow analysis of end-users of CDS (physicians)<br />2) Characterize the adaptation of a pneumonia standard care pathway to multiple settings through user-centered design of clinical decision support (CDS) for pneumonia across multiple VA emergency departments. <br />	- start with pilot “original” EBI<br />        - measure fidelity/adaptation throughout the iterative design process with stakeholders <br />3) Examine the adaptation and clinical impact of CDS for pneumonia across VA emergency departments through a stepped-wedge multi-site intervention.<br />	- formative evaluations during implementation<br />	- tracking of use, provider-level, and patient-level adaptation of CDS";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"8b7c2caac1acb441dc4cbf54fe3d4257";}s:4:"show";b:1;s:3:"cid";s:32:"ef1eee39318916242ebed92436a5d36c";}s:32:"9440610e45de601c6c7c7c919bdfbece";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"tlagu";s:4:"name";s:9:"Tara Lagu";s:4:"mail";s:28:"Tara.Lagu@baystatehealth.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536344551;}s:3:"raw";s:5019:"Lagu—Assignment #2
1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?
In a sense, the entire grant proposal aims to examine the fidelity to and adaptation of existing interventions to increase participation in cardiac rehabilitation. The objective of the proposal is to identify implementation strategies that increase participation in CR among patients with HF and then prioritize those strategies that are the most acceptable, feasible, and responsive to the needs of stakeholders. Prior research in hospitalized patients with coronary artery disease (CAD) suggests that strategies that increase CR participation include automatic referral (from the hospital), liaison-facilitated referral (a person comes to help the patient enroll), and scheduled CR appointments after discharge. For patients with CAD, prior research suggests that barriers may include cost (e.g., copays), transportation, distance to CR program, and lack of knowledge or understanding about the benefits of CR.  However, heart failure is a chronic illness (compared to acute conditions like myocardial infarction), and patients with heart failure are older, have more comorbid conditions, and often have poor functional status, all of which may represent barriers to participation. Additionally, there is a required 6-week waiting period following hospitalization may mean that referrals from outpatient providers play a more prominent role in recruitment strategies than has been observed for acute conditions. Thus, the strategies that result in high rates of CR participation in this population are unknown, but are likely to be adaptations of previously described strategies for CAD and cardiac surgery. However, the fidelity to the interventions described for other populations (not those with heart failure) is unknown. A single-center study of recently discharged patients with HF reported that predictors of CR attendance included discharge to home (vs. nursing facility), automatic referral during hospitalization, and physician recommendation, but given the small sample size, single-center nature, and lack of details provided about each strategy, this work can only suggest early hypotheses about facilitators and barriers to CR participation among patients with HF. 
2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?
We aim to understand the fidelity of the interventions in use and the degree to which these interventions have been adapted for this unique population (patients with heart failure). Based on our clinical experience in CR and our preliminary data, we suspect that the following strategies will emerge as being important to improving CR participation in patients with HF: the use of automatic referral in hospitals, the presence of dedicated inpatient CR staff members or other CR liaisons, aggressive methods of screening to identify all eligible patients in both inpatient and outpatient settings, the endorsement of CR champions and HF specialists, strong regional CR cooperation with minimal competition between health systems or physician groups, use of motivational interviewing or other behavioral interventions with patients to encourage attendance, and a high level of integration of the CR program into the health system (short distance from hospital, strong physician support, and CR team member integration into clinical care).  However, we know that each of these will be structurally and operationally different at each institution and will have been adapted to best serve the population of patients with HF, so we need to gain a better understanding of how they have been changed in response to the institution and clinical population. To do this, we will use the Consolidated Framework for Implementation Research (CFIR) to guide a qualitative study of hospitals that have high rates of use of CR for patients with HF. Because we are focused on fidelity and adaptation, we will obtain as many clarifying details about the interventions possible, including clear definitions of terms, specifics of temporality, involved personnel, intermediate outcomes affected, and rationale for use. We will also explore the role of environmental and community factors such as program availability, CR program cooperation, and the presence of risk contracts and bundled payments, such as accountable care organizations. We will also focus on the role of HF specialists in promoting CR, the role of the post-hospitalization clinic visit in facilitating referral, and the impact of the 6-week waiting period on patient interest and willingness to attend CR, as these are specific to the HF population.

";s:5:"xhtml";s:5037:"Lagu—Assignment #2<br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br />In a sense, the entire grant proposal aims to examine the fidelity to and adaptation of existing interventions to increase participation in cardiac rehabilitation. The objective of the proposal is to identify implementation strategies that increase participation in CR among patients with HF and then prioritize those strategies that are the most acceptable, feasible, and responsive to the needs of stakeholders. Prior research in hospitalized patients with coronary artery disease (CAD) suggests that strategies that increase CR participation include automatic referral (from the hospital), liaison-facilitated referral (a person comes to help the patient enroll), and scheduled CR appointments after discharge. For patients with CAD, prior research suggests that barriers may include cost (e.g., copays), transportation, distance to CR program, and lack of knowledge or understanding about the benefits of CR.  However, heart failure is a chronic illness (compared to acute conditions like myocardial infarction), and patients with heart failure are older, have more comorbid conditions, and often have poor functional status, all of which may represent barriers to participation. Additionally, there is a required 6-week waiting period following hospitalization may mean that referrals from outpatient providers play a more prominent role in recruitment strategies than has been observed for acute conditions. Thus, the strategies that result in high rates of CR participation in this population are unknown, but are likely to be adaptations of previously described strategies for CAD and cardiac surgery. However, the fidelity to the interventions described for other populations (not those with heart failure) is unknown. A single-center study of recently discharged patients with HF reported that predictors of CR attendance included discharge to home (vs. nursing facility), automatic referral during hospitalization, and physician recommendation, but given the small sample size, single-center nature, and lack of details provided about each strategy, this work can only suggest early hypotheses about facilitators and barriers to CR participation among patients with HF. <br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br />We aim to understand the fidelity of the interventions in use and the degree to which these interventions have been adapted for this unique population (patients with heart failure). Based on our clinical experience in CR and our preliminary data, we suspect that the following strategies will emerge as being important to improving CR participation in patients with HF: the use of automatic referral in hospitals, the presence of dedicated inpatient CR staff members or other CR liaisons, aggressive methods of screening to identify all eligible patients in both inpatient and outpatient settings, the endorsement of CR champions and HF specialists, strong regional CR cooperation with minimal competition between health systems or physician groups, use of motivational interviewing or other behavioral interventions with patients to encourage attendance, and a high level of integration of the CR program into the health system (short distance from hospital, strong physician support, and CR team member integration into clinical care).  However, we know that each of these will be structurally and operationally different at each institution and will have been adapted to best serve the population of patients with HF, so we need to gain a better understanding of how they have been changed in response to the institution and clinical population. To do this, we will use the Consolidated Framework for Implementation Research (CFIR) to guide a qualitative study of hospitals that have high rates of use of CR for patients with HF. Because we are focused on fidelity and adaptation, we will obtain as many clarifying details about the interventions possible, including clear definitions of terms, specifics of temporality, involved personnel, intermediate outcomes affected, and rationale for use. We will also explore the role of environmental and community factors such as program availability, CR program cooperation, and the presence of risk contracts and bundled payments, such as accountable care organizations. We will also focus on the role of HF specialists in promoting CR, the role of the post-hospitalization clinic visit in facilitating referral, and the impact of the 6-week waiting period on patient interest and willingness to attend CR, as these are specific to the HF population.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"2d3eb41927b1fe2eeefeb3787ee1ead1";}s:4:"show";b:1;s:3:"cid";s:32:"9440610e45de601c6c7c7c919bdfbece";}s:32:"917cc63bb972756847473d6415d6d079";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"kboockvar";s:4:"name";s:16:"Kenneth Boockvar";s:4:"mail";s:25:"kenneth.boockvar@mssm.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536348621;}s:3:"raw";s:4678:"Boockvar Assignment #2

1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

The evidence base to support health information exchange (HIE) is limited but its face validity is very high.  As pointed out by David Goodrich in feedback to assignment #1, HIE is a tool that can enhance many different health care activities.  Evidence from quasi-experimental studies show associations of HIE usage in outpatient and emergency room settings with reduction in lab and radiology testing, hospital use, and costs.  Our own evidence from a prospective controlled trial showed that HIE-enhanced medication reconciliation in a VA inpatient setting enabled increased detection of medication discrepancies.  This trial demonstrated that when more comprehensive medication information was available in the HIE, the intervention pharmacist detected more medication discrepancies, which suggests that level of fidelity (in this case availability of targeted HIE information) is associated with better individual level outcomes.

We have followed up this previous study with a current study which provides the basis for my TIDIRH concept.  In this study we are testing whether HIE notification to VA primary providers of non-VA hospitalization of Veterans, followed by an evidence-based geriatrics care coordination intervention, improves outcomes (hospital readmission; VA follow-up) after non-VA hospital discharge, as compared to notification alone and no notification.  This study is on-going at 2 VA sites:  Bronx and Indianapolis.  The idea is that HIE “push” notification of selected health data is more effective at getting that data viewed than relying on providers to “pull” it from the HIE, and that an automatic evidence-based response (the geriatrics care coordination intervention) will be more effective than no standard response.

We propose in our TIDIRH concept to measure fidelity by measuring the extent to which providers received, viewed and acted on the HIE push notifications, as well as their degree of viewing and utilization of the HIE for decision-making.  We also propose to interview providers as well as to observe them while they interface with the HIE (and the VA’s electronic health record) in the course of making decisions on real cases, while “thinking aloud.”  The hypothesis is that we can associate greater engagement with HIE information (greater fidelity) with better decision-making.  These sessions will be audio recorded and synced with observers’ record of screen clicks.  Note that we are not proposing for this TIDIRH concept to examine the fidelity or adaptation of the geriatrics care coordination intervention, the implementation of which is being studied by others.

2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

Our implementation focus is on optimizing the receipt, viewing and acting on the HIE notification of non-VA hospitalization of Veterans by their VA providers, to improve coordination of care across VA and non-VA systems.  The current mode of implementation is that HIE notifications are received by one credentialed administrative assistant at each VA (a research assistant) who then records the notification in the Veteran’s VA medical record as a progress note containing basic information about when, where, and for what diagnosis the Veteran was admitted to the non-VA hospital.  Primary providers and one additional primary care team member are notified by including them as cosigners of the note.  Adaptations may be needed to this process, which would be guided by our Aim 1 interviews and classified according to the Stirman model.  Adaptations could include content of the notification message (more or less information), target recipient of the notification message (alternate clinician or administrative assistant), mode of communication (secure email versus chart note), and desired clinical scenarios that trigger notification (non-VA hospital admission, emergency department registration, hospital discharge, out of range lab results, etc.).  A context modification could restrict or expand the patient population to be monitored by HIE notification.  We could use the Rabin “Real-time adaptation tracking form” to record how the intervention is adapted as it is delivered.

";s:5:"xhtml";s:4736:"Boockvar Assignment #2<br /><br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />The evidence base to support health information exchange (HIE) is limited but its face validity is very high.  As pointed out by David Goodrich in feedback to assignment #1, HIE is a tool that can enhance many different health care activities.  Evidence from quasi-experimental studies show associations of HIE usage in outpatient and emergency room settings with reduction in lab and radiology testing, hospital use, and costs.  Our own evidence from a prospective controlled trial showed that HIE-enhanced medication reconciliation in a VA inpatient setting enabled increased detection of medication discrepancies.  This trial demonstrated that when more comprehensive medication information was available in the HIE, the intervention pharmacist detected more medication discrepancies, which suggests that level of fidelity (in this case availability of targeted HIE information) is associated with better individual level outcomes.<br /><br />We have followed up this previous study with a current study which provides the basis for my TIDIRH concept.  In this study we are testing whether HIE notification to VA primary providers of non-VA hospitalization of Veterans, followed by an evidence-based geriatrics care coordination intervention, improves outcomes (hospital readmission; VA follow-up) after non-VA hospital discharge, as compared to notification alone and no notification.  This study is on-going at 2 VA sites:  Bronx and Indianapolis.  The idea is that HIE “push” notification of selected health data is more effective at getting that data viewed than relying on providers to “pull” it from the HIE, and that an automatic evidence-based response (the geriatrics care coordination intervention) will be more effective than no standard response.<br /><br />We propose in our TIDIRH concept to measure fidelity by measuring the extent to which providers received, viewed and acted on the HIE push notifications, as well as their degree of viewing and utilization of the HIE for decision-making.  We also propose to interview providers as well as to observe them while they interface with the HIE (and the VA’s electronic health record) in the course of making decisions on real cases, while “thinking aloud.”  The hypothesis is that we can associate greater engagement with HIE information (greater fidelity) with better decision-making.  These sessions will be audio recorded and synced with observers’ record of screen clicks.  Note that we are not proposing for this TIDIRH concept to examine the fidelity or adaptation of the geriatrics care coordination intervention, the implementation of which is being studied by others.<br /><br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />Our implementation focus is on optimizing the receipt, viewing and acting on the HIE notification of non-VA hospitalization of Veterans by their VA providers, to improve coordination of care across VA and non-VA systems.  The current mode of implementation is that HIE notifications are received by one credentialed administrative assistant at each VA (a research assistant) who then records the notification in the Veteran’s VA medical record as a progress note containing basic information about when, where, and for what diagnosis the Veteran was admitted to the non-VA hospital.  Primary providers and one additional primary care team member are notified by including them as cosigners of the note.  Adaptations may be needed to this process, which would be guided by our Aim 1 interviews and classified according to the Stirman model.  Adaptations could include content of the notification message (more or less information), target recipient of the notification message (alternate clinician or administrative assistant), mode of communication (secure email versus chart note), and desired clinical scenarios that trigger notification (non-VA hospital admission, emergency department registration, hospital discharge, out of range lab results, etc.).  A context modification could restrict or expand the patient population to be monitored by HIE notification.  We could use the Rabin “Real-time adaptation tracking form” to record how the intervention is adapted as it is delivered.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"966b82f6179a0d7c72f163aa4a0639fd";}s:4:"show";b:1;s:3:"cid";s:32:"917cc63bb972756847473d6415d6d079";}s:32:"b2d70705ff54d008fc4d7f960b71bb50";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"kfisher";s:4:"name";s:15:"Kimberly Fisher";s:4:"mail";s:33:"Kimberly.Fisher@umassmemorial.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536348938;}s:3:"raw";s:3387:"Fisher Assignment #2:
1.  Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

The first stages of my project (currently underway) involve conducting the foundational work that will serve as the basis for the development of the evidence-based care transition intervention.  As such, the evidence-based intervention is not currently not fully defined which makes it challenging to consider the issue of fidelity measurement and monitoring. 
 
However, prior studies I have conducted using the positive deviance approach have identified factors and strategies that fall into the domains of process (what are the components of the care transition process at high-performing hospitals?), structure (staff and other resources), and systems or organizational culture (teamwork, education, communication, leadership support).  I therefore envision the intervention as most likely including components that target the domains of process (specific strategies to bring hospital practice into alignment with “ideal” care transition process) and one or more aspects of organizational culture.  Further, I currently envision a flexible intervention that includes a menu or array of options that hospitals might choose to adopt depending on their specific care transition gaps and context.  

Given this current conceptualization of the intervention, I found fidelity to be a less relevant concept as there are not currently “core” components of the intervention that have been defined or proven to be effective.  As a substitute for overall intervention fidelity, I found it instructive to consider the extent the strategies selected by a hospital match their care transitions gap, and of course which and how many strategies a hospital chooses to adopt (is adoption of more strategies associated with greater improvement in care transitions or is it more important to “close” care transition gaps?).  It will nonetheless be important to think about developing a measure of fidelity for each component of the intervention in order to capture how well specific components are put in place.  

2.  Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

It is expected that adaptations to specific components will be needed and may in fact be viewed as an important aspect of intervention development and pilot testing.  Depending on the form the intervention takes, adaptations may be required to care transition process components or organizational culture intervention components.  The process for guiding adaptations would ideally be purposeful and driven by input from stakeholders and potentially early findings.  However, it seems unavoidable that some adaptations will occur at the level of the end-user that may not be planned or guided by a process.  Capturing the who, what, why of the adaptations will be essential to using the adaptations (both planned and unplanned) to develop a sustainable, flexible intervention that can evolve as health systems and practices evolve.  
";s:5:"xhtml";s:3439:"Fisher Assignment #2:<br />1.  Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />The first stages of my project (currently underway) involve conducting the foundational work that will serve as the basis for the development of the evidence-based care transition intervention.  As such, the evidence-based intervention is not currently not fully defined which makes it challenging to consider the issue of fidelity measurement and monitoring. <br /> <br />However, prior studies I have conducted using the positive deviance approach have identified factors and strategies that fall into the domains of process (what are the components of the care transition process at high-performing hospitals?), structure (staff and other resources), and systems or organizational culture (teamwork, education, communication, leadership support).  I therefore envision the intervention as most likely including components that target the domains of process (specific strategies to bring hospital practice into alignment with “ideal” care transition process) and one or more aspects of organizational culture.  Further, I currently envision a flexible intervention that includes a menu or array of options that hospitals might choose to adopt depending on their specific care transition gaps and context.  <br /><br />Given this current conceptualization of the intervention, I found fidelity to be a less relevant concept as there are not currently “core” components of the intervention that have been defined or proven to be effective.  As a substitute for overall intervention fidelity, I found it instructive to consider the extent the strategies selected by a hospital match their care transitions gap, and of course which and how many strategies a hospital chooses to adopt (is adoption of more strategies associated with greater improvement in care transitions or is it more important to “close” care transition gaps?).  It will nonetheless be important to think about developing a measure of fidelity for each component of the intervention in order to capture how well specific components are put in place.  <br /><br />2.  Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />It is expected that adaptations to specific components will be needed and may in fact be viewed as an important aspect of intervention development and pilot testing.  Depending on the form the intervention takes, adaptations may be required to care transition process components or organizational culture intervention components.  The process for guiding adaptations would ideally be purposeful and driven by input from stakeholders and potentially early findings.  However, it seems unavoidable that some adaptations will occur at the level of the end-user that may not be planned or guided by a process.  Capturing the who, what, why of the adaptations will be essential to using the adaptations (both planned and unplanned) to develop a sustainable, flexible intervention that can evolve as health systems and practices evolve.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"c1a7faf8cd119d94da8024da05ef2cc2";}s:4:"show";b:1;s:3:"cid";s:32:"b2d70705ff54d008fc4d7f960b71bb50";}s:32:"85e7ba8fb289732d124e3aba13814478";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"sshohat";s:4:"name";s:20:"Sivan Spitzer-Shohat";s:4:"mail";s:30:"sivan.spitzer-shohat@biu.ac.il";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536349172;}s:3:"raw";s:3483:"
Spitzer-Shohat Assignment #2”

1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?]

The aim of our study is to develop and implement equity translation training, assessing its effectiveness by observing and evaluating the works of pilot implementation teams. We are currently developing the training module, with the hope of delivering it to the first cohort in early 2019. As such, we do not plan to measure fidelity in this study. However, in the future, as training is rolled out throughout the organization, as well as to other organizations, we will be monitoring fidelity by observing trainers and the training components delivered as well as collecting feedback from trainees through a self-report survey at the end of training.  

2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

We plan to observe the effect of the equity translation training developed on implementation teams, mutatis mutandis. Given that the implementation teams will work in different departmental settings, we would like to understand if and to what extent training will require adaptation. Using the Realist Evaluation framework, which aims to understand not only what works, but what works for whom in what circumstances and why, we will map the interconnection between: 
(a) Context, i.e. assessing the departmental context of the department in which the pilot team is facilitating the implementation of an equity lens to organizational process, by collecting social network data through a survey and interviews on the network structure, types of ties between employees, existing resources, and competing pressures.
(b) Mechanisms, which are the combination of implementation team members' reasoning, that is, their beliefs, attitudes, or the logic they apply to the implementation process, and assessing how this was complemented by the training they have undergone. We will assess this by measuring perceived team effectiveness as well as conducting interviews to better understand to what extent training assisted in mitigating or enhancing perceived facilitators and barriers of team members’ ability to devise and facilitate the implementation of an equity lens to departmental work processes. 
(c) Outcomes, assessing the outcomes of intervention teams’ work in devising and assisting in the implementation of applying an equity lens to the departmental work processes to map the uptake of this organizational change. Using the RE-AIM framework, we will conduct a survey to identify reach among departmental personnel, adoption, and use of the strategies the implementation teams devised through the Dong et al. (2008), innovation-fit tool. We will also conduct interviews with the department employees to understand their perceptions on relevance, efficacy and usability of the equity lens focused implementation strategies suggested by the pilot team.
The interplay between the three constructs, or context-mechanisms-outcome (CMO) configurations, will enable us to decipher the variability and the extent that training needs to be modified to account for context. 

";s:5:"xhtml";s:3544:"Spitzer-Shohat Assignment #2”<br /><br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?]<br /><br />The aim of our study is to develop and implement equity translation training, assessing its effectiveness by observing and evaluating the works of pilot implementation teams. We are currently developing the training module, with the hope of delivering it to the first cohort in early 2019. As such, we do not plan to measure fidelity in this study. However, in the future, as training is rolled out throughout the organization, as well as to other organizations, we will be monitoring fidelity by observing trainers and the training components delivered as well as collecting feedback from trainees through a self-report survey at the end of training.  <br /><br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />We plan to observe the effect of the equity translation training developed on implementation teams, mutatis mutandis. Given that the implementation teams will work in different departmental settings, we would like to understand if and to what extent training will require adaptation. Using the Realist Evaluation framework, which aims to understand not only what works, but what works for whom in what circumstances and why, we will map the interconnection between: <br />(a) Context, i.e. assessing the departmental context of the department in which the pilot team is facilitating the implementation of an equity lens to organizational process, by collecting social network data through a survey and interviews on the network structure, types of ties between employees, existing resources, and competing pressures.<br />(b) Mechanisms, which are the combination of implementation team members&#039; reasoning, that is, their beliefs, attitudes, or the logic they apply to the implementation process, and assessing how this was complemented by the training they have undergone. We will assess this by measuring perceived team effectiveness as well as conducting interviews to better understand to what extent training assisted in mitigating or enhancing perceived facilitators and barriers of team members’ ability to devise and facilitate the implementation of an equity lens to departmental work processes. <br />(c) Outcomes, assessing the outcomes of intervention teams’ work in devising and assisting in the implementation of applying an equity lens to the departmental work processes to map the uptake of this organizational change. Using the RE-AIM framework, we will conduct a survey to identify reach among departmental personnel, adoption, and use of the strategies the implementation teams devised through the Dong et al. (2008), innovation-fit tool. We will also conduct interviews with the department employees to understand their perceptions on relevance, efficacy and usability of the equity lens focused implementation strategies suggested by the pilot team.<br />The interplay between the three constructs, or context-mechanisms-outcome (CMO) configurations, will enable us to decipher the variability and the extent that training needs to be modified to account for context.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"35a1d6e0581e70fe0bb094164e698c6f";}s:4:"show";b:1;s:3:"cid";s:32:"85e7ba8fb289732d124e3aba13814478";}s:32:"2053caf744257a1914525ed2a5500d16";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jpittman";s:4:"name";s:13:"James Pittman";s:4:"mail";s:17:"jpittman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1536351633;s:8:"modified";i:1536351719;}s:3:"raw";s:2410:"Pittman - Assignment #2 

1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

Yes. The eScreening program is able to be used in a variety of settings because it has a forms editor that allows a non-programmer to make changes to the screens, medical record note, alerts, and Veteran printout using an human centered design what-you-see-is-what-you-get (WYSIWYG) interface. This feature makes eScreening adaptable to many settings for a variety of uses. The eScreening playbook defines minimum necessary for eScreening functionality, but does not define how eScreening should be used in a particular setting. The proposed study targets using eScreening for mental health screening and ongoing symptom monitoring. eScreening data will be used to identify what type of data eScreening is being collected and can be used to track uses that are outside of the intended targets for this study. Qualitative data from programs will also be collected via survey or interview to identify how eScreening is being used at each site. 

In addition, it may be useful to capture data on the fidelity to or adaptation of the RPIW process in the eScreening Playbook. We have created a checklist that captures each aspect of the modified eScreening implementation RPIW process that could be administered to RPIW facilitators. This form could be modified to explanation for aspects not covered in the program’s RPIW.  

2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

The eScreening program will not be modified. As stated above, the way that it is used may be adapted and would be captured for the study. The minimum roles necessary to use eScreening also cannot be adapted, but it would be important to capture from each program who is involved in each of the essential roles and would also be collected as part of the qualitative interview or survey. The RPIW process itself is designed to facilitate adaptation so that team members collaborate to develop solutions that work for their particular context. 
";s:5:"xhtml";s:2458:"Pittman - Assignment #2 <br /><br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />Yes. The eScreening program is able to be used in a variety of settings because it has a forms editor that allows a non-programmer to make changes to the screens, medical record note, alerts, and Veteran printout using an human centered design what-you-see-is-what-you-get (WYSIWYG) interface. This feature makes eScreening adaptable to many settings for a variety of uses. The eScreening playbook defines minimum necessary for eScreening functionality, but does not define how eScreening should be used in a particular setting. The proposed study targets using eScreening for mental health screening and ongoing symptom monitoring. eScreening data will be used to identify what type of data eScreening is being collected and can be used to track uses that are outside of the intended targets for this study. Qualitative data from programs will also be collected via survey or interview to identify how eScreening is being used at each site. <br /><br />In addition, it may be useful to capture data on the fidelity to or adaptation of the RPIW process in the eScreening Playbook. We have created a checklist that captures each aspect of the modified eScreening implementation RPIW process that could be administered to RPIW facilitators. This form could be modified to explanation for aspects not covered in the program’s RPIW.  <br /><br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />The eScreening program will not be modified. As stated above, the way that it is used may be adapted and would be captured for the study. The minimum roles necessary to use eScreening also cannot be adapted, but it would be important to capture from each program who is involved in each of the essential roles and would also be collected as part of the qualitative interview or survey. The RPIW process itself is designed to facilitate adaptation so that team members collaborate to develop solutions that work for their particular context.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"2053caf744257a1914525ed2a5500d16";}s:32:"2d3eb41927b1fe2eeefeb3787ee1ead1";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dgoodrich";s:4:"name";s:14:"David Goodrich";s:4:"mail";s:22:"David.Goodrich2@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1536792773;s:8:"modified";i:1536793689;}s:3:"raw";s:5072:"Tara, it's evident that you have thought a lot about the factors that influence CR for heart failure patients. What  strikes me from your overview of the state of the art, is that there is NOT a clear definition of what the clinical intervention is (core elements) for CR for heart failure. I suspect there is a clinical exercise protocol for CR but, the protocol really has not left the effectiveness phase and been pragmatically adapted for real world care and complexities. This is true of many exercise-based programs for secondary prevention. We've know CR is effective for at least two decades  but the research into both developing an engaging clinical intervention AND studying implementation factors/strategies to improve uptake of CR programming across specific settings and regions has been lacking. 

Low engagement of patients in CR programs may be an artifact of the fact that many of the cardiologists and cardiac rehab specialists who developed the exercise protocol were not interested in patient-level motivational strategies as part of effectiveness testing (motivational interviewing is hard without the proper context and theoretical training!). These protocols seem written from the perspective of " 'IF' you show up, we can make you better." Likewise, this disinterest in translational research is also reflected by the paucity of implementation trials that have tested strategies to overcome provider and organizational barriers to effective CR uptake and utilization. It creates a conundrum when you have a promising innovation that the researchers left half-baked.  

I would encourage you to think of two phases in your research:

1) Expand the operational core components of what comprises evidence-based CR car for heart failure. Based on my exercise science training, the CPG's and exercise protocols generally lack clear recommendations on  patient-level engagement practices or clinical care processes to optimize adherence (e.g., screening, decision making, referrals, motivational enhancement aspects, billing, etc.). Furthermore, beyond what is essential for optimizing treatment outcomes, what is negotiable or adaptable across settings while providing some adherence to the core elements (e.g. staffing mix, referral methods, timing of procedures, dose?)? Is an automatic referral or having a shared decision making conversation about CR while still in the hospital a core element or an adaptation? Likewise, is variation whether a patient is engaging in CR from a nursing home/assisted living context vs. home, an adaptation or does it reflect two different populations of CR patients requiring slightly different recommendations based on treatment complexity or care coordination? Bottom line - the essential components of good programming for CR for heart failure patients is still relatively undefined based on your description of the current state of the art. YOU, have the opportunity to partner with organizations or other experts to change that which, is quite exciting!! 

2) Once you have a clearly defined intervention with flexible adaptations (preferably a checklist to indicate the presence of or quality of core elements and adaptations), you can then specify the strategies to implement the intervention and address the barriers you've identified. These strategies may vary across the phase of implementation (pre-implementation, adoption, scale-up and spread, and sustainability/maintenance). Dense urban/suburban settings or regions will have different needs than rural or historically under-resourced regions. Byron Powell's work on customizing these strategies will be very relevant in the coming weeks. You can also leverage relationships of peers who are now working in parallel with your own research efforts (e.g., Mary Whooley in San Francisco and likely others in some of the large healthcare systems like Kaiser or Mayo Clinic). 

I realize it sounds ambitious to come up with new clinical practice recommendations but it is important to get all clinical stakeholders to agree on what needs to be implemented as target for best practice or quality programming. Once the core elements are defined, then adaptations can be identified. It seems like your planning lumps adaptations for local context as an implementation strategy. I would keep them separate distinctly separate (clinical intervention to yield patient outcomes vs. strategies to address provider/facility/external policy environment challenges). I hope this advice is not too long or intimidating... I think you have a lot of the information from the literature to start repackaging and defining what you might define as the necessary core elements should be. Once you've done that, it's easier to identify reasonable adaptions based on organizational constraints. I wouldn't worry about the implementation strategies for the time-being. There is an upcoming reading on implementation trials by Geoff Curran that will help make some of these issues clearer. In the meantime, I would be pleased to continue this conversation via the Wiki.
David

";s:5:"xhtml";s:5185:"Tara, it&#039;s evident that you have thought a lot about the factors that influence CR for heart failure patients. What  strikes me from your overview of the state of the art, is that there is NOT a clear definition of what the clinical intervention is (core elements) for CR for heart failure. I suspect there is a clinical exercise protocol for CR but, the protocol really has not left the effectiveness phase and been pragmatically adapted for real world care and complexities. This is true of many exercise-based programs for secondary prevention. We&#039;ve know CR is effective for at least two decades  but the research into both developing an engaging clinical intervention AND studying implementation factors/strategies to improve uptake of CR programming across specific settings and regions has been lacking. <br /><br />Low engagement of patients in CR programs may be an artifact of the fact that many of the cardiologists and cardiac rehab specialists who developed the exercise protocol were not interested in patient-level motivational strategies as part of effectiveness testing (motivational interviewing is hard without the proper context and theoretical training!). These protocols seem written from the perspective of &quot; &#039;IF&#039; you show up, we can make you better.&quot; Likewise, this disinterest in translational research is also reflected by the paucity of implementation trials that have tested strategies to overcome provider and organizational barriers to effective CR uptake and utilization. It creates a conundrum when you have a promising innovation that the researchers left half-baked.  <br /><br />I would encourage you to think of two phases in your research:<br /><br />1) Expand the operational core components of what comprises evidence-based CR car for heart failure. Based on my exercise science training, the CPG&#039;s and exercise protocols generally lack clear recommendations on  patient-level engagement practices or clinical care processes to optimize adherence (e.g., screening, decision making, referrals, motivational enhancement aspects, billing, etc.). Furthermore, beyond what is essential for optimizing treatment outcomes, what is negotiable or adaptable across settings while providing some adherence to the core elements (e.g. staffing mix, referral methods, timing of procedures, dose?)? Is an automatic referral or having a shared decision making conversation about CR while still in the hospital a core element or an adaptation? Likewise, is variation whether a patient is engaging in CR from a nursing home/assisted living context vs. home, an adaptation or does it reflect two different populations of CR patients requiring slightly different recommendations based on treatment complexity or care coordination? Bottom line - the essential components of good programming for CR for heart failure patients is still relatively undefined based on your description of the current state of the art. YOU, have the opportunity to partner with organizations or other experts to change that which, is quite exciting!! <br /><br />2) Once you have a clearly defined intervention with flexible adaptations (preferably a checklist to indicate the presence of or quality of core elements and adaptations), you can then specify the strategies to implement the intervention and address the barriers you&#039;ve identified. These strategies may vary across the phase of implementation (pre-implementation, adoption, scale-up and spread, and sustainability/maintenance). Dense urban/suburban settings or regions will have different needs than rural or historically under-resourced regions. Byron Powell&#039;s work on customizing these strategies will be very relevant in the coming weeks. You can also leverage relationships of peers who are now working in parallel with your own research efforts (e.g., Mary Whooley in San Francisco and likely others in some of the large healthcare systems like Kaiser or Mayo Clinic). <br /><br />I realize it sounds ambitious to come up with new clinical practice recommendations but it is important to get all clinical stakeholders to agree on what needs to be implemented as target for best practice or quality programming. Once the core elements are defined, then adaptations can be identified. It seems like your planning lumps adaptations for local context as an implementation strategy. I would keep them separate distinctly separate (clinical intervention to yield patient outcomes vs. strategies to address provider/facility/external policy environment challenges). I hope this advice is not too long or intimidating... I think you have a lot of the information from the literature to start repackaging and defining what you might define as the necessary core elements should be. Once you&#039;ve done that, it&#039;s easier to identify reasonable adaptions based on organizational constraints. I wouldn&#039;t worry about the implementation strategies for the time-being. There is an upcoming reading on implementation trials by Geoff Curran that will help make some of these issues clearer. In the meantime, I would be pleased to continue this conversation via the Wiki.<br />David";s:6:"parent";s:32:"9440610e45de601c6c7c7c919bdfbece";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"2d3eb41927b1fe2eeefeb3787ee1ead1";}s:32:"c1a7faf8cd119d94da8024da05ef2cc2";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dgoodrich";s:4:"name";s:14:"David Goodrich";s:4:"mail";s:22:"David.Goodrich2@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536795934;}s:3:"raw";s:2815:"Hi Kimberly, this topic is so important with the Baby Boomers entering our healthcare facilities with complex needs. I suspect one thing that you will consistently find across your positive deviants is the capability to monitor high risk patients with appropriate care management or care coordination strategies (panel management). What risk prediction models or tools help providers make good decisions to allocate scarce resources to certain patients over others? You've identified a number of clinical strategies and processes, structures, systems, and organizational factors that define "evidence-based" transition practices. You also allude to aspects of these components that may vary based on treatment intensity, condition, staffing mix, or hospital resources or organizational priority. 

Like Tara, you may have the opportunity to be a clinical innovator in terms of defining what these evidence-based practices and processes are. It is essential to define the fundamentals (core elements) because these are the standards or goals for care teams to achieve and, to be held accountable for maintaining. 

I would encourage you to view the implementation strategies as a related but separate issue. These are the interventions used to change provider and organizational behavior to carry out the core elements of best practices. Right now, if you were to distill the literature down to a set of guiding clinical processes/actions for optimal transitions, what would those be? Where would you like to see them disseminated to (who are the consumers of these recommendations?) Based on these fundamental principles/components of effective and reliable care transitions, it will be easier to discern care processes that are flexible and can accommodate adaptation without losing and in some cases, improving clinical effectiveness.  

If people don't understand what they're implementing as fundamental tenants of good care, the variation in quality and clinical outcomes related to care transitions will be perpetuated. Make good use of your time right now to operationalize a list of core elements that appear evidence-based across your scan of the literature and positive deviants. These elements are not written in stone but they provide a basis for conversations and measuring current levels of performance to further improve transition outcomes.  

Finally, the organizational culture factors are important but they are more likely to be the target of the implementation strategies proposed to address contextual variation (treated as either a barrier to or enabling factor depending on organizational strengths/weaknesses). Addressing these factors is different than addressing the clinical transition process itself and a topic for a future week. Let me know if this feedback has been helpful. 

-David";s:5:"xhtml";s:2890:"Hi Kimberly, this topic is so important with the Baby Boomers entering our healthcare facilities with complex needs. I suspect one thing that you will consistently find across your positive deviants is the capability to monitor high risk patients with appropriate care management or care coordination strategies (panel management). What risk prediction models or tools help providers make good decisions to allocate scarce resources to certain patients over others? You&#039;ve identified a number of clinical strategies and processes, structures, systems, and organizational factors that define &quot;evidence-based&quot; transition practices. You also allude to aspects of these components that may vary based on treatment intensity, condition, staffing mix, or hospital resources or organizational priority. <br /><br />Like Tara, you may have the opportunity to be a clinical innovator in terms of defining what these evidence-based practices and processes are. It is essential to define the fundamentals (core elements) because these are the standards or goals for care teams to achieve and, to be held accountable for maintaining. <br /><br />I would encourage you to view the implementation strategies as a related but separate issue. These are the interventions used to change provider and organizational behavior to carry out the core elements of best practices. Right now, if you were to distill the literature down to a set of guiding clinical processes/actions for optimal transitions, what would those be? Where would you like to see them disseminated to (who are the consumers of these recommendations?) Based on these fundamental principles/components of effective and reliable care transitions, it will be easier to discern care processes that are flexible and can accommodate adaptation without losing and in some cases, improving clinical effectiveness.  <br /><br />If people don&#039;t understand what they&#039;re implementing as fundamental tenants of good care, the variation in quality and clinical outcomes related to care transitions will be perpetuated. Make good use of your time right now to operationalize a list of core elements that appear evidence-based across your scan of the literature and positive deviants. These elements are not written in stone but they provide a basis for conversations and measuring current levels of performance to further improve transition outcomes.  <br /><br />Finally, the organizational culture factors are important but they are more likely to be the target of the implementation strategies proposed to address contextual variation (treated as either a barrier to or enabling factor depending on organizational strengths/weaknesses). Addressing these factors is different than addressing the clinical transition process itself and a topic for a future week. Let me know if this feedback has been helpful. <br /><br />-David";s:6:"parent";s:32:"b2d70705ff54d008fc4d7f960b71bb50";s:7:"replies";a:1:{i:0;s:32:"f3b2c3e81854ebf79f59d3b314a30320";}s:4:"show";b:1;s:3:"cid";s:32:"c1a7faf8cd119d94da8024da05ef2cc2";}s:32:"35a1d6e0581e70fe0bb094164e698c6f";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lducharme";s:4:"name";s:13:"Lori Ducharme";s:4:"mail";s:21:"lori.ducharme@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536955008;}s:3:"raw";s:1603:"Hi Sivan.  You've got quite an interesting project that has some unique features, so first apologies if some of these assignments have you putting square pegs in round holes.  :)  Because you’re taking a phased approach to this project, it’s good to know that you have a bookmark for fidelity monitoring at the future point in the project where it will make the most sense. But as you develop the training module, you will want to think about which are the core elements of the training program – what is mandatory and must be delivered as written, and how will the individuals delivering the training be instructed or monitored to ensure fidelity?  These are nitty-gritty logistical issues in a sense.  Your pilot data collection could (should?) be structured in such a way that it helps you answer those questions about what are the essential core components of the training.  So even if fidelity measurement is something that will happen “later,” it may be worth the exercise to start thinking about it now.  By contrast, you already seem to be anticipating the need for adaptation given departmental differences, and it’s great to have a framework (C/M/O) to think about that.  In the process of rolling out the training, you’ll want to be able to measure both fidelity and adaptation in such a way as to know whether any variation in outcomes was attributable to variation in how the training was delivered (i.e., variation in fidelity). 

I hope these readings are helpful for you; let us know if there are any topics where you need more examples to help shape your project.  -- Lori";s:5:"xhtml";s:1618:"Hi Sivan.  You&#039;ve got quite an interesting project that has some unique features, so first apologies if some of these assignments have you putting square pegs in round holes.  :)  Because you’re taking a phased approach to this project, it’s good to know that you have a bookmark for fidelity monitoring at the future point in the project where it will make the most sense. But as you develop the training module, you will want to think about which are the core elements of the training program – what is mandatory and must be delivered as written, and how will the individuals delivering the training be instructed or monitored to ensure fidelity?  These are nitty-gritty logistical issues in a sense.  Your pilot data collection could (should?) be structured in such a way that it helps you answer those questions about what are the essential core components of the training.  So even if fidelity measurement is something that will happen “later,” it may be worth the exercise to start thinking about it now.  By contrast, you already seem to be anticipating the need for adaptation given departmental differences, and it’s great to have a framework (C/M/O) to think about that.  In the process of rolling out the training, you’ll want to be able to measure both fidelity and adaptation in such a way as to know whether any variation in outcomes was attributable to variation in how the training was delivered (i.e., variation in fidelity). <br /><br />I hope these readings are helpful for you; let us know if there are any topics where you need more examples to help shape your project.  -- Lori";s:6:"parent";s:32:"85e7ba8fb289732d124e3aba13814478";s:7:"replies";a:1:{i:0;s:32:"4bdbf9702d98200a415969c1b79f6fdd";}s:4:"show";b:1;s:3:"cid";s:32:"35a1d6e0581e70fe0bb094164e698c6f";}s:32:"fb20b6c8e6631f18c01deec40f5bc7fe";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lducharme";s:4:"name";s:13:"Lori Ducharme";s:4:"mail";s:21:"lori.ducharme@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536956750;}s:3:"raw";s:1358:"Hi James.  Technology interventions such as eScreening are sometimes refreshing for implementation science, right?  These tools “are what they are” so we don’t have to worry too much about them being reprogrammed such that they don’t serve their intended purpose.  So in some sense, fidelity may be less of a concern because the user has no option but to use the forms and fields that appear in front of them. But are there ways in which more general fidelity issues might creep into the clinic where it’s being implemented?  (Reminders don’t get sent, staff not on board, patients would rather talk to the nurse? -- places where the process breaks down) While the screening tool itself is locked down, I wonder whether there are potential fidelity issues in the implementation process itself to consider.  You’ve touched on some of this in thinking about the implementation playbook, and seem to have a good handle on it.

I do appreciate your consideration of other (unanticipated) ways in which clinicians may use the data resulting from the screenings. In some respects this isn’t quite fidelity or adaptation, but it’s important behavior to capture – and may actually help inform your implementation process, for example, by suggesting other benefits that can be important for getting clinics on board with this new process. -- Lori
";s:5:"xhtml";s:1367:"Hi James.  Technology interventions such as eScreening are sometimes refreshing for implementation science, right?  These tools “are what they are” so we don’t have to worry too much about them being reprogrammed such that they don’t serve their intended purpose.  So in some sense, fidelity may be less of a concern because the user has no option but to use the forms and fields that appear in front of them. But are there ways in which more general fidelity issues might creep into the clinic where it’s being implemented?  (Reminders don’t get sent, staff not on board, patients would rather talk to the nurse? -- places where the process breaks down) While the screening tool itself is locked down, I wonder whether there are potential fidelity issues in the implementation process itself to consider.  You’ve touched on some of this in thinking about the implementation playbook, and seem to have a good handle on it.<br /><br />I do appreciate your consideration of other (unanticipated) ways in which clinicians may use the data resulting from the screenings. In some respects this isn’t quite fidelity or adaptation, but it’s important behavior to capture – and may actually help inform your implementation process, for example, by suggesting other benefits that can be important for getting clinics on board with this new process. -- Lori";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"6618419db18e995e044fd672da003048";}s:4:"show";b:1;s:3:"cid";s:32:"fb20b6c8e6631f18c01deec40f5bc7fe";}s:32:"8b7c2caac1acb441dc4cbf54fe3d4257";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"bjpowell";s:4:"name";s:12:"Byron Powell";s:4:"mail";s:16:"bjpowell@unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537022297;}s:3:"raw";s:2001:"Hi Barb,

First of all, I love the user-centered design approach, and I think there is an opportunity to have that approach inform both your clinical intervention and your implementation strategies. There are calls for more systematic approaches to adapting clinical interventions and designing and tailoring implementation strategies, and what you present may directly address those priorities. I'm excited to learn more about the specific steps you're using, but really like the idea of framing this as a methods advance.

Second, as it stands, I'm not clear what types of implementation strategies you are thinking about using (this just echoes David's earlier question), but I imagine you will have time to think about that in more detail in subsequent assignments.

Third, I was curious as to whether there was a measure of fidelity for the CDS intervention. It is great that you will systematically track fidelity and adaptation, but I was curious as to how you would accomplish that. 

Fourth, nice job of identifying potential modifications at different levels. 

Fifth, just a few comments on the current aims. I find your first aim hard to digest (it seems like a lot packed into a single aim), and I'm wondering if there is a more direct, succinct way to phrase. I also wondered if the 3rd aim can just focus on examining the clinical impact. If I'm following the aims correctly, Aim 1 is really about optimizing the CDS, Aim 2 could be about examining implementation outcomes and determinants (i.e., I wonder if you could even move some of the CFIR stuff into Aim 2), and then Aim 3 is about examining clinical outcomes. With this flow, Aim 1 is your formative work, and Aim 2 and 3 are part of the trial. Maybe this doesn't track with what you're trying to accomplish though. In some ways, it feel like it could be Hybrid 1 trial if I'm understanding the current state of the evidence for the CDS intervention. Happy to discuss further if helpful! 

Good work and hope all is well!
Byron
";s:5:"xhtml";s:2110:"Hi Barb,<br /><br />First of all, I love the user-centered design approach, and I think there is an opportunity to have that approach inform both your clinical intervention and your implementation strategies. There are calls for more systematic approaches to adapting clinical interventions and designing and tailoring implementation strategies, and what you present may directly address those priorities. I&#039;m excited to learn more about the specific steps you&#039;re using, but really like the idea of framing this as a methods advance.<br /><br />Second, as it stands, I&#039;m not clear what types of implementation strategies you are thinking about using (this just echoes David&#039;s earlier question), but I imagine you will have time to think about that in more detail in subsequent assignments.<br /><br />Third, I was curious as to whether there was a measure of fidelity for the CDS intervention. It is great that you will systematically track fidelity and adaptation, but I was curious as to how you would accomplish that. <br /><br />Fourth, nice job of identifying potential modifications at different levels. <br /><br />Fifth, just a few comments on the current aims. I find your first aim hard to digest (it seems like a lot packed into a single aim), and I&#039;m wondering if there is a more direct, succinct way to phrase. I also wondered if the 3rd aim can just focus on examining the clinical impact. If I&#039;m following the aims correctly, Aim 1 is really about optimizing the CDS, Aim 2 could be about examining implementation outcomes and determinants (i.e., I wonder if you could even move some of the CFIR stuff into Aim 2), and then Aim 3 is about examining clinical outcomes. With this flow, Aim 1 is your formative work, and Aim 2 and 3 are part of the trial. Maybe this doesn&#039;t track with what you&#039;re trying to accomplish though. In some ways, it feel like it could be Hybrid 1 trial if I&#039;m understanding the current state of the evidence for the CDS intervention. Happy to discuss further if helpful! <br /><br />Good work and hope all is well!<br />Byron";s:6:"parent";s:32:"ef1eee39318916242ebed92436a5d36c";s:7:"replies";a:1:{i:0;s:32:"ca1a30c58a2c1ee8481b67ea42231f98";}s:4:"show";b:1;s:3:"cid";s:32:"8b7c2caac1acb441dc4cbf54fe3d4257";}s:32:"966b82f6179a0d7c72f163aa4a0639fd";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"bjpowell";s:4:"name";s:12:"Byron Powell";s:4:"mail";s:16:"bjpowell@unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537031046;}s:3:"raw";s:2086:"Hi Ken,

This is a really interesting study, and I just have a few comments on both this assignment and assignment 1, which I read again in preparation for giving feedback here.

1) It wasn't very clear to me how you're using RE-AIM to "improve implementation of HIE." I know you'll have more time to elaborate in subsequent assignments, but it would be great to be very clear about which elements of RE-AIM you're using and how it is informing your project.
2) I wasn't sure whether you were using a framework to guide the assessment of barriers in Aim 1. It would seem like it would be beneficial to draw upon previous work to categorize potential barriers by employing a framework such as CFIR, TDF, TICD checklist, EPIS, etc.). I thought it could be helpful for you to take a look at Nilsen's (2015) categories of theories, models, and frameworks, and to differentiate between use of a determinant framework (if you use one) and your evaluation framework (RE-AIM). 
3) You note that you'll use "standard qualitative methods" to analyze interview data. I think you may need more detail on your actual approach to qualitative data analysis, and in my view, it would be most helpful to use an approach that would allow for both deductive analysis (using an existing framework to guide your analysis) and inductive analysis. 
4) I'm curious exactly how you'll propose strategies to improve HIE implementation in Aim 2. Would be great to use a systematic process to develop those strategies, which I think is something we can talk about in subsequent weeks. 
5) I'd like to see more detail about how you will measure fidelity (i.e., how will you know if they utilize HIE for decision making, etc.).  
5) I love your use of the "think aloud" approach to examining potential barriers to implementation. Really smart and I think it will yield rich data on barriers and practitioners' ability to implement with fidelity. 
6) Nice use of Stirman and Rabin approaches for documenting adaptations. 

Nice work and I'll look forward to learning more in the coming weeks and months! 

Best,
Byron";s:5:"xhtml";s:2251:"Hi Ken,<br /><br />This is a really interesting study, and I just have a few comments on both this assignment and assignment 1, which I read again in preparation for giving feedback here.<br /><br />1) It wasn&#039;t very clear to me how you&#039;re using RE-AIM to &quot;improve implementation of HIE.&quot; I know you&#039;ll have more time to elaborate in subsequent assignments, but it would be great to be very clear about which elements of RE-AIM you&#039;re using and how it is informing your project.<br />2) I wasn&#039;t sure whether you were using a framework to guide the assessment of barriers in Aim 1. It would seem like it would be beneficial to draw upon previous work to categorize potential barriers by employing a framework such as CFIR, TDF, TICD checklist, EPIS, etc.). I thought it could be helpful for you to take a look at Nilsen&#039;s (2015) categories of theories, models, and frameworks, and to differentiate between use of a determinant framework (if you use one) and your evaluation framework (RE-AIM). <br />3) You note that you&#039;ll use &quot;standard qualitative methods&quot; to analyze interview data. I think you may need more detail on your actual approach to qualitative data analysis, and in my view, it would be most helpful to use an approach that would allow for both deductive analysis (using an existing framework to guide your analysis) and inductive analysis. <br />4) I&#039;m curious exactly how you&#039;ll propose strategies to improve HIE implementation in Aim 2. Would be great to use a systematic process to develop those strategies, which I think is something we can talk about in subsequent weeks. <br />5) I&#039;d like to see more detail about how you will measure fidelity (i.e., how will you know if they utilize HIE for decision making, etc.).  <br />5) I love your use of the &quot;think aloud&quot; approach to examining potential barriers to implementation. Really smart and I think it will yield rich data on barriers and practitioners&#039; ability to implement with fidelity. <br />6) Nice use of Stirman and Rabin approaches for documenting adaptations. <br /><br />Nice work and I&#039;ll look forward to learning more in the coming weeks and months! <br /><br />Best,<br />Byron";s:6:"parent";s:32:"917cc63bb972756847473d6415d6d079";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"966b82f6179a0d7c72f163aa4a0639fd";}s:32:"8606dc649d85232a48a52f91e4816067";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jpittman";s:4:"name";s:13:"James Pittman";s:4:"mail";s:17:"jpittman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537372783;}s:3:"raw";s:211:"Hi BP, 

Sorry for the delayed response. I just wanted to thank you for the thoughtful questions and let you know that I am slowly working through them to help improve my AIMS. More to follow. 

Thank you,
James";s:5:"xhtml";s:236:"Hi BP, <br /><br />Sorry for the delayed response. I just wanted to thank you for the thoughtful questions and let you know that I am slowly working through them to help improve my AIMS. More to follow. <br /><br />Thank you,<br />James";s:6:"parent";s:32:"513ae4a492c994356a55fc59c593bba4";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"8606dc649d85232a48a52f91e4816067";}s:32:"6618419db18e995e044fd672da003048";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jpittman";s:4:"name";s:13:"James Pittman";s:4:"mail";s:17:"jpittman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537373062;}s:3:"raw";s:177:"Hi Lori, 

Great point about the general fidelity issues -- I hadn't thought of it in that way. I agree that the issues you raised will be important to capture. 

Thanks!
-James";s:5:"xhtml";s:207:"Hi Lori, <br /><br />Great point about the general fidelity issues -- I hadn&#039;t thought of it in that way. I agree that the issues you raised will be important to capture. <br /><br />Thanks!<br />-James";s:6:"parent";s:32:"fb20b6c8e6631f18c01deec40f5bc7fe";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"6618419db18e995e044fd672da003048";}s:32:"87667ca23ad4eb1ab3b256cc10e3373e";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"tlagu";s:4:"name";s:9:"Tara Lagu";s:4:"mail";s:28:"Tara.Lagu@baystatehealth.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537381526;}s:3:"raw";s:4648:"Lagu Assignment #3a and #3b:

After last week's feedback I feel like I need to totally rewrite my aims! Assuming that I am NOT doing that, however (at least for the moment), I based my answers on my existing aims. 

1.	Which model or combination of models is most applicable to your proposed study and why?
We propose to conduct a qualitative analysis to identify the strategies that lead to higher rates of referral to cardiac rehabilitation among patients with heart failure, and this is an examination of a complex series of events (admission, referral, attendance, retention, etc) that require coordination across multiple care settings. Referral and attendance are affected by patient factors, clinical factors, environment, community factors, etc. Therefore, we propose to use the Consolidated Framework for Implementation Research (CFIR) to guide our analysis. CFIR provides a comprehensive taxonomy of constructs and will help facilitate clarity and understanding of complex implementation processes.  Aspects of our interview guide will examine elements of the CFIR taxonomy: Intervention Characteristics, Individuals/Team, Inner Setting, and Outer Setting. We will focus on a few specific topics: the role of HF specialists in promoting CR, the role of the post-hospitalization clinic visit in facilitating referral, and the impact of the 6-week waiting period on patient interest and willingness to attend CR, as these specific to the HF population, but will also explore the role of environmental and community factors such as program availability, CR program cooperation, and the presence of risk contracts and bundled payments, such as accountable care organizations. Potential barriers to be explored include provider beliefs about CR, uncertainty regarding eligibility or timing, and a variety of patient factors (e.g., lack of interest, fear of dyspnea in CR, health literacy, driving distances). 

How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 

The qualitative work will provide data that will inform our 3rd and 4th aims. For the third Aim, we plan to convene a Delphi panel of experts to review results. Using the themes we identified in Aim 2 as starting point, we will use the Expert Recommendations for Implementing Change (ERIC)  to compile the data into a list of strategies that can be further examined by the Delphi panel. The Delphi panel will assess acceptability, appropriateness, and feasibility of defined strategies. After 2 rounds of ratings, we will compile a final list of recommendations that we will disseminate as part of Aim 4. Our ultimate goal is to identify strategies for increasing referral to CR for patients with HF that are transferrable and generalizable to other settings. 

Assignment #3b
1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?
Aim 1: Risk-standardized rates of cardiac rehabilitation participation at the level of the HRR (using Medicare data to calculate rates)
Aim 2: Strategies/processes of high performers; barriers and facilitators to referral; patient, system, community and environmental factors that affect referral (qualitative interviews)
Aim 3: Synopsis summarizing the Delphi panel’s work, including name and definition of each strategy and examples of strategies in real-world settings. As above, the Delphi panel will assess acceptability, appropriateness, and feasibility of defined strategies.
Aim 4: Pilot the identified strategies among a subset of CR programs, will identify outcomes once we know the strategies we are piloting

What processes are you measuring in your study, how are you measuring them, and why are you measuring them?
We aren’t really measuring actual processes in Aims 1-3, although, as above, the Delphi panel will assess acceptability, appropriateness, and feasibility of described strategies. In Aim 4, we will measure implementation of various strategies, but we don’t yet have a plan of how we are measuring yet because we are using Aims 1-3 to identify the strategies that we will pilot in Aim 4.

Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If 
We don’t yet have a plan of how we are measuring yet because we are using Aims 1-3 to identify the strategies that we will pilot in Aim 4, but this is something that we must absolutely consider as the pilot is designed.
";s:5:"xhtml";s:4762:"Lagu Assignment #3a and #3b:<br /><br />After last week&#039;s feedback I feel like I need to totally rewrite my aims! Assuming that I am NOT doing that, however (at least for the moment), I based my answers on my existing aims. <br /><br />1.	Which model or combination of models is most applicable to your proposed study and why?<br />We propose to conduct a qualitative analysis to identify the strategies that lead to higher rates of referral to cardiac rehabilitation among patients with heart failure, and this is an examination of a complex series of events (admission, referral, attendance, retention, etc) that require coordination across multiple care settings. Referral and attendance are affected by patient factors, clinical factors, environment, community factors, etc. Therefore, we propose to use the Consolidated Framework for Implementation Research (CFIR) to guide our analysis. CFIR provides a comprehensive taxonomy of constructs and will help facilitate clarity and understanding of complex implementation processes.  Aspects of our interview guide will examine elements of the CFIR taxonomy: Intervention Characteristics, Individuals/Team, Inner Setting, and Outer Setting. We will focus on a few specific topics: the role of HF specialists in promoting CR, the role of the post-hospitalization clinic visit in facilitating referral, and the impact of the 6-week waiting period on patient interest and willingness to attend CR, as these specific to the HF population, but will also explore the role of environmental and community factors such as program availability, CR program cooperation, and the presence of risk contracts and bundled payments, such as accountable care organizations. Potential barriers to be explored include provider beliefs about CR, uncertainty regarding eligibility or timing, and a variety of patient factors (e.g., lack of interest, fear of dyspnea in CR, health literacy, driving distances). <br /><br />How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br /><br />The qualitative work will provide data that will inform our 3rd and 4th aims. For the third Aim, we plan to convene a Delphi panel of experts to review results. Using the themes we identified in Aim 2 as starting point, we will use the Expert Recommendations for Implementing Change (ERIC)  to compile the data into a list of strategies that can be further examined by the Delphi panel. The Delphi panel will assess acceptability, appropriateness, and feasibility of defined strategies. After 2 rounds of ratings, we will compile a final list of recommendations that we will disseminate as part of Aim 4. Our ultimate goal is to identify strategies for increasing referral to CR for patients with HF that are transferrable and generalizable to other settings. <br /><br />Assignment #3b<br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br />Aim 1: Risk-standardized rates of cardiac rehabilitation participation at the level of the HRR (using Medicare data to calculate rates)<br />Aim 2: Strategies/processes of high performers; barriers and facilitators to referral; patient, system, community and environmental factors that affect referral (qualitative interviews)<br />Aim 3: Synopsis summarizing the Delphi panel’s work, including name and definition of each strategy and examples of strategies in real-world settings. As above, the Delphi panel will assess acceptability, appropriateness, and feasibility of defined strategies.<br />Aim 4: Pilot the identified strategies among a subset of CR programs, will identify outcomes once we know the strategies we are piloting<br /><br />What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br />We aren’t really measuring actual processes in Aims 1-3, although, as above, the Delphi panel will assess acceptability, appropriateness, and feasibility of described strategies. In Aim 4, we will measure implementation of various strategies, but we don’t yet have a plan of how we are measuring yet because we are using Aims 1-3 to identify the strategies that we will pilot in Aim 4.<br /><br />Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If <br />We don’t yet have a plan of how we are measuring yet because we are using Aims 1-3 to identify the strategies that we will pilot in Aim 4, but this is something that we must absolutely consider as the pilot is designed.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"052e225b8c5f3340388a381118365887";}s:4:"show";b:1;s:3:"cid";s:32:"87667ca23ad4eb1ab3b256cc10e3373e";}s:32:"ca1a30c58a2c1ee8481b67ea42231f98";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"bjones";s:4:"name";s:13:"Barbara Jones";s:4:"mail";s:26:"Barbara.Jones@hsc.utah.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537393222;}s:3:"raw";s:93:"Thanks so much for your input Byron!  That is great advice about the aims. I will work on it!";s:5:"xhtml";s:93:"Thanks so much for your input Byron!  That is great advice about the aims. I will work on it!";s:6:"parent";s:32:"8b7c2caac1acb441dc4cbf54fe3d4257";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"ca1a30c58a2c1ee8481b67ea42231f98";}s:32:"71bffaefb8a3d7082667c56ca2301ae9";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"bjones";s:4:"name";s:13:"Barbara Jones";s:4:"mail";s:26:"Barbara.Jones@hsc.utah.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537436524;}s:3:"raw";s:6683:"Jones Assignment #3a - Models:
1.	Which model or combination of models is most applicable to your proposed study and why?
The goal of the project is to implement and examine an evidence-based computerized decision support tool for pneumonia that supports and examines the adaptation of standard practice recommendations to individual patients across settings in the VA. We hypothesize that several components of the current practice guidelines are based upon evidence that is uncertain, or may require substantial adaptation to settings and patients that are underrepresented in the evidence base (such as rural and small hospitals). We have therefore selected a determinant framework (CFIR) to identify the factors associated with implementation, and classic theory (Rogers) to describe the implementation and adaptation process within a DOI framework, with an emphasis on intervention characteristics (to identify components with uncertain evidence or applicability to different settings) and re-invention/adaptation (for individual provider characteristics, inner/outer setting, that influence fidelity and adaptation of components of standard practice).
One challenging part for me is that the success of the implementation will also in part determined by providers’ attitudes toward their EHR and health informatics tools, and the CDS’s usability. This leads me to seek other frameworks outside of implementation research that could help describe the implementation of the CDS: 1) user-centered design (I think I would describe this as a process framework – it is a well-validated process of design and implementation that involves cognitive work analyses, iterative design with end users, and formative evaluations; adaptation can be measured during this process) ; 2) the TURF EHR/health IT usability framework, as well as cognitive principles underlying human-computer interaction, visual display of information and user interface design (I think these would be determinant frameworks); and 3) additional frameworks from cognitive psychology such as information processing, dual process cognition, and self efficacy and motivation. I fear this may have led to possible framework overload.

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 
The CFIR and Rogers frameworks have definitely guided our general hypotheses regarding the interaction between intervention characteristics and setting. We have designed and piloted an interview that combines a cognitive work analysis (user-centered design) with the CFIR implementation constructs (focusing on provider and inner/outer setting constructs that influence implementation and adaptation) which seems to help us get at provider mental models, workflow, and attitudes toward both pneumonia guidelines and health IT. It has also been useful to plan the user-centered design processes around tracking adaptations as which implementation constructs they might fit into (ex – substitution of antimicrobial recommendations due to lack of confidence in guidelines by local antibiotic steward and clinical leadership, versus making severity-of-illness assessment voluntary).


Assignment #3b - Measures & Evaluations:
1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?
Well validated pneumonia clinical outcome metrics: 30-day mortality (this has been previously demonstrated to be reduced with guideline implementation), hospitalization of low-risk patients (has been demonstrated with a prospective implementation of severity of illness guidelines); 7-day secondary hospitalization among outpatients (a recommended metric for “under-hospitalization”). Site-specific process measures (such as influenza testing/treatment, specific abx stewardship goals) will be established through discussion with local clinical leaders. 
Dissemination/implementation: CDS use (tracked automatically by ; adoption of components of guidelines (site-of-care, antimicrobial selection); free text of clinical reasoning for adaptations with thematic analysis using concept mapping (this is one of those “used once and not well validated” measures – but it is a novel way to identify adaptation constructs not previously identified). 

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?
Well validated pneumonia and sepsis process metrics: Pneumonia guideline adherence (empiric antimicrobial selection, site-of-care decision-making) (VA data); IVF resuscitation and time to antibiotics among patients with sepsis (VA data). We also have explored additional decision-making metrics including antimicrobial-pathogen (“bug-drug” matching – alignment between antimicrobial selection and pathogens identifies. All metrics will be aggregated at the setting and the provider level. 
3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?
Provider-reported satisfaction, self-efficacy and user experience through CDS usability surveys.
- Revised aims  (still wordy!):
1)	Optimize an evidence-based computerized clinical decision support tool for pneumonia that incorporates and measures contextual differences in mental models of disease, information needs and constraints, and determinants of implementation/adoption for VA facilities demonstrating variation in pneumonia care processes. Ho: components of current pneumonia standard-practice guidelines will require substantial adaptation to integrate into workflow and mental models of providers for patients at underrepresented settings
2)	Examine the adoption of components of pneumonia standard care through the implementation of CDS across VA facilities demonstrating variation in care processes. Ho1: physicians who interact with a usable CDS will provide important information regarding the appropriateness and adaptation of guideline-based care in different contexts. Ho2: some components guideline-based pneumonia care will demonstrate lower fidelity in settings that are underrepresented in the evidence (ie rural versus urban facilities; populations with low SES; facilities without subspecialty care). Ho3: adoption/use of CDS will be determined by interpersonal networks rather than mass communication in rural > urban settings.
3)	Examine impact of CDS on pneumonia care processes, patient clinical outcomes, and provider satisfaction. 
";s:5:"xhtml";s:6793:"Jones Assignment #3a - Models:<br />1.	Which model or combination of models is most applicable to your proposed study and why?<br />The goal of the project is to implement and examine an evidence-based computerized decision support tool for pneumonia that supports and examines the adaptation of standard practice recommendations to individual patients across settings in the VA. We hypothesize that several components of the current practice guidelines are based upon evidence that is uncertain, or may require substantial adaptation to settings and patients that are underrepresented in the evidence base (such as rural and small hospitals). We have therefore selected a determinant framework (CFIR) to identify the factors associated with implementation, and classic theory (Rogers) to describe the implementation and adaptation process within a DOI framework, with an emphasis on intervention characteristics (to identify components with uncertain evidence or applicability to different settings) and re-invention/adaptation (for individual provider characteristics, inner/outer setting, that influence fidelity and adaptation of components of standard practice).<br />One challenging part for me is that the success of the implementation will also in part determined by providers’ attitudes toward their EHR and health informatics tools, and the CDS’s usability. This leads me to seek other frameworks outside of implementation research that could help describe the implementation of the CDS: 1) user-centered design (I think I would describe this as a process framework – it is a well-validated process of design and implementation that involves cognitive work analyses, iterative design with end users, and formative evaluations; adaptation can be measured during this process) ; 2) the TURF EHR/health IT usability framework, as well as cognitive principles underlying human-computer interaction, visual display of information and user interface design (I think these would be determinant frameworks); and 3) additional frameworks from cognitive psychology such as information processing, dual process cognition, and self efficacy and motivation. I fear this may have led to possible framework overload.<br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br />The CFIR and Rogers frameworks have definitely guided our general hypotheses regarding the interaction between intervention characteristics and setting. We have designed and piloted an interview that combines a cognitive work analysis (user-centered design) with the CFIR implementation constructs (focusing on provider and inner/outer setting constructs that influence implementation and adaptation) which seems to help us get at provider mental models, workflow, and attitudes toward both pneumonia guidelines and health IT. It has also been useful to plan the user-centered design processes around tracking adaptations as which implementation constructs they might fit into (ex – substitution of antimicrobial recommendations due to lack of confidence in guidelines by local antibiotic steward and clinical leadership, versus making severity-of-illness assessment voluntary).<br /><br /><br />Assignment #3b - Measures &amp; Evaluations:<br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br />Well validated pneumonia clinical outcome metrics: 30-day mortality (this has been previously demonstrated to be reduced with guideline implementation), hospitalization of low-risk patients (has been demonstrated with a prospective implementation of severity of illness guidelines); 7-day secondary hospitalization among outpatients (a recommended metric for “under-hospitalization”). Site-specific process measures (such as influenza testing/treatment, specific abx stewardship goals) will be established through discussion with local clinical leaders. <br />Dissemination/implementation: CDS use (tracked automatically by ; adoption of components of guidelines (site-of-care, antimicrobial selection); free text of clinical reasoning for adaptations with thematic analysis using concept mapping (this is one of those “used once and not well validated” measures – but it is a novel way to identify adaptation constructs not previously identified). <br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br />Well validated pneumonia and sepsis process metrics: Pneumonia guideline adherence (empiric antimicrobial selection, site-of-care decision-making) (VA data); IVF resuscitation and time to antibiotics among patients with sepsis (VA data). We also have explored additional decision-making metrics including antimicrobial-pathogen (“bug-drug” matching – alignment between antimicrobial selection and pathogens identifies. All metrics will be aggregated at the setting and the provider level. <br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br />Provider-reported satisfaction, self-efficacy and user experience through CDS usability surveys.<br />- Revised aims  (still wordy!):<br />1)	Optimize an evidence-based computerized clinical decision support tool for pneumonia that incorporates and measures contextual differences in mental models of disease, information needs and constraints, and determinants of implementation/adoption for VA facilities demonstrating variation in pneumonia care processes. Ho: components of current pneumonia standard-practice guidelines will require substantial adaptation to integrate into workflow and mental models of providers for patients at underrepresented settings<br />2)	Examine the adoption of components of pneumonia standard care through the implementation of CDS across VA facilities demonstrating variation in care processes. Ho1: physicians who interact with a usable CDS will provide important information regarding the appropriateness and adaptation of guideline-based care in different contexts. Ho2: some components guideline-based pneumonia care will demonstrate lower fidelity in settings that are underrepresented in the evidence (ie rural versus urban facilities; populations with low SES; facilities without subspecialty care). Ho3: adoption/use of CDS will be determined by interpersonal networks rather than mass communication in rural &gt; urban settings.<br />3)	Examine impact of CDS on pneumonia care processes, patient clinical outcomes, and provider satisfaction.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"7d04937316f5c7bc3fc7c4abf497c6f0";}s:4:"show";b:1;s:3:"cid";s:32:"71bffaefb8a3d7082667c56ca2301ae9";}s:32:"f3b2c3e81854ebf79f59d3b314a30320";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"kfisher";s:4:"name";s:15:"Kimberly Fisher";s:4:"mail";s:33:"Kimberly.Fisher@umassmemorial.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537475493;}s:3:"raw";s:318:"Dear David,
Thank you so much for this input - this input/feedback is extremely helpful and timely as I am thick in the interviews with positive deviants so thinking about and defining the practices/processes separate from the implementation strategies is a useful way to organize my thinking and approach.
Thanks!
Kim";s:5:"xhtml";s:333:"Dear David,<br />Thank you so much for this input - this input/feedback is extremely helpful and timely as I am thick in the interviews with positive deviants so thinking about and defining the practices/processes separate from the implementation strategies is a useful way to organize my thinking and approach.<br />Thanks!<br />Kim";s:6:"parent";s:32:"c1a7faf8cd119d94da8024da05ef2cc2";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"f3b2c3e81854ebf79f59d3b314a30320";}s:32:"babb5d4e42a6f0ada0172f8c9726857d";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"sshohat";s:4:"name";s:20:"Sivan Spitzer-Shohat";s:4:"mail";s:30:"sivan.spitzer-shohat@biu.ac.il";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537537826;}s:3:"raw";s:6471:"Spitzer-Shohat Assignment 3:

Assignment #3a - Models:
1.	Which model or combination of models is most applicable to your proposed study and why?

Our study aims to assess the effect equity translation training has on pilot teams’ ability to design and facilitate the implementation of an equity lens in departmental work processes. The overarching theoretical framework guiding our study is Realistic Evaluation (RE), which aims to understand not only what works, but what works for whom in what circumstances and why. Similar to the CFIR model, it discerns between context or organizational outer and/or inner setting, mechanisms or individuals’ characteristics, and outcomes or the effect of the intervention. The uniqueness of the RE framework is that it does not assume one possible relationship between context, mechanisms and outcome, but rather that programs ‘work’ in different ways and can trigger different change mechanisms for different participants. As such, it assists in identifying different configurations of program theories driving context-mechanisms-outcome configurations to understand why an intervention worked. 
As we aim to assess the effectiveness of training on facilitating implementation of equity across different organizational departments in an organization-wide initiative, it is imperative to understand how the departmental inner setting conditions teams’ logic, reasoning and choices in designing and implementing equity, and how, in turn, this creates a causative mechanism of change in the department employees’ response. Implementing organization-wide change requires changing the culture and work processes across the entire organization. Therefore, variability in implementation strategies and outcomes has implications for understanding  the training’s practicality, as well as generalizability to other organizations.

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

Realistic evaluation draws causative explanations through the generation of program theories, assessing context-mechanisms-outcomes configurations (CMOc.) The CMOc’s generated in the study will assist in informing: 
1.	How the organizational departmental setting and pilot teams’ characteristics affect the selection of different implementation strategies.  Some contextual factors may enable particular mechanisms to be triggered.  Other aspects of the context may prevent particular mechanisms from being triggered.  This triggers different interaction between context and mechanism, and impacts outcomes. Contextual departmental factors including organizational culture, perceived need for change, competing pressures, and social ties among departmental personnel may create a difference in the number, type and breadth of equity focused interventions. Additionally, team factors such as the social ties within pilot team and between teams and department personnel,  as well as pilot teams’ perceived effectiveness, i.e. their knowledge, skills, goal agreement and perceived ability to devise and lead equity focused interventions, may also affect the number and type of strategies chosen. 
2.	What key aspects of training need to remain the same across the organization, i.e. fidelity, versus those that require adaptability and contextualization to the unique departmental setting.


Assignment #3b - Measures & Evaluations:
What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

The study outcomes measures are twofold and relate to both the pilot team members and selected organizational departments: 
(a)	Team level outcomes will assess to what degree did translation training assist pilot implementation team members in mitigating organizational sensemaking. Interviews conducted with implementation team members, will assess the extent training provided tools and skills facilitating a process of organizational sense giving in which team members are clear how to implement equity and translate it from a value laden concept to everyday work processes. 
(b)	Department level outcomes will be assessed using the RE-AIM framework. We will conduct a survey to identify reach among departmental personnel, adoption, and use of the strategies the implementation teams devised through the Dong et al. (2008), innovation-fit tool as well as interviews with department personnel.

What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

Process measures of the study will document pilot teams' work through: 
1.	Content analysis of teams 12 monthly reports based on their departmental equity implementation plan. The organization recently introduced an 'implementation plan' tool that documents:
(a) Identification, acquisition and use of relevant stratified data to assess where an equity lens is required.
(b)	Major strategies devised and timeline for implementation
(c)	Identification and involvement of departmental stakeholders
(d)	Organizational resources required and their acquisition
2.	Observation of the quarterly governance committee meetings in which teams report back on progress and share facilitators and barriers encountered.
3.	Interviews with individual team members to identify levers and barriers during implementation.

Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

Given competing priorities as the well as pilot teams members’ involvement which may require additional time beyond the defined organizational time commitment, an unintended consequence may be burnout. Through an online survey, we will assess burnout among pilot teams’ members using the Maslach Burnout Inventory (MBI, health services version), a standard tool that measures aspects of workplace stress. 

While we did not think of assessing co-benefits, it would be interesting to understand to what extent pilot team members perceive the training to be valuable and transferable for mitigating sense making in other situations. Often, in organizational change initiatives, executive management outlines the general goal entrusting mid-managers to translate and implement change processes across the organization. 
";s:5:"xhtml";s:6673:"Spitzer-Shohat Assignment 3:<br /><br />Assignment #3a - Models:<br />1.	Which model or combination of models is most applicable to your proposed study and why?<br /><br />Our study aims to assess the effect equity translation training has on pilot teams’ ability to design and facilitate the implementation of an equity lens in departmental work processes. The overarching theoretical framework guiding our study is Realistic Evaluation (RE), which aims to understand not only what works, but what works for whom in what circumstances and why. Similar to the CFIR model, it discerns between context or organizational outer and/or inner setting, mechanisms or individuals’ characteristics, and outcomes or the effect of the intervention. The uniqueness of the RE framework is that it does not assume one possible relationship between context, mechanisms and outcome, but rather that programs ‘work’ in different ways and can trigger different change mechanisms for different participants. As such, it assists in identifying different configurations of program theories driving context-mechanisms-outcome configurations to understand why an intervention worked. <br />As we aim to assess the effectiveness of training on facilitating implementation of equity across different organizational departments in an organization-wide initiative, it is imperative to understand how the departmental inner setting conditions teams’ logic, reasoning and choices in designing and implementing equity, and how, in turn, this creates a causative mechanism of change in the department employees’ response. Implementing organization-wide change requires changing the culture and work processes across the entire organization. Therefore, variability in implementation strategies and outcomes has implications for understanding  the training’s practicality, as well as generalizability to other organizations.<br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />Realistic evaluation draws causative explanations through the generation of program theories, assessing context-mechanisms-outcomes configurations (CMOc.) The CMOc’s generated in the study will assist in informing: <br />1.	How the organizational departmental setting and pilot teams’ characteristics affect the selection of different implementation strategies.  Some contextual factors may enable particular mechanisms to be triggered.  Other aspects of the context may prevent particular mechanisms from being triggered.  This triggers different interaction between context and mechanism, and impacts outcomes. Contextual departmental factors including organizational culture, perceived need for change, competing pressures, and social ties among departmental personnel may create a difference in the number, type and breadth of equity focused interventions. Additionally, team factors such as the social ties within pilot team and between teams and department personnel,  as well as pilot teams’ perceived effectiveness, i.e. their knowledge, skills, goal agreement and perceived ability to devise and lead equity focused interventions, may also affect the number and type of strategies chosen. <br />2.	What key aspects of training need to remain the same across the organization, i.e. fidelity, versus those that require adaptability and contextualization to the unique departmental setting.<br /><br /><br />Assignment #3b - Measures &amp; Evaluations:<br />What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />The study outcomes measures are twofold and relate to both the pilot team members and selected organizational departments: <br />(a)	Team level outcomes will assess to what degree did translation training assist pilot implementation team members in mitigating organizational sensemaking. Interviews conducted with implementation team members, will assess the extent training provided tools and skills facilitating a process of organizational sense giving in which team members are clear how to implement equity and translate it from a value laden concept to everyday work processes. <br />(b)	Department level outcomes will be assessed using the RE-AIM framework. We will conduct a survey to identify reach among departmental personnel, adoption, and use of the strategies the implementation teams devised through the Dong et al. (2008), innovation-fit tool as well as interviews with department personnel.<br /><br />What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />Process measures of the study will document pilot teams&#039; work through: <br />1.	Content analysis of teams 12 monthly reports based on their departmental equity implementation plan. The organization recently introduced an &#039;implementation plan&#039; tool that documents:<br />(a) Identification, acquisition and use of relevant stratified data to assess where an equity lens is required.<br />(b)	Major strategies devised and timeline for implementation<br />(c)	Identification and involvement of departmental stakeholders<br />(d)	Organizational resources required and their acquisition<br />2.	Observation of the quarterly governance committee meetings in which teams report back on progress and share facilitators and barriers encountered.<br />3.	Interviews with individual team members to identify levers and barriers during implementation.<br /><br />Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />Given competing priorities as the well as pilot teams members’ involvement which may require additional time beyond the defined organizational time commitment, an unintended consequence may be burnout. Through an online survey, we will assess burnout among pilot teams’ members using the Maslach Burnout Inventory (MBI, health services version), a standard tool that measures aspects of workplace stress. <br /><br />While we did not think of assessing co-benefits, it would be interesting to understand to what extent pilot team members perceive the training to be valuable and transferable for mitigating sense making in other situations. Often, in organizational change initiatives, executive management outlines the general goal entrusting mid-managers to translate and implement change processes across the organization.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"59048673f2f7047017e244d37a7bc747";}s:4:"show";b:1;s:3:"cid";s:32:"babb5d4e42a6f0ada0172f8c9726857d";}s:32:"4bdbf9702d98200a415969c1b79f6fdd";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"sshohat";s:4:"name";s:20:"Sivan Spitzer-Shohat";s:4:"mail";s:30:"sivan.spitzer-shohat@biu.ac.il";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537538045;}s:3:"raw";s:131:"Hi Lori,
Thank you for your feedback. Any additional readings you think may be of help in shaping my project would be great.

Sivan";s:5:"xhtml";s:146:"Hi Lori,<br />Thank you for your feedback. Any additional readings you think may be of help in shaping my project would be great.<br /><br />Sivan";s:6:"parent";s:32:"35a1d6e0581e70fe0bb094164e698c6f";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"4bdbf9702d98200a415969c1b79f6fdd";}s:32:"bd93cf399700e9d27f3b1273dfc1ae58";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"kfisher";s:4:"name";s:15:"Kimberly Fisher";s:4:"mail";s:33:"Kimberly.Fisher@umassmemorial.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537555870;}s:3:"raw";s:5155:"Assignment #3a - Models:
1. Which model or combination of models is most applicable to your proposed study and why?

The CFIR model is most applicable to my proposed study because it incorporates a full range of constructs that are expected to be determinant of the adoption and implementation of a care transitions intervention.  Furthermore, the CFIR can be used in the development stages of this intervention to prompt consideration of these important constructs.  Finally, Roger’s Diffusion of Innovation Theory is highly relevant to the implementation of a care transitions intervention; in particular, characteristics of key elements (innovation or intervention characteristics, characteristics of the adopters, communication channels, time, and social system) are expected to facilitate or impede the process of adoption of this intervention.  The inclusion of these key elements in the CFIR strengthens the relevance and usefulness of this framework to my project.  

In addition, my study is also based on the theory of Positive Deviance – the cornerstone of which is that solutions to a problem may be found within the community of interest by studying high-performers.  Thus, by using the findings from our ongoing study of high-performers of the Care Transition Measure to inform intervention development, we will develop an intervention that is based on what high performers in the real world are already doing.  This approach (in theory at least) is expected to result in an intervention that is more likely to be adopted by other members of this community (hospitals), as compared to an externally developed intervention.  

2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

Because my project includes intervention development, the constructs within the Intervention Characteristics domain are especially relevant to consider at this stage.  For example, creation of a tool that hospitals can use to identify gaps in their care transition process in comparison to a core set of evidence-based high quality care transition best practices, will allow a degree of adaptability to allow the intervention to meet local needs.  Similarly, incorporating constructs in the CFIR Process domain will be essential to maximizing the likelihood of successful implementation.  Finally, our preliminary data (and other published literature) suggest that the organizational context will be an important determinant or enabler of intervention implementation and effect.  The constructs within the Inner Setting domain will guide collection of information relating to the organization context which will be important in understanding the implementation and impact of the intervention.  
  
Assignment #3b - Measures & Evaluations:
1. What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

As currently proposed (and funded), my project does not evaluate outcomes, but focuses on defining successful care transition strategies.  However, I now recognize the importance and value in measuring acceptability, appropriateness and feasibility (and/or cost) of the strategies we identify in Aim 1 as this information may be predictive of adoption.  

In thinking about the next steps (intervention development and piloting), I envision that the primary outcome measure will be the hospital Care Transition Measure (or change following intervention), with hospital readmission rate and length of stay as secondary outcomes.  Dissemination and implementation outcomes to be measured which could serve to explain the impact (or lack thereof) on the clinical/system outcomes could include number of strategies adopted by each hospital, whether hospital care transition approach (post-intervention) met all key components of fundamental care transition processes.  

2.What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

We will be measuring which effective care transition processes are in use by hospitals so we can understand what hospitals are currently doing and how that deviates from what we define to be “best practices”.  These specific processes are still to be defined (aims 1 and 2).  We will be assessing whether these are put into place following the intervention.   Currently, these are planned to be measured via self-report.  Although this has limitations, there is currently no other means of collecting this information about hospitals.  

3.Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

As above, potential co-benefits could include reduced length of stay and/or reduction in hospital readmission rate.  As we formulate the intervention and pilot, it will be important to think about potential co-benefits or unintended consequences of the implementation of this, but do not currently have a plan for this.  
";s:5:"xhtml";s:5276:"Assignment #3a - Models:<br />1. Which model or combination of models is most applicable to your proposed study and why?<br /><br />The CFIR model is most applicable to my proposed study because it incorporates a full range of constructs that are expected to be determinant of the adoption and implementation of a care transitions intervention.  Furthermore, the CFIR can be used in the development stages of this intervention to prompt consideration of these important constructs.  Finally, Roger’s Diffusion of Innovation Theory is highly relevant to the implementation of a care transitions intervention; in particular, characteristics of key elements (innovation or intervention characteristics, characteristics of the adopters, communication channels, time, and social system) are expected to facilitate or impede the process of adoption of this intervention.  The inclusion of these key elements in the CFIR strengthens the relevance and usefulness of this framework to my project.  <br /><br />In addition, my study is also based on the theory of Positive Deviance – the cornerstone of which is that solutions to a problem may be found within the community of interest by studying high-performers.  Thus, by using the findings from our ongoing study of high-performers of the Care Transition Measure to inform intervention development, we will develop an intervention that is based on what high performers in the real world are already doing.  This approach (in theory at least) is expected to result in an intervention that is more likely to be adopted by other members of this community (hospitals), as compared to an externally developed intervention.  <br /><br />2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />Because my project includes intervention development, the constructs within the Intervention Characteristics domain are especially relevant to consider at this stage.  For example, creation of a tool that hospitals can use to identify gaps in their care transition process in comparison to a core set of evidence-based high quality care transition best practices, will allow a degree of adaptability to allow the intervention to meet local needs.  Similarly, incorporating constructs in the CFIR Process domain will be essential to maximizing the likelihood of successful implementation.  Finally, our preliminary data (and other published literature) suggest that the organizational context will be an important determinant or enabler of intervention implementation and effect.  The constructs within the Inner Setting domain will guide collection of information relating to the organization context which will be important in understanding the implementation and impact of the intervention.  <br />  <br />Assignment #3b - Measures &amp; Evaluations:<br />1. What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />As currently proposed (and funded), my project does not evaluate outcomes, but focuses on defining successful care transition strategies.  However, I now recognize the importance and value in measuring acceptability, appropriateness and feasibility (and/or cost) of the strategies we identify in Aim 1 as this information may be predictive of adoption.  <br /><br />In thinking about the next steps (intervention development and piloting), I envision that the primary outcome measure will be the hospital Care Transition Measure (or change following intervention), with hospital readmission rate and length of stay as secondary outcomes.  Dissemination and implementation outcomes to be measured which could serve to explain the impact (or lack thereof) on the clinical/system outcomes could include number of strategies adopted by each hospital, whether hospital care transition approach (post-intervention) met all key components of fundamental care transition processes.  <br /><br />2.What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />We will be measuring which effective care transition processes are in use by hospitals so we can understand what hospitals are currently doing and how that deviates from what we define to be “best practices”.  These specific processes are still to be defined (aims 1 and 2).  We will be assessing whether these are put into place following the intervention.   Currently, these are planned to be measured via self-report.  Although this has limitations, there is currently no other means of collecting this information about hospitals.  <br /><br />3.Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />As above, potential co-benefits could include reduced length of stay and/or reduction in hospital readmission rate.  As we formulate the intervention and pilot, it will be important to think about potential co-benefits or unintended consequences of the implementation of this, but do not currently have a plan for this.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"0cf1a4b6c684b86070915ec89a959967";}s:4:"show";b:1;s:3:"cid";s:32:"bd93cf399700e9d27f3b1273dfc1ae58";}s:32:"54873db1b5398bbfe3824ffc673a841f";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jpittman";s:4:"name";s:13:"James Pittman";s:4:"mail";s:17:"jpittman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537556034;}s:3:"raw";s:6214:"Pittman Assignment #3a - Models:

1.	Which model or combination of models is most applicable to your proposed study and why?
Testing strategy and evaluating process

I had previously selected the Practical, Robust, Implementation, and Sustainability Model (PRISM) for this project. Byron asked why I had chosen that model for this study in his response to my first assignment. If reflection, I think I chose it because it is a nice integration of models, specifically Diffusion of Innovations and RE-AIM. It was also because PRISM was suggested by a mentor. 

After reading the articles for this assignment, I wonder if it might make more sense select CFIR to help capture overarching information on the factors that influence implementation outcomes for eScreening. CFIR is widely used, particularly in the VA, which may be more familiar to reviewers on the committee. Several Intervention characteristics for example intervention source, evidence strength, adaptability, and cost may be particularly important to measure for this tool. Outer setting constructs like patient needs and policies/incentives also seem specifically relevant to VA. The same is true for inner setting constructs like culture, readiness for implementation, available resources, and access to information. Characteristics of individuals and process constructs will also be important to consider. In reviewing the domains, the eScreening playbook was designed to address aspects of all of these domains specifically.

To compliment the CFIR model, I could use Reach Effectiveness Adoption Implementation and Maintenance (RE-AIM), another well-known framework within the VA system, to guide the evaluation aspects of this study.  The RE-AIM.org website has many resources and there are already tools developed that could be used for this study. 

In order to help capture adaptations to implementation, I could use the VA Triple Aim QUERI modified Stirman adaptation framework. Using the D&I models in Health Research Practice website I also identified The Facilitating Adoption of Best (FAB) practices model, which could both help guide the nuanced development of the facilitation component of this study (feedback I received from the first call) and be used as a focused implementation measure.   

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 

Since CFIR is a determinant model, it could be used to formulate hypotheses related to AIM: 1) Evaluate the use of facilitation and RPIW strategies for implementing eScreening in multiple VHA sites in mental health outpatient teams, transition care management teams, primary care, and specialty programs. For example, I could hypothesize that CFIR domains at baseline will predict implementation outcomes and potentially effectiveness outcomes.  CFIR domains would also directly inform some of  the measurement tools for the study. The CFIR domains could potentially be used to evaluate the eScreening playbook by capturing pre-post change on some of the domains that the playbook targets. 

The REAIM model will inform measures for the study and guide how I define successful implementation. Though I know little about it now, I think the FAB practices model will likely shape how I develop the facilitation intervention and also how it is tracked and measured over time.  

Pittman- Assignment #3b - Measures & Evaluations:

1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

I plan to collect implementation outcomes, service outcomes, and possibly patient outcomes as part of this study. I would likely capture these outcomes using quantitative and qualitative measurement. CFIR and RE-AIM will inform quantitative measures of many of the implementation outcomes. I can also use objective eScreening system data to capture reach/penetration for each of the clinical programs. Implementation outcomes are important for this study because the VA is planning for national roll-out of an enterprise version of eScreening and these outcomes will help inform how best to implement eScreening nationally. Aims 2 and 3 of my proposal are service outcomes. Quantitative data from the electronic medical record could be used to capture efficiency and effectiveness, and timeliness data. Qualitative/quantitative data from staff/providers could also be used for process outcomes. Service outcomes provide support for the utility/effectiveness of eScreening and will help inform recommendations for best programs to utilize eScreening and policy to guide eScreening utilization.  It may not be feasible as part of this study to measure patient outcomes, but the eScreening software could be used to capture satisfaction data from patients who use eScreening. On the other hand, it may not be that useful because it would not be easily comparable to care as usual. 

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

I’m not sure I fully understand this question. RPIW and facilitation processes will be measures using the Stirman framework for adaptations, and the FAB model for facilitation. I have not identified the specific measures, but I can see potential for measuring this through either qualitative or quantitative means. This will be important to identify which parts of the implementation strategies are most useful. 
 
3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? 
Observations from study staff and qualitative interviews with staff at implementation sites could be used to identify co-benefits or unintended consequences of the implementation. Depending on the benefits/consequences identified, quantitative means/measures could also be employed. Capturing these outcomes could lead to adaptation of the implementation strategy or the eScreening tool itself and could inform policy recommendations or future directions for research. 
";s:5:"xhtml";s:6370:"Pittman Assignment #3a - Models:<br /><br />1.	Which model or combination of models is most applicable to your proposed study and why?<br />Testing strategy and evaluating process<br /><br />I had previously selected the Practical, Robust, Implementation, and Sustainability Model (PRISM) for this project. Byron asked why I had chosen that model for this study in his response to my first assignment. If reflection, I think I chose it because it is a nice integration of models, specifically Diffusion of Innovations and RE-AIM. It was also because PRISM was suggested by a mentor. <br /><br />After reading the articles for this assignment, I wonder if it might make more sense select CFIR to help capture overarching information on the factors that influence implementation outcomes for eScreening. CFIR is widely used, particularly in the VA, which may be more familiar to reviewers on the committee. Several Intervention characteristics for example intervention source, evidence strength, adaptability, and cost may be particularly important to measure for this tool. Outer setting constructs like patient needs and policies/incentives also seem specifically relevant to VA. The same is true for inner setting constructs like culture, readiness for implementation, available resources, and access to information. Characteristics of individuals and process constructs will also be important to consider. In reviewing the domains, the eScreening playbook was designed to address aspects of all of these domains specifically.<br /><br />To compliment the CFIR model, I could use Reach Effectiveness Adoption Implementation and Maintenance (RE-AIM), another well-known framework within the VA system, to guide the evaluation aspects of this study.  The RE-AIM.org website has many resources and there are already tools developed that could be used for this study. <br /><br />In order to help capture adaptations to implementation, I could use the VA Triple Aim QUERI modified Stirman adaptation framework. Using the D&amp;I models in Health Research Practice website I also identified The Facilitating Adoption of Best (FAB) practices model, which could both help guide the nuanced development of the facilitation component of this study (feedback I received from the first call) and be used as a focused implementation measure.   <br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br /><br />Since CFIR is a determinant model, it could be used to formulate hypotheses related to AIM: 1) Evaluate the use of facilitation and RPIW strategies for implementing eScreening in multiple VHA sites in mental health outpatient teams, transition care management teams, primary care, and specialty programs. For example, I could hypothesize that CFIR domains at baseline will predict implementation outcomes and potentially effectiveness outcomes.  CFIR domains would also directly inform some of  the measurement tools for the study. The CFIR domains could potentially be used to evaluate the eScreening playbook by capturing pre-post change on some of the domains that the playbook targets. <br /><br />The REAIM model will inform measures for the study and guide how I define successful implementation. Though I know little about it now, I think the FAB practices model will likely shape how I develop the facilitation intervention and also how it is tracked and measured over time.  <br /><br />Pittman- Assignment #3b - Measures &amp; Evaluations:<br /><br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />I plan to collect implementation outcomes, service outcomes, and possibly patient outcomes as part of this study. I would likely capture these outcomes using quantitative and qualitative measurement. CFIR and RE-AIM will inform quantitative measures of many of the implementation outcomes. I can also use objective eScreening system data to capture reach/penetration for each of the clinical programs. Implementation outcomes are important for this study because the VA is planning for national roll-out of an enterprise version of eScreening and these outcomes will help inform how best to implement eScreening nationally. Aims 2 and 3 of my proposal are service outcomes. Quantitative data from the electronic medical record could be used to capture efficiency and effectiveness, and timeliness data. Qualitative/quantitative data from staff/providers could also be used for process outcomes. Service outcomes provide support for the utility/effectiveness of eScreening and will help inform recommendations for best programs to utilize eScreening and policy to guide eScreening utilization.  It may not be feasible as part of this study to measure patient outcomes, but the eScreening software could be used to capture satisfaction data from patients who use eScreening. On the other hand, it may not be that useful because it would not be easily comparable to care as usual. <br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />I’m not sure I fully understand this question. RPIW and facilitation processes will be measures using the Stirman framework for adaptations, and the FAB model for facilitation. I have not identified the specific measures, but I can see potential for measuring this through either qualitative or quantitative means. This will be important to identify which parts of the implementation strategies are most useful. <br /> <br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? <br />Observations from study staff and qualitative interviews with staff at implementation sites could be used to identify co-benefits or unintended consequences of the implementation. Depending on the benefits/consequences identified, quantitative means/measures could also be employed. Capturing these outcomes could lead to adaptation of the implementation strategy or the eScreening tool itself and could inform policy recommendations or future directions for research.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"a5ab182500c48f3255ab0f3928db70a9";}s:4:"show";b:1;s:3:"cid";s:32:"54873db1b5398bbfe3824ffc673a841f";}s:32:"6a3bace45a251a5500ed6a359b214fd8";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"kboockvar";s:4:"name";s:16:"Kenneth Boockvar";s:4:"mail";s:25:"kenneth.boockvar@mssm.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537565650;}s:3:"raw";s:7087:"
Boockvar—Assignment #3a - Models:

1.	Which model or combination of models is most applicable to your proposed study and why?

We selected the Consolidated Framework for Implementation Research (CFIR) to provide an overall background understanding of health services implementation.  The CFIR helps visualize a broad landscape of implementation and helps ensures that we consider intervention characteristics, outer setting, inner setting, individual characteristics, and implementation process in studying our informatics interventions.  Positive attributes of the CFIR are:  1) it uses plainer language (less jargon) than some other models, which is important for a peer review process in which not all reviewers are implementation science experts, and 2) it appears to be a thoughtful amalgamation of models that came before.  We also selected the RE-AIM framework as an evaluative framework because 1) our study is a hybrid design and there is an effectiveness aim, and 2) RE-AIM has been applied to informatics implementation cases.

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

In addition to the above, both models (but in particular the CFIR) have helped guide the qualitative aim of this hybrid study in terms of who we will interview (managers, providers, patients), what questions to ask, our approach to coding open-ended responses, and discovery of themes.

Boockvar--Assignment #3b - Measures & Evaluations:

1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

The following are proposed outcome measures for an effectiveness aim:  

1) Hospital admission or readmission.  Data on VA and non-VA hospital use will be retrieved through the local health information exchange (HIE), VA electronic health record (CPRS), and data warehouses.  We chose hospital admission or readmission as an outcome because a) it is the most commonly reported outcome in studies of geriatric care transitions interventions and a common outcome reported in studies of HIE, enabling comparison of our study findings with others, b) it is frequent, c) it is important to veteran patients, providers, and policymakers, d) it can be ascertained objectively with low risk of bias, and e) it is a driver of high costs.  

2) Scheduled follow-up.  Timely follow-up will be defined as a follow-up visit with any VA provider within 30 days of non-VA hospital discharge or ED visit.  Timely phone call will be defined as VA primary care phone call within 7 days of non-VA hospital discharge or ED visit.  Unscheduled ED visits will be defined as any VA or non-VA ED visits that occur during the follow up period that do not result in hospital admission.  We will obtain VA encounter and phone call dates from CPRS, and non-VA encounter dates from the local HIE. 
 
3) High-risk medication discrepancies.  These are defined as the number of discrepancies in medications classified as high risk for hospitalized older adults, including opioid analgesics, insulin, non-steroidal anti-inflammatory drugs, digoxin, antipsychotics, sedatives/hypnotics, and anticoagulants. In prior studies, the number of high-risk discrepancies was a significant predictor of an adverse drug event (OR 1.71; 95% CI 1.28-2.28), indicating an additional 71% risk of an adverse drug event with each additional high-risk discrepancy. We will obtain a count of high-risk discrepancies based on medical record review and patient or caregiver interview 30 days after non-VA hospital discharge.

4) Care Transitions Measure.  This measure of condition self-knowledge and transitional care quality from the patient’s perspective is ascertained by patient or caregiver interview 30 days after non-VA hospital discharge. It has good construct validity; inter-item Spearman correlations with a gold standard instrument ranged from 0.388–0.594. We will use an adapted 3-item version which includes items such as: “After I left the hospital, I had all the information I needed to be able to take care of myself” with the response options strongly agree, agree, disagree, strongly disagree, and don’t know.  We chose to use the 3-item rather than a 15-item version as the shorter instrument demonstrates excellent correlation with the longer version but with lower respondent burden.

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

Electronic Health Record meaningful use.  We will ascertain the following process measures associated with CMS’ Meaningful Use program that are relevant to HIE notification and care transitions coordination:  1) percentage of patients in whom a medication reconciliation is documented within 30 days after non-VA hospital discharge and the reconciliation includes non-VA medication information (by medical record review; CMS goal > 50%); 2) percentage of patients in whom documentation of non-VA hospital encounter is present in CPRS within 30 days after non-VA hospital encounter (by medical record review; CMS goal > 50%); and 3) percentage of patients who received patient-specific education resources within 30 days after non-VA hospital discharge (by patient interview; CMS goal > 10%).

Process measures.  Our process measures provide information on processes influenced by HIE-notifications and by care transitions intervention components.  We will calculate frequency of HIE-access by providers through system audits, which is a measure of adoption or, at the organization level, penetration.  We will calculate frequency of veteran contacts (in person and telephone) by primary care team and care transitions coordinator, using CPRS record review and coordinator logs, which is a measure of fidelity.  We will calculate percentage of care transitions intervention components delivered that were indicated using coordinator logs, another measure of fidelity.    

3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

One of the common unintended adverse consequences of informatics interventions is the additional time it may require of providers, time which may affect performance on other tasks or affect job satisfaction.  For example, accessing health information from other electronic sources and responding to health notifications from these sources are added tasks, though in some cases may save time if they reduce the amount of effort required to obtain needed information or help coordinate care.  To assess this tradeoff, in our qualitative aim, we will observe providers interacting with case material and track time.  We will also ask providers to estimate the amount of time they spend seeking information with and without access to HIE.  We will also ask how the HIE impacts other work tasks, workflow, and/or communications.

";s:5:"xhtml";s:7257:"Boockvar—Assignment #3a - Models:<br /><br />1.	Which model or combination of models is most applicable to your proposed study and why?<br /><br />We selected the Consolidated Framework for Implementation Research (CFIR) to provide an overall background understanding of health services implementation.  The CFIR helps visualize a broad landscape of implementation and helps ensures that we consider intervention characteristics, outer setting, inner setting, individual characteristics, and implementation process in studying our informatics interventions.  Positive attributes of the CFIR are:  1) it uses plainer language (less jargon) than some other models, which is important for a peer review process in which not all reviewers are implementation science experts, and 2) it appears to be a thoughtful amalgamation of models that came before.  We also selected the RE-AIM framework as an evaluative framework because 1) our study is a hybrid design and there is an effectiveness aim, and 2) RE-AIM has been applied to informatics implementation cases.<br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />In addition to the above, both models (but in particular the CFIR) have helped guide the qualitative aim of this hybrid study in terms of who we will interview (managers, providers, patients), what questions to ask, our approach to coding open-ended responses, and discovery of themes.<br /><br />Boockvar--Assignment #3b - Measures &amp; Evaluations:<br /><br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />The following are proposed outcome measures for an effectiveness aim:  <br /><br />1) Hospital admission or readmission.  Data on VA and non-VA hospital use will be retrieved through the local health information exchange (HIE), VA electronic health record (CPRS), and data warehouses.  We chose hospital admission or readmission as an outcome because a) it is the most commonly reported outcome in studies of geriatric care transitions interventions and a common outcome reported in studies of HIE, enabling comparison of our study findings with others, b) it is frequent, c) it is important to veteran patients, providers, and policymakers, d) it can be ascertained objectively with low risk of bias, and e) it is a driver of high costs.  <br /><br />2) Scheduled follow-up.  Timely follow-up will be defined as a follow-up visit with any VA provider within 30 days of non-VA hospital discharge or ED visit.  Timely phone call will be defined as VA primary care phone call within 7 days of non-VA hospital discharge or ED visit.  Unscheduled ED visits will be defined as any VA or non-VA ED visits that occur during the follow up period that do not result in hospital admission.  We will obtain VA encounter and phone call dates from CPRS, and non-VA encounter dates from the local HIE. <br /> <br />3) High-risk medication discrepancies.  These are defined as the number of discrepancies in medications classified as high risk for hospitalized older adults, including opioid analgesics, insulin, non-steroidal anti-inflammatory drugs, digoxin, antipsychotics, sedatives/hypnotics, and anticoagulants. In prior studies, the number of high-risk discrepancies was a significant predictor of an adverse drug event (OR 1.71; 95% CI 1.28-2.28), indicating an additional 71% risk of an adverse drug event with each additional high-risk discrepancy. We will obtain a count of high-risk discrepancies based on medical record review and patient or caregiver interview 30 days after non-VA hospital discharge.<br /><br />4) Care Transitions Measure.  This measure of condition self-knowledge and transitional care quality from the patient’s perspective is ascertained by patient or caregiver interview 30 days after non-VA hospital discharge. It has good construct validity; inter-item Spearman correlations with a gold standard instrument ranged from 0.388–0.594. We will use an adapted 3-item version which includes items such as: “After I left the hospital, I had all the information I needed to be able to take care of myself” with the response options strongly agree, agree, disagree, strongly disagree, and don’t know.  We chose to use the 3-item rather than a 15-item version as the shorter instrument demonstrates excellent correlation with the longer version but with lower respondent burden.<br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />Electronic Health Record meaningful use.  We will ascertain the following process measures associated with CMS’ Meaningful Use program that are relevant to HIE notification and care transitions coordination:  1) percentage of patients in whom a medication reconciliation is documented within 30 days after non-VA hospital discharge and the reconciliation includes non-VA medication information (by medical record review; CMS goal &gt; 50%); 2) percentage of patients in whom documentation of non-VA hospital encounter is present in CPRS within 30 days after non-VA hospital encounter (by medical record review; CMS goal &gt; 50%); and 3) percentage of patients who received patient-specific education resources within 30 days after non-VA hospital discharge (by patient interview; CMS goal &gt; 10%).<br /><br />Process measures.  Our process measures provide information on processes influenced by HIE-notifications and by care transitions intervention components.  We will calculate frequency of HIE-access by providers through system audits, which is a measure of adoption or, at the organization level, penetration.  We will calculate frequency of veteran contacts (in person and telephone) by primary care team and care transitions coordinator, using CPRS record review and coordinator logs, which is a measure of fidelity.  We will calculate percentage of care transitions intervention components delivered that were indicated using coordinator logs, another measure of fidelity.    <br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />One of the common unintended adverse consequences of informatics interventions is the additional time it may require of providers, time which may affect performance on other tasks or affect job satisfaction.  For example, accessing health information from other electronic sources and responding to health notifications from these sources are added tasks, though in some cases may save time if they reduce the amount of effort required to obtain needed information or help coordinate care.  To assess this tradeoff, in our qualitative aim, we will observe providers interacting with case material and track time.  We will also ask providers to estimate the amount of time they spend seeking information with and without access to HIE.  We will also ask how the HIE impacts other work tasks, workflow, and/or communications.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"8e85fe30472c2c974d68d3e32e38f968";}s:4:"show";b:1;s:3:"cid";s:32:"6a3bace45a251a5500ed6a359b214fd8";}s:32:"59048673f2f7047017e244d37a7bc747";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dgoodrich";s:4:"name";s:14:"David Goodrich";s:4:"mail";s:22:"David.Goodrich2@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538089916;}s:3:"raw";s:5000:"Hi Sivan,

There are a lot of moving parts in your equity translational plan! Echoing prior comments by Byron and Lori, I see a lot of the theoretical elements that can be measured but the timing and means by which these constructs will assessed to inform your implementation efforts is not entirely clear to me. Notably, I am a little confused as to why your third project aim includes a separate evaluation "phase?" Implementation studies usually feature an aim that focuses on effectiveness outcomes, and at least one aim that focuses on process evaluation to identify and address the multi-level contextual factors relevant to implementing an intervention. 

It might be helpful to reframe your aims need to reflect methods to 1) rigorously pilot and assess factors related to the training intervention (Aim 1); 2) evaluate the intervention effectiveness over time in 3 departments (Aim 2) and 3) identify and develop strategies to mitigate implementation barriers of the health equity training in the 3 departments (Aim 3). As an implementation study, it would be nice to see some of Proctor's implementation outcomes mentioned here such as feasibility, acceptability, appropriateness, etc. These overlap with some of the individual level determinants (attitudes, beliefs, biases, decisions, etc.) Given all the theoretical constructs being assessed, I encourage you to operationalize which constructs are being measured or observed formatively at specific timepoints during the implementation with the 3 departments to inform subsequent spread to other departments, as well as which outcomes will be summative outcome or process measures.

Your plan is really fascinating because it's an organizational culture intervention that you hope to see manifest in health outcomes that reflect equity across your diverse patient population. It seems like you have identified a number of determinants that may influence the effectiveness of your equity training at the departmental unit level (culture, relative priority, etc.), implementation team level/and individual implementers, and the departmental personnel asked to integrate the training into their daily work processes. 

It would also be helpful to be very concrete in which determinants are being assessed and why based on the literature versus unspecified contextual constructs and mechanisms. Specifically, consider delineating: 1) what are the key health equity outcomes that you desire to change at an organizational level by the intervention?; 2) what are the process measures (formative vs. summative) for each aim for each level of stakeholder involved in the process (trainer, implementer, employee, patient); and how will these measures be assessed (interview, focus group, observation, survey). What is slightly confusing is that the RE framework seems to identify "CMOs" inductively. For an implementation project of the scale you are proposing, it seems like a review of the literature would identify salient constructs and mechanisms of change that both your training and implementation strategies would seek to target a priori (so that you can measure them with the appropriate methods). Your formative process measures would allow you to make tactical adjustments/corrections to unanticipated barriers or challenges and, learn of possible causal mechnanisms. Implementation science determinants frameworks like CFIR or the Theoretical Domains Framework (TDF) are useful for planning in advance to target organizational- or individual-level barriers/facilitators respectively, and to ensure that your implementation study plan rigorously captures this information in a systematic manner. However, you need to be very parsimonious in the selection of your methods. Be mindful that each assessment you ask employees or implementers to do is a burden. It helps to determine if there are organizational culture/employee assessments that are conducted by the health system that you could use that get at constructs relevant to your training. 

Implementation studies are meant to be pragmatic - using efficient, brief, and existing measures (see Russ Glasgow's work for more about pragmatic research). Also related to Russ and RE-AIM, it is unclear why you have not specified how effectiveness and maintenance/sustainability of your training will be assessed using this framework. If you are not going to assess all elements of the framework, you could simply rephrase reach as "penetration" of the training in each department. There are synonyms for adoption and implementation as well. I realize that these comments represent a lot of things to consider but the goal of my suggestions is to encourage you to drop the theoretical jargon and develop a description of your plan that can be picked up by your non-academic colleagues and readily understood. Happy to follow-up to clarify any comments or to provide specific suggestions that can help clarify the operationalization of constructs and assessments.

Best regards,
David


";s:5:"xhtml";s:5107:"Hi Sivan,<br /><br />There are a lot of moving parts in your equity translational plan! Echoing prior comments by Byron and Lori, I see a lot of the theoretical elements that can be measured but the timing and means by which these constructs will assessed to inform your implementation efforts is not entirely clear to me. Notably, I am a little confused as to why your third project aim includes a separate evaluation &quot;phase?&quot; Implementation studies usually feature an aim that focuses on effectiveness outcomes, and at least one aim that focuses on process evaluation to identify and address the multi-level contextual factors relevant to implementing an intervention. <br /><br />It might be helpful to reframe your aims need to reflect methods to 1) rigorously pilot and assess factors related to the training intervention (Aim 1); 2) evaluate the intervention effectiveness over time in 3 departments (Aim 2) and 3) identify and develop strategies to mitigate implementation barriers of the health equity training in the 3 departments (Aim 3). As an implementation study, it would be nice to see some of Proctor&#039;s implementation outcomes mentioned here such as feasibility, acceptability, appropriateness, etc. These overlap with some of the individual level determinants (attitudes, beliefs, biases, decisions, etc.) Given all the theoretical constructs being assessed, I encourage you to operationalize which constructs are being measured or observed formatively at specific timepoints during the implementation with the 3 departments to inform subsequent spread to other departments, as well as which outcomes will be summative outcome or process measures.<br /><br />Your plan is really fascinating because it&#039;s an organizational culture intervention that you hope to see manifest in health outcomes that reflect equity across your diverse patient population. It seems like you have identified a number of determinants that may influence the effectiveness of your equity training at the departmental unit level (culture, relative priority, etc.), implementation team level/and individual implementers, and the departmental personnel asked to integrate the training into their daily work processes. <br /><br />It would also be helpful to be very concrete in which determinants are being assessed and why based on the literature versus unspecified contextual constructs and mechanisms. Specifically, consider delineating: 1) what are the key health equity outcomes that you desire to change at an organizational level by the intervention?; 2) what are the process measures (formative vs. summative) for each aim for each level of stakeholder involved in the process (trainer, implementer, employee, patient); and how will these measures be assessed (interview, focus group, observation, survey). What is slightly confusing is that the RE framework seems to identify &quot;CMOs&quot; inductively. For an implementation project of the scale you are proposing, it seems like a review of the literature would identify salient constructs and mechanisms of change that both your training and implementation strategies would seek to target a priori (so that you can measure them with the appropriate methods). Your formative process measures would allow you to make tactical adjustments/corrections to unanticipated barriers or challenges and, learn of possible causal mechnanisms. Implementation science determinants frameworks like CFIR or the Theoretical Domains Framework (TDF) are useful for planning in advance to target organizational- or individual-level barriers/facilitators respectively, and to ensure that your implementation study plan rigorously captures this information in a systematic manner. However, you need to be very parsimonious in the selection of your methods. Be mindful that each assessment you ask employees or implementers to do is a burden. It helps to determine if there are organizational culture/employee assessments that are conducted by the health system that you could use that get at constructs relevant to your training. <br /><br />Implementation studies are meant to be pragmatic - using efficient, brief, and existing measures (see Russ Glasgow&#039;s work for more about pragmatic research). Also related to Russ and RE-AIM, it is unclear why you have not specified how effectiveness and maintenance/sustainability of your training will be assessed using this framework. If you are not going to assess all elements of the framework, you could simply rephrase reach as &quot;penetration&quot; of the training in each department. There are synonyms for adoption and implementation as well. I realize that these comments represent a lot of things to consider but the goal of my suggestions is to encourage you to drop the theoretical jargon and develop a description of your plan that can be picked up by your non-academic colleagues and readily understood. Happy to follow-up to clarify any comments or to provide specific suggestions that can help clarify the operationalization of constructs and assessments.<br /><br />Best regards,<br />David";s:6:"parent";s:32:"babb5d4e42a6f0ada0172f8c9726857d";s:7:"replies";a:1:{i:0;s:32:"dfe28a72955444831c66becb770ccea0";}s:4:"show";b:1;s:3:"cid";s:32:"59048673f2f7047017e244d37a7bc747";}s:32:"7d04937316f5c7bc3fc7c4abf497c6f0";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lducharme";s:4:"name";s:13:"Lori Ducharme";s:4:"mail";s:21:"lori.ducharme@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538160960;}s:3:"raw";s:2423:"Hi Barbara – 
Framework overload indeed!  Let’s see if we can’t simplify this a bit.

First, I think you’ve got 2 different things going on here – one is concern about user feedback in the design of the CDS itself, and the other is concern about the implementation of that CDS.  Most of your response focuses on the former; I’d suggest revisiting it to see if you can shift the emphasis to the latter.  I think your narrative could be greatly simplified if for the purposes of TIDIRH you focused your models/measures around the implementation component.  I am assuming that once you’re ready to roll out an implementation plan, the CDS “is what it is” – its *use* may vary across providers or sites, but the guidelines and recommendations within it, and how one (mechanically) uses it, will be the same.
  
Provider attitudes toward the CDS (and toward EHR systems generally) as it affects the success of their implementation would seem to be covered adequately within CFIR already (Intervention Characteristics – Compatibility, Design Quality, Relative Advantage; Individual Level – Beliefs about the Intervention). CFIR has a number of advantages, including that the Rogers DOI components are baked in.  Another advantage is that the implementation research field has jumped on board and developed some excellent resources for utilizing the CFIR. If you haven’t already, check out https://cfirguide.org, within which you can click into each of the constructs to find links to qualitative and quantitative measures appropriate for each one.

In terms of proposed process measures, you articulated a number of clinical processes.  Are there *implementation* processes you would also measure – i.e., measures of planning, engaging, executing the implementation of the CDS within or across sites?  (This is the most difficult leap for most researchers to make – shifting the priority outcomes from clinical processes/outcomes to implementation processes/outcomes.)  Look at the CFIRGuide site for additional ideas there. Perhaps it’s helpful to think in terms of “how will we know when we’re finished implementing the CDS at this site -- what can we measure that will give an indication of progress?”  Saldana’s Stages of Implementation Completion may also be a helpful thought exercise for you in identifying measurable pieces of the implementation process.

I hope this helps!  Lori
";s:5:"xhtml";s:2467:"Hi Barbara – <br />Framework overload indeed!  Let’s see if we can’t simplify this a bit.<br /><br />First, I think you’ve got 2 different things going on here – one is concern about user feedback in the design of the CDS itself, and the other is concern about the implementation of that CDS.  Most of your response focuses on the former; I’d suggest revisiting it to see if you can shift the emphasis to the latter.  I think your narrative could be greatly simplified if for the purposes of TIDIRH you focused your models/measures around the implementation component.  I am assuming that once you’re ready to roll out an implementation plan, the CDS “is what it is” – its *use* may vary across providers or sites, but the guidelines and recommendations within it, and how one (mechanically) uses it, will be the same.<br />  <br />Provider attitudes toward the CDS (and toward EHR systems generally) as it affects the success of their implementation would seem to be covered adequately within CFIR already (Intervention Characteristics – Compatibility, Design Quality, Relative Advantage; Individual Level – Beliefs about the Intervention). CFIR has a number of advantages, including that the Rogers DOI components are baked in.  Another advantage is that the implementation research field has jumped on board and developed some excellent resources for utilizing the CFIR. If you haven’t already, check out https://cfirguide.org, within which you can click into each of the constructs to find links to qualitative and quantitative measures appropriate for each one.<br /><br />In terms of proposed process measures, you articulated a number of clinical processes.  Are there *implementation* processes you would also measure – i.e., measures of planning, engaging, executing the implementation of the CDS within or across sites?  (This is the most difficult leap for most researchers to make – shifting the priority outcomes from clinical processes/outcomes to implementation processes/outcomes.)  Look at the CFIRGuide site for additional ideas there. Perhaps it’s helpful to think in terms of “how will we know when we’re finished implementing the CDS at this site -- what can we measure that will give an indication of progress?”  Saldana’s Stages of Implementation Completion may also be a helpful thought exercise for you in identifying measurable pieces of the implementation process.<br /><br />I hope this helps!  Lori";s:6:"parent";s:32:"71bffaefb8a3d7082667c56ca2301ae9";s:7:"replies";a:1:{i:0;s:32:"744f3292a911d59174bbe6e95efabb01";}s:4:"show";b:1;s:3:"cid";s:32:"7d04937316f5c7bc3fc7c4abf497c6f0";}s:32:"8e85fe30472c2c974d68d3e32e38f968";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lducharme";s:4:"name";s:13:"Lori Ducharme";s:4:"mail";s:21:"lori.ducharme@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538164788;}s:3:"raw";s:1244:"Hi Kenneth.  Some food for thought:  You describe your project as a hybrid design, but I only see measures for an effectiveness aim.  I’m also a bit confused from reading your prior posts as to whether you will be deploying an active implementation intervention/strategy, or whether you are piggy-backing on implementation work that is already underway elsewhere in the VA.  In either case, for the sake of TIDIRH, I’d encourage you to think about how you can explicitly measure *implementation* process and outcomes, as distinct from (and in addition to) clinical processes/outcomes.  For starters, how would you operationalize the other elements of RE-AIM beyond the effectiveness measures?  If the implementation roll-out is happening outside of your control, how could you measure or characterize where each site is in the implementation process (early, midstream, late, fully implemented) and relate that to any changes observed in clinical processes/outcomes?

Finally, if you haven’t already seen it, you might want to check out the handy website, https://cfirguide.org.  They offer a host of qualitative and quantitative measures within each of the CFIR domains.  (I offer this only as a source of possible inspiration.) – Lori
";s:5:"xhtml";s:1253:"Hi Kenneth.  Some food for thought:  You describe your project as a hybrid design, but I only see measures for an effectiveness aim.  I’m also a bit confused from reading your prior posts as to whether you will be deploying an active implementation intervention/strategy, or whether you are piggy-backing on implementation work that is already underway elsewhere in the VA.  In either case, for the sake of TIDIRH, I’d encourage you to think about how you can explicitly measure *implementation* process and outcomes, as distinct from (and in addition to) clinical processes/outcomes.  For starters, how would you operationalize the other elements of RE-AIM beyond the effectiveness measures?  If the implementation roll-out is happening outside of your control, how could you measure or characterize where each site is in the implementation process (early, midstream, late, fully implemented) and relate that to any changes observed in clinical processes/outcomes?<br /><br />Finally, if you haven’t already seen it, you might want to check out the handy website, https://cfirguide.org.  They offer a host of qualitative and quantitative measures within each of the CFIR domains.  (I offer this only as a source of possible inspiration.) – Lori";s:6:"parent";s:32:"6a3bace45a251a5500ed6a359b214fd8";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"8e85fe30472c2c974d68d3e32e38f968";}s:32:"a5ab182500c48f3255ab0f3928db70a9";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dgoodrich";s:4:"name";s:14:"David Goodrich";s:4:"mail";s:22:"David.Goodrich2@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538166678;}s:3:"raw";s:5653:"Hi James, it sounds like you have been digesting a lot of the feedback from prior posts. I like your thought processes regarding the trade-offs between different models - RE-AIM, CFIR, FAB, and PRISM among others. Choosing the right assessment methods is definitely an art for implementation studies, particularly for ehealth studies. Your  proposal seems to be on the right track.

I would encourage you to look up Russ Glasgow's two reviews on applying RE-AIM to ehealth interventions as well as David C. Mohr's work on e-health/mhealth interventions for mental health populations if you haven't read these papers already. Given that you have proposed a Hybrid II study design, I am guessing that you have already read Geoff Curran's work? What I am not seeing in your Aims, models and measures, is a roughly equal assessment of contextual implementation factors vs. effectiveness outcomes. 

There are a lot of things going on with your SMART trial. You have the escreening intervention which results in initial screenings (I'm assuming for multiple mental health issues?) and ongoing panel management assessments. I am pleased you are thinking of applying Shannon Stirman's framework to track how different sites adapt the intervention to their facility/service. The Triple Aim paper (Rabin et al, 2018) does provide excellent examples of applying the model with RE-AIM in VA contexts. One question is how will you determine/track these adaptations? For example, if a site is not using the tool to properly do follow-up to a patient with elevated suicide risk, this might be a fidelity problem. Likewise, if a particularly effective site has created care teams to help patients complete the tablets prior to appointments, how will you learn about this positive adaptation (interview, survey, focus group?) These are workflow or local processes involving the tool that influence its effective use in care (3b.2). Looking into these determinants of care does not come through in your aims (Aim 1 seems very focused on comparing implementation strategies and not assessing contextual factors affecting the screening tool use). You asked in 3b what was meant by implementation processes and this is a good example.) Also, when assessing for unintended consequences - what would happen if the tablets at particular site developed a glitch and could not be used for period of time? Would screening stop? Would staff become reliant on the tablets and less capable with manual paper and pencil assessments? That might be an unforeseen consequence...

There are always a myriad of possible contextual factors influencing adoption, use, and sustained integration of an intervention like the screening tool at sites and services. I like the parsimony of the FAB model and it seems like you can selectively integrate  specific constructs from CFIR, Theoretical Domains Framework, and other determinants that you might identify from the implementation science, ehealth, and mental health literature to the FAB. It would be nice to know how you will systematically assess the influence of these determinants over the course of the study through surveys, interviews, or focus groups in relation to use of your implementation sites. Further, how will you chose a representative sample of sites to gather contextual data? You have acknowledged that patient level user data may be hard to collect but what about staff/provider/clinic leaders' perspectives? As we review designs next week, you might think about this. Frequently, in hybrid designs, qualitative or mixed methods process evaluations of contextual factors is a third Aim.

Your study also involves implementation strategies (Aim 1), particularly complex, multifaceted strategies like RPIW, internal facilitation, and external facilitation. Right now, I am not sure how these will be sequenced over time but you might specify what your implementation outcomes are (I could not tell find them) AND, identify how you will track discrete implementation strategies used in each of these broad strategies. I would encourage you to look at Byron Powell's work with the ERIC project on specifying discrete strategies as well as Jennifer Leeman's work which also calls for operationalizing strategies including WHO does the implementation strategy and at which PHASE of the implementation adoption. For example, the implementation strategies to get your tool ready for deployment in clinics may be different than strategies used to encourage ongoing measurement-based care in the face of constant staff turnover. 

Finally, I would suspect operational partners would want to know a little more than just service outcomes as a measure of intervention impact. For the two levels of screening, can you determine referrals to specialty care services for a med consults, suicide risk mitigation, psychotherapy, SUD tx, etc. which are patient level outcomes? Likewise, I'm wondering if these "effectiveness" outcomes might part of a two part aim (e.g., Aim 2a, 2b)? Fortunately, it seems like these data can be readily pulled from VA databases which can help you focus your resources on how to measure factors affecting implementation strategy effectiveness and site level contextual factors influencing effective use of the tool. 

This is a very timely project and I'm excited to see how things turn out. I'm not sure how many of your assessment strategies are locked into stone because the tools are ready for implementation at the next wave of facilities but I hope that this feedback provided some new ideas. Let me know if you have further questions and please fill out the feedback survey.

Warm regards,
 David
";s:5:"xhtml";s:5797:"Hi James, it sounds like you have been digesting a lot of the feedback from prior posts. I like your thought processes regarding the trade-offs between different models - RE-AIM, CFIR, FAB, and PRISM among others. Choosing the right assessment methods is definitely an art for implementation studies, particularly for ehealth studies. Your  proposal seems to be on the right track.<br /><br />I would encourage you to look up Russ Glasgow&#039;s two reviews on applying RE-AIM to ehealth interventions as well as David C. Mohr&#039;s work on e-health/mhealth interventions for mental health populations if you haven&#039;t read these papers already. Given that you have proposed a Hybrid II study design, I am guessing that you have already read Geoff Curran&#039;s work? What I am not seeing in your Aims, models and measures, is a roughly equal assessment of contextual implementation factors vs. effectiveness outcomes. <br /><br />There are a lot of things going on with your SMART trial. You have the escreening intervention which results in initial screenings (I&#039;m assuming for multiple mental health issues?) and ongoing panel management assessments. I am pleased you are thinking of applying Shannon Stirman&#039;s framework to track how different sites adapt the intervention to their facility/service. The Triple Aim paper (Rabin et al, 2018) does provide excellent examples of applying the model with RE-AIM in VA contexts. One question is how will you determine/track these adaptations? For example, if a site is not using the tool to properly do follow-up to a patient with elevated suicide risk, this might be a fidelity problem. Likewise, if a particularly effective site has created care teams to help patients complete the tablets prior to appointments, how will you learn about this positive adaptation (interview, survey, focus group?) These are workflow or local processes involving the tool that influence its effective use in care (3b.2). Looking into these determinants of care does not come through in your aims (Aim 1 seems very focused on comparing implementation strategies and not assessing contextual factors affecting the screening tool use). You asked in 3b what was meant by implementation processes and this is a good example.) Also, when assessing for unintended consequences - what would happen if the tablets at particular site developed a glitch and could not be used for period of time? Would screening stop? Would staff become reliant on the tablets and less capable with manual paper and pencil assessments? That might be an unforeseen consequence...<br /><br />There are always a myriad of possible contextual factors influencing adoption, use, and sustained integration of an intervention like the screening tool at sites and services. I like the parsimony of the FAB model and it seems like you can selectively integrate  specific constructs from CFIR, Theoretical Domains Framework, and other determinants that you might identify from the implementation science, ehealth, and mental health literature to the FAB. It would be nice to know how you will systematically assess the influence of these determinants over the course of the study through surveys, interviews, or focus groups in relation to use of your implementation sites. Further, how will you chose a representative sample of sites to gather contextual data? You have acknowledged that patient level user data may be hard to collect but what about staff/provider/clinic leaders&#039; perspectives? As we review designs next week, you might think about this. Frequently, in hybrid designs, qualitative or mixed methods process evaluations of contextual factors is a third Aim.<br /><br />Your study also involves implementation strategies (Aim 1), particularly complex, multifaceted strategies like RPIW, internal facilitation, and external facilitation. Right now, I am not sure how these will be sequenced over time but you might specify what your implementation outcomes are (I could not tell find them) AND, identify how you will track discrete implementation strategies used in each of these broad strategies. I would encourage you to look at Byron Powell&#039;s work with the ERIC project on specifying discrete strategies as well as Jennifer Leeman&#039;s work which also calls for operationalizing strategies including WHO does the implementation strategy and at which PHASE of the implementation adoption. For example, the implementation strategies to get your tool ready for deployment in clinics may be different than strategies used to encourage ongoing measurement-based care in the face of constant staff turnover. <br /><br />Finally, I would suspect operational partners would want to know a little more than just service outcomes as a measure of intervention impact. For the two levels of screening, can you determine referrals to specialty care services for a med consults, suicide risk mitigation, psychotherapy, SUD tx, etc. which are patient level outcomes? Likewise, I&#039;m wondering if these &quot;effectiveness&quot; outcomes might part of a two part aim (e.g., Aim 2a, 2b)? Fortunately, it seems like these data can be readily pulled from VA databases which can help you focus your resources on how to measure factors affecting implementation strategy effectiveness and site level contextual factors influencing effective use of the tool. <br /><br />This is a very timely project and I&#039;m excited to see how things turn out. I&#039;m not sure how many of your assessment strategies are locked into stone because the tools are ready for implementation at the next wave of facilities but I hope that this feedback provided some new ideas. Let me know if you have further questions and please fill out the feedback survey.<br /><br />Warm regards,<br /> David";s:6:"parent";s:32:"54873db1b5398bbfe3824ffc673a841f";s:7:"replies";a:1:{i:0;s:32:"032f174d70ffa078f1e8e05aaf64c3ed";}s:4:"show";b:1;s:3:"cid";s:32:"a5ab182500c48f3255ab0f3928db70a9";}s:32:"0cf1a4b6c684b86070915ec89a959967";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"bjpowell";s:4:"name";s:12:"Byron Powell";s:4:"mail";s:16:"bjpowell@unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538274136;}s:3:"raw";s:2851:"Hi Kimberly,

I really like your use of the positive deviance approach! I think when we had our first call I hadn't read your assignment/title of your project yet, and as you described your project, I suggested that you might consider using a positive deviance approach. Forgive me for not realizing you were formally using this approach! I have always appreciated the idea behind the approach, and will look forward to learning more from your project and your previous use of PD at TIDIRH.

One of the things that I was curious about as I read your previous assignments was how you use your qualitative work to glean enough detail about the specific interventions that are used to improve care transitions. In some of the work that I have been involved with, it has been really challenging to collect data about interventions and implementation strategies that is detailed enough to allow for replication in research and practice. If this is a challenge for you, I thought that some of the reporting guidelines for interventions and implementation strategies (e.g., https://www.ncbi.nlm.nih.gov/pubmed/24609605, https://www.ncbi.nlm.nih.gov/pubmed/24289295) might be helpful in guiding interviews and/or the analysis of data. I'd love to learn more about how you've approached this.

I know you are primarily focused on identifying effective care transition intervention components, but I wonder if you can also begin identifying some of the promising implementation strategies as well in your qual work (i.e., thinking about both the "what" and the "how"). Doing so could give you a bit of a head start once you move to formalize and implement a care transitions intervention. As your previous assignments are written, I was at times confused about whether you were differentiating the two types of interventions. 

The CFIR definitely seems appropriate in terms of identifying potential determinants, though I didn't quite understand how you were using it and how it linked to some of the things you propose to measure. It would be nice to see some more alignment there. I'm also not sure how helpful the CFIR process domain is, as I've always found it to be the least well developed of the five domains. You might consider other process models (Nilsen 2015 could be a good reference to some) that are operationalized a bit more.

If you do decide to formally assess acceptability, appropriateness, and feasibility, you could consider doing so using a mixed methods design. We've recently published some measures of these constructs (led by Bryan Weiner), which are published here: https://www.ncbi.nlm.nih.gov/pubmed/28851459 

Looking forward to learning more about your work, and the positive deviance approach you're taking. If you have any key readings (from your own work or others) that you would suggest, I'd love to see them! 

Best,
Byron";s:5:"xhtml";s:2991:"Hi Kimberly,<br /><br />I really like your use of the positive deviance approach! I think when we had our first call I hadn&#039;t read your assignment/title of your project yet, and as you described your project, I suggested that you might consider using a positive deviance approach. Forgive me for not realizing you were formally using this approach! I have always appreciated the idea behind the approach, and will look forward to learning more from your project and your previous use of PD at TIDIRH.<br /><br />One of the things that I was curious about as I read your previous assignments was how you use your qualitative work to glean enough detail about the specific interventions that are used to improve care transitions. In some of the work that I have been involved with, it has been really challenging to collect data about interventions and implementation strategies that is detailed enough to allow for replication in research and practice. If this is a challenge for you, I thought that some of the reporting guidelines for interventions and implementation strategies (e.g., https://www.ncbi.nlm.nih.gov/pubmed/24609605, https://www.ncbi.nlm.nih.gov/pubmed/24289295) might be helpful in guiding interviews and/or the analysis of data. I&#039;d love to learn more about how you&#039;ve approached this.<br /><br />I know you are primarily focused on identifying effective care transition intervention components, but I wonder if you can also begin identifying some of the promising implementation strategies as well in your qual work (i.e., thinking about both the &quot;what&quot; and the &quot;how&quot;). Doing so could give you a bit of a head start once you move to formalize and implement a care transitions intervention. As your previous assignments are written, I was at times confused about whether you were differentiating the two types of interventions. <br /><br />The CFIR definitely seems appropriate in terms of identifying potential determinants, though I didn&#039;t quite understand how you were using it and how it linked to some of the things you propose to measure. It would be nice to see some more alignment there. I&#039;m also not sure how helpful the CFIR process domain is, as I&#039;ve always found it to be the least well developed of the five domains. You might consider other process models (Nilsen 2015 could be a good reference to some) that are operationalized a bit more.<br /><br />If you do decide to formally assess acceptability, appropriateness, and feasibility, you could consider doing so using a mixed methods design. We&#039;ve recently published some measures of these constructs (led by Bryan Weiner), which are published here: https://www.ncbi.nlm.nih.gov/pubmed/28851459 <br /><br />Looking forward to learning more about your work, and the positive deviance approach you&#039;re taking. If you have any key readings (from your own work or others) that you would suggest, I&#039;d love to see them! <br /><br />Best,<br />Byron";s:6:"parent";s:32:"bd93cf399700e9d27f3b1273dfc1ae58";s:7:"replies";a:1:{i:0;s:32:"baf487302893c387b8edf0c9d42999ae";}s:4:"show";b:1;s:3:"cid";s:32:"0cf1a4b6c684b86070915ec89a959967";}s:32:"052e225b8c5f3340388a381118365887";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"bjpowell";s:4:"name";s:12:"Byron Powell";s:4:"mail";s:16:"bjpowell@unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538284881;}s:3:"raw";s:2218:"Hi Tara,

Great to read about your project in more depth.

First, I really like David's suggestions for focusing first on core components of the intervention. It will ultimately be much easier to identify implementation strategies when you know more about the intervention being implemented. I'm not sure how this fits with your project (as I can't remember if this is your funded study or another one that you're working on), but perhaps we can discuss this more in the coming weeks. 

Second, as Lori mentioned, I hope you are talking with Kimberly given your similar approach. Two of the things that I mentioned to her may also be relevant to you:

I have found it challenging to collect data about interventions and implementation strategies that is detailed enough to allow for replication in research and practice. If this is a challenge for you, I thought that some of the reporting guidelines for interventions and implementation strategies (e.g., https://www.ncbi.nlm.nih.gov/pubmed/24609605, https://www.ncbi.nlm.nih.gov/pubmed/24289295) might be helpful in guiding interviews and/or the analysis of data. They could also be useful as you specify the strategies identified and prioritized in your Delphi.

As you assess acceptability, appropriateness, and feasibility in your Delphi process, you could consider using measures of these constructs that we developed (Weiner et al. 2017), which are published here: https://www.ncbi.nlm.nih.gov/pubmed/28851459 

Finally, in terms of your Delphi method and the identification of strategies, I was curious how you would integrate existing evidence for implementation strategies, how you would balance and prioritize competing perspectives between your different stakeholder groups, how you would ensure that the strategies are well-aligned to known determinants, and how you would specify the mechanisms through which the strategies operate (i.e., how you would think about how the strategies work). You could consider emerging from the process with a logic model of the strategies (or the intervention if you start with David's suggestion) that clearly shows how the strategy/intervention might work. 

Looking forward to discussing this further!

Best,
Byron


";s:5:"xhtml";s:2325:"Hi Tara,<br /><br />Great to read about your project in more depth.<br /><br />First, I really like David&#039;s suggestions for focusing first on core components of the intervention. It will ultimately be much easier to identify implementation strategies when you know more about the intervention being implemented. I&#039;m not sure how this fits with your project (as I can&#039;t remember if this is your funded study or another one that you&#039;re working on), but perhaps we can discuss this more in the coming weeks. <br /><br />Second, as Lori mentioned, I hope you are talking with Kimberly given your similar approach. Two of the things that I mentioned to her may also be relevant to you:<br /><br />I have found it challenging to collect data about interventions and implementation strategies that is detailed enough to allow for replication in research and practice. If this is a challenge for you, I thought that some of the reporting guidelines for interventions and implementation strategies (e.g., https://www.ncbi.nlm.nih.gov/pubmed/24609605, https://www.ncbi.nlm.nih.gov/pubmed/24289295) might be helpful in guiding interviews and/or the analysis of data. They could also be useful as you specify the strategies identified and prioritized in your Delphi.<br /><br />As you assess acceptability, appropriateness, and feasibility in your Delphi process, you could consider using measures of these constructs that we developed (Weiner et al. 2017), which are published here: https://www.ncbi.nlm.nih.gov/pubmed/28851459 <br /><br />Finally, in terms of your Delphi method and the identification of strategies, I was curious how you would integrate existing evidence for implementation strategies, how you would balance and prioritize competing perspectives between your different stakeholder groups, how you would ensure that the strategies are well-aligned to known determinants, and how you would specify the mechanisms through which the strategies operate (i.e., how you would think about how the strategies work). You could consider emerging from the process with a logic model of the strategies (or the intervention if you start with David&#039;s suggestion) that clearly shows how the strategy/intervention might work. <br /><br />Looking forward to discussing this further!<br /><br />Best,<br />Byron";s:6:"parent";s:32:"87667ca23ad4eb1ab3b256cc10e3373e";s:7:"replies";a:1:{i:0;s:32:"3473554bcb1ea0c09ed1485054dfd9e2";}s:4:"show";b:1;s:3:"cid";s:32:"052e225b8c5f3340388a381118365887";}s:32:"744f3292a911d59174bbe6e95efabb01";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"bjones";s:4:"name";s:13:"Barbara Jones";s:4:"mail";s:26:"Barbara.Jones@hsc.utah.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538432666;}s:3:"raw";s:1698:"Thank you very much for your comments Lori!  That is great advice. You are right, it would be a lot simpler to view the CDS as "it is what it is" once implementation is initiated. However, at the risk of complicating things, (perhaps too much), the CDS that I envision is one that can change dramatically across settings, and we ultimately learn a lot from providers who  individualize the evidence represented in the guidelines to their patients, consciously deviating from guidelines when they feel they are inappropriate. I am hoping to measure the adaptations of the CDS itself across different settings throughout the user-centered design process, as a reflection of how mental models of pneumonia change in the minds of users across settings.  There are indeed core components of the CDS that reflect the generally-accepted model of pneumonia disease and management, but they really are just a place to start. Do you think that the flexibility of user-centered design is something that can be accommodated without adding too much complexity?
The Saldana stages of implementation were great to review and consider for tangible implementation outcomes. I think I would be measuring overall uptake and feasibility by tracking use; what I think I'm most likely to see by allowing a very flexible/adaptable CDS is a loss of fidelity, although that is exactly what is the most interesting to me. Sustainability and cost are exciting things to measure for CDS, as CDS-embedded evidence-based practice should theoretically be more sustainable and less costly than interventions that use other implementation approaches. 
Thank you for your stimulating comments! It is really helping guide this work.
";s:5:"xhtml";s:1722:"Thank you very much for your comments Lori!  That is great advice. You are right, it would be a lot simpler to view the CDS as &quot;it is what it is&quot; once implementation is initiated. However, at the risk of complicating things, (perhaps too much), the CDS that I envision is one that can change dramatically across settings, and we ultimately learn a lot from providers who  individualize the evidence represented in the guidelines to their patients, consciously deviating from guidelines when they feel they are inappropriate. I am hoping to measure the adaptations of the CDS itself across different settings throughout the user-centered design process, as a reflection of how mental models of pneumonia change in the minds of users across settings.  There are indeed core components of the CDS that reflect the generally-accepted model of pneumonia disease and management, but they really are just a place to start. Do you think that the flexibility of user-centered design is something that can be accommodated without adding too much complexity?<br />The Saldana stages of implementation were great to review and consider for tangible implementation outcomes. I think I would be measuring overall uptake and feasibility by tracking use; what I think I&#039;m most likely to see by allowing a very flexible/adaptable CDS is a loss of fidelity, although that is exactly what is the most interesting to me. Sustainability and cost are exciting things to measure for CDS, as CDS-embedded evidence-based practice should theoretically be more sustainable and less costly than interventions that use other implementation approaches. <br />Thank you for your stimulating comments! It is really helping guide this work.";s:6:"parent";s:32:"7d04937316f5c7bc3fc7c4abf497c6f0";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"744f3292a911d59174bbe6e95efabb01";}s:32:"3473554bcb1ea0c09ed1485054dfd9e2";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"tlagu";s:4:"name";s:9:"Tara Lagu";s:4:"mail";s:28:"Tara.Lagu@baystatehealth.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538577463;}s:3:"raw";s:409:"Excellent points:
1. I am going to talk to Kim Fisher offline to discuss 
2. I think for the Nov 9th proposal I need to incorporate yours and David's comments and re-think the Aims
3.Thanks for the ref on acceptability/feasibility/appropriateness!
4. Excellent questions re: Delphi. I hadn't thought about most of them but I think a logic model is a great idea-intervention probably given David's suggestion..";s:5:"xhtml";s:444:"Excellent points:<br />1. I am going to talk to Kim Fisher offline to discuss <br />2. I think for the Nov 9th proposal I need to incorporate yours and David&#039;s comments and re-think the Aims<br />3.Thanks for the ref on acceptability/feasibility/appropriateness!<br />4. Excellent questions re: Delphi. I hadn&#039;t thought about most of them but I think a logic model is a great idea-intervention probably given David&#039;s suggestion..";s:6:"parent";s:32:"052e225b8c5f3340388a381118365887";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"3473554bcb1ea0c09ed1485054dfd9e2";}s:32:"baf487302893c387b8edf0c9d42999ae";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"tlagu";s:4:"name";s:9:"Tara Lagu";s:4:"mail";s:28:"Tara.Lagu@baystatehealth.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538577528;}s:3:"raw";s:135:"Hi, Kim-Its Tara! It sounds like we are encountering a lot of the same issues with our proposals-we should chat, maybe offline! Thanks.";s:5:"xhtml";s:135:"Hi, Kim-Its Tara! It sounds like we are encountering a lot of the same issues with our proposals-we should chat, maybe offline! Thanks.";s:6:"parent";s:32:"0cf1a4b6c684b86070915ec89a959967";s:7:"replies";a:1:{i:0;s:32:"889a342729618929a74176471139091d";}s:4:"show";b:1;s:3:"cid";s:32:"baf487302893c387b8edf0c9d42999ae";}s:32:"889a342729618929a74176471139091d";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"kfisher";s:4:"name";s:15:"Kimberly Fisher";s:4:"mail";s:33:"Kimberly.Fisher@umassmemorial.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538582073;}s:3:"raw";s:245:"Hi Tara - that's a great idea!  This week is shot for me, but next week (other than Tuesday) or the week after are both good.  Let me know some days/times that are good for you.  Maybe we can even teleconference and actually see each other!  Kim";s:5:"xhtml";s:250:"Hi Tara - that&#039;s a great idea!  This week is shot for me, but next week (other than Tuesday) or the week after are both good.  Let me know some days/times that are good for you.  Maybe we can even teleconference and actually see each other!  Kim";s:6:"parent";s:32:"baf487302893c387b8edf0c9d42999ae";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"889a342729618929a74176471139091d";}s:32:"032f174d70ffa078f1e8e05aaf64c3ed";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jpittman";s:4:"name";s:13:"James Pittman";s:4:"mail";s:17:"jpittman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538582133;}s:3:"raw";s:372:"Thank you for the detailed response, very helpful! There is a lot for me to follow up on in this, and I am sure that I will have questions as I continue to develop this proposal. The good news is that there is still a lot of flexibility for how I approach this going forward, so all ideas and suggestions are welcome. I will complete the survey now.

Thanks again, 
James ";s:5:"xhtml";s:386:"Thank you for the detailed response, very helpful! There is a lot for me to follow up on in this, and I am sure that I will have questions as I continue to develop this proposal. The good news is that there is still a lot of flexibility for how I approach this going forward, so all ideas and suggestions are welcome. I will complete the survey now.<br /><br />Thanks again, <br />James";s:6:"parent";s:32:"a5ab182500c48f3255ab0f3928db70a9";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"032f174d70ffa078f1e8e05aaf64c3ed";}s:32:"ac4d9460cb759bc7d97c4654882b6033";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jpittman";s:4:"name";s:13:"James Pittman";s:4:"mail";s:17:"jpittman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538686745;}s:3:"raw";s:2276:"Pittman – Assignment # 4

1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

I proposed to conduct a mixed method hybrid type 2 implementation/effectiveness SMART trial. This was modeled after Kilbourne’s (2014) ADEPT study. My intention is to have approximately 80 clinical programs/teams (there could be several at each VA Healthcare System). I have not completely thought through all the logistics, but I am working on developing a flowchart to help work out the details. The general idea is to provide the eScreening and the eScreening playbook to all programs/teams at baseline. Programs/teams that have not implemented eScreening after three months, would then be randomized to facilitation using the site’s system redesign team to conduct a modified RPIW for implementation or facilitation and eScreening expert team support via regular phone calls. Programs randomized to receive facilitation RPIW only that have not implemented eScreening after another 3 (or 6?) months will be randomized again to either continue with systems redesign RPIW support only or to have both. I believe that this approach will provide useful information about the value of the eScreening playbook, the Value of systems redesign internal RPIW facilitator and the external eScreening expert team support in implementation. This approach should also be useful in identifying which types of programs need which level of support for eScreening implementation and can inform a strategy for national enterprise roll-out of eScreening. 

2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? 

I plan to use a convergent mixed methods approach for this study.  Qualitative data (interview) will be used to gain staff perspectives on eScreening and on the process of implementation. Qualitative results would be combined with quantitative measures such as use of eScreening, satisfaction ratings of eScreening, and the impact of eScreening on workflow and patient outcomes. Together these will more fully address the aims of the study. 
";s:5:"xhtml";s:2314:"Pittman – Assignment # 4<br /><br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />I proposed to conduct a mixed method hybrid type 2 implementation/effectiveness SMART trial. This was modeled after Kilbourne’s (2014) ADEPT study. My intention is to have approximately 80 clinical programs/teams (there could be several at each VA Healthcare System). I have not completely thought through all the logistics, but I am working on developing a flowchart to help work out the details. The general idea is to provide the eScreening and the eScreening playbook to all programs/teams at baseline. Programs/teams that have not implemented eScreening after three months, would then be randomized to facilitation using the site’s system redesign team to conduct a modified RPIW for implementation or facilitation and eScreening expert team support via regular phone calls. Programs randomized to receive facilitation RPIW only that have not implemented eScreening after another 3 (or 6?) months will be randomized again to either continue with systems redesign RPIW support only or to have both. I believe that this approach will provide useful information about the value of the eScreening playbook, the Value of systems redesign internal RPIW facilitator and the external eScreening expert team support in implementation. This approach should also be useful in identifying which types of programs need which level of support for eScreening implementation and can inform a strategy for national enterprise roll-out of eScreening. <br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? <br /><br />I plan to use a convergent mixed methods approach for this study.  Qualitative data (interview) will be used to gain staff perspectives on eScreening and on the process of implementation. Qualitative results would be combined with quantitative measures such as use of eScreening, satisfaction ratings of eScreening, and the impact of eScreening on workflow and patient outcomes. Together these will more fully address the aims of the study.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"f8fac9dbf137bfd658a4c9a10751fd17";}s:4:"show";b:1;s:3:"cid";s:32:"ac4d9460cb759bc7d97c4654882b6033";}s:32:"dfe28a72955444831c66becb770ccea0";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"sshohat";s:4:"name";s:20:"Sivan Spitzer-Shohat";s:4:"mail";s:30:"sivan.spitzer-shohat@biu.ac.il";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538703343;}s:3:"raw";s:2877:"Dear David,
Thank you so much for your in-depth feedback. It has really given me allot to think about as to how I may better express the projects’ aims and assessed measures. As Byron, Lori and you have expressed that parts of my proposed intervention are still unclear, I’d like to explain again in brief. 

The intervention proposed, stems from an evaluation we conducted of UChicago Medicine’s equity intervention after the first 5 years of implementation. The strategy the organization devised for leading organizational-wide change and becoming an equitable organization was based on a cultural competence course developed in house, that aimed to create change agents who will promote equity focused change interventions in their different departments. Our evaluation highlighted the problem of moving from awareness to action as 1) the existing hierarchical organizational culture does not have support in place to properly encourage bottom-up change agents, and 2) people in diverse sectors of the organization who do not deal directly with patient care do not perceive that issues of equity necessarily relate to their daily work nor do they feel they understand what needs to be done to implement equity as part of their everyday work processes.
The training developed around organizational sensemaking, i.e. implementation strategy, aims to assist the organization’s selected governance committee members in leading change in selected departments through facilitation and guidance to department employees in the translation and contextualization of equity to existing work processes. 
It is important for me to reiterate that at this phase, given the top-down rollout strategy of the organization, the focus is on the meso organizational level which is made up of eight cross cutting departments who do not deal directly with patient care, but oversee operations such as HR, Operational Excellence, Clinical Effectiveness, Information Systems etc. Therefore this is the reason the intervention outcomes I propose to assess relate to trainees and employees and not patients and focus primarily on perceived- effectiveness, innovation-fit and penetration. 

I think the major challenge I am grappling with is the organization’s aim to advance organization-wide equity which is not linked to specific clinical outcomes. Hence, identifying an objective organizational outcome effected is difficult, especially given the need to contextualize equity to different departments. For example, can we compare objectively between departments on the number of work process that an equity lens was added to? Some departments may have a harder time in thinking what are the relevant equity issues to their work processes, some may have only a few where equity is relevant. Your suggestions and thoughts David, Lori and Byron would be really helpful.
Thank you,
Sivan




";s:5:"xhtml";s:2917:"Dear David,<br />Thank you so much for your in-depth feedback. It has really given me allot to think about as to how I may better express the projects’ aims and assessed measures. As Byron, Lori and you have expressed that parts of my proposed intervention are still unclear, I’d like to explain again in brief. <br /><br />The intervention proposed, stems from an evaluation we conducted of UChicago Medicine’s equity intervention after the first 5 years of implementation. The strategy the organization devised for leading organizational-wide change and becoming an equitable organization was based on a cultural competence course developed in house, that aimed to create change agents who will promote equity focused change interventions in their different departments. Our evaluation highlighted the problem of moving from awareness to action as 1) the existing hierarchical organizational culture does not have support in place to properly encourage bottom-up change agents, and 2) people in diverse sectors of the organization who do not deal directly with patient care do not perceive that issues of equity necessarily relate to their daily work nor do they feel they understand what needs to be done to implement equity as part of their everyday work processes.<br />The training developed around organizational sensemaking, i.e. implementation strategy, aims to assist the organization’s selected governance committee members in leading change in selected departments through facilitation and guidance to department employees in the translation and contextualization of equity to existing work processes. <br />It is important for me to reiterate that at this phase, given the top-down rollout strategy of the organization, the focus is on the meso organizational level which is made up of eight cross cutting departments who do not deal directly with patient care, but oversee operations such as HR, Operational Excellence, Clinical Effectiveness, Information Systems etc. Therefore this is the reason the intervention outcomes I propose to assess relate to trainees and employees and not patients and focus primarily on perceived- effectiveness, innovation-fit and penetration. <br /><br />I think the major challenge I am grappling with is the organization’s aim to advance organization-wide equity which is not linked to specific clinical outcomes. Hence, identifying an objective organizational outcome effected is difficult, especially given the need to contextualize equity to different departments. For example, can we compare objectively between departments on the number of work process that an equity lens was added to? Some departments may have a harder time in thinking what are the relevant equity issues to their work processes, some may have only a few where equity is relevant. Your suggestions and thoughts David, Lori and Byron would be really helpful.<br />Thank you,<br />Sivan";s:6:"parent";s:32:"59048673f2f7047017e244d37a7bc747";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"dfe28a72955444831c66becb770ccea0";}s:32:"8a3ad3593905177b4b65a6fd25e99a67";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"sshohat";s:4:"name";s:20:"Sivan Spitzer-Shohat";s:4:"mail";s:30:"sivan.spitzer-shohat@biu.ac.il";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538706644;}s:3:"raw";s:2493:"Spitzer-Shohat Assignment #4:
What is your proposed study design? Why is that the best design to answer your research questions or hypotheses? Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?

We aim to pilot an equity translation training strategy and assess its effectiveness in advancing the implementation of an equity lens to everyday work processes using a convergent mixed-methods design. To assess the impact of the training on trainees, we will evaluate pilot teams’ perceived team effectiveness through an administered survey as well as interviews. The quantitative survey will obtain information on the extent to which team members perceive that following training they had the knowledge, skills and organizational support to facilitate and assist departments in implementing an equity lens to their daily work processes. Qualitative data collected through interviews will complement quantitative data by shedding light on what new knowledge and/or skills were acquired, how they assisted trainees in the facilitation process and what issues deemed important for successful implementation are not addressed by the training. 
We also plan to assess appropriateness of the organization’s equity initiative, following pilot teams’ work through department employees’ perceptions on innovation-fit. Using the Dong et al (2008) questionnaire, we will assess innovation-fit before and after facilitation. Baseline data has already been collected in the evaluation of Phase 1 of the initiative. Interviews with key department informants will contribute to understanding to what extent and how facilitation increased innovation-fit through concrete examples of departmental implementation efforts. 
Feasibility and Adoption will be assessed through content analysis of teams’ organizational implementation plans over 12 months, detailing the strategies devised and organizational resources required. Interviews with both implementation team members and department employees will enable to compare between the diverse views of both facilitator and implementor on what may be the levers and barriers to successful adoption. Penetration will also be assessed through content analysis of the implementation plan reports assessing the breadth of the organizational work-processes addressed.  


";s:5:"xhtml";s:2513:"Spitzer-Shohat Assignment #4:<br />What is your proposed study design? Why is that the best design to answer your research questions or hypotheses? Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br /><br />We aim to pilot an equity translation training strategy and assess its effectiveness in advancing the implementation of an equity lens to everyday work processes using a convergent mixed-methods design. To assess the impact of the training on trainees, we will evaluate pilot teams’ perceived team effectiveness through an administered survey as well as interviews. The quantitative survey will obtain information on the extent to which team members perceive that following training they had the knowledge, skills and organizational support to facilitate and assist departments in implementing an equity lens to their daily work processes. Qualitative data collected through interviews will complement quantitative data by shedding light on what new knowledge and/or skills were acquired, how they assisted trainees in the facilitation process and what issues deemed important for successful implementation are not addressed by the training. <br />We also plan to assess appropriateness of the organization’s equity initiative, following pilot teams’ work through department employees’ perceptions on innovation-fit. Using the Dong et al (2008) questionnaire, we will assess innovation-fit before and after facilitation. Baseline data has already been collected in the evaluation of Phase 1 of the initiative. Interviews with key department informants will contribute to understanding to what extent and how facilitation increased innovation-fit through concrete examples of departmental implementation efforts. <br />Feasibility and Adoption will be assessed through content analysis of teams’ organizational implementation plans over 12 months, detailing the strategies devised and organizational resources required. Interviews with both implementation team members and department employees will enable to compare between the diverse views of both facilitator and implementor on what may be the levers and barriers to successful adoption. Penetration will also be assessed through content analysis of the implementation plan reports assessing the breadth of the organizational work-processes addressed.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"ac732dfdcf66fa44e1576cbdd51f8bc6";}s:4:"show";b:1;s:3:"cid";s:32:"8a3ad3593905177b4b65a6fd25e99a67";}s:32:"74887bb78aab3f568344b959020986f6";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"tlagu";s:4:"name";s:9:"Tara Lagu";s:4:"mail";s:28:"Tara.Lagu@baystatehealth.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538764544;}s:3:"raw";s:2270:"Lagu-Assignment #4
1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?
The proposed study design is a mixed-methods positive deviance design in which we identify “high performers’ using Medicare data and then interview “positive deviants” about their implementation strategies and finally use a Delphi method to summarize our recommendations, with a goal of increasing referral and use of Cardiac Rehabilitation by patients with heart failure. However, as David pointed out in response to our assignment 2, there is an issue-we don’t have clear guidelines as to the best methods for referring to/motivating patients to attend CR in patients with HF. So, in an ideal world, a hybrid implementation-effectiveness trial (probably type 2) would be the best approach because it would address the evidence gap and would allow us to speed the translation of findings into practice. However, this grant was written in response to an RFA that did not allow clinical trials using the new NIH definition of clinical trials, meaning that we really couldn’t do any kind of intervention and can’t do a clinical trial. So I am likely not going to be able to this study design for this application (if the grant isn't scored, I might give up on this mechanism and rethink the whole thing, but if it gets a good score...I might be stuck). So, I am going to have to stick with observational methods, and it seems like the positive deviance approach was the best way to understand the implementation strategies that lead to the greatest use of CR among patients with HF.

2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?
This is definitely a mixed-methods approach, in that it we are using quantitative, qualitative, and survey methods. This uses an explanatory sequential design, so we identify high performers, use qualitative methods to identify the implementation strategies of high performers, and then use the Delphi panel to further evaluate the strategies for feasibility, acceptability, and appropriateness.
";s:5:"xhtml";s:2299:"Lagu-Assignment #4<br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br />The proposed study design is a mixed-methods positive deviance design in which we identify “high performers’ using Medicare data and then interview “positive deviants” about their implementation strategies and finally use a Delphi method to summarize our recommendations, with a goal of increasing referral and use of Cardiac Rehabilitation by patients with heart failure. However, as David pointed out in response to our assignment 2, there is an issue-we don’t have clear guidelines as to the best methods for referring to/motivating patients to attend CR in patients with HF. So, in an ideal world, a hybrid implementation-effectiveness trial (probably type 2) would be the best approach because it would address the evidence gap and would allow us to speed the translation of findings into practice. However, this grant was written in response to an RFA that did not allow clinical trials using the new NIH definition of clinical trials, meaning that we really couldn’t do any kind of intervention and can’t do a clinical trial. So I am likely not going to be able to this study design for this application (if the grant isn&#039;t scored, I might give up on this mechanism and rethink the whole thing, but if it gets a good score...I might be stuck). So, I am going to have to stick with observational methods, and it seems like the positive deviance approach was the best way to understand the implementation strategies that lead to the greatest use of CR among patients with HF.<br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br />This is definitely a mixed-methods approach, in that it we are using quantitative, qualitative, and survey methods. This uses an explanatory sequential design, so we identify high performers, use qualitative methods to identify the implementation strategies of high performers, and then use the Delphi panel to further evaluate the strategies for feasibility, acceptability, and appropriateness.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"e138965d8a1ab7903edfd52cc8824714";}s:4:"show";b:1;s:3:"cid";s:32:"74887bb78aab3f568344b959020986f6";}s:32:"150e4fb1c2f44ce1ffcb9aaae88e551c";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"kfisher";s:4:"name";s:15:"Kimberly Fisher";s:4:"mail";s:33:"Kimberly.Fisher@umassmemorial.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538768608;}s:3:"raw";s:2419:"1.What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

The intervention phase of my project is still hypothetical and less developed (woefully un-developed would be another way to put it) which makes it harder to specify a proposed study design.  Within that limitation, I expect to propose a hybrid type I or II design as the intervention is not yet evidence-based.  Therefore, efficacy/effectiveness and implementation testing will occur concurrently.  

I am currently envisioning an intervention that: 1) specifies core elements or domains fundamental to high quality care transitions (e.g. interdisciplinary communication, medication reconciliation and education, case manager assessment of all patients’ care transition needs with more intensive targeting of case manager/SW resources to high-risk patients, systems/processes to bridge from acute to post-acute setting such as post-discharge calls), 2) includes an easy to use instrument that will allow hospitals to easily assess for domains in need of improvement, and 3) toolkits (as yet to be defined, possibly to include peer learning opportunities or webinars) to support implementation of systems/processes to address gaps in each core domain that hospitals can select based on their needs.  Because of the possibility to tailor what aspects of step 3 hospitals use based on their deficiencies, the SMART or adaptive trial design (which I had previously never heard of) is appealing and may offer some advantages, but I haven’t fully developed the intervention sufficiently to be ready to apply such a design or specify how I would use such a design.      

2.  Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?

A sequential mixed methods design in which qualitative evaluation data is collected to understand why hospitals adopted specific approaches to improve care transition quality, as well as barriers and facilitators that were encountered would be helpful to explain the findings (e.g., if a specific hospital does not improve their care transitions, it may be that they didn’t adopt any of the recommended practices due to insurmountable barriers that would be important to understand).  
";s:5:"xhtml";s:2456:"1.What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />The intervention phase of my project is still hypothetical and less developed (woefully un-developed would be another way to put it) which makes it harder to specify a proposed study design.  Within that limitation, I expect to propose a hybrid type I or II design as the intervention is not yet evidence-based.  Therefore, efficacy/effectiveness and implementation testing will occur concurrently.  <br /><br />I am currently envisioning an intervention that: 1) specifies core elements or domains fundamental to high quality care transitions (e.g. interdisciplinary communication, medication reconciliation and education, case manager assessment of all patients’ care transition needs with more intensive targeting of case manager/SW resources to high-risk patients, systems/processes to bridge from acute to post-acute setting such as post-discharge calls), 2) includes an easy to use instrument that will allow hospitals to easily assess for domains in need of improvement, and 3) toolkits (as yet to be defined, possibly to include peer learning opportunities or webinars) to support implementation of systems/processes to address gaps in each core domain that hospitals can select based on their needs.  Because of the possibility to tailor what aspects of step 3 hospitals use based on their deficiencies, the SMART or adaptive trial design (which I had previously never heard of) is appealing and may offer some advantages, but I haven’t fully developed the intervention sufficiently to be ready to apply such a design or specify how I would use such a design.      <br /><br />2.  Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br /><br />A sequential mixed methods design in which qualitative evaluation data is collected to understand why hospitals adopted specific approaches to improve care transition quality, as well as barriers and facilitators that were encountered would be helpful to explain the findings (e.g., if a specific hospital does not improve their care transitions, it may be that they didn’t adopt any of the recommended practices due to insurmountable barriers that would be important to understand).";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"34c8f2fae4aeabdb2e181c7d5c2eb911";}s:4:"show";b:1;s:3:"cid";s:32:"150e4fb1c2f44ce1ffcb9aaae88e551c";}s:32:"13731ddf9155d5d8e37e5965d447bb57";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"kboockvar";s:4:"name";s:16:"Kenneth Boockvar";s:4:"mail";s:25:"kenneth.boockvar@mssm.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538772872;}s:3:"raw";s:3224:"Boockvar--Assignment #4 - Design

What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

Our proposed study design is a hybrid type 1 design as described by Curran et al.  This design is broadly described as “testing a clinical intervention while gathering information on its delivery during the effectiveness trial and on its potential for implementation in a real world setting.”  We selected this design because 1) the electronic health information exchange (HIE) notification of non-system (non-VA) hospital or ED use has strong face validity, 2) the geriatrics care coordination intervention has a strong base of evidence that supports its applicability to this new setting and scenario, and 3) both interventions are minimal risk.  We believe additional effectiveness testing is needed to test the impact of the linkage of these 2 interventions, which is a complex endeavor.

We are using a sequential study strategy in which we are asking the following research questions:  1) does electronic HIE notification, with or without geriatrics care coordination, improve processes and outcomes after a non-VA hospital encounter (implementation and effectiveness questions ), followed by 2) what are potential barriers and facilitators to electronic HIE notification in the VA setting (implementation question), and 3) what modifications should be made to HIE notification and/or the geriatrics care coordination intervention to optimize implementation (implementation question).  

Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?

We plan to utilize mixed methods.  We will use quantitative methods to determine the impact of our interventions on hospital readmission (effectiveness question) and timely telephone and in-person follow-up at the VA hospital (implementation question).   To do this we have proposed a cluster-randomized trial in which patients are assigned to one of 3 study groups (HIE primary care notification plus geriatrics care coordination, HIE notification alone, and neither) clustered by their primary care provider.  We selected the primary care provider as the level of randomization in order to avoid contamination (in which a patient assigned to one group receives components of care associated with another group) and to avoid unblinding.  We will account for correlation of outcomes within cluster using hierarchical modeling.

We will use qualitative methods to determine potential barriers, facilitators, and modifications to the interventions after or at the end of the effectiveness study.  We will interview primary care and other stakeholder VA staff, audiotape, and transcribe the interviews.  We will integrate the mixed methods by using this qualitative component sequentially to answer “why” questions; e.g., if we find that there was minimal primary care response to the HIE notification (e.g., no timely telephone or in-person follow-up) we will ask participants why and what modifications might improve these processes.
";s:5:"xhtml";s:3283:"Boockvar--Assignment #4 - Design<br /><br />What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />Our proposed study design is a hybrid type 1 design as described by Curran et al.  This design is broadly described as “testing a clinical intervention while gathering information on its delivery during the effectiveness trial and on its potential for implementation in a real world setting.”  We selected this design because 1) the electronic health information exchange (HIE) notification of non-system (non-VA) hospital or ED use has strong face validity, 2) the geriatrics care coordination intervention has a strong base of evidence that supports its applicability to this new setting and scenario, and 3) both interventions are minimal risk.  We believe additional effectiveness testing is needed to test the impact of the linkage of these 2 interventions, which is a complex endeavor.<br /><br />We are using a sequential study strategy in which we are asking the following research questions:  1) does electronic HIE notification, with or without geriatrics care coordination, improve processes and outcomes after a non-VA hospital encounter (implementation and effectiveness questions ), followed by 2) what are potential barriers and facilitators to electronic HIE notification in the VA setting (implementation question), and 3) what modifications should be made to HIE notification and/or the geriatrics care coordination intervention to optimize implementation (implementation question).  <br /><br />Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br /><br />We plan to utilize mixed methods.  We will use quantitative methods to determine the impact of our interventions on hospital readmission (effectiveness question) and timely telephone and in-person follow-up at the VA hospital (implementation question).   To do this we have proposed a cluster-randomized trial in which patients are assigned to one of 3 study groups (HIE primary care notification plus geriatrics care coordination, HIE notification alone, and neither) clustered by their primary care provider.  We selected the primary care provider as the level of randomization in order to avoid contamination (in which a patient assigned to one group receives components of care associated with another group) and to avoid unblinding.  We will account for correlation of outcomes within cluster using hierarchical modeling.<br /><br />We will use qualitative methods to determine potential barriers, facilitators, and modifications to the interventions after or at the end of the effectiveness study.  We will interview primary care and other stakeholder VA staff, audiotape, and transcribe the interviews.  We will integrate the mixed methods by using this qualitative component sequentially to answer “why” questions; e.g., if we find that there was minimal primary care response to the HIE notification (e.g., no timely telephone or in-person follow-up) we will ask participants why and what modifications might improve these processes.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"4db101be6daec9e802bb54afe454a22c";}s:4:"show";b:1;s:3:"cid";s:32:"13731ddf9155d5d8e37e5965d447bb57";}s:32:"d8cf9431d5bc816f65321d7cd2f49c00";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"bjones";s:4:"name";s:13:"Barbara Jones";s:4:"mail";s:26:"Barbara.Jones@hsc.utah.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538802070;}s:3:"raw";s:3370:"Jones - Assignment #4: Design
1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?
I definitely think that the best study design for a multi-site implementation of evidence-based CDS for pneumonia is the hybrid Type 2, in which we would test both the effectiveness of a clinical pathway for pneumonia as well as the informatics-based implementation. There is 1) strong face validity that many of the components of pneumonia recommendations should be applicable to multiple settings as well as 2) indirect evidence (evidence of improvement of clinical outcomes with adherence to pathways). There is 3) minimal risk to a CDS intervention; there is 4) implementation momentum (the recent guidelines list adaptation and implementation of pneumonia guidelines as a grade 1a recommendation); there are 5) reasonable expectations that a EHR-based CDS would be supportable in new settings (because the VA shares a common EHR, each site has clinical applications coordinators, and informatics tools developed in one VA are share-able with other VA’s, the digital and human infrastructure for the CDS exist to support implementation), AND there 6) is reason to gather more data on the clinical intervention: while there is adequate evidence to support timely accurate diagnosis, first-line antibiotics, and site-of-care decision-making based upon objective severity assessment across all settings (yet a gap between these recommendations and actual practice), the thresholds at which to determine site-of-care and different antimicrobials are less clear and could vary by setting, and there is uncertainty in the optimal risks/ benefits of emphasizing time-to-antibiotics versus accurate diagnosis and judicious antimicrobial use (since there is tension between these 2 goals). Thus there are areas of uncertainty in the effectiveness of components of the evidence-based intervention, as well as gaps in the implementation of even the most certain components of the intervention.
2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?
Yes! We are currently conducting  a mixed methods study that aims to examine sources of variation in pneumonia care processes across VA facilities that integrates a qualitative component (semi-structured interviews with emergency department physicians and clinical leadership) with a quantitative component (hierarchical regression model using known patient, provider and setting factors that influence treatment decisions) to understand contextual features at VA facilities that demonstrate variation in hospitalization and antimicrobial selection. This preliminary work will serve as the exploratory and preparation phases of the implementation study. I have not worked out the details of how to continue the qualitative component during the implementation, but additional semi-structure interviews with end-users of the CDS at the setting during the implementation will be a main component of the formative evaluation of tool CDS to examine providers' experiences at different sites. I am envisioning working in qualitative measures of fidelity and sustainment here, but could use advice!
";s:5:"xhtml";s:3394:"Jones - Assignment #4: Design<br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br />I definitely think that the best study design for a multi-site implementation of evidence-based CDS for pneumonia is the hybrid Type 2, in which we would test both the effectiveness of a clinical pathway for pneumonia as well as the informatics-based implementation. There is 1) strong face validity that many of the components of pneumonia recommendations should be applicable to multiple settings as well as 2) indirect evidence (evidence of improvement of clinical outcomes with adherence to pathways). There is 3) minimal risk to a CDS intervention; there is 4) implementation momentum (the recent guidelines list adaptation and implementation of pneumonia guidelines as a grade 1a recommendation); there are 5) reasonable expectations that a EHR-based CDS would be supportable in new settings (because the VA shares a common EHR, each site has clinical applications coordinators, and informatics tools developed in one VA are share-able with other VA’s, the digital and human infrastructure for the CDS exist to support implementation), AND there 6) is reason to gather more data on the clinical intervention: while there is adequate evidence to support timely accurate diagnosis, first-line antibiotics, and site-of-care decision-making based upon objective severity assessment across all settings (yet a gap between these recommendations and actual practice), the thresholds at which to determine site-of-care and different antimicrobials are less clear and could vary by setting, and there is uncertainty in the optimal risks/ benefits of emphasizing time-to-antibiotics versus accurate diagnosis and judicious antimicrobial use (since there is tension between these 2 goals). Thus there are areas of uncertainty in the effectiveness of components of the evidence-based intervention, as well as gaps in the implementation of even the most certain components of the intervention.<br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br />Yes! We are currently conducting  a mixed methods study that aims to examine sources of variation in pneumonia care processes across VA facilities that integrates a qualitative component (semi-structured interviews with emergency department physicians and clinical leadership) with a quantitative component (hierarchical regression model using known patient, provider and setting factors that influence treatment decisions) to understand contextual features at VA facilities that demonstrate variation in hospitalization and antimicrobial selection. This preliminary work will serve as the exploratory and preparation phases of the implementation study. I have not worked out the details of how to continue the qualitative component during the implementation, but additional semi-structure interviews with end-users of the CDS at the setting during the implementation will be a main component of the formative evaluation of tool CDS to examine providers&#039; experiences at different sites. I am envisioning working in qualitative measures of fidelity and sustainment here, but could use advice!";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"9caca5bc02ecced42f9f8d71c8a8ff2b";}s:4:"show";b:1;s:3:"cid";s:32:"d8cf9431d5bc816f65321d7cd2f49c00";}s:32:"e138965d8a1ab7903edfd52cc8824714";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dgoodrich";s:4:"name";s:14:"David Goodrich";s:4:"mail";s:22:"David.Goodrich2@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539211007;}s:3:"raw";s:3183:"Hi Tara, its too bad you can't do an observational study in which you compare outcomes of HF patients getting referred to CR based on a change in state, CMS, or health provider policy/incentives. Are you aware of any regional practices that might associated with greater uptake of your CR for heart failure patients? Natural experiments are great to observe!  

You might take a moment to consider the information you will gather from your design. I'm a big believer in understanding the qualities of excellent positive deviants; however, there may be a big divide between the work flows of positive deviants and what Roger's calls the later adopters or laggards. Something you might think about (only to be provocative), what are the organizational and provider level interventions (i.e., implementation strategies) needed to get "low performing" sites up to positive deviance levels? 

If you learn about the characteristics of "high performers," you will still need to take that information and figure out how to get the low performers to innovate. A study that comes to mind that Bryon was part of related to rapid implementation of hepatitis C treatment in VA (see Shari Rogal et al., https://www.ncbi.nlm.nih.gov/pubmed/28494811). One of the interesting things Shari and her colleagues found was that low performing sites had less "implementation capacity" and therefore used more basic strategies to implement the clinical treatment vs the high performing sites which used a higher number of strategies and varied the use of these strategies based on where they were in adopting, scaling up and sustaining the innovation. Thus, to be contrarian, is it possible that your design might miss capturing what the barriers or normative capacity for CR referral is in most regions? I think referral to CR has improved in general (from watching my father's experience) from a decade ago but I'm wondering what are factors holding back CR, aside of the 6 week delay in starting care (basic infrastructure, cultural factors - e.g., value/priority for CR)? Another issue for consideration - what IF there is not a lot of variance in performance between the positive deviants (e.g., who say have 17% referral rates) vs. the median or average referral rate of say 6%? Are there similar models of care for another chronic disease in older comorbid adults that you could learn from as another case of positive deviance? 

Finally, I have to confess that I'm less up to date on NIH RFAs for your area of clinical expertise but I bet Lori and some of the other NIH faculty can work to help id some possible funding mechanisms to keep this line of research moving from observation to intervention. I think your research will only become more salient as baby-boomers age and start to require these rehabilitative treatments to reduce costs and utilization. My father's HF is the result of chemotherapy for lymphoma. I wonder if NCI might support some interventional research if cancer survivors with HF were included??

I really am excited about where you are going with this project and line of research. Please let me know if you have follow-up questions or thoughts. 

Warm regards,
David



";s:5:"xhtml";s:3299:"Hi Tara, its too bad you can&#039;t do an observational study in which you compare outcomes of HF patients getting referred to CR based on a change in state, CMS, or health provider policy/incentives. Are you aware of any regional practices that might associated with greater uptake of your CR for heart failure patients? Natural experiments are great to observe!  <br /><br />You might take a moment to consider the information you will gather from your design. I&#039;m a big believer in understanding the qualities of excellent positive deviants; however, there may be a big divide between the work flows of positive deviants and what Roger&#039;s calls the later adopters or laggards. Something you might think about (only to be provocative), what are the organizational and provider level interventions (i.e., implementation strategies) needed to get &quot;low performing&quot; sites up to positive deviance levels? <br /><br />If you learn about the characteristics of &quot;high performers,&quot; you will still need to take that information and figure out how to get the low performers to innovate. A study that comes to mind that Bryon was part of related to rapid implementation of hepatitis C treatment in VA (see Shari Rogal et al., https://www.ncbi.nlm.nih.gov/pubmed/28494811). One of the interesting things Shari and her colleagues found was that low performing sites had less &quot;implementation capacity&quot; and therefore used more basic strategies to implement the clinical treatment vs the high performing sites which used a higher number of strategies and varied the use of these strategies based on where they were in adopting, scaling up and sustaining the innovation. Thus, to be contrarian, is it possible that your design might miss capturing what the barriers or normative capacity for CR referral is in most regions? I think referral to CR has improved in general (from watching my father&#039;s experience) from a decade ago but I&#039;m wondering what are factors holding back CR, aside of the 6 week delay in starting care (basic infrastructure, cultural factors - e.g., value/priority for CR)? Another issue for consideration - what IF there is not a lot of variance in performance between the positive deviants (e.g., who say have 17% referral rates) vs. the median or average referral rate of say 6%? Are there similar models of care for another chronic disease in older comorbid adults that you could learn from as another case of positive deviance? <br /><br />Finally, I have to confess that I&#039;m less up to date on NIH RFAs for your area of clinical expertise but I bet Lori and some of the other NIH faculty can work to help id some possible funding mechanisms to keep this line of research moving from observation to intervention. I think your research will only become more salient as baby-boomers age and start to require these rehabilitative treatments to reduce costs and utilization. My father&#039;s HF is the result of chemotherapy for lymphoma. I wonder if NCI might support some interventional research if cancer survivors with HF were included??<br /><br />I really am excited about where you are going with this project and line of research. Please let me know if you have follow-up questions or thoughts. <br /><br />Warm regards,<br />David";s:6:"parent";s:32:"74887bb78aab3f568344b959020986f6";s:7:"replies";a:2:{i:0;s:32:"f0e92693331910ab7afa9bd70f1c28e4";i:1;s:32:"f1b6cf8fcd1781651cab83514c971d60";}s:4:"show";b:1;s:3:"cid";s:32:"e138965d8a1ab7903edfd52cc8824714";}s:32:"34c8f2fae4aeabdb2e181c7d5c2eb911";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dgoodrich";s:4:"name";s:14:"David Goodrich";s:4:"mail";s:22:"David.Goodrich2@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539214499;}s:3:"raw";s:2516:"Hi Kimberly, it is okay if your intervention design is still a work in progress at this point but it sounds like your thoughts are crystalizing towards a pragmatic efficacy-effectiveness trial or, even a hybrid-type 1 implementation-effectiveness design? I would encourage you not to go with a SMART trial design. Generally, you need some preliminary data on the effectiveness of your clinical intervention/system redesign to inform these fancier adaptive treatments designs. 

It sounds like you are leaning towards developing the care transitions "intervention" itself and attempting to identify potential implementation barriers for down the road? The development of the instrument and toolkit therefore could be framed as outcomes (products) that will be developed as part of one or more of your aims to be used in a larger implementation trial. 

Your interventional aim seems like you might test your best guess (based on the literature) of a "bundle" of processes/workflows to improve care transition outcomes compared to treatment as usual or some other limited intervention. Your evaluation plan also seems heavily influenced by user centered design principles. Input from stakeholders regarding the care transition bundle's acceptability, feasibility, appropriateness, and usability made me think of a great paper on user center design by Lyon et al. (https://www.ncbi.nlm.nih.gov/pubmed/29456295 ). While this paper may not help with your overall design per se, it may inspire you to think with the end in mind about possible domains for the instrument or metrics to assess the real world usability of the care transition practices. It appears that your aims move from exploratory to testing in practice and, have concrete products as outcomes. I like your plan to sequentially use process evaluation methods to understand context factors affecting hospitals decisions to adopt and manage specific implementation barriers. I look forward in the coming weeks to see how your design evolves and aligns with your aims. Don't be afraid to start with a simple design for initial testing so long as you can start to measure efficacy/effectiveness of your proposed innovation and, identify relevant barriers/facilitators to larger spread and scale over multiple sites or regions. Care transitions is a complex clinical area and perhaps starting simple will reveal the best path forward for your research. Please follow-up if any of my ideas are unclear or could use further clarification. 

Warm regards,

David
";s:5:"xhtml";s:2585:"Hi Kimberly, it is okay if your intervention design is still a work in progress at this point but it sounds like your thoughts are crystalizing towards a pragmatic efficacy-effectiveness trial or, even a hybrid-type 1 implementation-effectiveness design? I would encourage you not to go with a SMART trial design. Generally, you need some preliminary data on the effectiveness of your clinical intervention/system redesign to inform these fancier adaptive treatments designs. <br /><br />It sounds like you are leaning towards developing the care transitions &quot;intervention&quot; itself and attempting to identify potential implementation barriers for down the road? The development of the instrument and toolkit therefore could be framed as outcomes (products) that will be developed as part of one or more of your aims to be used in a larger implementation trial. <br /><br />Your interventional aim seems like you might test your best guess (based on the literature) of a &quot;bundle&quot; of processes/workflows to improve care transition outcomes compared to treatment as usual or some other limited intervention. Your evaluation plan also seems heavily influenced by user centered design principles. Input from stakeholders regarding the care transition bundle&#039;s acceptability, feasibility, appropriateness, and usability made me think of a great paper on user center design by Lyon et al. (https://www.ncbi.nlm.nih.gov/pubmed/29456295 ). While this paper may not help with your overall design per se, it may inspire you to think with the end in mind about possible domains for the instrument or metrics to assess the real world usability of the care transition practices. It appears that your aims move from exploratory to testing in practice and, have concrete products as outcomes. I like your plan to sequentially use process evaluation methods to understand context factors affecting hospitals decisions to adopt and manage specific implementation barriers. I look forward in the coming weeks to see how your design evolves and aligns with your aims. Don&#039;t be afraid to start with a simple design for initial testing so long as you can start to measure efficacy/effectiveness of your proposed innovation and, identify relevant barriers/facilitators to larger spread and scale over multiple sites or regions. Care transitions is a complex clinical area and perhaps starting simple will reveal the best path forward for your research. Please follow-up if any of my ideas are unclear or could use further clarification. <br /><br />Warm regards,<br /><br />David";s:6:"parent";s:32:"150e4fb1c2f44ce1ffcb9aaae88e551c";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"34c8f2fae4aeabdb2e181c7d5c2eb911";}s:32:"f0e92693331910ab7afa9bd70f1c28e4";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lducharme";s:4:"name";s:13:"Lori Ducharme";s:4:"mail";s:21:"lori.ducharme@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539372807;}s:3:"raw";s:586:"Eavesdropping on this conversation...  Tara: if your application is not funded, or even for future reference, NIH will always have funding opportunity announcements (FOAs) available for investigator-initiated studies -- and for every "no clinical trials" FOA, there will likely be a "clinical trials optional" counterpart.  At the in-person meeting, I'd recommend that you touch base with Kate Stoney, one of  the NIH facilitators from NHLBI's cardiovascular division.  She's best positioned to talk about how your interests might align with her institute's priorities and FOAs. -- Lori";s:5:"xhtml";s:626:"Eavesdropping on this conversation...  Tara: if your application is not funded, or even for future reference, NIH will always have funding opportunity announcements (FOAs) available for investigator-initiated studies -- and for every &quot;no clinical trials&quot; FOA, there will likely be a &quot;clinical trials optional&quot; counterpart.  At the in-person meeting, I&#039;d recommend that you touch base with Kate Stoney, one of  the NIH facilitators from NHLBI&#039;s cardiovascular division.  She&#039;s best positioned to talk about how your interests might align with her institute&#039;s priorities and FOAs. -- Lori";s:6:"parent";s:32:"e138965d8a1ab7903edfd52cc8824714";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"f0e92693331910ab7afa9bd70f1c28e4";}s:32:"4db101be6daec9e802bb54afe454a22c";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lducharme";s:4:"name";s:13:"Lori Ducharme";s:4:"mail";s:21:"lori.ducharme@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539392553;}s:3:"raw";s:1028:"Hi Kenneth.  This concept is really taking shape nicely!  I went back and re-read each assignment, and given everything you’ve described in prior assignments, your proposed design fits logically – a cluster RCT randomizing providers to three arms, measuring both patient-level outcomes (to test effectiveness of the HIE notification with or without a care coordination intervention) and provider-level feedback on the HIE (to identify implementation issues) in this hybrid 1.  The use of qualitative methods with the providers – particularly the “think-alouds” you described previously – after the end of the effectiveness trial makes sense, so as not to influence their behavior outside of the intervention(s) to which they have been randomized.  (Did I get that right?)  I have very little to add here, because I think you’ve got it!  If you have specific questions around design issues that you need guidance on, do let us know.  Otherwise I am looking forward to reading the full concept in a few weeks! - Lori";s:5:"xhtml";s:1028:"Hi Kenneth.  This concept is really taking shape nicely!  I went back and re-read each assignment, and given everything you’ve described in prior assignments, your proposed design fits logically – a cluster RCT randomizing providers to three arms, measuring both patient-level outcomes (to test effectiveness of the HIE notification with or without a care coordination intervention) and provider-level feedback on the HIE (to identify implementation issues) in this hybrid 1.  The use of qualitative methods with the providers – particularly the “think-alouds” you described previously – after the end of the effectiveness trial makes sense, so as not to influence their behavior outside of the intervention(s) to which they have been randomized.  (Did I get that right?)  I have very little to add here, because I think you’ve got it!  If you have specific questions around design issues that you need guidance on, do let us know.  Otherwise I am looking forward to reading the full concept in a few weeks! - Lori";s:6:"parent";s:32:"13731ddf9155d5d8e37e5965d447bb57";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"4db101be6daec9e802bb54afe454a22c";}s:32:"ac732dfdcf66fa44e1576cbdd51f8bc6";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lducharme";s:4:"name";s:13:"Lori Ducharme";s:4:"mail";s:21:"lori.ducharme@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539397996;}s:3:"raw";s:2024:"Hi Sivan.  I confess, even after re-reading all the assignments, I am still struggling with understanding your project.  Your follow-up response to the previous assignment was helpful, though, in explaining that the departments you will initially be working with are fairly removed from patient interactions, so that explains the lack of patient-level outcome focus.  I am still struggling with whether training is the implementation strategy being tested, because elsewhere you refer to measuring whether/how the training influenced the pilot teams’ selection of implementation strategies.  Then I see references to “facilitation” apart from “training” (maybe these are meant to be interchangeable terms here?).  Anyway, on the one hand you can see that I am struggling to get past these issues to get to this week’s study design assignment, but on the other hand you should bookmark these issues for the next assignment on specifying implementation strategies.  All that aside for now, I agree with your characterization of this study design as convergent mixed-methods.  It’s not strictly observational but also not fully experimental (randomized), and there is a bit of exploratory/pilot work to be done on a training (facilitation?) intervention that is not yet fully developed.  Is the outcome focus strictly on the pilot team members' self-efficacy in delivering the training, or is there intended to be some measure of the actual influence of the training on departmental behavior re: equity?  In the final version of your concept, I’d also like to see you include more detail about the number and size of the pilot teams and the anticipated number of respondents to both the surveys and the qualitative interviews – I’m returning to David’s concern about respondent burden and also feasibility for the research team.  (It seems I am all over the map with comments this week.  Apologies for that.  I’ll look forward to seeing you further hone this concept over the next several weeks.) - Lori";s:5:"xhtml";s:2029:"Hi Sivan.  I confess, even after re-reading all the assignments, I am still struggling with understanding your project.  Your follow-up response to the previous assignment was helpful, though, in explaining that the departments you will initially be working with are fairly removed from patient interactions, so that explains the lack of patient-level outcome focus.  I am still struggling with whether training is the implementation strategy being tested, because elsewhere you refer to measuring whether/how the training influenced the pilot teams’ selection of implementation strategies.  Then I see references to “facilitation” apart from “training” (maybe these are meant to be interchangeable terms here?).  Anyway, on the one hand you can see that I am struggling to get past these issues to get to this week’s study design assignment, but on the other hand you should bookmark these issues for the next assignment on specifying implementation strategies.  All that aside for now, I agree with your characterization of this study design as convergent mixed-methods.  It’s not strictly observational but also not fully experimental (randomized), and there is a bit of exploratory/pilot work to be done on a training (facilitation?) intervention that is not yet fully developed.  Is the outcome focus strictly on the pilot team members&#039; self-efficacy in delivering the training, or is there intended to be some measure of the actual influence of the training on departmental behavior re: equity?  In the final version of your concept, I’d also like to see you include more detail about the number and size of the pilot teams and the anticipated number of respondents to both the surveys and the qualitative interviews – I’m returning to David’s concern about respondent burden and also feasibility for the research team.  (It seems I am all over the map with comments this week.  Apologies for that.  I’ll look forward to seeing you further hone this concept over the next several weeks.) - Lori";s:6:"parent";s:32:"8a3ad3593905177b4b65a6fd25e99a67";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"ac732dfdcf66fa44e1576cbdd51f8bc6";}s:32:"f1b6cf8fcd1781651cab83514c971d60";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"tlagu";s:4:"name";s:9:"Tara Lagu";s:4:"mail";s:28:"Tara.Lagu@baystatehealth.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539624882;}s:3:"raw";s:509:"Thanks, David! I will try to consider/incorporate as many of your comments as possible in my revised concept note and future grant applications. I will also (as it turns out) get my score from NIH on this topic by probably the beginning of next week-so I will also have that to consider going forward (if the score really isn't good, i might totally revamp and consider another mechanism. if it's close, I will likely try to revise more modestly with the goal of resubmitting to the same mechanism next year).";s:5:"xhtml";s:519:"Thanks, David! I will try to consider/incorporate as many of your comments as possible in my revised concept note and future grant applications. I will also (as it turns out) get my score from NIH on this topic by probably the beginning of next week-so I will also have that to consider going forward (if the score really isn&#039;t good, i might totally revamp and consider another mechanism. if it&#039;s close, I will likely try to revise more modestly with the goal of resubmitting to the same mechanism next year).";s:6:"parent";s:32:"e138965d8a1ab7903edfd52cc8824714";s:7:"replies";a:1:{i:0;s:32:"92c3cefb9ae742fabf5aaf69b36d4a18";}s:4:"show";b:1;s:3:"cid";s:32:"f1b6cf8fcd1781651cab83514c971d60";}s:32:"f8fac9dbf137bfd658a4c9a10751fd17";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"bjpowell";s:4:"name";s:12:"Byron Powell";s:4:"mail";s:16:"bjpowell@unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539707213;}s:3:"raw";s:2324:"Hi James,

Sorry for the delayed response on this. This looks good to me. I just have a couple of comments, and I think in part I am still maybe hung up on two points that I raised earlier. (Also, take this with a grain of salt if not helpful, as I know you're more familiar with ADEPT, facilitation, and the VA context than I am). With respect to the design, I think it will be really important to be explicit about your rationale for using a SMART trial and why you are selecting it vs. other alternatives. To me, this is partly tied to justifying the appropriateness of the implementation strategies and what we need to learn about them to advance the field and to improve implementation with the VA. Is it just a question of how intensive the implementation strategy needs to be? What can you learn about the specific implementation strategies you're using? What can we learn about facilitation that can be applied to other areas within the VA and beyond? (I'm curious how the VA is using facilitation broadly (i.e., is it being used both as a general implementation capacity building strategy as well as for specific interventions/disease conditions?). In my view, SMART trials seem really promising, but as I noted before, it also seems like a really large, ambitious trial out of the gate. Thus, justifying that you have sufficient data to support the specific strategies you're proposing and having a clear rationale for their inclusion seems critical, as does having a clear rationale for the design. I might suggest adding a section to your proposal that describes its advantages and provides a narrative of why it was chosen over alternatives. 

I'm glad you're using mixed methods, and just had a couple of thoughts there. I'd suggest using formal nomenclature such as that proposed by Palinkas et al. (2011) to discuss the structure, process, and function of your mixed methods design. I tend to think of convergent designs as using multiple methods to answer the same question, and it seems like you're doing that in part, but also potentially using qualitative results in other ways (e.g., you might be using them for functions such as "expansion" or "complementarity").

Good work, and again, looking forward to learning more about your project and how you're thinking about SMART designs soon! 

Best,
Byron";s:5:"xhtml";s:2434:"Hi James,<br /><br />Sorry for the delayed response on this. This looks good to me. I just have a couple of comments, and I think in part I am still maybe hung up on two points that I raised earlier. (Also, take this with a grain of salt if not helpful, as I know you&#039;re more familiar with ADEPT, facilitation, and the VA context than I am). With respect to the design, I think it will be really important to be explicit about your rationale for using a SMART trial and why you are selecting it vs. other alternatives. To me, this is partly tied to justifying the appropriateness of the implementation strategies and what we need to learn about them to advance the field and to improve implementation with the VA. Is it just a question of how intensive the implementation strategy needs to be? What can you learn about the specific implementation strategies you&#039;re using? What can we learn about facilitation that can be applied to other areas within the VA and beyond? (I&#039;m curious how the VA is using facilitation broadly (i.e., is it being used both as a general implementation capacity building strategy as well as for specific interventions/disease conditions?). In my view, SMART trials seem really promising, but as I noted before, it also seems like a really large, ambitious trial out of the gate. Thus, justifying that you have sufficient data to support the specific strategies you&#039;re proposing and having a clear rationale for their inclusion seems critical, as does having a clear rationale for the design. I might suggest adding a section to your proposal that describes its advantages and provides a narrative of why it was chosen over alternatives. <br /><br />I&#039;m glad you&#039;re using mixed methods, and just had a couple of thoughts there. I&#039;d suggest using formal nomenclature such as that proposed by Palinkas et al. (2011) to discuss the structure, process, and function of your mixed methods design. I tend to think of convergent designs as using multiple methods to answer the same question, and it seems like you&#039;re doing that in part, but also potentially using qualitative results in other ways (e.g., you might be using them for functions such as &quot;expansion&quot; or &quot;complementarity&quot;).<br /><br />Good work, and again, looking forward to learning more about your project and how you&#039;re thinking about SMART designs soon! <br /><br />Best,<br />Byron";s:6:"parent";s:32:"ac4d9460cb759bc7d97c4654882b6033";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"f8fac9dbf137bfd658a4c9a10751fd17";}s:32:"9caca5bc02ecced42f9f8d71c8a8ff2b";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"bjpowell";s:4:"name";s:12:"Byron Powell";s:4:"mail";s:16:"bjpowell@unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539708454;}s:3:"raw";s:1858:"Hi Barbara,

Forgive my delayed response to this assignment! First, I really like how you laid out the rationale for a Hybrid 2 study. It makes sense to me, though it isn't totally clear to me that a Hybrid 1 wouldn't also be appropriate given the uncertainty about the components of the intervention. If you do end up going with a Hybrid 2, I think the next question is what type of Hybrid 2? Geoff describes a couple of variants in his paper. Would you be thinking more about a design in which you randomize both the intervention and the implementation strategy, or would the implementation strategy be evaluated in a more exploratory manner while the study's primary aim focuses on establishing the effectiveness of the intervention? It would be helpful for you to provide more specifics about how you imagine this unfolding.

For the mixed methods portion, I would make the same suggestion that I mentioned to James in that it would be good to draw upon some standard nomenclature for the structure, function, and process of your mixed methods work (from Palinkas et al., 2011 or someone else like Creswell, etc.). Even creating a simple table of the type of quantitative and qualitative data that you have can be helpful in examining how the different types can be used for the different functions that Palinkas describes. Happy to talk through more if you have a chance to look at that. Another thing that I should have mentioned to James as well is that you don't necessarily have to take the designs described in those papers as rigid (i.e., you can be creative of course in combining them in more complex/flexible ways to meet the needs of your project); however, I think they provide a nice starting point. 

Really looking forward to the in person meeting to discuss further, but happy to chat beforehand if helpful. Hope you're well!

Best,
Byron";s:5:"xhtml";s:1928:"Hi Barbara,<br /><br />Forgive my delayed response to this assignment! First, I really like how you laid out the rationale for a Hybrid 2 study. It makes sense to me, though it isn&#039;t totally clear to me that a Hybrid 1 wouldn&#039;t also be appropriate given the uncertainty about the components of the intervention. If you do end up going with a Hybrid 2, I think the next question is what type of Hybrid 2? Geoff describes a couple of variants in his paper. Would you be thinking more about a design in which you randomize both the intervention and the implementation strategy, or would the implementation strategy be evaluated in a more exploratory manner while the study&#039;s primary aim focuses on establishing the effectiveness of the intervention? It would be helpful for you to provide more specifics about how you imagine this unfolding.<br /><br />For the mixed methods portion, I would make the same suggestion that I mentioned to James in that it would be good to draw upon some standard nomenclature for the structure, function, and process of your mixed methods work (from Palinkas et al., 2011 or someone else like Creswell, etc.). Even creating a simple table of the type of quantitative and qualitative data that you have can be helpful in examining how the different types can be used for the different functions that Palinkas describes. Happy to talk through more if you have a chance to look at that. Another thing that I should have mentioned to James as well is that you don&#039;t necessarily have to take the designs described in those papers as rigid (i.e., you can be creative of course in combining them in more complex/flexible ways to meet the needs of your project); however, I think they provide a nice starting point. <br /><br />Really looking forward to the in person meeting to discuss further, but happy to chat beforehand if helpful. Hope you&#039;re well!<br /><br />Best,<br />Byron";s:6:"parent";s:32:"d8cf9431d5bc816f65321d7cd2f49c00";s:7:"replies";a:1:{i:0;s:32:"91130d91269f22a6fc8820b3d586ed54";}s:4:"show";b:1;s:3:"cid";s:32:"9caca5bc02ecced42f9f8d71c8a8ff2b";}s:32:"0fe5327e80144c35e1db1b51385f10e5";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"tlagu";s:4:"name";s:9:"Tara Lagu";s:4:"mail";s:28:"Tara.Lagu@baystatehealth.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1539804974;s:8:"modified";i:1539876298;}s:3:"raw";s:3778:"Lagu-Assignment 5

If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

As mentioned by David previously (assignment 2?), we will have to rethink this for the revised concept note, with the idea that we may look at 1.) components of CR; 2.) Implementation strategies for each component; and 3.) Adaptations of these strategies for specific settings. However, the ultimate goal of the original proposed study was to identify implementation strategies employed by the high-performing hospitals that can be generalizable to other settings. While we will have to rework the way we get to this point (as above, maybe first using the interviews to identify core components and then describing strategies for each component) will use qualitative interviews, guided by the CFIR, to identify the themes and contextual factors that are common to high performers. In response to feedback on a previous assignment, I believe we should also interview low performers to assess barriers (e.g., "implementation capacity.") Then, using these themes starting point, we will use the Expert Recommendations for Implementing Change (ERIC) to compile the data into a list of strategies that can be further examined by a Delphi panel. Because of the risk that items on such a list could be inconsistently understood, we will give each strategy a definition to ensure clarity and consistency (e.g., from ERIC, “audit and provide feedback about clinicians CR referrals” would be further defined for the purposes of the Delphi panel as “Audit and Feedback: any summary about the use of CR over a specified period of time that is provided in a written, electronic, or verbal format.”). We will also follow guidelines (published by Drs. Proctor, Powell, and McMillen) for naming, defining, and operationalizing implementation strategies in terms of seven dimensions: actor, the action, action targets, temporality, dose, implementation outcomes addressed, and theoretical justification. 

How might you link specific implementation strategies to the context in which your work is set?
In order to consider how the strategies could be applied in real-world settings, we will use both the qualitative interviews and the delphi panel to identify and characterize "adaptations." While it feels somewhat subjective/difficult to define where a strategy ends and an adaptation begins, we will use our qualitative analysis and delphi panel to help define this. To further assess setting/context, we will have the delphi panel (representing a wide variety of different viewpoints-patients, clinicians, policymakers, etc), for around each identified strategy and adaption, use a set of recently-validated tools (recently published, Weiner/Powell, 2017) to assess acceptability, appropriateness, and feasibility of defined strategies (and adaptations? Not sure). The instrument defines acceptability as the “extent to which tools are agreeable,” appropriateness the “extent to which the tools are suitable for a purpose” (in this case, of recruiting patients with HF to CR), and feasibility is the “extent to which tools are practical at the system level.” Each participant on our Delphi panel of experts will be asked to rate each strategy/adaptation using an online survey. We will then create summary acceptability, appropriateness and feasibility scores, averaging ratings across items and raters, and ranking each strategy on the three constructs.  We will review ratings and comments with the full investigative team and make additional revisions for another Delphi round. Finally, we will emerge with a set of strategies that can be included in a toolkit or tested in a hybrid type 2 trial.	 

";s:5:"xhtml";s:3829:"Lagu-Assignment 5<br /><br />If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />As mentioned by David previously (assignment 2?), we will have to rethink this for the revised concept note, with the idea that we may look at 1.) components of CR; 2.) Implementation strategies for each component; and 3.) Adaptations of these strategies for specific settings. However, the ultimate goal of the original proposed study was to identify implementation strategies employed by the high-performing hospitals that can be generalizable to other settings. While we will have to rework the way we get to this point (as above, maybe first using the interviews to identify core components and then describing strategies for each component) will use qualitative interviews, guided by the CFIR, to identify the themes and contextual factors that are common to high performers. In response to feedback on a previous assignment, I believe we should also interview low performers to assess barriers (e.g., &quot;implementation capacity.&quot;) Then, using these themes starting point, we will use the Expert Recommendations for Implementing Change (ERIC) to compile the data into a list of strategies that can be further examined by a Delphi panel. Because of the risk that items on such a list could be inconsistently understood, we will give each strategy a definition to ensure clarity and consistency (e.g., from ERIC, “audit and provide feedback about clinicians CR referrals” would be further defined for the purposes of the Delphi panel as “Audit and Feedback: any summary about the use of CR over a specified period of time that is provided in a written, electronic, or verbal format.”). We will also follow guidelines (published by Drs. Proctor, Powell, and McMillen) for naming, defining, and operationalizing implementation strategies in terms of seven dimensions: actor, the action, action targets, temporality, dose, implementation outcomes addressed, and theoretical justification. <br /><br />How might you link specific implementation strategies to the context in which your work is set?<br />In order to consider how the strategies could be applied in real-world settings, we will use both the qualitative interviews and the delphi panel to identify and characterize &quot;adaptations.&quot; While it feels somewhat subjective/difficult to define where a strategy ends and an adaptation begins, we will use our qualitative analysis and delphi panel to help define this. To further assess setting/context, we will have the delphi panel (representing a wide variety of different viewpoints-patients, clinicians, policymakers, etc), for around each identified strategy and adaption, use a set of recently-validated tools (recently published, Weiner/Powell, 2017) to assess acceptability, appropriateness, and feasibility of defined strategies (and adaptations? Not sure). The instrument defines acceptability as the “extent to which tools are agreeable,” appropriateness the “extent to which the tools are suitable for a purpose” (in this case, of recruiting patients with HF to CR), and feasibility is the “extent to which tools are practical at the system level.” Each participant on our Delphi panel of experts will be asked to rate each strategy/adaptation using an online survey. We will then create summary acceptability, appropriateness and feasibility scores, averaging ratings across items and raters, and ranking each strategy on the three constructs.  We will review ratings and comments with the full investigative team and make additional revisions for another Delphi round. Finally, we will emerge with a set of strategies that can be included in a toolkit or tested in a hybrid type 2 trial.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"6dd1d3bd34b04fb1ca487df04a6a7880";}s:4:"show";b:1;s:3:"cid";s:32:"0fe5327e80144c35e1db1b51385f10e5";}s:32:"92c3cefb9ae742fabf5aaf69b36d4a18";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"tlagu";s:4:"name";s:9:"Tara Lagu";s:4:"mail";s:28:"Tara.Lagu@baystatehealth.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539875285;}s:3:"raw";s:76:"Also, thanks Lori-I will DEFINITELY talk to Kate. I appreciate the thought!
";s:5:"xhtml";s:75:"Also, thanks Lori-I will DEFINITELY talk to Kate. I appreciate the thought!";s:6:"parent";s:32:"f1b6cf8fcd1781651cab83514c971d60";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"92c3cefb9ae742fabf5aaf69b36d4a18";}s:32:"db7ae40680229fded7d04e021ab9a25c";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jpittman";s:4:"name";s:13:"James Pittman";s:4:"mail";s:17:"jpittman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539885662;}s:3:"raw";s:3385:"Pittman -  Assignment #5:

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

My project will test a multicomponent implementation strategy. Based on the Powell et al., 2015 reading I have identified several components to my strategy -- listed below. I selected the RPIW as an approach because it contains several strategies that may be important in implementation success. I also chose this strategy because the VA uses lean six sigma (and RPIW) as part of their portfolio for systems re-design. Each hospital system as a systems redesign director familiar with the RPIW process and language. This seemed like a natural fit for the VA. I have adapted the RPIW to be focused specifically on implementation of eScreening, and so it was also necessary to develop a blueprint for the systems redesign expert to follow. 

-	Develop educational materials/formal educational blueprint – we have already developed the eScreening Playbook (can be found here: http://vaww.sandiego.portal.va.gov/eScreening/SitePages/Home.aspx)  which is a blueprint for how to implement eScreening including conducting an RPIW tailored specifically for eScreening implementation. The playbook also recommends other strategies to help facilitate implementation including: capture and share local knowledge via cross site communication, conduct educational meetings, develop educational materials, and prepare patients.

-	Facilitation (Internal and External) – I propose to use the local systems redesign experts at each VA to conduct the modified RPIW using the RPIW playbook. External facilitation will be support from the eScreening team via telephone calls at a TBD dose. The details of the facilitation will need to be better clarified and detailed. 

-	The RPIW contains several discrete implementation strategies within it as well. These include:  assess for readiness and identify barriers and facilitators, organize clinician implementation team meetings, promote adaptability, and tailor strategies. 

-	Provide technical assistance – technical assistance will be available through the VA OIT. 

2.	How might you link specific implementation strategies to the context in which your work is set?

One way to link the strategies to the context of implementation site is by using the table in Proctor et al., 2013 to specify what is intended by each. The playbook/blueprint that we developed is fairly flexible in that it describes domains for consideration but it does not proscribe the activity. For example, the playbook talks about value propositions when presenting to stakeholders and provides examples, but leaves open how to tailor the message to various stakeholders. The purpose of the study is in part to identify which of the strategies work for various clinical groups. We may find that teams with certain characteristics need benefit more or less from different implementation strategies. 

3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?
 
My proposal includes selecting implementation strategies. 
";s:5:"xhtml";s:3483:"Pittman -  Assignment #5:<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />My project will test a multicomponent implementation strategy. Based on the Powell et al., 2015 reading I have identified several components to my strategy -- listed below. I selected the RPIW as an approach because it contains several strategies that may be important in implementation success. I also chose this strategy because the VA uses lean six sigma (and RPIW) as part of their portfolio for systems re-design. Each hospital system as a systems redesign director familiar with the RPIW process and language. This seemed like a natural fit for the VA. I have adapted the RPIW to be focused specifically on implementation of eScreening, and so it was also necessary to develop a blueprint for the systems redesign expert to follow. <br /><br />-	Develop educational materials/formal educational blueprint – we have already developed the eScreening Playbook (can be found here: http://vaww.sandiego.portal.va.gov/eScreening/SitePages/Home.aspx)  which is a blueprint for how to implement eScreening including conducting an RPIW tailored specifically for eScreening implementation. The playbook also recommends other strategies to help facilitate implementation including: capture and share local knowledge via cross site communication, conduct educational meetings, develop educational materials, and prepare patients.<br /><br />-	Facilitation (Internal and External) – I propose to use the local systems redesign experts at each VA to conduct the modified RPIW using the RPIW playbook. External facilitation will be support from the eScreening team via telephone calls at a TBD dose. The details of the facilitation will need to be better clarified and detailed. <br /><br />-	The RPIW contains several discrete implementation strategies within it as well. These include:  assess for readiness and identify barriers and facilitators, organize clinician implementation team meetings, promote adaptability, and tailor strategies. <br /><br />-	Provide technical assistance – technical assistance will be available through the VA OIT. <br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />One way to link the strategies to the context of implementation site is by using the table in Proctor et al., 2013 to specify what is intended by each. The playbook/blueprint that we developed is fairly flexible in that it describes domains for consideration but it does not proscribe the activity. For example, the playbook talks about value propositions when presenting to stakeholders and provides examples, but leaves open how to tailor the message to various stakeholders. The purpose of the study is in part to identify which of the strategies work for various clinical groups. We may find that teams with certain characteristics need benefit more or less from different implementation strategies. <br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /> <br />My proposal includes selecting implementation strategies.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"fb4ad14ce02e26b9fc34d65afa55bb47";}s:4:"show";b:1;s:3:"cid";s:32:"db7ae40680229fded7d04e021ab9a25c";}s:32:"91130d91269f22a6fc8820b3d586ed54";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"bjones";s:4:"name";s:13:"Barbara Jones";s:4:"mail";s:26:"Barbara.Jones@hsc.utah.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539889094;}s:3:"raw";s:727:"Thank so much for your input Byron! This is great advice. You could be right about changing it to a hybrid I. For a hybrid type II, I had thought about a cluster randomized control trial with 2x2 factorial design, with facilities randomized to the CDS intervention, then among those receiving the intervention, 2 different types of implementation, one centered around user-centered design with frequent, iterative adaptation, and one without much adaptation. Although I'm most interested in the questions surrounding the effectiveness of CDS in the first place, I feel that you could study the different implementation strategies that are taken as well. But maybe this is do-able in a hybrid I.  I will keep thinking about it! ";s:5:"xhtml";s:731:"Thank so much for your input Byron! This is great advice. You could be right about changing it to a hybrid I. For a hybrid type II, I had thought about a cluster randomized control trial with 2x2 factorial design, with facilities randomized to the CDS intervention, then among those receiving the intervention, 2 different types of implementation, one centered around user-centered design with frequent, iterative adaptation, and one without much adaptation. Although I&#039;m most interested in the questions surrounding the effectiveness of CDS in the first place, I feel that you could study the different implementation strategies that are taken as well. But maybe this is do-able in a hybrid I.  I will keep thinking about it!";s:6:"parent";s:32:"9caca5bc02ecced42f9f8d71c8a8ff2b";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"91130d91269f22a6fc8820b3d586ed54";}s:32:"b62c85e1a1f2df794e6ffa746e2489c8";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"bjones";s:4:"name";s:13:"Barbara Jones";s:4:"mail";s:26:"Barbara.Jones@hsc.utah.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539889152;}s:3:"raw";s:4032:"Jones - Assignment #5:
1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?
- Our study is a multi-site implementation of an evidence-based computerized clinical decision support (CDS) tool for pneumonia. While one could consider the CDS as an implementation strategy for guideline/standard practice in pneumonia, as I’m currently thinking about it, I think the CDS fits better as the intervention itself. I have been trying to intersect the principles of user-centered design of CDS as kind of an over-arching implementation strategy, and considering whether this is something that I could test in a hybrid I or II design (ie, VA’s could be randomized to either having CDS implemented in a user-centered design process, versus receiving the CDS “as is” with minimal adaptation but lots of technical support and facilitation). What do you think?
I really liked how Dr. Adsul recommended matching selection strategies to determinants of successful implementation, so I have tried that below. I am still a bit unclear about how to measure them.
1)	Knowledge: most physicians are not expecting the existence of diagnosis-specific, EHR-embedded CDS across the VA, especially rural settings. → *Identify and prepare clinical champions at each site, *promote network weaving, embed the CDS into their EHR workflow to make it easy to use(I was looking for a term for about behavioral economics strategies for this one?), and use the *train-the-trainer strategies to raise awareness. 
2)	Motivation/Relative advantage/tension for change: This is big determinant. Usable CDS is still kind of new to physicians, and most feel they do just fine without it. →*Audit and feedback prior to the implementation on facility- and provider-specific process measures for pneumonia seems to work at my pilot VA site, especially for antibiotic prescribing. This *engages not only the end-user providers but also the *clinical leaders (who pay attention to facility-level metrics such as N patient tested/treated for influenza, documentation of severity-of-illness, overall rates of broad-spectrum antibiotics). 
3)	Beliefs/attitudes: Physicians and facility leaders not involved in the components of a guideline-based CDS for pneumonia might not believe that it will fit with their patients/processes/setting. → *Promote adaptability; *build a coalition through pre-implementation stakeholder interviews and audit-and-feedback; *conduct local consensus discussions; *identify and prepare champions.
4)	System of Care:  this is another big one. Each VAMC usually has to program their own decision support (the pitfall of customizability), and lot of smaller VA settings have fewer human resources (clinical applications coordinators- CAC’s-  who program local decision support tools). → *Develop resource-sharing agreements and *centralize & provide technical assistance (Sites will have full access to a centralized CAC, and they will work with local CACs as well). We will *change (and centralize!) record systems through the CDS to make audit-and-feedback more useful and relevant. 
2.	How might you link specific implementation strategies to the context in which your work is set?
- I hope I am understanding the word “context” to be meant as setting here? I think the rural facilities will have very different patterns of determinants. They may have different attitudes toward relative advantage/tension for change, patterns of adoption (perhaps more based upon interpersonal connections) and different needs more technical assistance. 

3.	If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?
N/A – this will be a prospective, cluster randomized trial.
";s:5:"xhtml";s:4095:"Jones - Assignment #5:<br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br />- Our study is a multi-site implementation of an evidence-based computerized clinical decision support (CDS) tool for pneumonia. While one could consider the CDS as an implementation strategy for guideline/standard practice in pneumonia, as I’m currently thinking about it, I think the CDS fits better as the intervention itself. I have been trying to intersect the principles of user-centered design of CDS as kind of an over-arching implementation strategy, and considering whether this is something that I could test in a hybrid I or II design (ie, VA’s could be randomized to either having CDS implemented in a user-centered design process, versus receiving the CDS “as is” with minimal adaptation but lots of technical support and facilitation). What do you think?<br />I really liked how Dr. Adsul recommended matching selection strategies to determinants of successful implementation, so I have tried that below. I am still a bit unclear about how to measure them.<br />1)	Knowledge: most physicians are not expecting the existence of diagnosis-specific, EHR-embedded CDS across the VA, especially rural settings. → *Identify and prepare clinical champions at each site, *promote network weaving, embed the CDS into their EHR workflow to make it easy to use(I was looking for a term for about behavioral economics strategies for this one?), and use the *train-the-trainer strategies to raise awareness. <br />2)	Motivation/Relative advantage/tension for change: This is big determinant. Usable CDS is still kind of new to physicians, and most feel they do just fine without it. →*Audit and feedback prior to the implementation on facility- and provider-specific process measures for pneumonia seems to work at my pilot VA site, especially for antibiotic prescribing. This *engages not only the end-user providers but also the *clinical leaders (who pay attention to facility-level metrics such as N patient tested/treated for influenza, documentation of severity-of-illness, overall rates of broad-spectrum antibiotics). <br />3)	Beliefs/attitudes: Physicians and facility leaders not involved in the components of a guideline-based CDS for pneumonia might not believe that it will fit with their patients/processes/setting. → *Promote adaptability; *build a coalition through pre-implementation stakeholder interviews and audit-and-feedback; *conduct local consensus discussions; *identify and prepare champions.<br />4)	System of Care:  this is another big one. Each VAMC usually has to program their own decision support (the pitfall of customizability), and lot of smaller VA settings have fewer human resources (clinical applications coordinators- CAC’s-  who program local decision support tools). → *Develop resource-sharing agreements and *centralize &amp; provide technical assistance (Sites will have full access to a centralized CAC, and they will work with local CACs as well). We will *change (and centralize!) record systems through the CDS to make audit-and-feedback more useful and relevant. <br />2.	How might you link specific implementation strategies to the context in which your work is set?<br />- I hope I am understanding the word “context” to be meant as setting here? I think the rural facilities will have very different patterns of determinants. They may have different attitudes toward relative advantage/tension for change, patterns of adoption (perhaps more based upon interpersonal connections) and different needs more technical assistance. <br /><br />3.	If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br />N/A – this will be a prospective, cluster randomized trial.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"7b35db4ad76b71e3170faf77dea2c8b8";}s:4:"show";b:1;s:3:"cid";s:32:"b62c85e1a1f2df794e6ffa746e2489c8";}s:32:"225778658174adb8246efe8541cc0a6f";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"sshohat";s:4:"name";s:20:"Sivan Spitzer-Shohat";s:4:"mail";s:30:"sivan.spitzer-shohat@biu.ac.il";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539958719;}s:3:"raw";s:4209:"Spitzer-Shohat Assignment #5:

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?
The goal of the project is to facilitate implementation of equity throughout UChicago Medicine as it aims to become an equitable organization. The first phase of our research illuminated the chasm between the organization’s implementation strategy of cultural competence training and implementation of an equity lens to daily organizational processes. This gap occurred because employees did not know how to translate equity from value to action, reiterating that while they understood the importance of equity they just didn’t know what to do. 
In phase two, we are proposing sequential implementation strategies to assist in the facilitation of equity throughout the organization through:
a.	The development of equity translation training based on organizational sense making processes - Sensemaking occurs when people are presented with ambiguous events. As sensemaking is an organizational process that is not tied to a specific procedure, process or clinical outcome, it can equip people who undergo the training with the skill of how to translate new concepts to differential organizational contexts. The training will provide a sensemaking model for understanding the relevance of equity to different situations, a framework for information gathering, how to evaluate this information, and the construction and appraisal of alternative courses of action. A generalized mental model of organizational sensemaking will be used together with case-based learning on equity translation using examples from the organization such as how an equity lens was applied to the work of the analytics department's predictive algorithm work process. The aim of the one-day training is that trainees will be able to become facilitators and guide different organizational departments in the implementation of an equity lens to everyday practices. The training module is currently in development. 
b.	Training 25 Diversity and Equity committee members, the organizational governance committee, who have volunteered to take part in 5 ‘pilot implementation’ teams - The reason for creating these teams is to move the governance committee from awareness to action by making members active partners in the implementation process. Team members, representing different departments and positions ranging from mid to executive management, will be trained on organizational sensemaking strategies. Pilot teams will have ongoing mentoring by a member from the Diversity and Equity department that will be assigned to each of the teams.
c.	Facilitation will be used to promote implementation of equity throughout the organization. Each of the pilot teams will work with one of the organization’s cross-cutting departments. Using the organizational sense making model they will assist departments in identifying how equity is relevant to the department’s work processes, devising an implementation plan, and implementing equity to everyday work processes. 

2.	How might you link specific implementation strategies to the context in which your work is set?
The implementation strategies selected consider different aspects of the organizational context. Training on organizational sensemaking models aims to solve not only the problem of how to translate an ambiguous concept, but how to do so in multiple organizational contexts that are often removed from direct patient care.
Facilitation was selected as an implementation strategy due to the organization’s complexity. With over 11,000 employees creating organization-wide implementation requires a strategy to permeate equity. We believe that there is dual benefit in training ‘pilot implementation’ teams as facilitators. One benefit is that these teams will help in advancing implementation of equity in the departments they will work with. We are also hoping that as a result of training, team members, who hold managerial position throughout the organization, will be able to identify how equity is relevant to work processes in their own departments and suggest changes.  
";s:5:"xhtml";s:4266:"Spitzer-Shohat Assignment #5:<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br />The goal of the project is to facilitate implementation of equity throughout UChicago Medicine as it aims to become an equitable organization. The first phase of our research illuminated the chasm between the organization’s implementation strategy of cultural competence training and implementation of an equity lens to daily organizational processes. This gap occurred because employees did not know how to translate equity from value to action, reiterating that while they understood the importance of equity they just didn’t know what to do. <br />In phase two, we are proposing sequential implementation strategies to assist in the facilitation of equity throughout the organization through:<br />a.	The development of equity translation training based on organizational sense making processes - Sensemaking occurs when people are presented with ambiguous events. As sensemaking is an organizational process that is not tied to a specific procedure, process or clinical outcome, it can equip people who undergo the training with the skill of how to translate new concepts to differential organizational contexts. The training will provide a sensemaking model for understanding the relevance of equity to different situations, a framework for information gathering, how to evaluate this information, and the construction and appraisal of alternative courses of action. A generalized mental model of organizational sensemaking will be used together with case-based learning on equity translation using examples from the organization such as how an equity lens was applied to the work of the analytics department&#039;s predictive algorithm work process. The aim of the one-day training is that trainees will be able to become facilitators and guide different organizational departments in the implementation of an equity lens to everyday practices. The training module is currently in development. <br />b.	Training 25 Diversity and Equity committee members, the organizational governance committee, who have volunteered to take part in 5 ‘pilot implementation’ teams - The reason for creating these teams is to move the governance committee from awareness to action by making members active partners in the implementation process. Team members, representing different departments and positions ranging from mid to executive management, will be trained on organizational sensemaking strategies. Pilot teams will have ongoing mentoring by a member from the Diversity and Equity department that will be assigned to each of the teams.<br />c.	Facilitation will be used to promote implementation of equity throughout the organization. Each of the pilot teams will work with one of the organization’s cross-cutting departments. Using the organizational sense making model they will assist departments in identifying how equity is relevant to the department’s work processes, devising an implementation plan, and implementing equity to everyday work processes. <br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br />The implementation strategies selected consider different aspects of the organizational context. Training on organizational sensemaking models aims to solve not only the problem of how to translate an ambiguous concept, but how to do so in multiple organizational contexts that are often removed from direct patient care.<br />Facilitation was selected as an implementation strategy due to the organization’s complexity. With over 11,000 employees creating organization-wide implementation requires a strategy to permeate equity. We believe that there is dual benefit in training ‘pilot implementation’ teams as facilitators. One benefit is that these teams will help in advancing implementation of equity in the departments they will work with. We are also hoping that as a result of training, team members, who hold managerial position throughout the organization, will be able to identify how equity is relevant to work processes in their own departments and suggest changes.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"a492c8c311cc04dfbf038a030a8fa283";}s:4:"show";b:1;s:3:"cid";s:32:"225778658174adb8246efe8541cc0a6f";}s:32:"953c9bffc416269b9143686df6f8a08b";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"kfisher";s:4:"name";s:15:"Kimberly Fisher";s:4:"mail";s:33:"Kimberly.Fisher@umassmemorial.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539971100;}s:3:"raw";s:2434:"Assignment #5 - Fisher

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

This is not entirely relevant to my proposed project because it is still in the formative stages.  The current work is focused on understanding and describing the care transition processes of high-performing hospitals.  This approach could be conceptualized as “capture and share local knowledge”, using the lens high-performers.  

Using the findings from this stage, we will characterize the process and context of high performers and intend to develop a set of tools/products that can be used to help other hospitals to: 1) assess how their practice compares to the “idealized” care transition process created through our study of positive deviants, 
2) identify gaps in their practice compared to an idealized care transition, and 3) put in place processes to address and close gaps identified.  How the third point is achieved will depend on the tools we develop and what the gaps and barriers are.  Ideally, the strategies for closing these gaps will be matched to what the gap is and the barriers, and possibly could be selected from a menu of a suite of tools, consistent with the strategy of “promoting adaptability”.  

2.	How might you link specific implementation strategies to the context in which your work is set?

I’m not sure I have a specific answer for this, other than to say that the strategies will need to be matched to what the need/barrier is for a specific hospital.  For example, if a given care transition process is lacking in a post-discharge component, an intervention strategy of educating the staff on medication reconciliation would not be appropriate or linked to the context.  

3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?

This is not applicable to my proposed study, but I think one area it is relevant will be capturing how many components or elements of an ideal care transition process are in place at each hospital at the end of the intervention and whether that is associated with care transition performance.  

";s:5:"xhtml";s:2505:"Assignment #5 - Fisher<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />This is not entirely relevant to my proposed project because it is still in the formative stages.  The current work is focused on understanding and describing the care transition processes of high-performing hospitals.  This approach could be conceptualized as “capture and share local knowledge”, using the lens high-performers.  <br /><br />Using the findings from this stage, we will characterize the process and context of high performers and intend to develop a set of tools/products that can be used to help other hospitals to: 1) assess how their practice compares to the “idealized” care transition process created through our study of positive deviants, <br />2) identify gaps in their practice compared to an idealized care transition, and 3) put in place processes to address and close gaps identified.  How the third point is achieved will depend on the tools we develop and what the gaps and barriers are.  Ideally, the strategies for closing these gaps will be matched to what the gap is and the barriers, and possibly could be selected from a menu of a suite of tools, consistent with the strategy of “promoting adaptability”.  <br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />I’m not sure I have a specific answer for this, other than to say that the strategies will need to be matched to what the need/barrier is for a specific hospital.  For example, if a given care transition process is lacking in a post-discharge component, an intervention strategy of educating the staff on medication reconciliation would not be appropriate or linked to the context.  <br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /><br />This is not applicable to my proposed study, but I think one area it is relevant will be capturing how many components or elements of an ideal care transition process are in place at each hospital at the end of the intervention and whether that is associated with care transition performance.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"c688b656051e0e043fe0f14f14e30047";}s:4:"show";b:1;s:3:"cid";s:32:"953c9bffc416269b9143686df6f8a08b";}s:32:"32bde823e3d45894621f2aadf89dc64a";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"kboockvar";s:4:"name";s:16:"Kenneth Boockvar";s:4:"mail";s:25:"kenneth.boockvar@mssm.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539984723;}s:3:"raw";s:3641:"Boockvar-Assignment #5:

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

We selected implementation strategies based on theory, evidence, and stakeholder perspectives, resembling an “Intervention Mapping” approach.  We developed a model of adverse events from lapses in information exchange (theory) and mapped interventions to it that are accepted as effective (electronic notification with/without a geriatrics care transitions intervention).  In prior work we have collected stakeholder perspectives through interviews with healthcare managers and personnel across several disciplines (MD, nurse, pharmacy, informatics).  We will be focusing on the following implementation strategies:  1) assessing barriers and facilitators by interviewing recipients of the electronic notification and conducting qualitative analysis of interview transcripts, 2) capturing and sharing knowledge across the 2 study sites through every other week phone calls, in which site teams share strategies for overcoming barriers to implementing electronic health information exchange and the transitions intervention, 3) centralizing technical assistance in which all database management that supports the evaluation is provided by a database programmer at one of the 2 sites, 4) facilitation of relay of clinical data to VA providers through real-time electronic notification when patients experience a hospital encounter outside the VA system, 5) preparing patients to be active participants by implementing a geriatrics care transitions intervention which is designed to teach patients strategies for pro-actively coordinating their care between systems (Coleman Care Transitions Intervention model), 6) obtaining patient feedback by asking them open- and closed-ended questions about the perceived impact of and acceptance of the sharing of their health information across systems, 7) tailoring notification and care transitions strategies to the needs and capabilities of each of the 2 sites while at the same time ensuring fidelity to the core aspects of the interventions, and 8) using an implementation advisor via regular check-ins with the Coleman Care Transitions dissemination group.

2.	How might you link specific implementation strategies to the context in which your work is set?

Several of the implementation strategies are linked to the context of VA health care.  For example, centralizing technical assistance across distant geographic sites (Bronx and Indianapolis VAs) is possible because the VA can share files and applications securely nationwide across its sites.  The strategy of electronic facilitation of relay of clinical data is linked to the VA’s recent initiatives to enable bidirectional VA-non-VA electronic data exchange.  The strategy of preparing patients to be active participants addresses prior experience that suggests that many VA patients are not pro-active in managing their own care.  Finally, the strategy of assessing barriers and facilitators will identify not only barriers and facilitators that are broadly applicable across most healthcare contexts but also those that are specific to the VA context.

3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?

Not applicable since we are selecting implementation strategies.
";s:5:"xhtml";s:3700:"Boockvar-Assignment #5:<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />We selected implementation strategies based on theory, evidence, and stakeholder perspectives, resembling an “Intervention Mapping” approach.  We developed a model of adverse events from lapses in information exchange (theory) and mapped interventions to it that are accepted as effective (electronic notification with/without a geriatrics care transitions intervention).  In prior work we have collected stakeholder perspectives through interviews with healthcare managers and personnel across several disciplines (MD, nurse, pharmacy, informatics).  We will be focusing on the following implementation strategies:  1) assessing barriers and facilitators by interviewing recipients of the electronic notification and conducting qualitative analysis of interview transcripts, 2) capturing and sharing knowledge across the 2 study sites through every other week phone calls, in which site teams share strategies for overcoming barriers to implementing electronic health information exchange and the transitions intervention, 3) centralizing technical assistance in which all database management that supports the evaluation is provided by a database programmer at one of the 2 sites, 4) facilitation of relay of clinical data to VA providers through real-time electronic notification when patients experience a hospital encounter outside the VA system, 5) preparing patients to be active participants by implementing a geriatrics care transitions intervention which is designed to teach patients strategies for pro-actively coordinating their care between systems (Coleman Care Transitions Intervention model), 6) obtaining patient feedback by asking them open- and closed-ended questions about the perceived impact of and acceptance of the sharing of their health information across systems, 7) tailoring notification and care transitions strategies to the needs and capabilities of each of the 2 sites while at the same time ensuring fidelity to the core aspects of the interventions, and 8) using an implementation advisor via regular check-ins with the Coleman Care Transitions dissemination group.<br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />Several of the implementation strategies are linked to the context of VA health care.  For example, centralizing technical assistance across distant geographic sites (Bronx and Indianapolis VAs) is possible because the VA can share files and applications securely nationwide across its sites.  The strategy of electronic facilitation of relay of clinical data is linked to the VA’s recent initiatives to enable bidirectional VA-non-VA electronic data exchange.  The strategy of preparing patients to be active participants addresses prior experience that suggests that many VA patients are not pro-active in managing their own care.  Finally, the strategy of assessing barriers and facilitators will identify not only barriers and facilitators that are broadly applicable across most healthcare contexts but also those that are specific to the VA context.<br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /><br />Not applicable since we are selecting implementation strategies.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"32bde823e3d45894621f2aadf89dc64a";}s:32:"c688b656051e0e043fe0f14f14e30047";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lducharme";s:4:"name";s:13:"Lori Ducharme";s:4:"mail";s:21:"lori.ducharme@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1540596223;s:8:"modified";i:1540681058;}s:3:"raw";s:1647:"Hi Kimberly,  

I appreciate you thinking through all of these assignments even though they may not all equally apply to the stage of your project that you are at right now.  I hope that the TIDIRH resources are giving you a head start on thinking about implementation strategies when you’re ready to move into that phase of your project.

Obviously there’s a lot riding on your initial work with the high performing hospitals!  I am curious whether you’ll emerge with what is essentially a list of things those hospitals do that result in effective care transitions, or if you also need to identify specific strategies the under-performers can use to help them accomplish those things -- especially if some of those strategies are unique to particular hospitals. For instance, if High Performing Hospital says that the key to their success is that they have a dedicated care transitions team, your toolkit will need to say more than just "Get a dedicated care transitions team," but also offer concrete steps for doing that.  In other words, will your toolkit say “if you have X deficit, do Y” or will it need to say “if you have X deficit, you should do Y, and here is a strategy for doing Y”?  (And where will those strategies come from?)  If it’s the latter, those are also part of your implementation strategy (the toolkit + strategies for doing what's recommended in the toolkit).  At some point I know it feels like turtles all the way down, but just want to be sure your formative work will set you up for success in the next phase.

Looking forward to reading your revised concept and to seeing you here in December.

Lori";s:5:"xhtml";s:1702:"Hi Kimberly,  <br /><br />I appreciate you thinking through all of these assignments even though they may not all equally apply to the stage of your project that you are at right now.  I hope that the TIDIRH resources are giving you a head start on thinking about implementation strategies when you’re ready to move into that phase of your project.<br /><br />Obviously there’s a lot riding on your initial work with the high performing hospitals!  I am curious whether you’ll emerge with what is essentially a list of things those hospitals do that result in effective care transitions, or if you also need to identify specific strategies the under-performers can use to help them accomplish those things -- especially if some of those strategies are unique to particular hospitals. For instance, if High Performing Hospital says that the key to their success is that they have a dedicated care transitions team, your toolkit will need to say more than just &quot;Get a dedicated care transitions team,&quot; but also offer concrete steps for doing that.  In other words, will your toolkit say “if you have X deficit, do Y” or will it need to say “if you have X deficit, you should do Y, and here is a strategy for doing Y”?  (And where will those strategies come from?)  If it’s the latter, those are also part of your implementation strategy (the toolkit + strategies for doing what&#039;s recommended in the toolkit).  At some point I know it feels like turtles all the way down, but just want to be sure your formative work will set you up for success in the next phase.<br /><br />Looking forward to reading your revised concept and to seeing you here in December.<br /><br />Lori";s:6:"parent";s:32:"953c9bffc416269b9143686df6f8a08b";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"c688b656051e0e043fe0f14f14e30047";}s:32:"fb4ad14ce02e26b9fc34d65afa55bb47";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dgoodrich";s:4:"name";s:14:"David Goodrich";s:4:"mail";s:22:"David.Goodrich2@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540660342;}s:3:"raw";s:6455:"Hi James, your proposal is really moving along! My feedback on this assignment is going to focus on the practical and theoretical linkages between the implementation strategies and your intervention.

I like the use of the RPIW because it is a multi-component strategy that enables sites to evaluate how they are currently screening for mental health issues and aims to develop an intervention strategy that works for the stakeholders at the facility. One potential problem with proposing this strategy is that many systems redesign engineers/teams at individual facilities may have a long cue of projects to work with and your initiative may need to wait in line while priority projects for a VAMC's leadership go before e-screening. 

Have you explored whether the systems redesign engineers want or are able to be internal facilitators? RPIW's usually occur at the beginning of a project but are led by a clinical team leader who champions the project. Relatedly, it seems like an e-screening project would or should be championed by someone in the Mental Health Service or PC-MHI team who is familiar with the clinical issues of screening and monitoring mental health disorders. I am not sure the systems redesign engineer or their employees view it to be their role to lead such initiatives rather than to provide the technical support and guidance to enable a clinical champion to lead a multidisciplinary team to make the process or system redesign. 

This takes us back to Byron's earlier comments that it is not clear exactly what the IF and EF personnel are doing and how they interact with each other. Both roles are supposed to address specific barriers to e-screening that have been identified in prior work. Reviewers want to understand that implementation resources are being used efficiently and cost-effectively. These roles are not coherently operationalized from either a conceptual or practical perspective particularly with respect to a SMART design. I would encourage you to read up on the necessary data you need to determine your power for a SMART design (see work by Susan Murphy, Danny Almirall, Billi Nahum-Shani, and Shawna Smith at the University of Michigan/Harvard. I would add that Danny and Shawna would be happy to consult with you on using an adaptive design because that's what they been funded to do by NIH).

What troubles me is the lack of integration between relevant national Offices championing this rollout of e-screening, VISN level leaders, and the local facilities. Usually, external facilitators float between these organizational levels of influence and work with IFs to develop feasible solutions to implementation barriers that are identified regionally or locally. However, how do the EF's identify low performing sites and what strategies will they use to help the IF remediate an underperforming site? How often will EF's interact with sites, is there a community of practice (e.g., virtual learning collaborative) to share implementation successes, failures, and educational/technical information, and what is the system for monitoring implementation and e-screening uptake? More precisely, it is not evident that the project incorporates a performance monitoring system (i.e., audit and feedback) to assess the progress of implementation and the effectiveness of e-screening on clinical outcomes of interest to relevant clinical stakeholders. is there an accountability mechanism from Central Office that EFs can use to motivate local implementation teams that have been slow to implement the screening? 

While e-screening sounds fairly automated and straightforward, this is about changing work flows, clinical processes and motivating people to see the advantage of this innovation. In other words, this is organizational behavior change and the EFs and IFs need specific skills, strategies, and information to be successful in implementing this complex intervention at scale. I would review some protocol papers from VA studies that have successfully used QUERI Practice Facilitation and zero in on specification of the roles and activities to not only explain what these people will be doing but why. VA grant reviewers are increasingly reluctant to fund implementation studies if the implementation strategies are not well specified. RPIWs and Practice Facilitation are multicomponent implementation strategies that are frequently used as "black box" strategies to the detriment of the grant proposal. Strong grants proposals succinctly operationalize anticipated discrete strategies used by facilitators across phases of the implementation process which indicates to reviewers that a research team actually possesses the relevant knowledge about the intervention and implementation strategies to carry out the grant.  

What's increasingly clear in the literature is that education and training strategies like the playbook (toolkit?) are insufficient  to implement complex interventions. Traditional quality improvement strategies guided by Lean or 6 Sigma (RPIW) are often only as effective as the QI project leader's skills to change the interpersonal connections between disciplines of the stakeholders involved to fundamentally alter the way the "system" functions. Both strategies are sufficient to get the initiative going but the real goal of an implementation trial is to use methods that build the change capacity to address the anticipated barriers and solutions (implementation strategies) that are important once a plan to start local implementation is initiated. Facilitators are basically coaches to this organizational change process. In addition to looking at protocol papers to look at how your implementation strategies have been operationalized by prior studies, you might look to barriers identified in prior mental health screening initiatives that involved technology and then link how your strategies will address these determinants. 

James, please forgive me for a long-winded answer but its something I spend a lot of time thinking about. The Center for Evaluation and Implementation Resources was created as a resource to VA operations and researchers to support the effective use of implementation strategies. In short, you have a resource to go to offline after TIDIRH to get additional feedback and advice to support your research proposals! Please feel free to follow up if you have questions or desire introductions to Danny and Shawna. 

Best regards,

David
";s:5:"xhtml";s:6604:"Hi James, your proposal is really moving along! My feedback on this assignment is going to focus on the practical and theoretical linkages between the implementation strategies and your intervention.<br /><br />I like the use of the RPIW because it is a multi-component strategy that enables sites to evaluate how they are currently screening for mental health issues and aims to develop an intervention strategy that works for the stakeholders at the facility. One potential problem with proposing this strategy is that many systems redesign engineers/teams at individual facilities may have a long cue of projects to work with and your initiative may need to wait in line while priority projects for a VAMC&#039;s leadership go before e-screening. <br /><br />Have you explored whether the systems redesign engineers want or are able to be internal facilitators? RPIW&#039;s usually occur at the beginning of a project but are led by a clinical team leader who champions the project. Relatedly, it seems like an e-screening project would or should be championed by someone in the Mental Health Service or PC-MHI team who is familiar with the clinical issues of screening and monitoring mental health disorders. I am not sure the systems redesign engineer or their employees view it to be their role to lead such initiatives rather than to provide the technical support and guidance to enable a clinical champion to lead a multidisciplinary team to make the process or system redesign. <br /><br />This takes us back to Byron&#039;s earlier comments that it is not clear exactly what the IF and EF personnel are doing and how they interact with each other. Both roles are supposed to address specific barriers to e-screening that have been identified in prior work. Reviewers want to understand that implementation resources are being used efficiently and cost-effectively. These roles are not coherently operationalized from either a conceptual or practical perspective particularly with respect to a SMART design. I would encourage you to read up on the necessary data you need to determine your power for a SMART design (see work by Susan Murphy, Danny Almirall, Billi Nahum-Shani, and Shawna Smith at the University of Michigan/Harvard. I would add that Danny and Shawna would be happy to consult with you on using an adaptive design because that&#039;s what they been funded to do by NIH).<br /><br />What troubles me is the lack of integration between relevant national Offices championing this rollout of e-screening, VISN level leaders, and the local facilities. Usually, external facilitators float between these organizational levels of influence and work with IFs to develop feasible solutions to implementation barriers that are identified regionally or locally. However, how do the EF&#039;s identify low performing sites and what strategies will they use to help the IF remediate an underperforming site? How often will EF&#039;s interact with sites, is there a community of practice (e.g., virtual learning collaborative) to share implementation successes, failures, and educational/technical information, and what is the system for monitoring implementation and e-screening uptake? More precisely, it is not evident that the project incorporates a performance monitoring system (i.e., audit and feedback) to assess the progress of implementation and the effectiveness of e-screening on clinical outcomes of interest to relevant clinical stakeholders. is there an accountability mechanism from Central Office that EFs can use to motivate local implementation teams that have been slow to implement the screening? <br /><br />While e-screening sounds fairly automated and straightforward, this is about changing work flows, clinical processes and motivating people to see the advantage of this innovation. In other words, this is organizational behavior change and the EFs and IFs need specific skills, strategies, and information to be successful in implementing this complex intervention at scale. I would review some protocol papers from VA studies that have successfully used QUERI Practice Facilitation and zero in on specification of the roles and activities to not only explain what these people will be doing but why. VA grant reviewers are increasingly reluctant to fund implementation studies if the implementation strategies are not well specified. RPIWs and Practice Facilitation are multicomponent implementation strategies that are frequently used as &quot;black box&quot; strategies to the detriment of the grant proposal. Strong grants proposals succinctly operationalize anticipated discrete strategies used by facilitators across phases of the implementation process which indicates to reviewers that a research team actually possesses the relevant knowledge about the intervention and implementation strategies to carry out the grant.  <br /><br />What&#039;s increasingly clear in the literature is that education and training strategies like the playbook (toolkit?) are insufficient  to implement complex interventions. Traditional quality improvement strategies guided by Lean or 6 Sigma (RPIW) are often only as effective as the QI project leader&#039;s skills to change the interpersonal connections between disciplines of the stakeholders involved to fundamentally alter the way the &quot;system&quot; functions. Both strategies are sufficient to get the initiative going but the real goal of an implementation trial is to use methods that build the change capacity to address the anticipated barriers and solutions (implementation strategies) that are important once a plan to start local implementation is initiated. Facilitators are basically coaches to this organizational change process. In addition to looking at protocol papers to look at how your implementation strategies have been operationalized by prior studies, you might look to barriers identified in prior mental health screening initiatives that involved technology and then link how your strategies will address these determinants. <br /><br />James, please forgive me for a long-winded answer but its something I spend a lot of time thinking about. The Center for Evaluation and Implementation Resources was created as a resource to VA operations and researchers to support the effective use of implementation strategies. In short, you have a resource to go to offline after TIDIRH to get additional feedback and advice to support your research proposals! Please feel free to follow up if you have questions or desire introductions to Danny and Shawna. <br /><br />Best regards,<br /><br />David";s:6:"parent";s:32:"db7ae40680229fded7d04e021ab9a25c";s:7:"replies";a:1:{i:0;s:32:"6c7cc70f27b4b626c86763fac54d04ac";}s:4:"show";b:1;s:3:"cid";s:32:"fb4ad14ce02e26b9fc34d65afa55bb47";}s:32:"7b35db4ad76b71e3170faf77dea2c8b8";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dgoodrich";s:4:"name";s:14:"David Goodrich";s:4:"mail";s:22:"David.Goodrich2@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540666030;}s:3:"raw";s:2625:"Hi Barbara,

I really like how you mapped your strategy to known determinants (barriers and solutions) to electronic clinical decision support interventions! In the adoption and spread phases, it's clear that electronic interventions need to be perceived as value-added and evidence-based. Your user center design approach does seem to be an implementation strategy because if the CDS is not simple and easy to use, physicians may resist adopting it. Additionally, the intervention does need to seamlessly integrate into added work flows without adding too much extra time to clinical encounters and decision-making. UCD seems to be a critical adoption phase strategy that is iterative and cyclical before it can be considered complete. You might think about how you get to a satisfactory level of usability, acceptability, and perceived value or satisfaction among end users as an implementation target for adapting the CDS to a site?  A couple additional questions come to mind as I read your summary:

* Have you considered any kind of knowledge sharing implementation strategy to share solutions, best practices and adaptations? (e.g., community of practice, virtual break through collaborative). See some of the infection control initiatives led by hospitalists and nurses in VHA/National Center for Patient Safety.
* It sounds like the centralized technical assistance will be your team and a CAC? You mention facilitation which begs the question who will be doing this facilitation and what discrete strategies would you anticipate them doing with sites?
* Aside of motivating physicians do adopt and utilize the CDS to prevent complicated and serious cases of pneumonia, do you have top down support for your CDS implementation from a national program office to foster accountability and extrinsic motivation among participating ED teams?
* Assuming all goes well following initial deployment and adoption by sites; what are the strategies that will need to be in place to ensure sustainability and long-term integration with routine care? In other words, how will you market and educate/train new staff and trainees in the face of high turnover in some facilities? 
* Also, do you have an aspirational aim for improvement over standard care that will help motivate physicians/staff as they receive A&F from your team or other source after your trial is over?

Overall, things seem to be coming together with your project. Please feel free to reach out VA CEIR (my office) if you have further questions about implementation strategies and evaluation considerations for this or future proposals. 

Warm regards,

David";s:5:"xhtml";s:2704:"Hi Barbara,<br /><br />I really like how you mapped your strategy to known determinants (barriers and solutions) to electronic clinical decision support interventions! In the adoption and spread phases, it&#039;s clear that electronic interventions need to be perceived as value-added and evidence-based. Your user center design approach does seem to be an implementation strategy because if the CDS is not simple and easy to use, physicians may resist adopting it. Additionally, the intervention does need to seamlessly integrate into added work flows without adding too much extra time to clinical encounters and decision-making. UCD seems to be a critical adoption phase strategy that is iterative and cyclical before it can be considered complete. You might think about how you get to a satisfactory level of usability, acceptability, and perceived value or satisfaction among end users as an implementation target for adapting the CDS to a site?  A couple additional questions come to mind as I read your summary:<br /><br />* Have you considered any kind of knowledge sharing implementation strategy to share solutions, best practices and adaptations? (e.g., community of practice, virtual break through collaborative). See some of the infection control initiatives led by hospitalists and nurses in VHA/National Center for Patient Safety.<br />* It sounds like the centralized technical assistance will be your team and a CAC? You mention facilitation which begs the question who will be doing this facilitation and what discrete strategies would you anticipate them doing with sites?<br />* Aside of motivating physicians do adopt and utilize the CDS to prevent complicated and serious cases of pneumonia, do you have top down support for your CDS implementation from a national program office to foster accountability and extrinsic motivation among participating ED teams?<br />* Assuming all goes well following initial deployment and adoption by sites; what are the strategies that will need to be in place to ensure sustainability and long-term integration with routine care? In other words, how will you market and educate/train new staff and trainees in the face of high turnover in some facilities? <br />* Also, do you have an aspirational aim for improvement over standard care that will help motivate physicians/staff as they receive A&amp;F from your team or other source after your trial is over?<br /><br />Overall, things seem to be coming together with your project. Please feel free to reach out VA CEIR (my office) if you have further questions about implementation strategies and evaluation considerations for this or future proposals. <br /><br />Warm regards,<br /><br />David";s:6:"parent";s:32:"b62c85e1a1f2df794e6ffa746e2489c8";s:7:"replies";a:1:{i:0;s:32:"739e7e22077dec81a676ada6235248cd";}s:4:"show";b:1;s:3:"cid";s:32:"7b35db4ad76b71e3170faf77dea2c8b8";}s:32:"6dd1d3bd34b04fb1ca487df04a6a7880";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lducharme";s:4:"name";s:13:"Lori Ducharme";s:4:"mail";s:21:"lori.ducharme@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540681015;}s:3:"raw";s:2038:"Hi Tara,

I’ve re-read all of these assignments and it’s great to see the evolution of your thinking about this project, as well as better understand how you’re constrained by the grant mechanism currently under review.  I think my only caution here would be that, when you’ve compiled a list of strategies (which might be thought of as “best practices” of the high performing hospitals), also think about whether there are any additional implementation strategies or “ingredients” that might be needed to get the low performers to put those practices in place.  One of those has already been mentioned – there may be a need to engage them in particular ways in order to build up their implementation capacity to a certain threshold first – that’s a form of implementation strategy on which further strategies would build.  I’m also thinking more concretely, e.g., if your Delphi agrees that utilizing motivational interviewing is important for linking patients to CR, your toolkit would not simply suggest “do motivational interviewing,” but presumably also provide guidance around how to integrate motivational interviewing into the clinic workflow, how to ensure appropriate training and fidelity, etc.  (Imagine your response each time a clinic says, “okay but *how* do we do that?  Do we train all our staff?  Can we assign one nurse to this? Can we use a liaison service?  How do we pay for this? What about X or Y reason why that won’t work here?”)  This gets to be fairly tedious, I know, but just wanted to get you to think about exactly how to specify the full complement of implementation strategies in all of their nitpicky detail.

I agree that you’re on to something here, and research in this area could be positioned to make a big impact. I hope all of these assignments have helped you think through the pieces either for the pending application, or for the next one you’ll write.  Looking forward to seeing the final revised concept, and to chatting at the in-person meeting.  Lori
";s:5:"xhtml";s:2057:"Hi Tara,<br /><br />I’ve re-read all of these assignments and it’s great to see the evolution of your thinking about this project, as well as better understand how you’re constrained by the grant mechanism currently under review.  I think my only caution here would be that, when you’ve compiled a list of strategies (which might be thought of as “best practices” of the high performing hospitals), also think about whether there are any additional implementation strategies or “ingredients” that might be needed to get the low performers to put those practices in place.  One of those has already been mentioned – there may be a need to engage them in particular ways in order to build up their implementation capacity to a certain threshold first – that’s a form of implementation strategy on which further strategies would build.  I’m also thinking more concretely, e.g., if your Delphi agrees that utilizing motivational interviewing is important for linking patients to CR, your toolkit would not simply suggest “do motivational interviewing,” but presumably also provide guidance around how to integrate motivational interviewing into the clinic workflow, how to ensure appropriate training and fidelity, etc.  (Imagine your response each time a clinic says, “okay but *how* do we do that?  Do we train all our staff?  Can we assign one nurse to this? Can we use a liaison service?  How do we pay for this? What about X or Y reason why that won’t work here?”)  This gets to be fairly tedious, I know, but just wanted to get you to think about exactly how to specify the full complement of implementation strategies in all of their nitpicky detail.<br /><br />I agree that you’re on to something here, and research in this area could be positioned to make a big impact. I hope all of these assignments have helped you think through the pieces either for the pending application, or for the next one you’ll write.  Looking forward to seeing the final revised concept, and to chatting at the in-person meeting.  Lori";s:6:"parent";s:32:"0fe5327e80144c35e1db1b51385f10e5";s:7:"replies";a:1:{i:0;s:32:"5033e1668c034395d01639521502b99a";}s:4:"show";b:1;s:3:"cid";s:32:"6dd1d3bd34b04fb1ca487df04a6a7880";}s:32:"a492c8c311cc04dfbf038a030a8fa283";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"bjpowell";s:4:"name";s:12:"Byron Powell";s:4:"mail";s:16:"bjpowell@unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540823209;}s:3:"raw";s:3896:"Dear Sivan,

I hope you're well! Thanks so much for this assignment and your willingness to roll with us as we provide feedback on your work! I have a few thoughts in reflecting on your assignments. Forgive me if I'm missing the point or misinterpreting the state of the evidence.

Your comment that “employees did not know how to translate equity from value to action” suggests to me that the intervention needs to be made more concrete (i.e., that you’re trying to implement something without clear dimensions). 

I think concrete examples about the kind of skills you want participants to develop as they are taught the sensemaking model would be helpful, as would specific examples of the types of problems addressed. For instance, you give the example of applying this “lens” to a predictive algorithm work process. How might this play out? Anything that you can do to make the “what” (or as Geoff Curran has started saying “the thing”) that you implementing more clear would be helpful. I realize that part of the beauty of what you’re attempting is to have a general approach that will allow participants to fit these principles to their own contexts; however, having one concrete example that you could carry throughout a proposal might be nice and really helpful to the reader. 

I wonder if you could be more explicit about “who needs to change what” in order to achieve something as broad as health equity (or the more intermediate outcomes that you wish to obtain). It is clear (and perhaps not very surprising) that training would not be sufficient to achieve the outcomes that you’ve identified, but it is less clear to me what the specific determinants of the behaviors that you’d like to see changed are. Being more specific on that regard would allow you to really link the implementation strategies you are proposing to the determinants more clearly. 

I am still not totally clear that you have an intervention yet that has demonstrated effectiveness (as I noted in my first comments). Thus, what is framed as implementation strategies are really the beginnings of an organizational intervention to improve equity/culture around equity. Am I off base here? I wonder if what you are trying to do is similar (in a way anyway) to what Charles Glisson and colleagues have done with the ARC (Availability, Responsiveness, & Continuity) intervention/organizational implementation strategy. ARC is designed to improve organizational culture and climate, and has been shown to improve implementation and clinical outcomes as well. It was really initially developed as an organizational intervention, and (to my understanding), later pitched as an organizational implementation strategy. In fact, it has been used as a strategy to implement specific EBPs and as a more general strategy to try to improve the use of evidence-based approaches in children’s services. This might seem like a huge tangent, but I’m wondering if there would be ways to draw upon this work to help you frame the equity work in a way that is consistent with the implementation literature. A link to a book that describes the model is here: https://www.amazon.com/Building-Cultures-Climates-Effective-Services/dp/0190455284?keywords=building+cultures+and+climates+for+effective+human+services&qid=1540823164&sr=8-1-spell&ref=sr_1_1 

Finally, I thought of this article (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3673273/) as potentially helpful as you think about defining the intervention and the multiple considerations for implementation and sustainment. I have always found it helpful, but it has been a while since I read it so sorry if it is a miss. 

I’m really looking forward to hearing more about this and talking through what this can look like. Hopefully we can take some time on our next call to discuss, and of course, at the in-person meeting. 

Best,
 Byron";s:5:"xhtml";s:4007:"Dear Sivan,<br /><br />I hope you&#039;re well! Thanks so much for this assignment and your willingness to roll with us as we provide feedback on your work! I have a few thoughts in reflecting on your assignments. Forgive me if I&#039;m missing the point or misinterpreting the state of the evidence.<br /><br />Your comment that “employees did not know how to translate equity from value to action” suggests to me that the intervention needs to be made more concrete (i.e., that you’re trying to implement something without clear dimensions). <br /><br />I think concrete examples about the kind of skills you want participants to develop as they are taught the sensemaking model would be helpful, as would specific examples of the types of problems addressed. For instance, you give the example of applying this “lens” to a predictive algorithm work process. How might this play out? Anything that you can do to make the “what” (or as Geoff Curran has started saying “the thing”) that you implementing more clear would be helpful. I realize that part of the beauty of what you’re attempting is to have a general approach that will allow participants to fit these principles to their own contexts; however, having one concrete example that you could carry throughout a proposal might be nice and really helpful to the reader. <br /><br />I wonder if you could be more explicit about “who needs to change what” in order to achieve something as broad as health equity (or the more intermediate outcomes that you wish to obtain). It is clear (and perhaps not very surprising) that training would not be sufficient to achieve the outcomes that you’ve identified, but it is less clear to me what the specific determinants of the behaviors that you’d like to see changed are. Being more specific on that regard would allow you to really link the implementation strategies you are proposing to the determinants more clearly. <br /><br />I am still not totally clear that you have an intervention yet that has demonstrated effectiveness (as I noted in my first comments). Thus, what is framed as implementation strategies are really the beginnings of an organizational intervention to improve equity/culture around equity. Am I off base here? I wonder if what you are trying to do is similar (in a way anyway) to what Charles Glisson and colleagues have done with the ARC (Availability, Responsiveness, &amp; Continuity) intervention/organizational implementation strategy. ARC is designed to improve organizational culture and climate, and has been shown to improve implementation and clinical outcomes as well. It was really initially developed as an organizational intervention, and (to my understanding), later pitched as an organizational implementation strategy. In fact, it has been used as a strategy to implement specific EBPs and as a more general strategy to try to improve the use of evidence-based approaches in children’s services. This might seem like a huge tangent, but I’m wondering if there would be ways to draw upon this work to help you frame the equity work in a way that is consistent with the implementation literature. A link to a book that describes the model is here: https://www.amazon.com/Building-Cultures-Climates-Effective-Services/dp/0190455284?keywords=building+cultures+and+climates+for+effective+human+services&amp;qid=1540823164&amp;sr=8-1-spell&amp;ref=sr_1_1 <br /><br />Finally, I thought of this article (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3673273/) as potentially helpful as you think about defining the intervention and the multiple considerations for implementation and sustainment. I have always found it helpful, but it has been a while since I read it so sorry if it is a miss. <br /><br />I’m really looking forward to hearing more about this and talking through what this can look like. Hopefully we can take some time on our next call to discuss, and of course, at the in-person meeting. <br /><br />Best,<br /> Byron";s:6:"parent";s:32:"225778658174adb8246efe8541cc0a6f";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"a492c8c311cc04dfbf038a030a8fa283";}s:32:"72383671ab5ea9ef051afbfa296028b4";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"bjpowell";s:4:"name";s:12:"Byron Powell";s:4:"mail";s:16:"bjpowell@unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540824770;}s:3:"raw";s:1014:"Hi Kenneth,

I hope all is well! Nice job with this assignment. You clearly describe your approach and the implementation strategies that you'll use. I'd love to hear more about the approach/modified version of Intervention Mapping. Also, while the strategies seem really well-justified, you might consider (at this point or further on down the road) specifying them using established reporting guidelines (e.g., https://www.ncbi.nlm.nih.gov/pubmed/24289295, TIDieR, etc.) to enhance replicability. I love that you have a balance between well described strategies that are based upon preliminary studies and that you are still building in the opportunity to tailor strategies based upon your work to assess barriers/facilitators in this study. What methods/processes will you have in place to track the strategies that emerge from this process (or more organically from VA sites)? Nicely done and looking forward to learning from your approach on our call and/or in the in-person meeting in December. 

Best,
Byron";s:5:"xhtml";s:1049:"Hi Kenneth,<br /><br />I hope all is well! Nice job with this assignment. You clearly describe your approach and the implementation strategies that you&#039;ll use. I&#039;d love to hear more about the approach/modified version of Intervention Mapping. Also, while the strategies seem really well-justified, you might consider (at this point or further on down the road) specifying them using established reporting guidelines (e.g., https://www.ncbi.nlm.nih.gov/pubmed/24289295, TIDieR, etc.) to enhance replicability. I love that you have a balance between well described strategies that are based upon preliminary studies and that you are still building in the opportunity to tailor strategies based upon your work to assess barriers/facilitators in this study. What methods/processes will you have in place to track the strategies that emerge from this process (or more organically from VA sites)? Nicely done and looking forward to learning from your approach on our call and/or in the in-person meeting in December. <br /><br />Best,<br />Byron";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"72383671ab5ea9ef051afbfa296028b4";}s:32:"6c7cc70f27b4b626c86763fac54d04ac";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jpittman";s:4:"name";s:13:"James Pittman";s:4:"mail";s:17:"jpittman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1541019182;}s:3:"raw";s:281:"Thank you, David. It's clear I have a lot more to do/think through before I have something feasible/fundable. I hope we will have time to talk in D.C. Perhaps we could meet during D & I ahead of the TIDIRH if you are around? Either way, I will definitely follow up. 

Best, 
James ";s:5:"xhtml";s:304:"Thank you, David. It&#039;s clear I have a lot more to do/think through before I have something feasible/fundable. I hope we will have time to talk in D.C. Perhaps we could meet during D &amp; I ahead of the TIDIRH if you are around? Either way, I will definitely follow up. <br /><br />Best, <br />James";s:6:"parent";s:32:"fb4ad14ce02e26b9fc34d65afa55bb47";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"6c7cc70f27b4b626c86763fac54d04ac";}s:32:"5033e1668c034395d01639521502b99a";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"tlagu";s:4:"name";s:9:"Tara Lagu";s:4:"mail";s:28:"Tara.Lagu@baystatehealth.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1541628777;}s:3:"raw";s:23:"Thanks so much! Me too!";s:5:"xhtml";s:23:"Thanks so much! Me too!";s:6:"parent";s:32:"6dd1d3bd34b04fb1ca487df04a6a7880";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"5033e1668c034395d01639521502b99a";}s:32:"739e7e22077dec81a676ada6235248cd";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"bjones";s:4:"name";s:13:"Barbara Jones";s:4:"mail";s:26:"Barbara.Jones@hsc.utah.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1541709788;}s:3:"raw";s:2818:"Thank you very much for your excellent advice David!
That is a great idea about knowledge-sharing - I have discussed pneumonia CDS with VA informatics leaders who have created "knowledge artifacts" for CDS in other areas, and there so far is no VA-approved standard for pneumonia, so each setting is creating its own adaptation of guidelines. The center I am part of (IDEAS) is involved in many antibiotic-stewardship efforts, particularly surrounding acute respiratory infections (ARIs), in which they are testing different implementation strategies to reduce unnecessary antibiotic prescribing - so far nothing for pneumonia, although a pneumonia CDS could complement the ARI CDS' that are currently being tested.  I have a CDA surrounding a mixed methods examination of practice variation in pneumonia across the VA - so I'm excited to interview providers and clinical leaders at different VA's which I think will be a good opportunity to examine existing CDS at these facilities. Where there is antibiotic stewardship, there seems to be more CDS around infectious disease - so I think that the existing pneumonia CDS that I do find will be in urban/teaching facilities with specialty services.
Regarding top-down involvement, I am in contact with the national antibiotic stewardship folks quite a bit and think I could generate support there, but have not managed to make good connections yet with national pulmonary/critical care or emergency department leadership yet. I was hoping to generate some data from my CDA to share with them next year, before I introduce myself. What do you think?
I feel like the sustainability/sustainment question could be an entire project on its own. Once embedded, the CDS may always be there, but factors influencing the rate at which the CDS' use and impact will degrade, or whether it is sustained, are unknown! If we design the tracking system well - it would collect utilization as well as fidelity for different management recommendations -- I would like to explore the targeting of sustainment efforts based upon information from the tracking system.
As for an over-arching aspirational aim, yes! - Pneumonia mortality is truly variable, likely driven by differences in management of sepsis, respiratory failure, and appropriate antibiotics. I think there are many mechanisms of death in pneumonia that are unknown - studying the facilities and providers that seem to have better outcomes for their patients could unearth new ways to improve outcomes.  Also, from my preliminary work, ED physicians receive very little feedback about the outcomes of their paitnets, so exploring how A/F with ED physicians might improve pneumonia care is exciting to me. 
I will definitely reach out to you in the future, thank you for the offer! I really appreciate your advice.
- barb  
";s:5:"xhtml";s:2875:"Thank you very much for your excellent advice David!<br />That is a great idea about knowledge-sharing - I have discussed pneumonia CDS with VA informatics leaders who have created &quot;knowledge artifacts&quot; for CDS in other areas, and there so far is no VA-approved standard for pneumonia, so each setting is creating its own adaptation of guidelines. The center I am part of (IDEAS) is involved in many antibiotic-stewardship efforts, particularly surrounding acute respiratory infections (ARIs), in which they are testing different implementation strategies to reduce unnecessary antibiotic prescribing - so far nothing for pneumonia, although a pneumonia CDS could complement the ARI CDS&#039; that are currently being tested.  I have a CDA surrounding a mixed methods examination of practice variation in pneumonia across the VA - so I&#039;m excited to interview providers and clinical leaders at different VA&#039;s which I think will be a good opportunity to examine existing CDS at these facilities. Where there is antibiotic stewardship, there seems to be more CDS around infectious disease - so I think that the existing pneumonia CDS that I do find will be in urban/teaching facilities with specialty services.<br />Regarding top-down involvement, I am in contact with the national antibiotic stewardship folks quite a bit and think I could generate support there, but have not managed to make good connections yet with national pulmonary/critical care or emergency department leadership yet. I was hoping to generate some data from my CDA to share with them next year, before I introduce myself. What do you think?<br />I feel like the sustainability/sustainment question could be an entire project on its own. Once embedded, the CDS may always be there, but factors influencing the rate at which the CDS&#039; use and impact will degrade, or whether it is sustained, are unknown! If we design the tracking system well - it would collect utilization as well as fidelity for different management recommendations -- I would like to explore the targeting of sustainment efforts based upon information from the tracking system.<br />As for an over-arching aspirational aim, yes! - Pneumonia mortality is truly variable, likely driven by differences in management of sepsis, respiratory failure, and appropriate antibiotics. I think there are many mechanisms of death in pneumonia that are unknown - studying the facilities and providers that seem to have better outcomes for their patients could unearth new ways to improve outcomes.  Also, from my preliminary work, ED physicians receive very little feedback about the outcomes of their paitnets, so exploring how A/F with ED physicians might improve pneumonia care is exciting to me. <br />I will definitely reach out to you in the future, thank you for the offer! I really appreciate your advice.<br />- barb";s:6:"parent";s:32:"7b35db4ad76b71e3170faf77dea2c8b8";s:7:"replies";a:1:{i:0;s:32:"ea343a4b86da185310bdce0be41d5aa8";}s:4:"show";b:1;s:3:"cid";s:32:"739e7e22077dec81a676ada6235248cd";}s:32:"ea343a4b86da185310bdce0be41d5aa8";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dgoodrich";s:4:"name";s:14:"David Goodrich";s:4:"mail";s:22:"David.Goodrich2@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1543519892;}s:3:"raw";s:1101:"Barb, my apologies for the delayed response to your comment. While I don't have direct answers for most of the questions you raise, I think you are asking the right ones at the right time. I do feel like it would be helpful for you to work with your mentors to see if they can connect you with appropriate leaders in VACO to align your research interests with at least one clinical priority topic related to ED treatment or infection control. I am less familiar with ED issues in VA but I know some clinical researchers here in Ann Arbor who do wonderful work on infection control including Jack Iwashyna, Sarah Krein, Sanjay Saint, and others. Locally, our Center for Clinical Management investigators try to align CDA's with national priorities right from the start and hopefully, this research also aligns with community health needs in your local region or with your NIH collaborators.  I hope that Nick Bowersox, Christian Helfrich, Lindsey Zimmerman and I can give you some added advice that really helps take your concept paper to the next level. Look forward to talking more next week!
-David ";s:5:"xhtml";s:1115:"Barb, my apologies for the delayed response to your comment. While I don&#039;t have direct answers for most of the questions you raise, I think you are asking the right ones at the right time. I do feel like it would be helpful for you to work with your mentors to see if they can connect you with appropriate leaders in VACO to align your research interests with at least one clinical priority topic related to ED treatment or infection control. I am less familiar with ED issues in VA but I know some clinical researchers here in Ann Arbor who do wonderful work on infection control including Jack Iwashyna, Sarah Krein, Sanjay Saint, and others. Locally, our Center for Clinical Management investigators try to align CDA&#039;s with national priorities right from the start and hopefully, this research also aligns with community health needs in your local region or with your NIH collaborators.  I hope that Nick Bowersox, Christian Helfrich, Lindsey Zimmerman and I can give you some added advice that really helps take your concept paper to the next level. Look forward to talking more next week!<br />-David";s:6:"parent";s:32:"739e7e22077dec81a676ada6235248cd";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"ea343a4b86da185310bdce0be41d5aa8";}}s:11:"subscribers";N;}