a:5:{s:5:"title";N;s:6:"status";i:1;s:6:"number";i:75;s:8:"comments";a:75:{s:32:"b98ced6117ceba48ab634cfcf00fa724";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1534880964;}s:3:"raw";s:368:"KUPFER - Pre-Assignment Welcome!!
I just want to welcome everyone to the HIV/AIDS Group.  We look forward to reading and responding to your assignments and working with you on TIDIRH.   You should have gotten a calendar invite from Erin Spaniol for   our small group introductory call  from 11:30 to 1:30 EST on September 4th.  Talk to you then!  

Best,

Linda       ";s:5:"xhtml";s:386:"KUPFER - Pre-Assignment Welcome!!<br />I just want to welcome everyone to the HIV/AIDS Group.  We look forward to reading and responding to your assignments and working with you on TIDIRH.   You should have gotten a calendar invite from Erin Spaniol for   our small group introductory call  from 11:30 to 1:30 EST on September 4th.  Talk to you then!  <br /><br />Best,<br /><br />Linda";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"b98ced6117ceba48ab634cfcf00fa724";}s:32:"87417dbea2b01906d069218b9e05b3d3";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"mmarzan";s:4:"name";s:24:"Melissa Marzan-Rodriguez";s:4:"mail";s:22:"melissa.marzan@upr.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535121055;}s:3:"raw";s:3675:"Marzan-Rodriguez - Assignment #1a

1. Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&I-focused specific aims pages, it should generally address the following 5 elements (and questions). 

My response to this is:

Background: Sexual and ethnic minorities adolescents are at increased risk for HIV infection. Epidemiological trends in United States (US) have shown an increase in incidence among Adolescent Men who have Sex with Men (AMSM). The use of health services and information delivered or enhanced through the Internet or related technologies –known as eHealth- is an important strategy to reduce HIV disparities and to engage with some minority populations such as Spanish-speaking Latino AMSM. The Centers for Disease Control and Prevention (CDC) suggest that social aspects of the HIV epidemic such as: stigma, fear, homophobia, isolation, and lack of support may also place youths at higher risk for HIV. In 2016, a total of 39,782 HIV diagnoses were reported in the US. From that total, 8,451 were among youth aged 13-24 years old; 6,848 were among Young Gay and Bisexual Men (YGBM); and 1,821 were Hispanic/Latino males.  Between 2011 and 2015, in the US, HIV diagnoses increased 19% among young (13-24 yrs. old) Hispanic/Latino gay and bisexual men (CDC, 2018). AMSM experience a dramatic health disparity as they represent 2% of young people but account for almost 80% of HIV diagnoses. Despite this disproportionate burden, there is a conspicuous lack of evidence-based HIV prevention programs.

SMART Program: eHealth interventions represent an excellent modality for delivering AMSM specific intervention material where youth “are”. The SMART Program (Sexual Minority Adolescent Risk Taking) is an eHealth intervention focused on HIV Prevention among AMSM. The SMART Program package includes: (1) a universally-delivered, brief, online sexual health education program designed for sexual and gender minority youths regardless of whether they are sexually active (Queer Sex Ed-QSE); (2) a more intensive online intervention designed for diverse AMSM engaging in HIV transmission risk behaviors (Keep It Up!), and (3) the most intensive tool is a motivational interviewing (MI) intervention that will be delivered by MI therapists via online video chat (Young Men's Health Project). The SMART Program will be available for Spanish-speaking Latino AMSM in the US by the fall of 2018. As part of this process, assessing its feasibility and documenting the implementation science strategies will facilitate its expansion to other Spanish-speaking populations. 

Implementation Gap: eHealth- is an important strategy to reduce HIV disparities and to engage with some minority populations such as Spanish-speaking Latino AMSM. The main implementation gaps for this eHealth tool are (1) there are no published evidence of the development and implementation of eHealth tools for an HIV Prevention Program tailored for Spanish-speaking Latino AMSM and (2) no prior validation process of  cultural adaptation of the SMART program.
Research Question: What are the facilitators and barriers for the implementation of an eHealth HIV Prevention Program tailored to Spanish-speaking Latino AMSM in the US?

Specific Aims: (1) to evaluate the feasibility of the SMART Program among Spanish-speaking Latino AMSM in the US; (2) to describe the acceptability and demand among Spanish-speaking Latino AMSM and stakeholders who participated in the implementation process of the SMART Program &; (3) to develop public health policy implementation strategies for HIV Prevention among Spanish-speaking Latino AMSM. 

 
";s:5:"xhtml";s:3748:"Marzan-Rodriguez - Assignment #1a<br /><br />1. Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&amp;I-focused specific aims pages, it should generally address the following 5 elements (and questions). <br /><br />My response to this is:<br /><br />Background: Sexual and ethnic minorities adolescents are at increased risk for HIV infection. Epidemiological trends in United States (US) have shown an increase in incidence among Adolescent Men who have Sex with Men (AMSM). The use of health services and information delivered or enhanced through the Internet or related technologies –known as eHealth- is an important strategy to reduce HIV disparities and to engage with some minority populations such as Spanish-speaking Latino AMSM. The Centers for Disease Control and Prevention (CDC) suggest that social aspects of the HIV epidemic such as: stigma, fear, homophobia, isolation, and lack of support may also place youths at higher risk for HIV. In 2016, a total of 39,782 HIV diagnoses were reported in the US. From that total, 8,451 were among youth aged 13-24 years old; 6,848 were among Young Gay and Bisexual Men (YGBM); and 1,821 were Hispanic/Latino males.  Between 2011 and 2015, in the US, HIV diagnoses increased 19% among young (13-24 yrs. old) Hispanic/Latino gay and bisexual men (CDC, 2018). AMSM experience a dramatic health disparity as they represent 2% of young people but account for almost 80% of HIV diagnoses. Despite this disproportionate burden, there is a conspicuous lack of evidence-based HIV prevention programs.<br /><br />SMART Program: eHealth interventions represent an excellent modality for delivering AMSM specific intervention material where youth “are”. The SMART Program (Sexual Minority Adolescent Risk Taking) is an eHealth intervention focused on HIV Prevention among AMSM. The SMART Program package includes: (1) a universally-delivered, brief, online sexual health education program designed for sexual and gender minority youths regardless of whether they are sexually active (Queer Sex Ed-QSE); (2) a more intensive online intervention designed for diverse AMSM engaging in HIV transmission risk behaviors (Keep It Up!), and (3) the most intensive tool is a motivational interviewing (MI) intervention that will be delivered by MI therapists via online video chat (Young Men&#039;s Health Project). The SMART Program will be available for Spanish-speaking Latino AMSM in the US by the fall of 2018. As part of this process, assessing its feasibility and documenting the implementation science strategies will facilitate its expansion to other Spanish-speaking populations. <br /><br />Implementation Gap: eHealth- is an important strategy to reduce HIV disparities and to engage with some minority populations such as Spanish-speaking Latino AMSM. The main implementation gaps for this eHealth tool are (1) there are no published evidence of the development and implementation of eHealth tools for an HIV Prevention Program tailored for Spanish-speaking Latino AMSM and (2) no prior validation process of  cultural adaptation of the SMART program.<br />Research Question: What are the facilitators and barriers for the implementation of an eHealth HIV Prevention Program tailored to Spanish-speaking Latino AMSM in the US?<br /><br />Specific Aims: (1) to evaluate the feasibility of the SMART Program among Spanish-speaking Latino AMSM in the US; (2) to describe the acceptability and demand among Spanish-speaking Latino AMSM and stakeholders who participated in the implementation process of the SMART Program &amp;; (3) to develop public health policy implementation strategies for HIV Prevention among Spanish-speaking Latino AMSM.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"35d7c10c25aa5d9388029cb9aa2d5f06";}s:4:"show";b:1;s:3:"cid";s:32:"87417dbea2b01906d069218b9e05b3d3";}s:32:"22c7c55c8f26270ecddd896924b0811b";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cpsaros";s:4:"name";s:16:"Christina Psaros";s:4:"mail";s:23:"CPSAROS@mgh.harvard.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535124436;}s:3:"raw";s:4689:"Psaros - Assignment #1A

1. Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&I-focused specific aims pages, it should generally address the following 5 elements (and questions). 
 
My response to this is… 

African-American (AA) women living in the Deep South are at high risk for HIV acquisition, with an infection rate 16 times that of white women. AA women accounted for 61% of new HIV infections among US women in 2015, and HIV infection is the 12th leading cause of death among AA women in the Deep South Rural counties in particular represent an underserved area within the Deep South, which is currently seeing the highest rates of HIV infections per 100,000 persons in the US. As evidenced by the above statistics, traditional HIV prevention methods, including STI/HIV education, comprehensive behavioral interventions, and provision of condoms and microbicides have rendered minimal success in this population. More so, biomedical HIV prevention work in the US has generally focused on men who have sex with men (MSM), often leaving women (particularly women of racial/ethnic minority status) underserved. 
      HIV pre-exposure prophylaxis (PrEP) has become a promising biomedical prevention tool, with over 92% efficacy as shown in numerous trials PrEP has the potential to curb incident HIV infection among at-risk women. However, while PrEP prescriptions for MSM are increasing, PrEP uptake among women, particularly AA women, has remained sparse, with recent Gilead data showing a decrease in prescriptions written for women in the past year. There is a dearth of research examining potential PrEP use and uptake with AA women, particularly those in rural areas who may have less access to healthcare and more limited knowledge of PrEP than women in urban areas. Demonstration projects are urgently needed to evaluate and overcome barriers to PrEP awareness, uptake and adherence in this population. 
      Our long-term goal is to successfully integrate PrEP into clinical care for AA women at risk for HIV infection in the rural South. The objective of this application is to pilot test PrEP implementation among women seeking care at a Federally Qualified Healthcare Center (FQHC) in rural Alabama and explore perceptions, facilitators, and barriers to the delivery and uptake of PrEP among AA women living in the Deep South and their healthcare providers. FQHCs offer a variety of health services ranging from primary care to family planning, and have been central in the provision of primary care to underserved populations, especially those living in rural areas of the U.S. 
In the proposed pilot implementation study, we will first conduct preliminary work to understand how to best implement PrEP services into FQHC clinics in Alabama. In the next phase, an existing protocol for PrEP delivery will be modified, implemented, and refined using an iterative process; both provider and patient level data on adherence to the PrEP care continuum will be collected. Thus, we propose the following specific aims:
1)	To conduct formative work using qualitative methods with various types of health care providers (N=15 individual provider interviews, or until thematic saturation) and AA HIV-uninfected women (N=15 individual interviews or until thematic saturation) to explore how to most effectively integrate PrEP services into a FQHC in rural AL and to develop culturally tailored and integrated PrEP services. 
2)	To dynamically adapt and implement a PrEP delivery protocol for AA women seeking care at FQHCs in Alabama using an iterative adaptation process.
2a)	To systematically document and evaluate adherence to the PrEP cascade among high-risk, AA HIV-uninfected women and their providers to evaluate feasibility of implementation.
2b) 	To explore and identify acceptability, barriers, and facilitators of PrEP use among high-risk, AA HIV-uninfected women offered PrEP and their providers using qualitative and quantitative methods. 
	With respect to expected outcomes, the proposed work is expected to significantly contribute to our understanding of factors impacting PrEP uptake and use among at-risk AA HIV-uninfected women, in particular in underserved, rural areas in the Deep South that are increasingly impacted by the HIV epidemic and have worse health outcomes relative to other areas in the US. Such results will positively impact the development of an R01 scale-out implementation trial using a stepped-wedge trial design and testing the dissemination of PrEP through primary care and reproductive health centers in rural areas of the Deep South to AA women at high-risk for HIV-infection.
 	";s:5:"xhtml";s:4760:"Psaros - Assignment #1A<br /><br />1. Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&amp;I-focused specific aims pages, it should generally address the following 5 elements (and questions). <br /> <br />My response to this is… <br /><br />African-American (AA) women living in the Deep South are at high risk for HIV acquisition, with an infection rate 16 times that of white women. AA women accounted for 61% of new HIV infections among US women in 2015, and HIV infection is the 12th leading cause of death among AA women in the Deep South Rural counties in particular represent an underserved area within the Deep South, which is currently seeing the highest rates of HIV infections per 100,000 persons in the US. As evidenced by the above statistics, traditional HIV prevention methods, including STI/HIV education, comprehensive behavioral interventions, and provision of condoms and microbicides have rendered minimal success in this population. More so, biomedical HIV prevention work in the US has generally focused on men who have sex with men (MSM), often leaving women (particularly women of racial/ethnic minority status) underserved. <br />      HIV pre-exposure prophylaxis (PrEP) has become a promising biomedical prevention tool, with over 92% efficacy as shown in numerous trials PrEP has the potential to curb incident HIV infection among at-risk women. However, while PrEP prescriptions for MSM are increasing, PrEP uptake among women, particularly AA women, has remained sparse, with recent Gilead data showing a decrease in prescriptions written for women in the past year. There is a dearth of research examining potential PrEP use and uptake with AA women, particularly those in rural areas who may have less access to healthcare and more limited knowledge of PrEP than women in urban areas. Demonstration projects are urgently needed to evaluate and overcome barriers to PrEP awareness, uptake and adherence in this population. <br />      Our long-term goal is to successfully integrate PrEP into clinical care for AA women at risk for HIV infection in the rural South. The objective of this application is to pilot test PrEP implementation among women seeking care at a Federally Qualified Healthcare Center (FQHC) in rural Alabama and explore perceptions, facilitators, and barriers to the delivery and uptake of PrEP among AA women living in the Deep South and their healthcare providers. FQHCs offer a variety of health services ranging from primary care to family planning, and have been central in the provision of primary care to underserved populations, especially those living in rural areas of the U.S. <br />In the proposed pilot implementation study, we will first conduct preliminary work to understand how to best implement PrEP services into FQHC clinics in Alabama. In the next phase, an existing protocol for PrEP delivery will be modified, implemented, and refined using an iterative process; both provider and patient level data on adherence to the PrEP care continuum will be collected. Thus, we propose the following specific aims:<br />1)	To conduct formative work using qualitative methods with various types of health care providers (N=15 individual provider interviews, or until thematic saturation) and AA HIV-uninfected women (N=15 individual interviews or until thematic saturation) to explore how to most effectively integrate PrEP services into a FQHC in rural AL and to develop culturally tailored and integrated PrEP services. <br />2)	To dynamically adapt and implement a PrEP delivery protocol for AA women seeking care at FQHCs in Alabama using an iterative adaptation process.<br />2a)	To systematically document and evaluate adherence to the PrEP cascade among high-risk, AA HIV-uninfected women and their providers to evaluate feasibility of implementation.<br />2b) 	To explore and identify acceptability, barriers, and facilitators of PrEP use among high-risk, AA HIV-uninfected women offered PrEP and their providers using qualitative and quantitative methods. <br />	With respect to expected outcomes, the proposed work is expected to significantly contribute to our understanding of factors impacting PrEP uptake and use among at-risk AA HIV-uninfected women, in particular in underserved, rural areas in the Deep South that are increasingly impacted by the HIV epidemic and have worse health outcomes relative to other areas in the US. Such results will positively impact the development of an R01 scale-out implementation trial using a stepped-wedge trial design and testing the dissemination of PrEP through primary care and reproductive health centers in rural areas of the Deep South to AA women at high-risk for HIV-infection.";s:6:"parent";N;s:7:"replies";a:2:{i:0;s:32:"59f6e88e0cbd1b14b788b7102a411abe";i:1;s:32:"ce3d534101594a406a2316dcc1b373b4";}s:4:"show";b:1;s:3:"cid";s:32:"22c7c55c8f26270ecddd896924b0811b";}s:32:"c6823b7e57ae6c23bb5e0e0159c0765f";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"kbeima";s:4:"name";s:19:"Kristin Beima-Sofie";s:4:"mail";s:13:"beimak@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535126464;}s:3:"raw";s:6715:"Beima-Sofie – Assignment #1a

Specific Aims 
Despite unprecedented progress curbing the HIV epidemic, evidence suggests that adolescents are falling behind. HIV-infected adolescents have low rates of retention, adherence and viral suppression when compared to children and adults. The majority of HIV-infected adolescents (84%) live in sub-Saharan Africa. Adolescence is a time of rapid transition and change, and behaviors adopted during adolescence impact lifelong health outcomes. Poor disclosure of HIV diagnosis to adolescents, as well as failure to successfully transition to adult care systems, are potential barriers to reaching ’90-90-90 targets in adolescents. Therefore, improving systems for disclosure and transition to adult care is a high priority. High volume HIV clinics in Africa require feasible and effective systematic approaches to improve disclosure and transition. Implementation science (IS) is a multidisciplinary field that seeks to provide generalizable knowledge about key stakeholder behaviors in order to close the gap between evidence and practice. 

The ATTACH study (NIH R01HD089850-01, PI: John-Stewart) is a cluster RCT to evaluate the effectiveness of an Adolescent Transition Package (ATP) to improve adolescent clinical outcomes among HIV-infected adolescents who transition to adult care. The proposed project leverages a timely opportunity to evaluate the context of implementation alongside the effectiveness outcomes in ATTACH, therefore expanding the original project into a hybrid effectiveness-implementation trial. The ATP being evaluated in ATTACH combines a validated US-based transition toolset and an evidence-based disclosure intervention from Namibia. While both tools are effective individually, and in their respective health system environments, the ATP combines them into a single package of tools being implemented in a new cultural and health systems context. This translation may influence implementation outcomes and failure to evaluate implementation outcomes alongside effectiveness outcomes may limit interpretation of intervention success and future scalability. Therefore, the goal of this study is to use the Consolidated Framework for Implementation Research (CFIR) to guide a mixed-methods evaluation of determinants impacting implementation outcomes during installation, initial implementation and full implementation of the ATP. We believe that variations in readiness for change, HCW training and competence, HCW workload, and perceptions of intervention complexity and adaptability will be associated with implementation outcomes. Specifically, the aims of this project are:

 Aim 1: To evaluate how facility-led intervention adaptations during installation influence acceptability, feasibility, and intervention fidelity of the ATP. CFIR will guide mixed methods identification of implementation determinants and adaptation strategies emerging through facility-led quality improvement approaches. During the first 6 months of ATP implementation, intervention facilities will participate in modified plan-do-study-act (PDSA) cycles. We will conduct and audio-record bi-monthly meetings to discuss implementation barriers and facilitators, and intervention adaptations. We will also conduct surveys with HCWs, directly observe patient/provider interactions, and review HCW implementation logs at 1 and 6 months post intervention implementation. 

Aim 2: To identify determinants affecting fidelity, adoption, penetration and sustainability of full ATP implementation. CFIR constructs will guide mixed methods identification of determinants influencing implementation outcomes at 2 years post-implementation. Information will be gathered across high-priority constructs from the five CFIR domains to understand individual and system-level determinants of improved uptake and sustained provision of the ATP.  We will measure organizational readiness for change before ATP implementation, review patient charts and observe patient/provider encounters to assess fidelity, and conduct individual interviews with HCWs. 

Aim 3: To identify necessary and sufficient conditions for successful implementation. We will use Comparative Qualitative Analysis (CQA), a method that rigorously combines qualitative and quantitative data, to identify patterns of explanatory factors influencing successful ATP implementation. Qualitative and quantitative data collected from aims 1 & 2 will be used to identify key barriers and facilitators of implementation success. Factors will be categorized and ranked alongside implementation outcomes to identify conditions necessary and sufficient for acceptability, feasibility, and sustainability to improve future implementation and scale-up.

Alternative Aim 3: To identify implementation determinants most likely to impact implementation outcomes during scale-up. We will use intervention mapping to characterize desired implementation outcomes, corresponding determinants, and change objectives to identify implementation strategies to be tested during future scale-up.

Evidence-based Intervention: The ATP includes components of evidence-based HIV disclosure and adolescent transition tools. 

Disclosure Tool: I was involved in evaluation of a nation disclosure intervention in Namibia. The intervention includes a staged disclosure cartoon book, child and caregiver readiness form, a monitoring form to track progress over visits, and a health care worker (HCW) training curriculum. The intervention increased confidence of caregivers and HCWs to disclose and was felt to improve child outcomes and adherence. This Namibia disclosure intervention tool was highlighted by UNAIDS Science Now (https://sciencenow.unaids.org/articlelist/803) with editorial noting needs for further implementation and evaluation studies. 

Transition Tool: The GOT Transition tool (http://www.gottransition.org) was developed to align with transition policies recommended by the American Academy of Pediatrics (AAP), American Academy of Family Physicians (AAFP), and American College of Physicians (ACP). The tool provides a framework that includes child, caregiver and HCW readiness for transition assessments and involves a several year process with repeated iterative planning towards transition. This model is useful both for children who switch from pediatric to adult health care providers and for children who retain their provider but transition to an adult approach to care, making it relevant to global health settings with integrated adult/child clinic services. Overburdened and lack of specialty cadres of healthcare providers will make direct translation to global settings, without adaptation, challenging. 

 
";s:5:"xhtml";s:6809:"Beima-Sofie – Assignment #1a<br /><br />Specific Aims <br />Despite unprecedented progress curbing the HIV epidemic, evidence suggests that adolescents are falling behind. HIV-infected adolescents have low rates of retention, adherence and viral suppression when compared to children and adults. The majority of HIV-infected adolescents (84%) live in sub-Saharan Africa. Adolescence is a time of rapid transition and change, and behaviors adopted during adolescence impact lifelong health outcomes. Poor disclosure of HIV diagnosis to adolescents, as well as failure to successfully transition to adult care systems, are potential barriers to reaching ’90-90-90 targets in adolescents. Therefore, improving systems for disclosure and transition to adult care is a high priority. High volume HIV clinics in Africa require feasible and effective systematic approaches to improve disclosure and transition. Implementation science (IS) is a multidisciplinary field that seeks to provide generalizable knowledge about key stakeholder behaviors in order to close the gap between evidence and practice. <br /><br />The ATTACH study (NIH R01HD089850-01, PI: John-Stewart) is a cluster RCT to evaluate the effectiveness of an Adolescent Transition Package (ATP) to improve adolescent clinical outcomes among HIV-infected adolescents who transition to adult care. The proposed project leverages a timely opportunity to evaluate the context of implementation alongside the effectiveness outcomes in ATTACH, therefore expanding the original project into a hybrid effectiveness-implementation trial. The ATP being evaluated in ATTACH combines a validated US-based transition toolset and an evidence-based disclosure intervention from Namibia. While both tools are effective individually, and in their respective health system environments, the ATP combines them into a single package of tools being implemented in a new cultural and health systems context. This translation may influence implementation outcomes and failure to evaluate implementation outcomes alongside effectiveness outcomes may limit interpretation of intervention success and future scalability. Therefore, the goal of this study is to use the Consolidated Framework for Implementation Research (CFIR) to guide a mixed-methods evaluation of determinants impacting implementation outcomes during installation, initial implementation and full implementation of the ATP. We believe that variations in readiness for change, HCW training and competence, HCW workload, and perceptions of intervention complexity and adaptability will be associated with implementation outcomes. Specifically, the aims of this project are:<br /><br /> Aim 1: To evaluate how facility-led intervention adaptations during installation influence acceptability, feasibility, and intervention fidelity of the ATP. CFIR will guide mixed methods identification of implementation determinants and adaptation strategies emerging through facility-led quality improvement approaches. During the first 6 months of ATP implementation, intervention facilities will participate in modified plan-do-study-act (PDSA) cycles. We will conduct and audio-record bi-monthly meetings to discuss implementation barriers and facilitators, and intervention adaptations. We will also conduct surveys with HCWs, directly observe patient/provider interactions, and review HCW implementation logs at 1 and 6 months post intervention implementation. <br /><br />Aim 2: To identify determinants affecting fidelity, adoption, penetration and sustainability of full ATP implementation. CFIR constructs will guide mixed methods identification of determinants influencing implementation outcomes at 2 years post-implementation. Information will be gathered across high-priority constructs from the five CFIR domains to understand individual and system-level determinants of improved uptake and sustained provision of the ATP.  We will measure organizational readiness for change before ATP implementation, review patient charts and observe patient/provider encounters to assess fidelity, and conduct individual interviews with HCWs. <br /><br />Aim 3: To identify necessary and sufficient conditions for successful implementation. We will use Comparative Qualitative Analysis (CQA), a method that rigorously combines qualitative and quantitative data, to identify patterns of explanatory factors influencing successful ATP implementation. Qualitative and quantitative data collected from aims 1 &amp; 2 will be used to identify key barriers and facilitators of implementation success. Factors will be categorized and ranked alongside implementation outcomes to identify conditions necessary and sufficient for acceptability, feasibility, and sustainability to improve future implementation and scale-up.<br /><br />Alternative Aim 3: To identify implementation determinants most likely to impact implementation outcomes during scale-up. We will use intervention mapping to characterize desired implementation outcomes, corresponding determinants, and change objectives to identify implementation strategies to be tested during future scale-up.<br /><br />Evidence-based Intervention: The ATP includes components of evidence-based HIV disclosure and adolescent transition tools. <br /><br />Disclosure Tool: I was involved in evaluation of a nation disclosure intervention in Namibia. The intervention includes a staged disclosure cartoon book, child and caregiver readiness form, a monitoring form to track progress over visits, and a health care worker (HCW) training curriculum. The intervention increased confidence of caregivers and HCWs to disclose and was felt to improve child outcomes and adherence. This Namibia disclosure intervention tool was highlighted by UNAIDS Science Now (https://sciencenow.unaids.org/articlelist/803) with editorial noting needs for further implementation and evaluation studies. <br /><br />Transition Tool: The GOT Transition tool (http://www.gottransition.org) was developed to align with transition policies recommended by the American Academy of Pediatrics (AAP), American Academy of Family Physicians (AAFP), and American College of Physicians (ACP). The tool provides a framework that includes child, caregiver and HCW readiness for transition assessments and involves a several year process with repeated iterative planning towards transition. This model is useful both for children who switch from pediatric to adult health care providers and for children who retain their provider but transition to an adult approach to care, making it relevant to global health settings with integrated adult/child clinic services. Overburdened and lack of specialty cadres of healthcare providers will make direct translation to global settings, without adaptation, challenging.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"97c58b1c2db90ae9070370aba89f966f";}s:4:"show";b:1;s:3:"cid";s:32:"c6823b7e57ae6c23bb5e0e0159c0765f";}s:32:"bc716ed0a70edd1abdf51d7a41e672b7";a:8:{s:4:"user";a:5:{s:2:"id";s:3:"dli";s:4:"name";s:9:"Dennis Li";s:4:"mail";s:23:"dennis@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535134156;}s:3:"raw";s:5916:"LI - Assignment #1a

1. Draft a specific aims page (2 pages maximum) of your proposed study.

My response to this is...

     Substantial investment has gone into developing and testing the efficacy of eHealth HIV prevention interventions [1-5], but these evidence-based interventions (EBIs) will not affect the spread of the epidemic unless they are effectively implemented to reach the most at-risk populations. Emerging evidence has shown that eHealth approaches can be highly effective in reducing HIV risk and improving care engagement [5-11]. Acceptability, interest, and actual use of eHealth HIV EBIs have also been high among racially diverse young men who have sex with men (YMSM) [12-14], who account for over 80% of new HIV diagnoses among U.S. youth [15]. Despite these promising characteristics, the implementation requirements for eHealth HIV EBIs do not fit within established models for public health scale up [16, 17], leaving it an open question as to how to move efficacious programs past the “proof-of-concept” stage. To date, almost no implementation research has examined what is needed to effectively deliver eHealth HIV EBIs in routine practice [8, 16-18].

     The research goal of the proposed study is to identify critical implementation strategies that could facilitate the dissemination and widespread use of effective eHealth HIV prevention interventions for YMSM in HIV service settings. Implementation strategies are the methods for enhancing adoption, implementation, and sustainment of a program or practice [19], but little direction exists on which and how many to select for a given implementation context [19, 20]. Therefore, establishing preliminary consensus among key HIV, eHealth, and implementation stakeholders around what strategies are necessary, acceptable, feasible, and appropriate [21] has the potential to guide the development of implementation plans and broadly improve the scalability and uptake of eHealth HIV interventions in real-world settings. The applicant will achieve the research goal by adapting the Expert Recommendations for Implementing Change (ERIC) protocol, a mixed-methods process for selecting implementation strategies for specific contexts [19, 22, 23], via two specific aims:

     Aim 1: Describe factors that may influence the adoption, implementation, and maintenance of eHealth HIV EBIs for YMSM within different HIV-service settings. This proposal builds on Keep It Up! (KIU!), an ongoing effectiveness‒implementation hybrid trial [24] of an efficacious online HIV intervention for YMSM [25]. Because eHealth HIV EBIs in general have yet to be widely disseminated, the applicant will (a) assess knowledge and awareness of eHealth among health departments, clinics, and community-based organizations (CBOs) currently providing HIV services, identified using purposive sampling. Guided by Proctor et al.’s implementation taxonomy, he will (b) examine HIV service providers’ attitudes toward eHealth HIV EBIs (e.g., acceptability, feasibility) and (c) identify processes, barriers, and facilitators of adopting, implementing, and maintaining eHealth HIV EBIs, informed by the Consolidated Framework for Implementation Research.

     Aim 2: Determine key strategies for disseminating and implementing eHealth HIV EBIs for YMSM in different HIV-service settings. Guided by the Replicating Effective Programs process model [26] and expert review, the applicant will map the taxonomy of 73 general implementation strategies developed in the original ERIC project [19] onto the identified barriers and facilitators from the first aim to (a) identify critical strategies most salient to eHealth and HIV practice settings. Then, a panel of research and practice experts—including the qualitative interview participants, eHealth HIV intervention developers, and eHealth implementation experts—will comment on and score the implementation strategies on characteristics such as importance, feasibility, and essentialness. We will also (b) explore differences in ratings by stakeholder background (researcher vs. service provider) and practice setting.

     In addition, the applicant will prepare for the development of a decision-support tool to help HIV practice settings plan for adoption and implementation of eHealth HIV EBIs. Decision-support tools are widely used in clinical care [27, 28] to facilitate changing healthcare practices. We will consolidate findings into an implementation guide for HIV practice settings interested in implementing eHealth HIV programs.

     The aims of this proposal respond directly to PAR-18-670 in that it will identify, develop, and refine strategies to disseminate and implement complex evidence-based interventions into public health practice. The proposed study will also serve as a platform for advanced postdoctoral training in mixed-methods research and implementation science for the applicant and provide him with mentorship on manuscript and grant writing in these areas. The training goal of the proposed study is to prepare the applicant for a career as an independent researcher focused on the implementation and dissemination challenges of eHealth HIV interventions.

     The research and training goals of this proposal are consistent with NIH OAR High Priorities. First, identifying implementation strategies for eHealth HIV interventions within different practice settings will help reduce incidence of HIV/AIDS by informing the implementation of interventions to improve HIV testing and entry into prevention services. It will also help reduce health disparities in the incidence of new HIV infections and in treatment outcomes for those living with HIV/AIDS, specifically among YMSM. Second, the training provided with the proposed study will enhance the capacity of the applicant to conduct future High Priority HIV/AIDS-related research as an independent scientist. ";s:5:"xhtml";s:6005:"LI - Assignment #1a<br /><br />1. Draft a specific aims page (2 pages maximum) of your proposed study.<br /><br />My response to this is...<br /><br />     Substantial investment has gone into developing and testing the efficacy of eHealth HIV prevention interventions [1-5], but these evidence-based interventions (EBIs) will not affect the spread of the epidemic unless they are effectively implemented to reach the most at-risk populations. Emerging evidence has shown that eHealth approaches can be highly effective in reducing HIV risk and improving care engagement [5-11]. Acceptability, interest, and actual use of eHealth HIV EBIs have also been high among racially diverse young men who have sex with men (YMSM) [12-14], who account for over 80% of new HIV diagnoses among U.S. youth [15]. Despite these promising characteristics, the implementation requirements for eHealth HIV EBIs do not fit within established models for public health scale up [16, 17], leaving it an open question as to how to move efficacious programs past the “proof-of-concept” stage. To date, almost no implementation research has examined what is needed to effectively deliver eHealth HIV EBIs in routine practice [8, 16-18].<br /><br />     The research goal of the proposed study is to identify critical implementation strategies that could facilitate the dissemination and widespread use of effective eHealth HIV prevention interventions for YMSM in HIV service settings. Implementation strategies are the methods for enhancing adoption, implementation, and sustainment of a program or practice [19], but little direction exists on which and how many to select for a given implementation context [19, 20]. Therefore, establishing preliminary consensus among key HIV, eHealth, and implementation stakeholders around what strategies are necessary, acceptable, feasible, and appropriate [21] has the potential to guide the development of implementation plans and broadly improve the scalability and uptake of eHealth HIV interventions in real-world settings. The applicant will achieve the research goal by adapting the Expert Recommendations for Implementing Change (ERIC) protocol, a mixed-methods process for selecting implementation strategies for specific contexts [19, 22, 23], via two specific aims:<br /><br />     Aim 1: Describe factors that may influence the adoption, implementation, and maintenance of eHealth HIV EBIs for YMSM within different HIV-service settings. This proposal builds on Keep It Up! (KIU!), an ongoing effectiveness‒implementation hybrid trial [24] of an efficacious online HIV intervention for YMSM [25]. Because eHealth HIV EBIs in general have yet to be widely disseminated, the applicant will (a) assess knowledge and awareness of eHealth among health departments, clinics, and community-based organizations (CBOs) currently providing HIV services, identified using purposive sampling. Guided by Proctor et al.’s implementation taxonomy, he will (b) examine HIV service providers’ attitudes toward eHealth HIV EBIs (e.g., acceptability, feasibility) and (c) identify processes, barriers, and facilitators of adopting, implementing, and maintaining eHealth HIV EBIs, informed by the Consolidated Framework for Implementation Research.<br /><br />     Aim 2: Determine key strategies for disseminating and implementing eHealth HIV EBIs for YMSM in different HIV-service settings. Guided by the Replicating Effective Programs process model [26] and expert review, the applicant will map the taxonomy of 73 general implementation strategies developed in the original ERIC project [19] onto the identified barriers and facilitators from the first aim to (a) identify critical strategies most salient to eHealth and HIV practice settings. Then, a panel of research and practice experts—including the qualitative interview participants, eHealth HIV intervention developers, and eHealth implementation experts—will comment on and score the implementation strategies on characteristics such as importance, feasibility, and essentialness. We will also (b) explore differences in ratings by stakeholder background (researcher vs. service provider) and practice setting.<br /><br />     In addition, the applicant will prepare for the development of a decision-support tool to help HIV practice settings plan for adoption and implementation of eHealth HIV EBIs. Decision-support tools are widely used in clinical care [27, 28] to facilitate changing healthcare practices. We will consolidate findings into an implementation guide for HIV practice settings interested in implementing eHealth HIV programs.<br /><br />     The aims of this proposal respond directly to PAR-18-670 in that it will identify, develop, and refine strategies to disseminate and implement complex evidence-based interventions into public health practice. The proposed study will also serve as a platform for advanced postdoctoral training in mixed-methods research and implementation science for the applicant and provide him with mentorship on manuscript and grant writing in these areas. The training goal of the proposed study is to prepare the applicant for a career as an independent researcher focused on the implementation and dissemination challenges of eHealth HIV interventions.<br /><br />     The research and training goals of this proposal are consistent with NIH OAR High Priorities. First, identifying implementation strategies for eHealth HIV interventions within different practice settings will help reduce incidence of HIV/AIDS by informing the implementation of interventions to improve HIV testing and entry into prevention services. It will also help reduce health disparities in the incidence of new HIV infections and in treatment outcomes for those living with HIV/AIDS, specifically among YMSM. Second, the training provided with the proposed study will enhance the capacity of the applicant to conduct future High Priority HIV/AIDS-related research as an independent scientist.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"f8df94dec7e408d4361b2e2175c25b42";}s:4:"show";b:1;s:3:"cid";s:32:"bc716ed0a70edd1abdf51d7a41e672b7";}s:32:"8d1bb7cf7c974f9dfce281e31c8b79cd";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jhaberer";s:4:"name";s:15:"Jessica Haberer";s:4:"mail";s:21:"JHABERER@PARTNERS.ORG";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535139555;}s:3:"raw";s:7853:"Haberer - Assignment #1A

1. Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&I-focused specific aims pages, it should generally address the following 5 elements (and questions). 

My response to this is… 

Specific Aims. High, sustained adherence to HIV antiretroviral therapy (ART) is critical for achieving viral suppression, which in turn leads to dramatic individual health benefits and reduction in secondary transmission of the virus (1,2). In most clinical settings, however, inadequate attention is paid to adherence. Health care providers may ask about missed doses or perform pill counts, yet these measures commonly overestimate adherence due to recall and social desirability biases (3,4). Pharmacy data can objectively detect gaps in refills, but typically weeks or months after the nonadherence occurs (5). Electronic adherence monitors (i.e., pill containers that record each opening as a proxy for medication ingestion) uniquely provide an understanding of day-to-day adherence patterns, which are particularly important for the risk of HIV viral rebound (6). These patterns enable delivery of data-informed interventions, including platforms for detailed counseling (e.g., adherence barriers discussed in the context of specific missed doses) and short message service (SMS) text messages for missed doses when using the real-time versions of electronic adherence monitors.

Multiple studies have shown electronic adherence monitoring plus these supportive interventions to be both feasible and acceptable and to improve adherence in numerous settings. One study conducted in China found an increase in adherence when SMS reminders triggered by real-time detection of missed doses were combined with data-informed counseling (i.e., day-by-day adherence records were used at clinic visits to develop solutions to adherence challenges) (7). A similarly designed study of triggered SMS reminders in South Africa (that did not include supported counseling) observed a decrease in treatment interruptions of >72 hours, although not in average adherence (8). Further, our research group conducted a pilot randomized controlled trial that tested real-time adherence monitoring plus SMS in Uganda and also found improved adherence (9). The type of SMS support in each study varied by local context and participant preference. Importantly, these devices have previously been too expensive for use in routine clinical care; however, recent developments in the technology have resulted in drastically reduced costs. Formerly, devices cost ~$150 each, but a new monitor called the evriMED1000 is currently available for $35.

Despite the evidence of improved adherence, the potential for electronic adherence monitoring and associated interventions has not been fully realized. Further research using implementation science methodology is needed to optimize these approaches for different contexts and, in particular, to enable their use in routine clinical care. This work is especially important for developing settings, which carry high disease burdens and where limited infrastructure creates fertile ground for technological development. The Consolidated Framework for Implementation Research (CFIR) is considered the gold standard framework for implementation science research and is composed of five domains (10): 1) intervention characteristics (evidentiary support, relative advantage, adaptability, trialability, and complexity); 2) the outer setting (patient needs and resources, organizational connectedness, peer pressure, external policy and incentives); 3) the inner setting (structural characteristics, networks and communications, culture, climate, readiness for implementation); 4) the characteristics of the individuals (knowledge, self-efficacy, stage of change, identification with organization); and 5) the process of implementation (adapting, executing, reflecting & evaluating, developing & tailoring).

In this study, we propose to explore the barriers and facilitators for uptake of real-time electronic adherence monitoring with the evriMED1000 plus adherence support for routine HIV clinical care in rural Uganda, using a multi-level iterative approach and following the CFIR.
   1. Define a preliminary implementation strategy. We will develop this strategy through formative research involving qualitative interviews with health care administrators/clinicians at multiple levels of the Ugandan health care system (N=35) and individuals taking ART (N=15). Content of the qualitative interviews will follow the above-noted domains of the CFIR. 
   2. Execute and optimize the implementation strategy. We will provide 30 individuals taking ART in rural Uganda with the evriMED1000 monitor and associated adherence support (as determined in the Aim 1 interviews). We will then utilize a mixed-methods approach based on CFIR domains to optimize implementation. Quantitative metrics will be collected and summarized at Month 3. We will also interview the same health care administrators/clinicians from Aim 1, as well as up to 15 of the individuals taking ART. A second iteration of this implementation strategy will be performed with 30 new individuals taking ART to fully optimize implementation.

The results of this will yield a robust implementation strategy for subsequent testing of the evriMED1000 plus support in an R01 cluster randomized trial.

References. 
1. Cohen MS, Chen YQ, McCauley M, Gamble T, Hosseinipour MC, Kumarasamy N, et al. Prevention of HIV-1 infection with early antiretroviral therapy. N Engl J Med. 2011;365(6):493-505. 
2.Conway B. The role of adherence to antiretroviral therapy in the management of HIV infection. J Acquir Immune Defic Syndr. 2007;45 Suppl 1:S14-8. 
3. Kagee A, Nel A. Assessing the association between self-report items for HIV pill adherence and biological
measures. AIDS care. 2012;24(11):1448-52. doi: 10.1080/09540121.2012.687816. PubMed PMID: 22670758.
4. Okatch H, Beiter K, Eby J, Chapman J, Marukutira T, Tshume O, Matshaba M, Anabwani GM, Gross R,
Lowenthal E. Brief Report: Apparent Antiretroviral Overadherence by Pill Count is Associated With HIV
Treatment Failure in Adolescents. J Acquir Immune Defic Syndr. 2016;72(5):542-5. 
5. Bisson GP, Gross R, Bellamy S, Chittams J, Hislop M, Regensberg L, Frank I, Maartens G, Nachega JB. Pharmacy refill adherence compared with CD4 count changes for monitoring HIV-infected adults on antiretroviral therapy. PLoS Med. 2008 May 20;5(5):e109.
6. Haberer JE, Musinguzi N, Boum Y 2nd, Siedner MJ, Mocello AR, Hunt PW, et al. Duration of Antiretroviral Therapy Adherence Interruption Is Associated With Risk of Virologic Rebound as Determined by Real-Time Adherence Monitoring in Rural Uganda. J Acquir Immune Defic Syndr. 2015;70(4):386-92. 
7. Sabin LL, Bachman DeSilva M, Gill CJ, Zhong L, Vian T, Xie W, et al. Improving Adherence to Antiretroviral Therapy With Triggered Real-time Text Message Reminders: The China Adherence Through Technology Study. J Acquir Immune Defic Syndr. 2015;69(5):551-9. 
8. Orrell C, Cohen K, Mauff K, Bangsberg DR, Maartens G, Wood R. A Randomized Controlled Trial of Real-Time Electronic Adherence Monitoring With Text Message Dosing Reminders in People Starting First-Line Antiretroviral Therapy. J Acquir Immune Defic Syndr. 2015;70(5):495-502. 
9. Haberer JE, Musiimenta A, Atukunda EC, Musinguzi N, Wyatt MA, Ware NC, et al. Short message service (SMS) reminders and real-time adherence monitoring improve antiretroviral therapy adherence in rural Uganda. AIDS. 2016;30(8):1295-300.
10. Damschroder LJ, Aron DC, Keith RE, Kirsh SR, Alexander JA, Lowery JC. Fostering implementation of health services research findings into practice: a consolidated framework for advancing implementation science. Implement Sci. 2009;4:50. ";s:5:"xhtml";s:8022:"Haberer - Assignment #1A<br /><br />1. Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&amp;I-focused specific aims pages, it should generally address the following 5 elements (and questions). <br /><br />My response to this is… <br /><br />Specific Aims. High, sustained adherence to HIV antiretroviral therapy (ART) is critical for achieving viral suppression, which in turn leads to dramatic individual health benefits and reduction in secondary transmission of the virus (1,2). In most clinical settings, however, inadequate attention is paid to adherence. Health care providers may ask about missed doses or perform pill counts, yet these measures commonly overestimate adherence due to recall and social desirability biases (3,4). Pharmacy data can objectively detect gaps in refills, but typically weeks or months after the nonadherence occurs (5). Electronic adherence monitors (i.e., pill containers that record each opening as a proxy for medication ingestion) uniquely provide an understanding of day-to-day adherence patterns, which are particularly important for the risk of HIV viral rebound (6). These patterns enable delivery of data-informed interventions, including platforms for detailed counseling (e.g., adherence barriers discussed in the context of specific missed doses) and short message service (SMS) text messages for missed doses when using the real-time versions of electronic adherence monitors.<br /><br />Multiple studies have shown electronic adherence monitoring plus these supportive interventions to be both feasible and acceptable and to improve adherence in numerous settings. One study conducted in China found an increase in adherence when SMS reminders triggered by real-time detection of missed doses were combined with data-informed counseling (i.e., day-by-day adherence records were used at clinic visits to develop solutions to adherence challenges) (7). A similarly designed study of triggered SMS reminders in South Africa (that did not include supported counseling) observed a decrease in treatment interruptions of &gt;72 hours, although not in average adherence (8). Further, our research group conducted a pilot randomized controlled trial that tested real-time adherence monitoring plus SMS in Uganda and also found improved adherence (9). The type of SMS support in each study varied by local context and participant preference. Importantly, these devices have previously been too expensive for use in routine clinical care; however, recent developments in the technology have resulted in drastically reduced costs. Formerly, devices cost ~$150 each, but a new monitor called the evriMED1000 is currently available for $35.<br /><br />Despite the evidence of improved adherence, the potential for electronic adherence monitoring and associated interventions has not been fully realized. Further research using implementation science methodology is needed to optimize these approaches for different contexts and, in particular, to enable their use in routine clinical care. This work is especially important for developing settings, which carry high disease burdens and where limited infrastructure creates fertile ground for technological development. The Consolidated Framework for Implementation Research (CFIR) is considered the gold standard framework for implementation science research and is composed of five domains (10): 1) intervention characteristics (evidentiary support, relative advantage, adaptability, trialability, and complexity); 2) the outer setting (patient needs and resources, organizational connectedness, peer pressure, external policy and incentives); 3) the inner setting (structural characteristics, networks and communications, culture, climate, readiness for implementation); 4) the characteristics of the individuals (knowledge, self-efficacy, stage of change, identification with organization); and 5) the process of implementation (adapting, executing, reflecting &amp; evaluating, developing &amp; tailoring).<br /><br />In this study, we propose to explore the barriers and facilitators for uptake of real-time electronic adherence monitoring with the evriMED1000 plus adherence support for routine HIV clinical care in rural Uganda, using a multi-level iterative approach and following the CFIR.<br />   1. Define a preliminary implementation strategy. We will develop this strategy through formative research involving qualitative interviews with health care administrators/clinicians at multiple levels of the Ugandan health care system (N=35) and individuals taking ART (N=15). Content of the qualitative interviews will follow the above-noted domains of the CFIR. <br />   2. Execute and optimize the implementation strategy. We will provide 30 individuals taking ART in rural Uganda with the evriMED1000 monitor and associated adherence support (as determined in the Aim 1 interviews). We will then utilize a mixed-methods approach based on CFIR domains to optimize implementation. Quantitative metrics will be collected and summarized at Month 3. We will also interview the same health care administrators/clinicians from Aim 1, as well as up to 15 of the individuals taking ART. A second iteration of this implementation strategy will be performed with 30 new individuals taking ART to fully optimize implementation.<br /><br />The results of this will yield a robust implementation strategy for subsequent testing of the evriMED1000 plus support in an R01 cluster randomized trial.<br /><br />References. <br />1. Cohen MS, Chen YQ, McCauley M, Gamble T, Hosseinipour MC, Kumarasamy N, et al. Prevention of HIV-1 infection with early antiretroviral therapy. N Engl J Med. 2011;365(6):493-505. <br />2.Conway B. The role of adherence to antiretroviral therapy in the management of HIV infection. J Acquir Immune Defic Syndr. 2007;45 Suppl 1:S14-8. <br />3. Kagee A, Nel A. Assessing the association between self-report items for HIV pill adherence and biological<br />measures. AIDS care. 2012;24(11):1448-52. doi: 10.1080/09540121.2012.687816. PubMed PMID: 22670758.<br />4. Okatch H, Beiter K, Eby J, Chapman J, Marukutira T, Tshume O, Matshaba M, Anabwani GM, Gross R,<br />Lowenthal E. Brief Report: Apparent Antiretroviral Overadherence by Pill Count is Associated With HIV<br />Treatment Failure in Adolescents. J Acquir Immune Defic Syndr. 2016;72(5):542-5. <br />5. Bisson GP, Gross R, Bellamy S, Chittams J, Hislop M, Regensberg L, Frank I, Maartens G, Nachega JB. Pharmacy refill adherence compared with CD4 count changes for monitoring HIV-infected adults on antiretroviral therapy. PLoS Med. 2008 May 20;5(5):e109.<br />6. Haberer JE, Musinguzi N, Boum Y 2nd, Siedner MJ, Mocello AR, Hunt PW, et al. Duration of Antiretroviral Therapy Adherence Interruption Is Associated With Risk of Virologic Rebound as Determined by Real-Time Adherence Monitoring in Rural Uganda. J Acquir Immune Defic Syndr. 2015;70(4):386-92. <br />7. Sabin LL, Bachman DeSilva M, Gill CJ, Zhong L, Vian T, Xie W, et al. Improving Adherence to Antiretroviral Therapy With Triggered Real-time Text Message Reminders: The China Adherence Through Technology Study. J Acquir Immune Defic Syndr. 2015;69(5):551-9. <br />8. Orrell C, Cohen K, Mauff K, Bangsberg DR, Maartens G, Wood R. A Randomized Controlled Trial of Real-Time Electronic Adherence Monitoring With Text Message Dosing Reminders in People Starting First-Line Antiretroviral Therapy. J Acquir Immune Defic Syndr. 2015;70(5):495-502. <br />9. Haberer JE, Musiimenta A, Atukunda EC, Musinguzi N, Wyatt MA, Ware NC, et al. Short message service (SMS) reminders and real-time adherence monitoring improve antiretroviral therapy adherence in rural Uganda. AIDS. 2016;30(8):1295-300.<br />10. Damschroder LJ, Aron DC, Keith RE, Kirsh SR, Alexander JA, Lowery JC. Fostering implementation of health services research findings into practice: a consolidated framework for advancing implementation science. Implement Sci. 2009;4:50.";s:6:"parent";N;s:7:"replies";a:2:{i:0;s:32:"2437dcff7772f46c21e9be5ec230709b";i:1;s:32:"ccded52ddfa335025124b3e070988540";}s:4:"show";b:1;s:3:"cid";s:32:"8d1bb7cf7c974f9dfce281e31c8b79cd";}s:32:"69575f3735bd49958a012b30cf7b4484";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"awagner";s:4:"name";s:13:"Anjuli Wagner";s:4:"mail";s:14:"anjuliw@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535140682;}s:3:"raw";s:5506:"WAGNER -- Assignment #1a

SIGNIFICANCE

Pregnancy and postpartum are high-risk periods for HIV acquisition and subsequent risk of mother-to-child-transmission of HIV. Female-controlled interventions like pre-exposure prophylaxis (PrEP) are needed to curb HIV incidence for this vulnerable and time-defined population. PrEP is recommended during pregnancy and postpartum in high incidence settings with safety data that supports these recommendations. The ongoing trial PrIMA (NIH R01: John-Stewart, Baeten) and recently completed implementation project PrIYA (PEPFAR DREAMS; John-Stewart, Baeten) have integrated PrEP delivery into routine antenatal care (ANC). The PrIYA project screened >10,000 women and initiated ~4,100 on PrEP and is the first and largest implementation project for PrEP delivery during pregnancy in the world. 

These large projects noted implementation challenges in integrating PrEP delivery into routine ANC. Our team noted that integration of PrEP into ANC added an additional 18 minutes for clients who initiate PrEP and 13 minutes for those who do not (Pintye, JAIDS, 2018). Additionally, there was substantial heterogeneity and gaps between clinics in the coverage of PrEP screening, limiting uptake of subsequent steps; coverage ranged between X% and Y% and was associated with X, Y, Z factors.  

It is in this context that we propose this career development award to use implementation science determinants and outcomes frameworks to identify, and subsequently test, specific implementation strategies to improve integration of PrEP delivery in ANC. This study will partner with several routine, programmatic, “real world” clinics to test strategies to overcome implementation barriers. As PrEP provision during pregnancy expands in Kenya, this is an opportune moment to leverage our team’s robust research infrastructure, strong stakeholder partnerships, and unique political will in Kenya to test specific implementation strategies to optimize scale up of this important intervention. 

An implementation science approach is ideal for these optimization efforts; our team has identified key determinants of implementation success during PrIMA and PrIYA, allowing efficient and informed selection of implementation strategies that merit testing. These implementation barriers include increased total number of HIV testing (HTS) sessions with static numbers of providers, additional risk screening questionnaires, and limited numbers of PrEP prescribers. Currently, ANC nurses are responsible for PrEP screening, risk assessment, counseling, prescribing, and dispensing. We propose to test 4 time-saving implementation strategies to improve coverage of PrEP screening: a) saliva-based self-testing for HIV, b) self-administered risk assessment, c) group-based PrEP counseling, and d) task-shifting PrEP screening and counseling to HTS providers. Robust evaluation of these efforts is required; RE-AIM is an evaluative implementation science framework to understand coverage as a function of reach, effectiveness, adoption, implementation, and maintenance. Finally, discrete event simulation (DES) modeling is a flexible, but under-utilized tool, to simulate and prioritize change concepts for future empiric testing within large implementation trials and evaluate cost-effectiveness and budget impact.

The proposed study would provide Dr. Wagner with training in implementation science evaluative frameworks, implementation strategy testing, health systems research, operational research modeling, and budget impact analysis. It would expand her scientific expertise from pediatric and adolescent HIV to include pregnant and postpartum populations and to expand her mentorship and collaborators in Kenya. Dr. Wagner would propose to test promising implementation strategies in an R01 following her career development award.

SPECIFIC AIMS

Aim 1: To test whether a) saliva-based self-testing for HIV, b) self-administered risk assessment, c) group-based PrEP counseling, and d) task-shifting PrEP screening and counseling to HTS providers reduces provider time spent during an ANC visit and improves the implementation outcomes of PrEP screening coverage. We will test each one of these 4 strategies in 3 clinics (12 total intervention clinics and 3 additional control clinics) using an interrupted time-series design with 1 year of baseline, 1 year of a single intervention, and 1 year of a combination of 2 strategies. The most successful strategies will be tested in combination in a future R01 trial.

Aim 2: To evaluate the implementation strategies using the RE-AIM (reach, effectiveness, adoption, implementation, and maintenance) framework. We will conduct a mixed-methods evaluation of the implementation process, identify dimensions of low performance, and utilize these results to modify future implementation strategy introduction and testing. These results will adapt the most promising strategies identified in Aim 1 for the future R01.

Aim 3: To determine the budget impact of each implementation strategy and simulate operational time gains using discrete event simulation (DES) models. We will create a DES model that simulates individual patient flow in a complex clinic system, and test (using the simulation model) micro-changes to clinic organization, staffing, and operations to relieve bottlenecks and improve system flow. We will additionally collect cost data to integrate into the DES model to evaluate the budget impact of each strategy.
";s:5:"xhtml";s:5608:"WAGNER -- Assignment #1a<br /><br />SIGNIFICANCE<br /><br />Pregnancy and postpartum are high-risk periods for HIV acquisition and subsequent risk of mother-to-child-transmission of HIV. Female-controlled interventions like pre-exposure prophylaxis (PrEP) are needed to curb HIV incidence for this vulnerable and time-defined population. PrEP is recommended during pregnancy and postpartum in high incidence settings with safety data that supports these recommendations. The ongoing trial PrIMA (NIH R01: John-Stewart, Baeten) and recently completed implementation project PrIYA (PEPFAR DREAMS; John-Stewart, Baeten) have integrated PrEP delivery into routine antenatal care (ANC). The PrIYA project screened &gt;10,000 women and initiated ~4,100 on PrEP and is the first and largest implementation project for PrEP delivery during pregnancy in the world. <br /><br />These large projects noted implementation challenges in integrating PrEP delivery into routine ANC. Our team noted that integration of PrEP into ANC added an additional 18 minutes for clients who initiate PrEP and 13 minutes for those who do not (Pintye, JAIDS, 2018). Additionally, there was substantial heterogeneity and gaps between clinics in the coverage of PrEP screening, limiting uptake of subsequent steps; coverage ranged between X% and Y% and was associated with X, Y, Z factors.  <br /><br />It is in this context that we propose this career development award to use implementation science determinants and outcomes frameworks to identify, and subsequently test, specific implementation strategies to improve integration of PrEP delivery in ANC. This study will partner with several routine, programmatic, “real world” clinics to test strategies to overcome implementation barriers. As PrEP provision during pregnancy expands in Kenya, this is an opportune moment to leverage our team’s robust research infrastructure, strong stakeholder partnerships, and unique political will in Kenya to test specific implementation strategies to optimize scale up of this important intervention. <br /><br />An implementation science approach is ideal for these optimization efforts; our team has identified key determinants of implementation success during PrIMA and PrIYA, allowing efficient and informed selection of implementation strategies that merit testing. These implementation barriers include increased total number of HIV testing (HTS) sessions with static numbers of providers, additional risk screening questionnaires, and limited numbers of PrEP prescribers. Currently, ANC nurses are responsible for PrEP screening, risk assessment, counseling, prescribing, and dispensing. We propose to test 4 time-saving implementation strategies to improve coverage of PrEP screening: a) saliva-based self-testing for HIV, b) self-administered risk assessment, c) group-based PrEP counseling, and d) task-shifting PrEP screening and counseling to HTS providers. Robust evaluation of these efforts is required; RE-AIM is an evaluative implementation science framework to understand coverage as a function of reach, effectiveness, adoption, implementation, and maintenance. Finally, discrete event simulation (DES) modeling is a flexible, but under-utilized tool, to simulate and prioritize change concepts for future empiric testing within large implementation trials and evaluate cost-effectiveness and budget impact.<br /><br />The proposed study would provide Dr. Wagner with training in implementation science evaluative frameworks, implementation strategy testing, health systems research, operational research modeling, and budget impact analysis. It would expand her scientific expertise from pediatric and adolescent HIV to include pregnant and postpartum populations and to expand her mentorship and collaborators in Kenya. Dr. Wagner would propose to test promising implementation strategies in an R01 following her career development award.<br /><br />SPECIFIC AIMS<br /><br />Aim 1: To test whether a) saliva-based self-testing for HIV, b) self-administered risk assessment, c) group-based PrEP counseling, and d) task-shifting PrEP screening and counseling to HTS providers reduces provider time spent during an ANC visit and improves the implementation outcomes of PrEP screening coverage. We will test each one of these 4 strategies in 3 clinics (12 total intervention clinics and 3 additional control clinics) using an interrupted time-series design with 1 year of baseline, 1 year of a single intervention, and 1 year of a combination of 2 strategies. The most successful strategies will be tested in combination in a future R01 trial.<br /><br />Aim 2: To evaluate the implementation strategies using the RE-AIM (reach, effectiveness, adoption, implementation, and maintenance) framework. We will conduct a mixed-methods evaluation of the implementation process, identify dimensions of low performance, and utilize these results to modify future implementation strategy introduction and testing. These results will adapt the most promising strategies identified in Aim 1 for the future R01.<br /><br />Aim 3: To determine the budget impact of each implementation strategy and simulate operational time gains using discrete event simulation (DES) models. We will create a DES model that simulates individual patient flow in a complex clinic system, and test (using the simulation model) micro-changes to clinic organization, staffing, and operations to relieve bottlenecks and improve system flow. We will additionally collect cost data to integrate into the DES model to evaluate the budget impact of each strategy.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"69575f3735bd49958a012b30cf7b4484";}s:32:"7c0e2beb9ef8381ae5d7fbfc39204e4b";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535398877;}s:3:"raw";s:458:"Hi Anjuli,

Thanks for this well thought out draft of your specific aims. You described the EBP, PrEP, the implementation gap and the aims clearly.   You may want to add a bit more about the context of the clinics in Kenya -- rural, urban etc., # of trained/untrained HCW in the clinic.   Also details about the method and populations in which you identified the barriers would be useful.   Do you think you need more formative research?  

Thanks,

Linda

 ";s:5:"xhtml";s:485:"Hi Anjuli,<br /><br />Thanks for this well thought out draft of your specific aims. You described the EBP, PrEP, the implementation gap and the aims clearly.   You may want to add a bit more about the context of the clinics in Kenya -- rural, urban etc., # of trained/untrained HCW in the clinic.   Also details about the method and populations in which you identified the barriers would be useful.   Do you think you need more formative research?  <br /><br />Thanks,<br /><br />Linda";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"325f0788e2d3a2b88c52ba7ff71d8344";}s:4:"show";b:1;s:3:"cid";s:32:"7c0e2beb9ef8381ae5d7fbfc39204e4b";}s:32:"97c58b1c2db90ae9070370aba89f966f";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535662601;}s:3:"raw";s:442:"Hi Kristin,

Thanks for your submission of the first part of the first assignment.  The project sounds very relevant and useful to the underlying RO1. I had a few questions.  Will you be accessing the acceptability of the intervention to the caregivers in your context? Are there other stakeholders who need to be part of your study from the start (MOH for example)?.  Will there be a cost analysis?  I believe you are off to a good start!   ";s:5:"xhtml";s:449:"Hi Kristin,<br /><br />Thanks for your submission of the first part of the first assignment.  The project sounds very relevant and useful to the underlying RO1. I had a few questions.  Will you be accessing the acceptability of the intervention to the caregivers in your context? Are there other stakeholders who need to be part of your study from the start (MOH for example)?.  Will there be a cost analysis?  I believe you are off to a good start!";s:6:"parent";s:32:"c6823b7e57ae6c23bb5e0e0159c0765f";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"97c58b1c2db90ae9070370aba89f966f";}s:32:"325f0788e2d3a2b88c52ba7ff71d8344";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"awagner";s:4:"name";s:13:"Anjuli Wagner";s:4:"mail";s:14:"anjuliw@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535673439;}s:3:"raw";s:5456:"Dear Dr. Kupfer,

Thank you for the thoughtful comments; they are much appreciated. I revised the aims this past week and have included a revised version here (more formative work, stronger focus on implementation strategies) in case you might have a chance to review. I will be sure to include details about the clinics (rural/urban, # trained/untrained HCW, etc.) in subsequent drafts.

Thank you for your support and guidance. 


 SIGNIFICANCE

Pregnancy and postpartum are high-risk periods for HIV acquisition and subsequent risk of mother-to-child-transmission of HIV. Pre-exposure prophylaxis (PrEP) is an effective, female-controlled, evidence-based intervention for HIV prevention that is recommended during pregnancy and postpartum in high incidence settings with safety data that supports these recommendations. However, PrEP is not routinely provided during the perinatal period in low-resource, high burden settings globally. 

The ongoing trial PrIMA (NIAID R01: John-Stewart, Baeten) and recently completed implementation project PrIYA (PEPFAR DREAMS; John-Stewart, Baeten) in Kenya have integrated PrEP delivery into routine antenatal care (ANC). The PrIYA project screened >10,000 women and initiated ~4,100 on PrEP and is the first and largest implementation project for PrEP delivery during pregnancy in the world. As PrEP provision during pregnancy expands, this is an opportune moment to leverage our team’s unique implementation experience, robust research infrastructure, and strong stakeholder partnerships and political will in Kenya to identify and test specific implementation strategies (ISs) to increase penetration and sustainability of this critical evidence-based intervention.

These large projects noted implementation barriers in integrating PrEP delivery into routine ANC. Integration added additional workload for overburdened HCW by increasing total number of HIV testing (HTS) sessions and adding risk screening questionnaires with static numbers of providers. Our team noted that integration of PrEP into ANC added an additional 18 minutes for clients who initiate PrEP and 13 minutes for those who do not (Pintye, JAIDS, 2018). Additionally, there was substantial heterogeneity and gaps between clinics in the coverage of PrEP screening, limiting uptake of subsequent steps; coverage ranged between X% and Y% and was associated with X, Y, Z factors. Health care workers (HCW) in these projects experimented organically with a range of ISs; systematically evaluating HCW experiences with the specific ISs used can efficiently identify modified and novel ISs that merit large scale testing. 

In this context, we propose this career development award to identify, and subsequently test, specific implementation strategies to improve penetration and sustainability of integrated PrEP delivery in ANC. This study will utilize the unique experiences of health care workers (HCW) who delivered PrEP in PrIYA and PrIMA to identify promising implementation strategies. We will then partner with several routine, programmatic, “real world” clinics to test 3 novel or modified implementation strategies to improve penetration and sustainability of PrEP integration into ANC, measured using PrEP screening. 

The proposed study would provide Dr. Wagner with training in implementation determinants and outcomes frameworks, implementation strategy identification and testing, health systems research, qualitative methods, and budget impact analysis. It would expand her scientific expertise from pediatric and adolescent HIV to include peripartum populations and to expand her mentorship and collaborators in Kenya. Dr. Wagner would propose to test promising implementation strategies in an R01 following her career development award.

SPECIFIC AIMS

Aim 1: To identify promising implementation strategies (ISs) to increase penetration and sustainability of PrEP integration into ANC. We will make use of the heterogeneous ISs used in PrIMA and PrIYA to characterize experiences with specific ISs and elucidate opinions on modifications to these specific ISs to further improve penetration and sustainability. We will use a mixed-methods approach, combining qualitative and quantitative data.

Aim 2: To test whether 3 new or modified implementation strategies improve penetration and sustainability of PrEP integration into ANC using a series of experiments. We will use either rapid-cycle, small-scale randomized controlled trials (RCTs) or interrupted time series (ITS) studies to test each of these ISs against standard practice; we will use 3 months of baseline and 3 months of intervention time for each experiment. We will evaluate each IS using a mixed-methods approach based in the RE-AIM framework, which is an evaluative implementation science framework to understand coverage as a function of reach, effectiveness, adoption, implementation, and maintenance.

Aim 3: To determine the budget impact of each implementation strategy and assess policymaker understanding and interpretation of budget impact analyses. We will conduct a budget impact analysis that assesses the affordability of each strategy within the context of a Kenyan public health care system. We will disseminate these results to key policymakers at national and county levels and conduct semi-structured in-depth interviews to assess policymaker understanding of the analysis, as well as likelihood of making changes based on these results.
";s:5:"xhtml";s:5583:"Dear Dr. Kupfer,<br /><br />Thank you for the thoughtful comments; they are much appreciated. I revised the aims this past week and have included a revised version here (more formative work, stronger focus on implementation strategies) in case you might have a chance to review. I will be sure to include details about the clinics (rural/urban, # trained/untrained HCW, etc.) in subsequent drafts.<br /><br />Thank you for your support and guidance. <br /><br /><br /> SIGNIFICANCE<br /><br />Pregnancy and postpartum are high-risk periods for HIV acquisition and subsequent risk of mother-to-child-transmission of HIV. Pre-exposure prophylaxis (PrEP) is an effective, female-controlled, evidence-based intervention for HIV prevention that is recommended during pregnancy and postpartum in high incidence settings with safety data that supports these recommendations. However, PrEP is not routinely provided during the perinatal period in low-resource, high burden settings globally. <br /><br />The ongoing trial PrIMA (NIAID R01: John-Stewart, Baeten) and recently completed implementation project PrIYA (PEPFAR DREAMS; John-Stewart, Baeten) in Kenya have integrated PrEP delivery into routine antenatal care (ANC). The PrIYA project screened &gt;10,000 women and initiated ~4,100 on PrEP and is the first and largest implementation project for PrEP delivery during pregnancy in the world. As PrEP provision during pregnancy expands, this is an opportune moment to leverage our team’s unique implementation experience, robust research infrastructure, and strong stakeholder partnerships and political will in Kenya to identify and test specific implementation strategies (ISs) to increase penetration and sustainability of this critical evidence-based intervention.<br /><br />These large projects noted implementation barriers in integrating PrEP delivery into routine ANC. Integration added additional workload for overburdened HCW by increasing total number of HIV testing (HTS) sessions and adding risk screening questionnaires with static numbers of providers. Our team noted that integration of PrEP into ANC added an additional 18 minutes for clients who initiate PrEP and 13 minutes for those who do not (Pintye, JAIDS, 2018). Additionally, there was substantial heterogeneity and gaps between clinics in the coverage of PrEP screening, limiting uptake of subsequent steps; coverage ranged between X% and Y% and was associated with X, Y, Z factors. Health care workers (HCW) in these projects experimented organically with a range of ISs; systematically evaluating HCW experiences with the specific ISs used can efficiently identify modified and novel ISs that merit large scale testing. <br /><br />In this context, we propose this career development award to identify, and subsequently test, specific implementation strategies to improve penetration and sustainability of integrated PrEP delivery in ANC. This study will utilize the unique experiences of health care workers (HCW) who delivered PrEP in PrIYA and PrIMA to identify promising implementation strategies. We will then partner with several routine, programmatic, “real world” clinics to test 3 novel or modified implementation strategies to improve penetration and sustainability of PrEP integration into ANC, measured using PrEP screening. <br /><br />The proposed study would provide Dr. Wagner with training in implementation determinants and outcomes frameworks, implementation strategy identification and testing, health systems research, qualitative methods, and budget impact analysis. It would expand her scientific expertise from pediatric and adolescent HIV to include peripartum populations and to expand her mentorship and collaborators in Kenya. Dr. Wagner would propose to test promising implementation strategies in an R01 following her career development award.<br /><br />SPECIFIC AIMS<br /><br />Aim 1: To identify promising implementation strategies (ISs) to increase penetration and sustainability of PrEP integration into ANC. We will make use of the heterogeneous ISs used in PrIMA and PrIYA to characterize experiences with specific ISs and elucidate opinions on modifications to these specific ISs to further improve penetration and sustainability. We will use a mixed-methods approach, combining qualitative and quantitative data.<br /><br />Aim 2: To test whether 3 new or modified implementation strategies improve penetration and sustainability of PrEP integration into ANC using a series of experiments. We will use either rapid-cycle, small-scale randomized controlled trials (RCTs) or interrupted time series (ITS) studies to test each of these ISs against standard practice; we will use 3 months of baseline and 3 months of intervention time for each experiment. We will evaluate each IS using a mixed-methods approach based in the RE-AIM framework, which is an evaluative implementation science framework to understand coverage as a function of reach, effectiveness, adoption, implementation, and maintenance.<br /><br />Aim 3: To determine the budget impact of each implementation strategy and assess policymaker understanding and interpretation of budget impact analyses. We will conduct a budget impact analysis that assesses the affordability of each strategy within the context of a Kenyan public health care system. We will disseminate these results to key policymakers at national and county levels and conduct semi-structured in-depth interviews to assess policymaker understanding of the analysis, as well as likelihood of making changes based on these results.";s:6:"parent";s:32:"7c0e2beb9ef8381ae5d7fbfc39204e4b";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"325f0788e2d3a2b88c52ba7ff71d8344";}s:32:"44a3d286b08b437180a846720c471964";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535733923;}s:3:"raw";s:392:"Hi  Anjuli,

Thanks for this revision!  I like the addition of the costing, and suggest that rather than only  dissemination to the policymakers at the end of the project, you bring them in at the start of the project (with other stakeholders) to consider any questions and specific concerns that they have with implementing PrEP in ANCs that you might be able to address in your project.    ";s:5:"xhtml";s:398:"Hi  Anjuli,<br /><br />Thanks for this revision!  I like the addition of the costing, and suggest that rather than only  dissemination to the policymakers at the end of the project, you bring them in at the start of the project (with other stakeholders) to consider any questions and specific concerns that they have with implementing PrEP in ANCs that you might be able to address in your project.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"44a3d286b08b437180a846720c471964";}s:32:"35d7c10c25aa5d9388029cb9aa2d5f06";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536008901;}s:3:"raw";s:819:"Hi Melissa,

Thanks so much for this response for Assignment 1A.  I am so sorry that this reply is so late and will totally understand if you can't change your presentation for tomorrow.  In reading your response, I found myself wondering in which populations the SMART program has been used thus far and if it indeed has proven to be efficacious.   I also was not certain where you would be testing SMART --   the US, but what state, city -- multiple?  Also, will you be testing all of the different versions to all the populations of Spanish speaking Latinos?  Have any similar programs been used in this population?  How will you look at demand for SMART?  And finally, how does AIM 3 fit into your project -- policy level changes?   We can discuss these tomorrow.  I look forward to speaking with you!

Best,

Linda";s:5:"xhtml";s:854:"Hi Melissa,<br /><br />Thanks so much for this response for Assignment 1A.  I am so sorry that this reply is so late and will totally understand if you can&#039;t change your presentation for tomorrow.  In reading your response, I found myself wondering in which populations the SMART program has been used thus far and if it indeed has proven to be efficacious.   I also was not certain where you would be testing SMART --   the US, but what state, city -- multiple?  Also, will you be testing all of the different versions to all the populations of Spanish speaking Latinos?  Have any similar programs been used in this population?  How will you look at demand for SMART?  And finally, how does AIM 3 fit into your project -- policy level changes?   We can discuss these tomorrow.  I look forward to speaking with you!<br /><br />Best,<br /><br />Linda";s:6:"parent";s:32:"87417dbea2b01906d069218b9e05b3d3";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"35d7c10c25aa5d9388029cb9aa2d5f06";}s:32:"59f6e88e0cbd1b14b788b7102a411abe";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536010081;}s:3:"raw";s:826:"Hi Christina, 
I am so sorry that this is late and do not expect that you will be able to incorporate these questions into your presentation tomorrow.  We can simply discuss these and other questions during the call.  First thanks for your thoughtful response to assignment #1A.  If you have not yet conducted a lit review of where PrEP has been used in similar populations you may want to do one prior to the formative work you propose.   The formative work seems very useful, -- you may need to be careful about the OMB paperwork reduction act and just interview 9 people in each group.  Is there a way to identify high-risk women in this population or will you want to give PrEP to all AA HIV-uninfected women who come into the clinic?   I look forward to speaking with you about your project tomorrow.

Best,

Linda

     ";s:5:"xhtml";s:844:"Hi Christina, <br />I am so sorry that this is late and do not expect that you will be able to incorporate these questions into your presentation tomorrow.  We can simply discuss these and other questions during the call.  First thanks for your thoughtful response to assignment #1A.  If you have not yet conducted a lit review of where PrEP has been used in similar populations you may want to do one prior to the formative work you propose.   The formative work seems very useful, -- you may need to be careful about the OMB paperwork reduction act and just interview 9 people in each group.  Is there a way to identify high-risk women in this population or will you want to give PrEP to all AA HIV-uninfected women who come into the clinic?   I look forward to speaking with you about your project tomorrow.<br /><br />Best,<br /><br />Linda";s:6:"parent";s:32:"22c7c55c8f26270ecddd896924b0811b";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"59f6e88e0cbd1b14b788b7102a411abe";}s:32:"f8df94dec7e408d4361b2e2175c25b42";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536011507;}s:3:"raw";s:434:"Hi Dennis,
I apologize for this late response to your assignment reply.  Lucky for us, your response seems very thoughtful and you seem to have a good handle on the aims of your  project.  One questions I had was are the eHealth EBI that you will use good for all patient populations or will you focus on something specifically for young men (adolescents?)?  

I look forward to discussing this more with you tomorrow.  

Best,

Linda";s:5:"xhtml";s:469:"Hi Dennis,<br />I apologize for this late response to your assignment reply.  Lucky for us, your response seems very thoughtful and you seem to have a good handle on the aims of your  project.  One questions I had was are the eHealth EBI that you will use good for all patient populations or will you focus on something specifically for young men (adolescents?)?  <br /><br />I look forward to discussing this more with you tomorrow.  <br /><br />Best,<br /><br />Linda";s:6:"parent";s:32:"bc716ed0a70edd1abdf51d7a41e672b7";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"f8df94dec7e408d4361b2e2175c25b42";}s:32:"2437dcff7772f46c21e9be5ec230709b";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536012124;}s:3:"raw";s:908:"
Hi Jessica,

I apologize for this very late response to your submission for Assignment 1A.  Do not feel you need to change your presentation for tomorrow -- we can discuss questions in real time!  When you do the qualitative interviews you need to check if you have to submit something to OMB for the Paperwork Reduction Act -- I an not sure what the status is now.  It used to be you could only interview 9 people without clearance.  Would you consider this a pilot project?  Will everything be done in one clinic?    Do you see doing this on a larger scale depending on your results?  I assume that the MOH will pay for the device, right?  If so will they be included in your formative research -- are they the HC administrators, one of the levels you refer to?  I think you have done a very nice job in articulating your aims.  I look forward to discussing this more with you tomorrow.     

Best,

Linda";s:5:"xhtml";s:937:"Hi Jessica,<br /><br />I apologize for this very late response to your submission for Assignment 1A.  Do not feel you need to change your presentation for tomorrow -- we can discuss questions in real time!  When you do the qualitative interviews you need to check if you have to submit something to OMB for the Paperwork Reduction Act -- I an not sure what the status is now.  It used to be you could only interview 9 people without clearance.  Would you consider this a pilot project?  Will everything be done in one clinic?    Do you see doing this on a larger scale depending on your results?  I assume that the MOH will pay for the device, right?  If so will they be included in your formative research -- are they the HC administrators, one of the levels you refer to?  I think you have done a very nice job in articulating your aims.  I look forward to discussing this more with you tomorrow.     <br /><br />Best,<br /><br />Linda";s:6:"parent";s:32:"8d1bb7cf7c974f9dfce281e31c8b79cd";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"2437dcff7772f46c21e9be5ec230709b";}s:32:"ce3d534101594a406a2316dcc1b373b4";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"twiley";s:4:"name";s:11:"Tisha Wiley";s:4:"mail";s:19:"tisha.wiley@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536075909;}s:3:"raw";s:743:"Hi Christina,
Apologies for the late feedback. A few pieces of feedback I would like to add to Linda's feedback: 
1. In my experience, the OMB paperwork reduction regulations do not tend to apply to grants, so you can keep your target for interviews higher than 9. 
2. I think more detail is needed on what the existing PrEP delivery protocol is, why it is inadequate, and what general direction you expect modifications to take. 
3. What activities would be involved in Aim 2b. Some degree of detail is needed. 
4. Aim 2a: What does this look like methodologically? How many participants? Followed for how-long? 
5. Generalizability: How many clinics will you engage? How do you expect these findings to generalize beyond the targeted FQHCs? ";s:5:"xhtml";s:777:"Hi Christina,<br />Apologies for the late feedback. A few pieces of feedback I would like to add to Linda&#039;s feedback: <br />1. In my experience, the OMB paperwork reduction regulations do not tend to apply to grants, so you can keep your target for interviews higher than 9. <br />2. I think more detail is needed on what the existing PrEP delivery protocol is, why it is inadequate, and what general direction you expect modifications to take. <br />3. What activities would be involved in Aim 2b. Some degree of detail is needed. <br />4. Aim 2a: What does this look like methodologically? How many participants? Followed for how-long? <br />5. Generalizability: How many clinics will you engage? How do you expect these findings to generalize beyond the targeted FQHCs?";s:6:"parent";s:32:"22c7c55c8f26270ecddd896924b0811b";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"ce3d534101594a406a2316dcc1b373b4";}s:32:"ccded52ddfa335025124b3e070988540";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"twiley";s:4:"name";s:11:"Tisha Wiley";s:4:"mail";s:19:"tisha.wiley@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536076838;}s:3:"raw";s:981:"Hi Jessica,
Apologies as well for the delayed response to your concept. A few pieces of feedback in addition to Linda's: 
1. In my experience, grants are typically not affected by the OMB regulations so formative work with >9 people tends to be fine so long as you obtain IRB approval. 
2. I think you are headed in the right direction, but a successful application will require more specificity on your expected implementation strategy. Qualitative work can be used to refine that strategy, but your application will need to propose a concrete starting point. 
3. The iterative development approach is strong, but may be too ambitious with a N=30 for a R34 or R21 implementation strategy. 
4. What are your targeted outcomes? Are they implementation outcomes or individual outcomes? 
5. You may want to give more detail regarding the expected implementation barriers. It is unclear to me from this what these might be and what you might do to overcome them. 

Warm regards, 
Tisha";s:5:"xhtml";s:1034:"Hi Jessica,<br />Apologies as well for the delayed response to your concept. A few pieces of feedback in addition to Linda&#039;s: <br />1. In my experience, grants are typically not affected by the OMB regulations so formative work with &gt;9 people tends to be fine so long as you obtain IRB approval. <br />2. I think you are headed in the right direction, but a successful application will require more specificity on your expected implementation strategy. Qualitative work can be used to refine that strategy, but your application will need to propose a concrete starting point. <br />3. The iterative development approach is strong, but may be too ambitious with a N=30 for a R34 or R21 implementation strategy. <br />4. What are your targeted outcomes? Are they implementation outcomes or individual outcomes? <br />5. You may want to give more detail regarding the expected implementation barriers. It is unclear to me from this what these might be and what you might do to overcome them. <br /><br />Warm regards, <br />Tisha";s:6:"parent";s:32:"8d1bb7cf7c974f9dfce281e31c8b79cd";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"ccded52ddfa335025124b3e070988540";}s:32:"ea49c13dd731a50cd1214c550fa8ecf9";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"mmarzan";s:4:"name";s:24:"Melissa Marzan-Rodriguez";s:4:"mail";s:22:"melissa.marzan@upr.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536263181;}s:3:"raw";s:1645:"Marzan-Rodriguez - Assignment #2

1.Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

It’s expected to measuring fidelity in the eHealth tool for Spanish speaking Latino-AMSM. The main method will be through data collected (e.g., self-report scales, survey). Another way is through the inclusion and exclusion criteria. There’s an assumption that the participants who will enroll in the intervention must be Spanish speaking Latino-AMSM (the targeted population study). Also cultural adaptation process will be performing to ensure intervention’s validity.

2.Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

A study designed for adolescent’s minority groups focus on sexuality issues might have some many challenges in the implementation process. We expect to have setting issues might be an area for adaptation (where and when participants can access the eHealth intervention). The main issue can be that parents and/or guardians would like to be part of the process. To address that setting issue, stakeholders and other key players’ needs to be part of the implementation process in order to adapt the conditions to make sure adolescents can navigate the eHealth intervention without any parent/guardian supervision. 
";s:5:"xhtml";s:1683:"Marzan-Rodriguez - Assignment #2<br /><br />1.Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />It’s expected to measuring fidelity in the eHealth tool for Spanish speaking Latino-AMSM. The main method will be through data collected (e.g., self-report scales, survey). Another way is through the inclusion and exclusion criteria. There’s an assumption that the participants who will enroll in the intervention must be Spanish speaking Latino-AMSM (the targeted population study). Also cultural adaptation process will be performing to ensure intervention’s validity.<br /><br />2.Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />A study designed for adolescent’s minority groups focus on sexuality issues might have some many challenges in the implementation process. We expect to have setting issues might be an area for adaptation (where and when participants can access the eHealth intervention). The main issue can be that parents and/or guardians would like to be part of the process. To address that setting issue, stakeholders and other key players’ needs to be part of the implementation process in order to adapt the conditions to make sure adolescents can navigate the eHealth intervention without any parent/guardian supervision.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"1de8c8c402ecd93603da9d8d5f1f93da";}s:4:"show";b:1;s:3:"cid";s:32:"ea49c13dd731a50cd1214c550fa8ecf9";}s:32:"51270a66e48639e82f2dfce9aea5bff8";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"kbeima";s:4:"name";s:19:"Kristin Beima-Sofie";s:4:"mail";s:13:"beimak@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536328869;}s:3:"raw";s:4667:"Beima-Sofie - Assignment #2:

1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

Yes, I plan to monitor fidelity in both aims 1 and 2 of my project. For both aims, I’m planning to use both self-report by the providers using both surveys and qualitative methods, and in in-person live observation of patient-provider interactions to measure fidelity. For the second aim and evaluation at the end of the implementation period, I’m planning to also measure fidelity through self-report of adolescents and caregivers who were exposed to the intervention, again using both surveys and qualitative methods. At this stage of grant planning, I’m not sure if it makes sense to also measure fidelity among caregivers for Aim 1. For the GOT transition tool, I’m not aware of any evaluations that associate implementation fidelity with outcomes. For the Naimbia Disclosure tool intervention, during the original evaluation in Namibia, we evaluated fidelity through provider self-report and found that one of the tools was not implemented with fidelity, and that had implications for how we measured health outcomes in the study. Providers were asked to read the cartoon comic book during one-on-one health interactions with the children and subsequently fill out the tracking form to identify exposure and progress towards full HIV disclosure. But, because the comic book describes general concepts about health, disease, and how medications work in the body, and doesn’t mention HIV, health providers used the book in group education sessions with children while they were waiting for appointments. During those situations, healthcare providers did not fill out the accompanying tracking form, indicating if a child had been exposed to the book. When we went to use the tracking forms to evaluate the impact of the intervention on clinical outcomes in children, we were not able to use the tracking forms to accurately identify children exposed and not exposed to the intervention given the way healthcare providers had adapted to providing the intervention. However, this didn’t directly impact the health outcomes in children, just our method of evaluation.  

2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

I anticipate that adaptations will need to be made to both the Namibia Disclosure Tool Intervention, as well as the GOT transition tool during implementation. Prior to implementation, both interventions are being adapted for the Kenyan setting through a 2-day workshop with 40 key stakeholders, including representatives from the MOH, partner organizations in Kenya, healthcare providers, caregivers and adolescents. We have permission to audio record this meeting, and plan to capture the adaptations here, which I would classify as being at the population level (from the Stirman article). We hope that this minimizes adaptations to the core components of the intervention that happen during implementation. But, we also plan to evaluate effectiveness of the intervention, through the original R01. For this new proposed grant, Aim 1 plans to evaluate the interaction between fidelity and adaptation during the initial 6 months of implementation, which seems very timely given what Chris discussed in the lecture, and the Gonzales article, about needing to think more about how these 2 concepts interact throughout implementation. I think that adaptations will mostly fall under the categories he described of mode of delivery and service setting. Related to the Stirman article, this process should track adaptations at the provider and unit level. The current proposal includes PDSA cycles in the clinics during the first 6 months of implementation to track provider and unit adaptations, using bi-monthly meetings the clinic implementing staff. We plan to record those meetings so they can be analyzed, as well as have healthcare providers keep weekly (real time) implementation tracking forms to monitor small changes that occur in delivery and why they made those changes. I liked the description of the 12 types of content modifications and the real time tracking tool described by Rabin that made use of those content modification types. I would like to use that tool, or an adapted version, to monitor adaptations over time. 
";s:5:"xhtml";s:4705:"Beima-Sofie - Assignment #2:<br /><br />1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />Yes, I plan to monitor fidelity in both aims 1 and 2 of my project. For both aims, I’m planning to use both self-report by the providers using both surveys and qualitative methods, and in in-person live observation of patient-provider interactions to measure fidelity. For the second aim and evaluation at the end of the implementation period, I’m planning to also measure fidelity through self-report of adolescents and caregivers who were exposed to the intervention, again using both surveys and qualitative methods. At this stage of grant planning, I’m not sure if it makes sense to also measure fidelity among caregivers for Aim 1. For the GOT transition tool, I’m not aware of any evaluations that associate implementation fidelity with outcomes. For the Naimbia Disclosure tool intervention, during the original evaluation in Namibia, we evaluated fidelity through provider self-report and found that one of the tools was not implemented with fidelity, and that had implications for how we measured health outcomes in the study. Providers were asked to read the cartoon comic book during one-on-one health interactions with the children and subsequently fill out the tracking form to identify exposure and progress towards full HIV disclosure. But, because the comic book describes general concepts about health, disease, and how medications work in the body, and doesn’t mention HIV, health providers used the book in group education sessions with children while they were waiting for appointments. During those situations, healthcare providers did not fill out the accompanying tracking form, indicating if a child had been exposed to the book. When we went to use the tracking forms to evaluate the impact of the intervention on clinical outcomes in children, we were not able to use the tracking forms to accurately identify children exposed and not exposed to the intervention given the way healthcare providers had adapted to providing the intervention. However, this didn’t directly impact the health outcomes in children, just our method of evaluation.  <br /><br />2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />I anticipate that adaptations will need to be made to both the Namibia Disclosure Tool Intervention, as well as the GOT transition tool during implementation. Prior to implementation, both interventions are being adapted for the Kenyan setting through a 2-day workshop with 40 key stakeholders, including representatives from the MOH, partner organizations in Kenya, healthcare providers, caregivers and adolescents. We have permission to audio record this meeting, and plan to capture the adaptations here, which I would classify as being at the population level (from the Stirman article). We hope that this minimizes adaptations to the core components of the intervention that happen during implementation. But, we also plan to evaluate effectiveness of the intervention, through the original R01. For this new proposed grant, Aim 1 plans to evaluate the interaction between fidelity and adaptation during the initial 6 months of implementation, which seems very timely given what Chris discussed in the lecture, and the Gonzales article, about needing to think more about how these 2 concepts interact throughout implementation. I think that adaptations will mostly fall under the categories he described of mode of delivery and service setting. Related to the Stirman article, this process should track adaptations at the provider and unit level. The current proposal includes PDSA cycles in the clinics during the first 6 months of implementation to track provider and unit adaptations, using bi-monthly meetings the clinic implementing staff. We plan to record those meetings so they can be analyzed, as well as have healthcare providers keep weekly (real time) implementation tracking forms to monitor small changes that occur in delivery and why they made those changes. I liked the description of the 12 types of content modifications and the real time tracking tool described by Rabin that made use of those content modification types. I would like to use that tool, or an adapted version, to monitor adaptations over time.";s:6:"parent";N;s:7:"replies";a:2:{i:0;s:32:"fbd182976411cd016cb063966f5887b5";i:1;s:32:"a2b8b5c9658fce291f2b749d7fa9cf36";}s:4:"show";b:1;s:3:"cid";s:32:"51270a66e48639e82f2dfce9aea5bff8";}s:32:"33dd9f6ed1516ba76c59b130a6cdf2bc";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cpsaros";s:4:"name";s:16:"Christina Psaros";s:4:"mail";s:23:"CPSAROS@mgh.harvard.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536336404;}s:3:"raw";s:3213:"Psaros - Assignment 2

1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

My response to this is…

There are a number of steps that are required in order to deliver PrEP safely and that are widely accepted as necessary, including testing for HIV, screening for HIV risk, providing education on PrEP, and routine laboratory monitoring. We plan to measure adherence to each of these steps; the means by which we will measure fidelity may depend on logistics of the study site. Ideally these behaviors / steps would be documented in the medical record and could be abstracted by study staff. In the event that is not possible, we would consider developing checklists to be completed by providers that assess which steps were completed, and reasons for failing to complete a given step (e.g., lack of time to assess interest in PrEP and/or HIV risk). 

2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

My response to this is…

Our study proposes to begin with a qualitative exploration of how a PrEP delivery protocol might need to be modified to fits the needs of providers at FQHCs as well as the needs of African American (AA) women living in the rural South who may be at risk for HIV. We plan to use these data to make adaptations to the PrEP delivery protocol, and create a first draft of the protocol. Given the minority status of our target population, it is anticipated that cultural adaptations will include process and content changes relevant to AA women at high-risk for HIV-infection. Based on previous intervention adaptations described in the literature the following changes are to be considered: 1) Reordering components; 2) forestalling or delaying certain components; 3) de-emphasis and emphasis; 4) augmentation (adding materials or interventions) of components; 5) and language adaptations. Adaptation will be considered on the patient, provider, and organizational level using the CFIR framework. Following the first draft of the adapted PrEP delivery protocol, an implementation resource team (IRT) will be convened to review proposed changes. The IRT will be comprised of experts in implementation science and PrEP delivery, representatives of the clinic administration, potential PrEP candidates, providers and members of the implementation team. A second draft of the adapted PrEP delivery protocol will integrate the recommended changes made and measures added by the IRT, again maintaining the core elements of the PrEP delivery protocol and considering the limitations of the setting in which we will operate. We will then implement the protocol. During the implementation phase, monthly assessments and feedback will be provided by the IRT (or more frequently if needed). Further need of adapting the PrEP protocol will be evaluated after each monthly assessment.
";s:5:"xhtml";s:3272:"Psaros - Assignment 2<br /><br />1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />My response to this is…<br /><br />There are a number of steps that are required in order to deliver PrEP safely and that are widely accepted as necessary, including testing for HIV, screening for HIV risk, providing education on PrEP, and routine laboratory monitoring. We plan to measure adherence to each of these steps; the means by which we will measure fidelity may depend on logistics of the study site. Ideally these behaviors / steps would be documented in the medical record and could be abstracted by study staff. In the event that is not possible, we would consider developing checklists to be completed by providers that assess which steps were completed, and reasons for failing to complete a given step (e.g., lack of time to assess interest in PrEP and/or HIV risk). <br /><br />2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />My response to this is…<br /><br />Our study proposes to begin with a qualitative exploration of how a PrEP delivery protocol might need to be modified to fits the needs of providers at FQHCs as well as the needs of African American (AA) women living in the rural South who may be at risk for HIV. We plan to use these data to make adaptations to the PrEP delivery protocol, and create a first draft of the protocol. Given the minority status of our target population, it is anticipated that cultural adaptations will include process and content changes relevant to AA women at high-risk for HIV-infection. Based on previous intervention adaptations described in the literature the following changes are to be considered: 1) Reordering components; 2) forestalling or delaying certain components; 3) de-emphasis and emphasis; 4) augmentation (adding materials or interventions) of components; 5) and language adaptations. Adaptation will be considered on the patient, provider, and organizational level using the CFIR framework. Following the first draft of the adapted PrEP delivery protocol, an implementation resource team (IRT) will be convened to review proposed changes. The IRT will be comprised of experts in implementation science and PrEP delivery, representatives of the clinic administration, potential PrEP candidates, providers and members of the implementation team. A second draft of the adapted PrEP delivery protocol will integrate the recommended changes made and measures added by the IRT, again maintaining the core elements of the PrEP delivery protocol and considering the limitations of the setting in which we will operate. We will then implement the protocol. During the implementation phase, monthly assessments and feedback will be provided by the IRT (or more frequently if needed). Further need of adapting the PrEP protocol will be evaluated after each monthly assessment.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"a0bd2df8c3e5837a45b3ef88c01101d3";}s:4:"show";b:1;s:3:"cid";s:32:"33dd9f6ed1516ba76c59b130a6cdf2bc";}s:32:"7bad6c5fcaadbebd0208c198b9956788";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"awagner";s:4:"name";s:13:"Anjuli Wagner";s:4:"mail";s:14:"anjuliw@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536367059;}s:3:"raw";s:2387:"Wagner -- Assignment 2

1.Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes

While I had not initially planned on systematically evaluating fidelity, I would like to add elements of fidelity and adaptation measurement to this application. For fidelity, I will plan to assess whether elements of the "PrEP visit" were fully delivered; these include HIV testing, screening for PrEP interest and/or HIV risk, counseling about PrEP (initiation and adherence). I will use a combination of A) patient report for a subset of individuals using questions like, "During your visit today, did anyone talk to you about something called PrEP? What information did they share with you? What did you understand clearly and what was unclear?"; B) direct observation of client-provider interactions; and C) abstraction of routine program data from registers noting HIV testing and counseling/dispensation of PrEP.


2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

I am having a hard time distinguishing between adaptations and implementation strategies. We are planning on testing 3 specific either novel or modified implementation strategies in Aim 2 of this proposal, which might include 3 types of task shifting: HIV risk assessment performed by HIV testing counselors instead of ANC nurses, self-administered HIV risk assessment, self-testing for HIV in waiting room. We would want to document how the fidelity to the intended IS naturally changed during the testing period, but I wonder if that is measuring adaptation of the EBI? If the changes that we are proposing to evaluate are adaptations instead of ISs, the process that we are using to guide the adaptations would be the mixed methods work from Aim 1, where we elicit information on strategies that health care workers organically developed and tried during the original projects to fit their specific contexts. Any clarification about what adaptation might look like in the context of testing ISs would be much appreciated.
";s:5:"xhtml";s:2451:"Wagner -- Assignment 2<br /><br />1.Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes<br /><br />While I had not initially planned on systematically evaluating fidelity, I would like to add elements of fidelity and adaptation measurement to this application. For fidelity, I will plan to assess whether elements of the &quot;PrEP visit&quot; were fully delivered; these include HIV testing, screening for PrEP interest and/or HIV risk, counseling about PrEP (initiation and adherence). I will use a combination of A) patient report for a subset of individuals using questions like, &quot;During your visit today, did anyone talk to you about something called PrEP? What information did they share with you? What did you understand clearly and what was unclear?&quot;; B) direct observation of client-provider interactions; and C) abstraction of routine program data from registers noting HIV testing and counseling/dispensation of PrEP.<br /><br /><br />2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />I am having a hard time distinguishing between adaptations and implementation strategies. We are planning on testing 3 specific either novel or modified implementation strategies in Aim 2 of this proposal, which might include 3 types of task shifting: HIV risk assessment performed by HIV testing counselors instead of ANC nurses, self-administered HIV risk assessment, self-testing for HIV in waiting room. We would want to document how the fidelity to the intended IS naturally changed during the testing period, but I wonder if that is measuring adaptation of the EBI? If the changes that we are proposing to evaluate are adaptations instead of ISs, the process that we are using to guide the adaptations would be the mixed methods work from Aim 1, where we elicit information on strategies that health care workers organically developed and tried during the original projects to fit their specific contexts. Any clarification about what adaptation might look like in the context of testing ISs would be much appreciated.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"eaba02c101f8a9cb6308cce3fc1b4046";}s:4:"show";b:1;s:3:"cid";s:32:"7bad6c5fcaadbebd0208c198b9956788";}s:32:"ca8330c787f5fb376192301593807764";a:8:{s:4:"user";a:5:{s:2:"id";s:3:"dli";s:4:"name";s:9:"Dennis Li";s:4:"mail";s:23:"dennis@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536383465;}s:3:"raw";s:2686:"LI – Assignment 2

1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

Because eHealth HIV interventions have yet to be widely disseminated and many HIV service settings do not yet have the capacity to implement them, my proposal seeks to explore how to best build that capacity and facilitate eventual adoption and implementation of these types of interventions. The EBI, Keep It Up!, will serve as the example EBI to be implemented but will not be actually delivered as part of this project. Therefore, fidelity will not be measured/monitored.

In general, however, eHealth programs have a lot of measures of fidelity that are not possible with traditional in-person interventions. For example, interactive components can be used to measure engagement (e.g., those who type in answers to free-text-response questions may be more engaged), and time stamps can be used to estimate exposure (e.g., did they see the content) and engagement (e.g., how long did they spend on the content).

For the Keep It Up! intervention specifically, the main efficacy trial only recently ended. No analysis of fidelity on outcomes has yet been conducted. For eHealth HIV interventions in general, little to no research has examined the effect of fidelity on outcomes, and there is a pervasive (and possibly premature) assumption that the automated components of digital interventions necessarily equate to high fidelity in intervention delivery. This is an area ripe for research.

2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

Because Keep It Up! will not actually be implemented as part of this project, no adaptations will be made at this time. In our other work, however, we will be delivering the program and will be making adaptations not only to the content but also the technology and container. Digital-technology-based interventions are especially susceptible to becoming outdated as software platforms, hardware, and the expectations of users change over time. Thus, eHealth interventions must be designed to be adaptable over time, as we are aiming to do in our other work. Content adaptations will be made systematically using Intervention Mapping, but after going through this Module, I will be applying a more rigorous tracking methods to the changes that are made.";s:5:"xhtml";s:2746:"LI – Assignment 2<br /><br />1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />Because eHealth HIV interventions have yet to be widely disseminated and many HIV service settings do not yet have the capacity to implement them, my proposal seeks to explore how to best build that capacity and facilitate eventual adoption and implementation of these types of interventions. The EBI, Keep It Up!, will serve as the example EBI to be implemented but will not be actually delivered as part of this project. Therefore, fidelity will not be measured/monitored.<br /><br />In general, however, eHealth programs have a lot of measures of fidelity that are not possible with traditional in-person interventions. For example, interactive components can be used to measure engagement (e.g., those who type in answers to free-text-response questions may be more engaged), and time stamps can be used to estimate exposure (e.g., did they see the content) and engagement (e.g., how long did they spend on the content).<br /><br />For the Keep It Up! intervention specifically, the main efficacy trial only recently ended. No analysis of fidelity on outcomes has yet been conducted. For eHealth HIV interventions in general, little to no research has examined the effect of fidelity on outcomes, and there is a pervasive (and possibly premature) assumption that the automated components of digital interventions necessarily equate to high fidelity in intervention delivery. This is an area ripe for research.<br /><br />2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />Because Keep It Up! will not actually be implemented as part of this project, no adaptations will be made at this time. In our other work, however, we will be delivering the program and will be making adaptations not only to the content but also the technology and container. Digital-technology-based interventions are especially susceptible to becoming outdated as software platforms, hardware, and the expectations of users change over time. Thus, eHealth interventions must be designed to be adaptable over time, as we are aiming to do in our other work. Content adaptations will be made systematically using Intervention Mapping, but after going through this Module, I will be applying a more rigorous tracking methods to the changes that are made.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"ca8ea0a00b2bf8010f91e16ce067ba55";}s:4:"show";b:1;s:3:"cid";s:32:"ca8330c787f5fb376192301593807764";}s:32:"dd4fcec4ac82ff427103da63b41206d1";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jhaberer";s:4:"name";s:15:"Jessica Haberer";s:4:"mail";s:21:"JHABERER@PARTNERS.ORG";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536717451;}s:3:"raw";s:3467:"Jessica Haberer- assignment #2

Brief summary
In Aim 1 of the study, we will be determining the combination of electronic adherence monitor features (e.g., alarms), SMS adherence reminders/messages to patients, and/or SMS notifications to social supporters that is likely to be acceptable and feasible within the routine Ugandan health care system. SMS may be one or two-way and interventions may be scheduled (e.g., daily) or triggered in real-time by the lack of expected adherence. In Aim 2, we will implement this defined monitoring and intervention strategy in a two-phase, iterative manner.

Fidelity
To assess fidelity of implementing the adherence monitoring and intervention strategy, we will measure the following:
Healthcare providers
•	Registration of the patient on the evriMED Caregiver app to indicate devices are available to patients (via the evriMED website)
•	Time spent with patient describing the monitor and intervention (via time and motion studies) 
•	Content of the description of the monitor and intervention (via time and motion studies)
•	Follow-up of two-way SMS requests (via SMS logs)
•	Use of the adherence data in follow-up visits with patients (via check box on adherence data printouts)
•	Time spent with patient presenting the adherence data (via time and motion studies) 
•	Content of the presentation of the adherence data (via time and motion studies)

Patient
•	Monitor opening events after leaving clinic (via the evriMED website)
•	# SMS reminders/messages received—automated, from healthcare providers, and from social supporters (via self-report at 3 months; could assess SMS logs although may overreach privacy protections)
•	Report of alarms for late/missed dosing (via self-report at 3 months)

Social supporter
•	# SMS notifications received (via self-report at 3 months)
•	# actions taken due to notifications (via self-report at 3 months)

Pros and cons of the above strategy: The self-reported data will reflect the participants viewpoints, but may lack validity. The time and motion studies will be more objective, but will realistically only capture a small portion of the sample. We will do them with the study is at “steady state” to hopefully capture representative values, but we will not be able to assess how representative they truly are.

The existing literature comments on various aspects of monitoring and intervention that may be tested in this study, pending the input from the formative Aim 1 work. Most show improvements in adherence.


Adaptations
As noted above, Aim 1 of the study is largely about adaptations of potentially effective aspects of the adherence monitor and intervention. This process is considering viewpoints at multiple levels of the healthcare system and multiple types of patients (e.g., recently initiated versus long-term use of ART). We will re-visit the adaptation in a second round of implementation after assessing key quantitative metrics and qualitative interviews informed by the CFIR. 

Pros and cons of the above strategy: All potential aspects of the monitoring and intervention are based on some prior evidence and are therefore likely to lead to high adherence (which we will be measuring). Adaptations will reflect the local culture and preferences, as well as available resources and infrastructure. We therefore believe theoretical congruence will remain, while achieving optimal acceptability and feasibility for implementation.
";s:5:"xhtml";s:3631:"Jessica Haberer- assignment #2<br /><br />Brief summary<br />In Aim 1 of the study, we will be determining the combination of electronic adherence monitor features (e.g., alarms), SMS adherence reminders/messages to patients, and/or SMS notifications to social supporters that is likely to be acceptable and feasible within the routine Ugandan health care system. SMS may be one or two-way and interventions may be scheduled (e.g., daily) or triggered in real-time by the lack of expected adherence. In Aim 2, we will implement this defined monitoring and intervention strategy in a two-phase, iterative manner.<br /><br />Fidelity<br />To assess fidelity of implementing the adherence monitoring and intervention strategy, we will measure the following:<br />Healthcare providers<br />•	Registration of the patient on the evriMED Caregiver app to indicate devices are available to patients (via the evriMED website)<br />•	Time spent with patient describing the monitor and intervention (via time and motion studies) <br />•	Content of the description of the monitor and intervention (via time and motion studies)<br />•	Follow-up of two-way SMS requests (via SMS logs)<br />•	Use of the adherence data in follow-up visits with patients (via check box on adherence data printouts)<br />•	Time spent with patient presenting the adherence data (via time and motion studies) <br />•	Content of the presentation of the adherence data (via time and motion studies)<br /><br />Patient<br />•	Monitor opening events after leaving clinic (via the evriMED website)<br />•	# SMS reminders/messages received—automated, from healthcare providers, and from social supporters (via self-report at 3 months; could assess SMS logs although may overreach privacy protections)<br />•	Report of alarms for late/missed dosing (via self-report at 3 months)<br /><br />Social supporter<br />•	# SMS notifications received (via self-report at 3 months)<br />•	# actions taken due to notifications (via self-report at 3 months)<br /><br />Pros and cons of the above strategy: The self-reported data will reflect the participants viewpoints, but may lack validity. The time and motion studies will be more objective, but will realistically only capture a small portion of the sample. We will do them with the study is at “steady state” to hopefully capture representative values, but we will not be able to assess how representative they truly are.<br /><br />The existing literature comments on various aspects of monitoring and intervention that may be tested in this study, pending the input from the formative Aim 1 work. Most show improvements in adherence.<br /><br /><br />Adaptations<br />As noted above, Aim 1 of the study is largely about adaptations of potentially effective aspects of the adherence monitor and intervention. This process is considering viewpoints at multiple levels of the healthcare system and multiple types of patients (e.g., recently initiated versus long-term use of ART). We will re-visit the adaptation in a second round of implementation after assessing key quantitative metrics and qualitative interviews informed by the CFIR. <br /><br />Pros and cons of the above strategy: All potential aspects of the monitoring and intervention are based on some prior evidence and are therefore likely to lead to high adherence (which we will be measuring). Adaptations will reflect the local culture and preferences, as well as available resources and infrastructure. We therefore believe theoretical congruence will remain, while achieving optimal acceptability and feasibility for implementation.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"1efa4a267f0968cd50e1d3f0edb0570e";}s:4:"show";b:1;s:3:"cid";s:32:"dd4fcec4ac82ff427103da63b41206d1";}s:32:"eaba02c101f8a9cb6308cce3fc1b4046";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536855749;}s:3:"raw";s:701:"Hi Amjuli

I believe that measuring fidelity will be key as you are adapting the intervention by task shifting.  Some people consider adaptation an implementation strategy or implementation outcome (chapter 17, Brownson et al, Editors, Dissemination and implementation research in health. 2018) .  I like looking at it that way.  The important thing that you realized already is that you have to look at fidelity to see what else might be changed (adapted) when you task shift.  You may need to add other implementation strategies to your intervention, like educating the counselors and the patients.   I hope this answers your question -- if you still have additional questions please let me know.   ";s:5:"xhtml";s:708:"Hi Amjuli<br /><br />I believe that measuring fidelity will be key as you are adapting the intervention by task shifting.  Some people consider adaptation an implementation strategy or implementation outcome (chapter 17, Brownson et al, Editors, Dissemination and implementation research in health. 2018) .  I like looking at it that way.  The important thing that you realized already is that you have to look at fidelity to see what else might be changed (adapted) when you task shift.  You may need to add other implementation strategies to your intervention, like educating the counselors and the patients.   I hope this answers your question -- if you still have additional questions please let me know.";s:6:"parent";s:32:"7bad6c5fcaadbebd0208c198b9956788";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"eaba02c101f8a9cb6308cce3fc1b4046";}s:32:"fbd182976411cd016cb063966f5887b5";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536859915;}s:3:"raw";s:707:"Hi Kristin,

I again have little to add! If the caregivers have a role in either of your EBIs then I suggest you do measure the fidelity of their roles.  Regarding GOT and implementation outcomes,  it would seem that in the "current assessment", the level of the implementation of some of the elements with fidelity might effect the acceptability and the sustainability of the intervention especially at the  practice level.   With the "measurement tool", would higher scores track with fidelity and sustainability? And since both tools have sections measuring inclusion of stakeholders (youth and families) would fidelity equal acceptability?  You workshops should help with this implementation outcome.   ";s:5:"xhtml";s:734:"Hi Kristin,<br /><br />I again have little to add! If the caregivers have a role in either of your EBIs then I suggest you do measure the fidelity of their roles.  Regarding GOT and implementation outcomes,  it would seem that in the &quot;current assessment&quot;, the level of the implementation of some of the elements with fidelity might effect the acceptability and the sustainability of the intervention especially at the  practice level.   With the &quot;measurement tool&quot;, would higher scores track with fidelity and sustainability? And since both tools have sections measuring inclusion of stakeholders (youth and families) would fidelity equal acceptability?  You workshops should help with this implementation outcome.";s:6:"parent";s:32:"51270a66e48639e82f2dfce9aea5bff8";s:7:"replies";a:1:{i:0;s:32:"a1537417e4279e9292906f751199c997";}s:4:"show";b:1;s:3:"cid";s:32:"fbd182976411cd016cb063966f5887b5";}s:32:"1de8c8c402ecd93603da9d8d5f1f93da";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536865349;}s:3:"raw";s:665:"Hello Melissa. I look forward to speaking tomorrow. 

I have a couple of questions about your response re: fidelity. Could you be more specific about fidelity in terms of how the ehealth intervention is delivered? Who will be completing the self-report scales, etc., and when?

A couple of follow-up questions about adaptation, as well. Please remind us, where would this study be conducted? (As compared to how/where the original intervention was tested?

You mention setting as a potential adaptation. So this addresses the "where" potential challenges of delivery. Are you anticipating the need for any changes in the intervention content? (besides language)? 

";s:5:"xhtml";s:702:"Hello Melissa. I look forward to speaking tomorrow. <br /><br />I have a couple of questions about your response re: fidelity. Could you be more specific about fidelity in terms of how the ehealth intervention is delivered? Who will be completing the self-report scales, etc., and when?<br /><br />A couple of follow-up questions about adaptation, as well. Please remind us, where would this study be conducted? (As compared to how/where the original intervention was tested?<br /><br />You mention setting as a potential adaptation. So this addresses the &quot;where&quot; potential challenges of delivery. Are you anticipating the need for any changes in the intervention content? (besides language)?";s:6:"parent";s:32:"ea49c13dd731a50cd1214c550fa8ecf9";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"1de8c8c402ecd93603da9d8d5f1f93da";}s:32:"a2b8b5c9658fce291f2b749d7fa9cf36";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536866126;}s:3:"raw";s:620:"Hello Kristin. I look forward to setting up a call sometime when it is convenient for us both, just to talk through your plans and study, etc., more generally (and your Aims). Apologies i wasn't able to provide input on your Aims yet, and missed the first call.

ANYHOW, my take is that this is a thorough response re: fidelity and adaptation. My only reaction (caution?) is that you will be awash in data, both quantitative and qualitative! I personally haven't done any CQA, so i look forward to learning how you combine and make sense of how these factors (and from different stakeholders) may affect implementation. ";s:5:"xhtml";s:639:"Hello Kristin. I look forward to setting up a call sometime when it is convenient for us both, just to talk through your plans and study, etc., more generally (and your Aims). Apologies i wasn&#039;t able to provide input on your Aims yet, and missed the first call.<br /><br />ANYHOW, my take is that this is a thorough response re: fidelity and adaptation. My only reaction (caution?) is that you will be awash in data, both quantitative and qualitative! I personally haven&#039;t done any CQA, so i look forward to learning how you combine and make sense of how these factors (and from different stakeholders) may affect implementation.";s:6:"parent";s:32:"51270a66e48639e82f2dfce9aea5bff8";s:7:"replies";a:1:{i:0;s:32:"f8fb104eac2f106475f9f04811a8cad2";}s:4:"show";b:1;s:3:"cid";s:32:"a2b8b5c9658fce291f2b749d7fa9cf36";}s:32:"a1537417e4279e9292906f751199c997";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"kbeima";s:4:"name";s:19:"Kristin Beima-Sofie";s:4:"mail";s:13:"beimak@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537084251;}s:3:"raw";s:873:"Hi Linda,
Thanks for your feedback on my response. I think my description of fidelity in caregivers wasn't worded great and could use more clarification. The caregivers are not directly involved in implementing the EBI, only HCWs, but they are third party observers of the HCW's delivery of the EBI to their adolescent. In that sense, I think that talking with them to understand their perception of how the EBI has been delivered to their adolescent gets around some of the self-report bias HCWs might have in reporting performance, without doing direct observation by an outsider to the patient/provider interaction. But, it also involves additional data collection and I think I'm already proposing quite a bit of data to collect. The other questions you pose require some thought, thanks for posing those and I'll definitely work on thinking through them more. 
Thanks!";s:5:"xhtml";s:903:"Hi Linda,<br />Thanks for your feedback on my response. I think my description of fidelity in caregivers wasn&#039;t worded great and could use more clarification. The caregivers are not directly involved in implementing the EBI, only HCWs, but they are third party observers of the HCW&#039;s delivery of the EBI to their adolescent. In that sense, I think that talking with them to understand their perception of how the EBI has been delivered to their adolescent gets around some of the self-report bias HCWs might have in reporting performance, without doing direct observation by an outsider to the patient/provider interaction. But, it also involves additional data collection and I think I&#039;m already proposing quite a bit of data to collect. The other questions you pose require some thought, thanks for posing those and I&#039;ll definitely work on thinking through them more. <br />Thanks!";s:6:"parent";s:32:"fbd182976411cd016cb063966f5887b5";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"a1537417e4279e9292906f751199c997";}s:32:"f8fb104eac2f106475f9f04811a8cad2";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"kbeima";s:4:"name";s:19:"Kristin Beima-Sofie";s:4:"mail";s:13:"beimak@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537084601;}s:3:"raw";s:499:"Hi Chris,
Thanks for the feedback. I would be happy to have a call sometime to discuss the proposed project, plans and aims, etc. 

Yes, it is a LOT of data and maybe too much? I'd be happy to hear suggestions of which pieces of proposed data collection seem duplicative, if any, and where data collection could be minimized while still ensuring that we capture enough data to measure the outcomes of interest. Since I'm new to IS, feedback on what sounds like "too much" is very helpful. 

Thanks! ";s:5:"xhtml";s:543:"Hi Chris,<br />Thanks for the feedback. I would be happy to have a call sometime to discuss the proposed project, plans and aims, etc. <br /><br />Yes, it is a LOT of data and maybe too much? I&#039;d be happy to hear suggestions of which pieces of proposed data collection seem duplicative, if any, and where data collection could be minimized while still ensuring that we capture enough data to measure the outcomes of interest. Since I&#039;m new to IS, feedback on what sounds like &quot;too much&quot; is very helpful. <br /><br />Thanks!";s:6:"parent";s:32:"a2b8b5c9658fce291f2b749d7fa9cf36";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"f8fb104eac2f106475f9f04811a8cad2";}s:32:"147881660748b960bf8b8068a303b990";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"kbeima";s:4:"name";s:19:"Kristin Beima-Sofie";s:4:"mail";s:13:"beimak@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537133827;}s:3:"raw";s:5942:"Beima-Sofie - Assignment #3:

Assignment #3a - Models:
1.	Which model or combination of models is most applicable to your proposed study and why?

While CFIR is not a model, it’s a framework, it seems to make the most logical sense to use to guide the development of my project because the goal is to understand determinants that influence implementation outcomes. Because it’s a meta-theoretical framework, it does also include elements of the diffusion of innovation theory (intervention), as well as the implementation theory of organizational readiness for change (inner setting). The article by Tabak and lecture by Wynne also noted that CFIR can be used as an evaluation framework explaining IS outcomes. I appreciate that it can be applied early to predict IS outcomes, as well as later to explain outcomes. I also feel most confident using this framework given that it’s one of the most commonly used IS frameworks in NIH grant applications. I think that a determinants framework also makes sense for this study, given that determinants frameworks acknowledge multiple ecological levels of influence and recognize the importance of evaluating the system as a holistic entity. 

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 

Picking the CFIR limits the constructs I can evaluate to those included in the 5 domains of the CFIR framework. However, since that framework is one of the broadest, I don’t see this as a limiting factor.  I’m still a little confused on how to map different frameworks, models, and theories onto IS outcomes, which might help me answer this question better, maybe something that can be discussed during the live webinar in a few weeks. 

Assignment #3b - Measures & Evaluations:
1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

Since this is a nested project, the original R01 grant will be measuring the clinical and service outcomes for the intervention including effectiveness, satisfaction, efficiency and patient-centeredness. This project will measure only the implementation outcomes. Specifically Aim 1 will evaluate implementation outcomes from early phases of implementation, as defined by Proctor et al, including acceptability, appropriateness, feasibility, and fidelity. I’m hoping to measure them through qualitative interviews, as well as through a survey administered to HCWs that includes Bryan Weiner’s validated tool for acceptability, feasibility and appropriateness. However, I think it will be interesting to see how the tool performs in a very different cultural setting (Kenya). I’m still reviewing and deciding on tools to measure fidelity and am open to suggestions of additional resources to explore. I think measuring the 4 constructs from each of the domains presented in Carroll’s article, of adherence and moderators, would be great to evaluate if I can find a validated tool that includes those domains. For Aim 2, I’m planning to evaluate the implementation outcomes associated with later stages of implementation (active implementation or sustainment) and would like to evaluate fidelity, adoption, penetration and sustainability. I will use administrative records (not self-report) to measure penetration, as discussed by Cara in her lecture. I’m planning to use self-report and direct observation to measure fidelity, review patient files to review adoption and penetration and look at patient files over time, during the 2 years of implementation, to understand sustainability. 

I think the ideas of construct validity, reliability, practicality and sensitivity are really important to consider. But it also feels daunting, as someone new to the field, to try and identify where to get the best measures for outcomes and how to perform some of our own construct verifications through the projects we’re developing. 

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

The main process we are measuring during Aim 1 is the process of adaptation of the intervention. I’m still exploring tools to use, but would refer back to last weeks’ reading and the tool described by Rabin to measure those adaptations. I’m measuring the process of adaptation because I think it’s really interesting and an area that needs more exploration, especially with the emphasis on implementing interventions with fidelity. During Aim 2, through the CFIR, I will also be measuring how implementation unfolds using constructs mapped to the process domain, including planning, engaging, executing, and evaluation. We will use tools from the CFIR website to come up with appropriate questions to ask to measure process constructs. 

3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? 

I wasn’t directly planning to measure any co-benefits or unintended consequences of implementation of the ATP (Adolescent Transition Package) because I hadn’t ever thought about it (but I’m considering it now). I think our HCW implementation logs tracking changes to implementation practices during the initial 6 months of implementation might provide some information on these outcomes. We might also capture some of this information in the qualitative interviews with key stakeholders at the end of the study, and could easily add in additional interview questions to the guides to specifically capture this information. In reflecting on this question, I do think it’s important to think about how implementing this intervention might negatively impact other services offered to adolescents, either negatively or positively.
";s:5:"xhtml";s:6065:"Beima-Sofie - Assignment #3:<br /><br />Assignment #3a - Models:<br />1.	Which model or combination of models is most applicable to your proposed study and why?<br /><br />While CFIR is not a model, it’s a framework, it seems to make the most logical sense to use to guide the development of my project because the goal is to understand determinants that influence implementation outcomes. Because it’s a meta-theoretical framework, it does also include elements of the diffusion of innovation theory (intervention), as well as the implementation theory of organizational readiness for change (inner setting). The article by Tabak and lecture by Wynne also noted that CFIR can be used as an evaluation framework explaining IS outcomes. I appreciate that it can be applied early to predict IS outcomes, as well as later to explain outcomes. I also feel most confident using this framework given that it’s one of the most commonly used IS frameworks in NIH grant applications. I think that a determinants framework also makes sense for this study, given that determinants frameworks acknowledge multiple ecological levels of influence and recognize the importance of evaluating the system as a holistic entity. <br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br /><br />Picking the CFIR limits the constructs I can evaluate to those included in the 5 domains of the CFIR framework. However, since that framework is one of the broadest, I don’t see this as a limiting factor.  I’m still a little confused on how to map different frameworks, models, and theories onto IS outcomes, which might help me answer this question better, maybe something that can be discussed during the live webinar in a few weeks. <br /><br />Assignment #3b - Measures &amp; Evaluations:<br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />Since this is a nested project, the original R01 grant will be measuring the clinical and service outcomes for the intervention including effectiveness, satisfaction, efficiency and patient-centeredness. This project will measure only the implementation outcomes. Specifically Aim 1 will evaluate implementation outcomes from early phases of implementation, as defined by Proctor et al, including acceptability, appropriateness, feasibility, and fidelity. I’m hoping to measure them through qualitative interviews, as well as through a survey administered to HCWs that includes Bryan Weiner’s validated tool for acceptability, feasibility and appropriateness. However, I think it will be interesting to see how the tool performs in a very different cultural setting (Kenya). I’m still reviewing and deciding on tools to measure fidelity and am open to suggestions of additional resources to explore. I think measuring the 4 constructs from each of the domains presented in Carroll’s article, of adherence and moderators, would be great to evaluate if I can find a validated tool that includes those domains. For Aim 2, I’m planning to evaluate the implementation outcomes associated with later stages of implementation (active implementation or sustainment) and would like to evaluate fidelity, adoption, penetration and sustainability. I will use administrative records (not self-report) to measure penetration, as discussed by Cara in her lecture. I’m planning to use self-report and direct observation to measure fidelity, review patient files to review adoption and penetration and look at patient files over time, during the 2 years of implementation, to understand sustainability. <br /><br />I think the ideas of construct validity, reliability, practicality and sensitivity are really important to consider. But it also feels daunting, as someone new to the field, to try and identify where to get the best measures for outcomes and how to perform some of our own construct verifications through the projects we’re developing. <br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />The main process we are measuring during Aim 1 is the process of adaptation of the intervention. I’m still exploring tools to use, but would refer back to last weeks’ reading and the tool described by Rabin to measure those adaptations. I’m measuring the process of adaptation because I think it’s really interesting and an area that needs more exploration, especially with the emphasis on implementing interventions with fidelity. During Aim 2, through the CFIR, I will also be measuring how implementation unfolds using constructs mapped to the process domain, including planning, engaging, executing, and evaluation. We will use tools from the CFIR website to come up with appropriate questions to ask to measure process constructs. <br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? <br /><br />I wasn’t directly planning to measure any co-benefits or unintended consequences of implementation of the ATP (Adolescent Transition Package) because I hadn’t ever thought about it (but I’m considering it now). I think our HCW implementation logs tracking changes to implementation practices during the initial 6 months of implementation might provide some information on these outcomes. We might also capture some of this information in the qualitative interviews with key stakeholders at the end of the study, and could easily add in additional interview questions to the guides to specifically capture this information. In reflecting on this question, I do think it’s important to think about how implementing this intervention might negatively impact other services offered to adolescents, either negatively or positively.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"ed486630ca99b2af21ed889adb02fbcc";}s:4:"show";b:1;s:3:"cid";s:32:"147881660748b960bf8b8068a303b990";}s:32:"ca8ea0a00b2bf8010f91e16ce067ba55";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537289509;}s:3:"raw";s:92:"Thanks for these thoughtful responses -- esp given the nature of the work you are proposing.";s:5:"xhtml";s:92:"Thanks for these thoughtful responses -- esp given the nature of the work you are proposing.";s:6:"parent";s:32:"ca8330c787f5fb376192301593807764";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"ca8ea0a00b2bf8010f91e16ce067ba55";}s:32:"a0bd2df8c3e5837a45b3ef88c01101d3";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537290054;}s:3:"raw";s:48:"Well put. No additional input on this response. ";s:5:"xhtml";s:47:"Well put. No additional input on this response.";s:6:"parent";s:32:"33dd9f6ed1516ba76c59b130a6cdf2bc";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"a0bd2df8c3e5837a45b3ef88c01101d3";}s:32:"0d75ec8fcbe1e469a14b8a94bb43b4fb";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jhaberer";s:4:"name";s:15:"Jessica Haberer";s:4:"mail";s:21:"JHABERER@PARTNERS.ORG";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537499125;}s:3:"raw";s:4456:"Jessica Haberer- Assignment #3

#3a - Models:
1. Which model or combination of models is most applicable to your proposed study and why?

I chose to use the Consolidated Framework for Implementation Research because I am most interested in barriers and enablers that influence implementation outcomes (i.e., the CFIR is a determinant framework). I also considered RE-AIM (an evaluation framework), but want to focus more on barriers and enablers.


2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 

The CFIR will help me understand the process of implementing the evriMED adherence monitor and associated interventions (e.g., SMS, data-informed counseling). Specifically, I am looking for the following:
1. intervention characteristics (support provided, relative advantage over standard of care); 
2. the outer setting (patient needs and resources, external policy and incentives); 
3. the inner setting (structural characteristics, culture, climate, readiness for implementation); 
4. the characteristics of the individuals (knowledge, self-efficacy); and 
5. the process of implementation (adapting, executing, reflecting & evaluating, developing & tailoring).


#3b - Measures & Evaluations:
1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

In our Aim 1 formative/qualitative work, we are looking at the following within the CFIR framework:
Intervention characteristics: Feedback on the electronic adherence monitor, SMS, and counseling specifications; perceived ease of use (optimizing the “fit” for the “need”)

Outer setting: Economic, political, and social context of health care policies for routine Ugandan HIV clinics (e.g., prioritization of limited funds, regional and national goals for types of care delivery)

Inner setting: Structural, political, and cultural context within routine clinics (e.g., availability of appropriate staff, electricity); prioritization of clinic-level goals

Individual: Beliefs about the intervention; self-efficacy in using each aspect of it; perceived barriers and facilitators (e.g., stigma, travel)

Implementation process: Recommendations for approaching execution of the intervention, necessary feedback at each level of involved stakeholders, timing


In Aim 2, where we will be implementing the adherence monitor and associated interventions, we will add the following metrics:

Intervention characteristics: # devices, SMS and counseling sessions deployed as planned; #technical failures; cost

Outer setting: Relative to other priorities, target cost given impact of the intervention at the regional and national levels; level of prioritization given deployment outcomes

Inner setting: # staff available and assigned to work with each aspect of the intervention; amount of time involved with each step; detraction from other care activities

Individual: Level and patterns of adherence; extent of modifiable barriers identified; # adverse events (e.g., unintended disclosure); degree of stigma reported

Implementation process: Challenges with deployment (e.g., importation of devices, execution of the SMS platform)


2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

We will be conducting time and motion studies to understand the standard of care, as well as implementation of the monitors and associated interventions. Specifically, we are interested in the inner setting metrics noted above. We will collect this data in a first implementation study until saturation of data points is achieved. Then, we will repeat it during the second implementation study to see if the metrics improve.


3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? 

As noted above, we are looking at detraction from other care activities. We anticipate increased efficiency of care delivery through this approach to adherence monitoring and support; however, the technology could be a distraction and/or more challenging to implement than anticipated. There may also be unanticipated costs, including actual costs (e.g., importation fees) and time (e.g., staffing). 
";s:5:"xhtml";s:4731:"Jessica Haberer- Assignment #3<br /><br />#3a - Models:<br />1. Which model or combination of models is most applicable to your proposed study and why?<br /><br />I chose to use the Consolidated Framework for Implementation Research because I am most interested in barriers and enablers that influence implementation outcomes (i.e., the CFIR is a determinant framework). I also considered RE-AIM (an evaluation framework), but want to focus more on barriers and enablers.<br /><br /><br />2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br /><br />The CFIR will help me understand the process of implementing the evriMED adherence monitor and associated interventions (e.g., SMS, data-informed counseling). Specifically, I am looking for the following:<br />1. intervention characteristics (support provided, relative advantage over standard of care); <br />2. the outer setting (patient needs and resources, external policy and incentives); <br />3. the inner setting (structural characteristics, culture, climate, readiness for implementation); <br />4. the characteristics of the individuals (knowledge, self-efficacy); and <br />5. the process of implementation (adapting, executing, reflecting &amp; evaluating, developing &amp; tailoring).<br /><br /><br />#3b - Measures &amp; Evaluations:<br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />In our Aim 1 formative/qualitative work, we are looking at the following within the CFIR framework:<br />Intervention characteristics: Feedback on the electronic adherence monitor, SMS, and counseling specifications; perceived ease of use (optimizing the “fit” for the “need”)<br /><br />Outer setting: Economic, political, and social context of health care policies for routine Ugandan HIV clinics (e.g., prioritization of limited funds, regional and national goals for types of care delivery)<br /><br />Inner setting: Structural, political, and cultural context within routine clinics (e.g., availability of appropriate staff, electricity); prioritization of clinic-level goals<br /><br />Individual: Beliefs about the intervention; self-efficacy in using each aspect of it; perceived barriers and facilitators (e.g., stigma, travel)<br /><br />Implementation process: Recommendations for approaching execution of the intervention, necessary feedback at each level of involved stakeholders, timing<br /><br /><br />In Aim 2, where we will be implementing the adherence monitor and associated interventions, we will add the following metrics:<br /><br />Intervention characteristics: # devices, SMS and counseling sessions deployed as planned; #technical failures; cost<br /><br />Outer setting: Relative to other priorities, target cost given impact of the intervention at the regional and national levels; level of prioritization given deployment outcomes<br /><br />Inner setting: # staff available and assigned to work with each aspect of the intervention; amount of time involved with each step; detraction from other care activities<br /><br />Individual: Level and patterns of adherence; extent of modifiable barriers identified; # adverse events (e.g., unintended disclosure); degree of stigma reported<br /><br />Implementation process: Challenges with deployment (e.g., importation of devices, execution of the SMS platform)<br /><br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />We will be conducting time and motion studies to understand the standard of care, as well as implementation of the monitors and associated interventions. Specifically, we are interested in the inner setting metrics noted above. We will collect this data in a first implementation study until saturation of data points is achieved. Then, we will repeat it during the second implementation study to see if the metrics improve.<br /><br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? <br /><br />As noted above, we are looking at detraction from other care activities. We anticipate increased efficiency of care delivery through this approach to adherence monitoring and support; however, the technology could be a distraction and/or more challenging to implement than anticipated. There may also be unanticipated costs, including actual costs (e.g., importation fees) and time (e.g., staffing).";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"c0d939d1413db78fd611ad94791f009d";}s:4:"show";b:1;s:3:"cid";s:32:"0d75ec8fcbe1e469a14b8a94bb43b4fb";}s:32:"13c65b61a7317a2b9b435588a5104a37";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"awagner";s:4:"name";s:13:"Anjuli Wagner";s:4:"mail";s:14:"anjuliw@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537579868;}s:3:"raw";s:6327:"Wagner -- Assignment #3

#3a - Models:
1. Which model or combination of models is most applicable to your proposed study and why?

(Relevant aims restated for ease of reference)

Aim 1: To identify promising implementation strategies (ISs) to increase penetration and sustainability of PrEP integration into ANC) will utilize CFIR 

Aim 2: To test whether 3 new or modified implementation strategies improve penetration and sustainability of PrEP integration into ANC using a series of experiments.

For Aim 1, I plan to use CFIR, which will primarily serve as a determinants framework (but additional as an evaluative framework to understand the impact of determinants on outcomes?). We will aim to understand what factors served as positive and negative determinants (barriers and facilitators) to PrEP integration into ANC, and how health care workers innovated and tried specific ISs to address these specific determinants. 

Initially, we had not planned on conducting this CFIR-guided formative work as a first aim, and instead jumping directly into testing specific ISs that we hypothesized would address determinants that were anecdotally (by not systematically) ascertained during the PrIMA and PrIYA projects. I think that this addition will help build the theoretical rationale for the ISs that we test in Aim 2. I appreciate that CFIR can serve both as an explanatory and predictive framework; I hope to use it to use it as an explanatory framework in understanding why the wide range of ISs tried had different effects in the historical observational data; I hope to use it as a predictive framework to hypothesize which ISs might overcome specific barriers to inform Aim 2. 

Additionally, from a practical standpoint, I appreciate the “meta” nature of the CFIR and the wide range of constructs available. I have enjoyed using the CFIR website to understand and select constructs to measure, and the automated question guide generators to create specific guides for a separate study that Kristin Beima-Sofie and I are collaborating on together. After watching a recent systems engineering intervention study, the concepts probed in the CFIR seem to match well to the determinants of success in that study. I look forward to learning more about this as I develop the current application about PrEP integration into ANC!

In Aim 2, I wonder whether the inclusion of RE-AIM would be additionally beneficial as an explanatory framework. While we will primarily be testing ~3 ISs on both system and implementation outcomes in mini-RCTs, I think that an explanation of why each test was or was not successful would strengthen our inference. Additionally, my initial reading of this framework seems to indicate that it is more quantitatively oriented than CFIR, which appeals to me. It seems to blend implementation and service outcomes, which I think are both equally relevant in Aim 2. 


2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 

As mentioned above, the use of CFIR will help to drive the measures in Aim 1, as well as select and develop hypotheses about specific ISs to be tested in Aim 2. The RE-AIM framework does not seem to impact my hypotheses, outcomes, or strategies. However, it will directly inform the selection of measures and processes to document in Aim 2; I have not yet explored the RE-AIM website to guide this measure selection, but look forward to doing so. 



#3b - Measures & Evaluations:
1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

The original PrIYA and PrIMA studies measured a wide range of service and client outcomes, including: efficiency & timeliness (using time and motion data in clinics and simulation models), safety (PrEP safety during pregnancy for maternal and infant outcomes), effectiveness (HIV incidence reduction, PrEP initiation and adherence), patient-centeredness & satisfaction (qualitative interviews about women and health care workers’ experiences delivering and receiving PrEP in ANC).

I will want to continue to measure a few of these outcomes during Aim 2, including efficiency & timeliness (using time and motion data), effectiveness (PrEP initiation and possibly adherence), and patient satisfaction (mini semi-structured exit questionnaires alongside time and motion data collection). 

Additionally, I will want to measure fidelity (and adaptation as described in last week’s response), acceptability, adoption, cost, penetration, and sustainability in Aims 2 & 3 (budget impact). I struggle to understand the distinctions between acceptability, feasibility, and appropriateness, so I will read further about how the two differ and which would be applicable to my work. 


2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

I am naturally fascinated by processes and look forward to measuring clinic flow (time and motion data and flow mapping), operationalization of changes with the introduction of new ISs (flow mapping and narrative explanations by health care workers), and operational data cascades (using routine program data sources to populate).


3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? 

Absolutely. Adding PrEP as an additional set of services (screening, counseling, prescribing, follow-up, documentation, etc.) into an already burdened ANC system is likely to create unintended consequences on service and client outcomes. I intend to measure those outcomes closely through Aim 2 using the aforementioned data sources. However, I am not yet sure how different ISs might offer co-benefit or unintended consequences on different implementation outcomes; I would ideally like to incorporate quantitative measures of penetration, sustainability, and adoption into a pre-post survey for each of the mini-RCTs in Aim 2. I will plan to measure costs for each of the different ISs tested and pay a keen eye to impacts of the ISs on the costs of other delivering other services.


";s:5:"xhtml";s:6559:"Wagner -- Assignment #3<br /><br />#3a - Models:<br />1. Which model or combination of models is most applicable to your proposed study and why?<br /><br />(Relevant aims restated for ease of reference)<br /><br />Aim 1: To identify promising implementation strategies (ISs) to increase penetration and sustainability of PrEP integration into ANC) will utilize CFIR <br /><br />Aim 2: To test whether 3 new or modified implementation strategies improve penetration and sustainability of PrEP integration into ANC using a series of experiments.<br /><br />For Aim 1, I plan to use CFIR, which will primarily serve as a determinants framework (but additional as an evaluative framework to understand the impact of determinants on outcomes?). We will aim to understand what factors served as positive and negative determinants (barriers and facilitators) to PrEP integration into ANC, and how health care workers innovated and tried specific ISs to address these specific determinants. <br /><br />Initially, we had not planned on conducting this CFIR-guided formative work as a first aim, and instead jumping directly into testing specific ISs that we hypothesized would address determinants that were anecdotally (by not systematically) ascertained during the PrIMA and PrIYA projects. I think that this addition will help build the theoretical rationale for the ISs that we test in Aim 2. I appreciate that CFIR can serve both as an explanatory and predictive framework; I hope to use it to use it as an explanatory framework in understanding why the wide range of ISs tried had different effects in the historical observational data; I hope to use it as a predictive framework to hypothesize which ISs might overcome specific barriers to inform Aim 2. <br /><br />Additionally, from a practical standpoint, I appreciate the “meta” nature of the CFIR and the wide range of constructs available. I have enjoyed using the CFIR website to understand and select constructs to measure, and the automated question guide generators to create specific guides for a separate study that Kristin Beima-Sofie and I are collaborating on together. After watching a recent systems engineering intervention study, the concepts probed in the CFIR seem to match well to the determinants of success in that study. I look forward to learning more about this as I develop the current application about PrEP integration into ANC!<br /><br />In Aim 2, I wonder whether the inclusion of RE-AIM would be additionally beneficial as an explanatory framework. While we will primarily be testing ~3 ISs on both system and implementation outcomes in mini-RCTs, I think that an explanation of why each test was or was not successful would strengthen our inference. Additionally, my initial reading of this framework seems to indicate that it is more quantitatively oriented than CFIR, which appeals to me. It seems to blend implementation and service outcomes, which I think are both equally relevant in Aim 2. <br /><br /><br />2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br /><br />As mentioned above, the use of CFIR will help to drive the measures in Aim 1, as well as select and develop hypotheses about specific ISs to be tested in Aim 2. The RE-AIM framework does not seem to impact my hypotheses, outcomes, or strategies. However, it will directly inform the selection of measures and processes to document in Aim 2; I have not yet explored the RE-AIM website to guide this measure selection, but look forward to doing so. <br /><br /><br /><br />#3b - Measures &amp; Evaluations:<br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />The original PrIYA and PrIMA studies measured a wide range of service and client outcomes, including: efficiency &amp; timeliness (using time and motion data in clinics and simulation models), safety (PrEP safety during pregnancy for maternal and infant outcomes), effectiveness (HIV incidence reduction, PrEP initiation and adherence), patient-centeredness &amp; satisfaction (qualitative interviews about women and health care workers’ experiences delivering and receiving PrEP in ANC).<br /><br />I will want to continue to measure a few of these outcomes during Aim 2, including efficiency &amp; timeliness (using time and motion data), effectiveness (PrEP initiation and possibly adherence), and patient satisfaction (mini semi-structured exit questionnaires alongside time and motion data collection). <br /><br />Additionally, I will want to measure fidelity (and adaptation as described in last week’s response), acceptability, adoption, cost, penetration, and sustainability in Aims 2 &amp; 3 (budget impact). I struggle to understand the distinctions between acceptability, feasibility, and appropriateness, so I will read further about how the two differ and which would be applicable to my work. <br /><br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />I am naturally fascinated by processes and look forward to measuring clinic flow (time and motion data and flow mapping), operationalization of changes with the introduction of new ISs (flow mapping and narrative explanations by health care workers), and operational data cascades (using routine program data sources to populate).<br /><br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? <br /><br />Absolutely. Adding PrEP as an additional set of services (screening, counseling, prescribing, follow-up, documentation, etc.) into an already burdened ANC system is likely to create unintended consequences on service and client outcomes. I intend to measure those outcomes closely through Aim 2 using the aforementioned data sources. However, I am not yet sure how different ISs might offer co-benefit or unintended consequences on different implementation outcomes; I would ideally like to incorporate quantitative measures of penetration, sustainability, and adoption into a pre-post survey for each of the mini-RCTs in Aim 2. I will plan to measure costs for each of the different ISs tested and pay a keen eye to impacts of the ISs on the costs of other delivering other services.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"250c9bac12041ddab391be66b22ac1c4";}s:4:"show";b:1;s:3:"cid";s:32:"13c65b61a7317a2b9b435588a5104a37";}s:32:"2ff3b0b3e44ddb0e56dc0c4a7e61a59b";a:8:{s:4:"user";a:5:{s:2:"id";s:3:"dli";s:4:"name";s:9:"Dennis Li";s:4:"mail";s:23:"dennis@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537589431;}s:3:"raw";s:5544:"LI = Assignment #3a - Models:

** 1. Which model or combination of models is most applicable to your proposed study and why?

In my first specific aim, I stated that I would examine HIV service providers’ attitudes toward eHealth HIV interventions (e.g., acceptability, feasibility), guided by Proctor et al.’s implementation outcomes taxonomy. Because I am not actually implementing a program as part of this proposed study, however, at most, I would be able to assess acceptability, appropriateness, and feasibility. I could supplement these measures with those from the Characteristics of Individuals domain from CFIR. Knowledge and beliefs about the intervention overlaps with those three Proctor outcomes, but self-efficacy to implement, individual stage of change, individual identification with the organization (and thus its HIV prevention mission), and other personal attributes like competence with technology could be good constructs to measure.

In part 2 of my first specific aim, I also said I would use CFIR to identify processes, barriers, and facilitators of adopting, implementing, and maintaining eHealth HIV interventions. I still think this makes sense because of how comprehensive CFIR is in these domains. Given that my goal is the identification of attitudes, barriers, and facilitators and not necessarily the examination of their relationships, I think a determine framework like CFIR, combined with some of Proctor’s implementation outcomes, is appropriate. Coincidentally, these two frameworks are the organizing frameworks for the SIRC Instrument Review Project.

** 2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

As noted in the Martinez et al. and Rabin et al. readings, the CFIR (and somewhat the Proctor outcomes) are widely known and used, so there would be access to better measures for many of the constructs. Aim 2 of my proposed study seeks to map implementation strategies onto the determinants identified in Aim 1. Because CFIR (and Proctor) constructs are well-defined (e.g., on the CFIR Wiki), it becomes easier to map the strategies to the appropriate determinants. For example, the definition for Knowledge and Beliefs about the Intervention says:

“Skill levels [in using the intervention] reflect the effectiveness of training. If this knowledge is not obtained prior to [implementation], rejection or discontinuance are likely. The competence of individuals to judge the effectiveness of an intervention is facilitated by their understanding of underlying principles that justify using the intervention.”

If Knowledge and Beliefs about the Intervention seems to be a strong determinant for eHealth HIV interventions, then an appropriately matched strategy might be training.

LI - Assignment #3b - Measures & Evaluations:

** 1. What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

As stated above, I will be measuring three of the Proctor et al. outcomes in my first aim: acceptability, appropriateness, and feasibility. Because eHealth HIV interventions have yet to be widely adopted, and the proposed study is not actually implementing the program, I am focused on measuring attitudinal outcomes from potential service providers (with another caveat being that I’m not treating these as outcomes in my study). I plan on assessing these constructs primarily using qualitative methods to understand why service providers might find eHealth HIV interventions acceptable/appropriate/feasible or not. From this week’s lesson, though, I think I can also use Weiner et al.’s Acceptability of Intervention Measure, Intervention Appropriateness Measure, and Feasibility of Intervention Measure, all three of which are pragmatic (4 items each) with good demonstrated psychometrics.

** 2. What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

In part 2 of my first aim I state that I will identify processes…of adopting, implementing, and maintaining eHealth HIV programs, informed by the CFIR. According to the CFIR wiki, process is “the single most difficult domain to define, measure, or evaluate in implementation research.” This module made me question if I should be measuring process with a quantitative instrument, but I actually don’t think that it would be appropriate for my research question to do so. My goal is to inform selection of implementation strategies for eHealth programs (aim 2) by learning about how HIV service providers had adopted other HIV programs in the past. Given the exploratory nature, a qualitative approach seems more suitable. On the Grid-Enabled Measures Database, Implementation Workspace, there is a semi-structured interview protocol for program adoption developed by Noonan et al. for community settings (https://www.gem-measures.org/public/MeasureDetail.aspx?mid=1284&cat=2) that I think I can adapt for my purposes.

** 3. Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

Because I am not planning to actually implement a program, any co-benefits or unintended consequences would only be perceived. Such data may come out in the qualitative interviews, but they are not a priority in my proposed study.
";s:5:"xhtml";s:5691:"LI = Assignment #3a - Models:<br /><br />** 1. Which model or combination of models is most applicable to your proposed study and why?<br /><br />In my first specific aim, I stated that I would examine HIV service providers’ attitudes toward eHealth HIV interventions (e.g., acceptability, feasibility), guided by Proctor et al.’s implementation outcomes taxonomy. Because I am not actually implementing a program as part of this proposed study, however, at most, I would be able to assess acceptability, appropriateness, and feasibility. I could supplement these measures with those from the Characteristics of Individuals domain from CFIR. Knowledge and beliefs about the intervention overlaps with those three Proctor outcomes, but self-efficacy to implement, individual stage of change, individual identification with the organization (and thus its HIV prevention mission), and other personal attributes like competence with technology could be good constructs to measure.<br /><br />In part 2 of my first specific aim, I also said I would use CFIR to identify processes, barriers, and facilitators of adopting, implementing, and maintaining eHealth HIV interventions. I still think this makes sense because of how comprehensive CFIR is in these domains. Given that my goal is the identification of attitudes, barriers, and facilitators and not necessarily the examination of their relationships, I think a determine framework like CFIR, combined with some of Proctor’s implementation outcomes, is appropriate. Coincidentally, these two frameworks are the organizing frameworks for the SIRC Instrument Review Project.<br /><br />** 2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />As noted in the Martinez et al. and Rabin et al. readings, the CFIR (and somewhat the Proctor outcomes) are widely known and used, so there would be access to better measures for many of the constructs. Aim 2 of my proposed study seeks to map implementation strategies onto the determinants identified in Aim 1. Because CFIR (and Proctor) constructs are well-defined (e.g., on the CFIR Wiki), it becomes easier to map the strategies to the appropriate determinants. For example, the definition for Knowledge and Beliefs about the Intervention says:<br /><br />“Skill levels [in using the intervention] reflect the effectiveness of training. If this knowledge is not obtained prior to [implementation], rejection or discontinuance are likely. The competence of individuals to judge the effectiveness of an intervention is facilitated by their understanding of underlying principles that justify using the intervention.”<br /><br />If Knowledge and Beliefs about the Intervention seems to be a strong determinant for eHealth HIV interventions, then an appropriately matched strategy might be training.<br /><br />LI - Assignment #3b - Measures &amp; Evaluations:<br /><br />** 1. What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />As stated above, I will be measuring three of the Proctor et al. outcomes in my first aim: acceptability, appropriateness, and feasibility. Because eHealth HIV interventions have yet to be widely adopted, and the proposed study is not actually implementing the program, I am focused on measuring attitudinal outcomes from potential service providers (with another caveat being that I’m not treating these as outcomes in my study). I plan on assessing these constructs primarily using qualitative methods to understand why service providers might find eHealth HIV interventions acceptable/appropriate/feasible or not. From this week’s lesson, though, I think I can also use Weiner et al.’s Acceptability of Intervention Measure, Intervention Appropriateness Measure, and Feasibility of Intervention Measure, all three of which are pragmatic (4 items each) with good demonstrated psychometrics.<br /><br />** 2. What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />In part 2 of my first aim I state that I will identify processes…of adopting, implementing, and maintaining eHealth HIV programs, informed by the CFIR. According to the CFIR wiki, process is “the single most difficult domain to define, measure, or evaluate in implementation research.” This module made me question if I should be measuring process with a quantitative instrument, but I actually don’t think that it would be appropriate for my research question to do so. My goal is to inform selection of implementation strategies for eHealth programs (aim 2) by learning about how HIV service providers had adopted other HIV programs in the past. Given the exploratory nature, a qualitative approach seems more suitable. On the Grid-Enabled Measures Database, Implementation Workspace, there is a semi-structured interview protocol for program adoption developed by Noonan et al. for community settings (https://www.gem-measures.org/public/MeasureDetail.aspx?mid=1284&amp;cat=2) that I think I can adapt for my purposes.<br /><br />** 3. Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />Because I am not planning to actually implement a program, any co-benefits or unintended consequences would only be perceived. Such data may come out in the qualitative interviews, but they are not a priority in my proposed study.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"fa779879dfd49807f80ae702fbd4bc40";}s:4:"show";b:1;s:3:"cid";s:32:"2ff3b0b3e44ddb0e56dc0c4a7e61a59b";}s:32:"2feca61bf4801d6f8c47d15f824deb9a";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cpsaros";s:4:"name";s:16:"Christina Psaros";s:4:"mail";s:23:"CPSAROS@mgh.harvard.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537817475;}s:3:"raw";s:5834:"Psaros = Assignment 3

3a

1.	Which model or combination of models is most applicable to your proposed study and why?

My response to this is…

We plan to use both the Consolidated Framework for Implementation Research (CFIR) and Dynamic Adaptation Process (DAP) frameworks. We chose CFIR as it provides a platform to guide intervention planning and implementation. By addressing “what works, where and why,” the CFIR is particularly useful in guiding formative evaluations, and contributes to a knowledge base critical for the implementation of interventions across diverse settings. It can inform data collection related to intervention acceptability and feasibility, context and sustainability, penetration and adoption, and assist in distinguishing core from adaptable intervention components. The DAP framework will be used in the adaptation of this application embedded within the CFIR implementation framework. It has been developed for the adaptation of evidence based interventions and provides a model framework that includes cultural adaptations, which we expect our intervention to include. The DAP provides a process for pre-assessment, convening an “implementation resource team” to guide the implementation process, and use of audit and feedback data to help guide appropriate EBI adaptation.

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 

My response to this is:

The models may inform our measures as early, formative work will help us understand how to most effectively integrate PrEP services into FQHCs for African American women. The formative work may suggest steps that need to be added into the PrEP delivery intervention, and thus, we will need to measure them as both outcomes and processes. In addition, we expect to measure cultural competency, depending on the formative work. The implementation resource team may have additional feedback that translates into steps in the intervention and these too will need to be measured. 

3b 

1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

My response to this is:

Our variables of interest largely correspond to our proposed models; we have not yet decided how to measure all outcomes. 

Intervention characteristics include (1) feasibility (number of participants screened; number of eligible participants enrolled; reasons for declining enrollment, leaving study, and/or discontinuing PrEP; recruitment/scheduling strategies; contacts with participants throughout study; feasibility of administering instruments; number of new HIV infections); PrEP uptake; PrEP adherence; clinic visit adherence; (2) acceptability: patient / provider perceptions of study and evaluation of PrEP delivery; (3) proposed modifications (both structural and didactic).

Outer setting variables include (1) patient needs and resources; and (2) stigma (likely the Scale of Stigmatizing Attitudes Towards People Living with HIV).

Inner setting variables include (1) patient satisfaction (likely with Client Satisfaction Questionnaire); (2) provider satisfaction (likely with Behavioral Interventionist Satisfaction Survey); (3) Structural characteristics (e.g., social structure, age, maturity, and size); (4) culture (likely with Culture Assessment Instrument); (5) readiness for change.

Individual characteristics include (1) demographics; (2) intimate partner violence (likely Abuse Assessment Screen); (3) mental health variables (e.g., depression, anxiety, PTSD); (4) sexual risk behavior; (5) substance use (likely ASI Lite); (5) individual stage of change.

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

My response to this is...

Process evaluation measures (feasibility assessments) will include number of participants screened for participation, number of eligible participants who enrolled, and the number of participants who agreed to use PrEP and adhered to their regimen as prescribed. We will also track reasons for declining enrollment and for prematurely leaving the study and/or discontinuing PrEP (permission to contact participants via phone will be obtained during the consent process). Recruitment and scheduling strategies and contacts with participants throughout the study will be documented as well as feasibility to administer survey instruments during assessments (e.g. length of time for conducting assessments). We will also collect data on acceptability measures; all participating patients and providers will be asked to consent to an in-depth phone interview addressing their perceptions of the study as a whole and an evaluation of the PrEP delivery protocol, including location of delivery.


3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

My response to this is: 

Every effort will be made to identify additional resources and services available to address and mitigate the impact of barriers and challenges identified. The knowledge and expertise of the implementation resource team convened for the adaptation of the intervention will be valuable to the improvement of patient care beyond the traditional clinic setting (e.g. linkage with community services unrelated to HIV care). Should contextual and structural factors that are beyond the scope of the resources in this application be discovered, the study team will communicate this issues with the appropriate local or state health departments to shape an environment supportive of PrEP services for AA women in Alabama.
";s:5:"xhtml";s:6048:"Psaros = Assignment 3<br /><br />3a<br /><br />1.	Which model or combination of models is most applicable to your proposed study and why?<br /><br />My response to this is…<br /><br />We plan to use both the Consolidated Framework for Implementation Research (CFIR) and Dynamic Adaptation Process (DAP) frameworks. We chose CFIR as it provides a platform to guide intervention planning and implementation. By addressing “what works, where and why,” the CFIR is particularly useful in guiding formative evaluations, and contributes to a knowledge base critical for the implementation of interventions across diverse settings. It can inform data collection related to intervention acceptability and feasibility, context and sustainability, penetration and adoption, and assist in distinguishing core from adaptable intervention components. The DAP framework will be used in the adaptation of this application embedded within the CFIR implementation framework. It has been developed for the adaptation of evidence based interventions and provides a model framework that includes cultural adaptations, which we expect our intervention to include. The DAP provides a process for pre-assessment, convening an “implementation resource team” to guide the implementation process, and use of audit and feedback data to help guide appropriate EBI adaptation.<br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br /><br />My response to this is:<br /><br />The models may inform our measures as early, formative work will help us understand how to most effectively integrate PrEP services into FQHCs for African American women. The formative work may suggest steps that need to be added into the PrEP delivery intervention, and thus, we will need to measure them as both outcomes and processes. In addition, we expect to measure cultural competency, depending on the formative work. The implementation resource team may have additional feedback that translates into steps in the intervention and these too will need to be measured. <br /><br />3b <br /><br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />My response to this is:<br /><br />Our variables of interest largely correspond to our proposed models; we have not yet decided how to measure all outcomes. <br /><br />Intervention characteristics include (1) feasibility (number of participants screened; number of eligible participants enrolled; reasons for declining enrollment, leaving study, and/or discontinuing PrEP; recruitment/scheduling strategies; contacts with participants throughout study; feasibility of administering instruments; number of new HIV infections); PrEP uptake; PrEP adherence; clinic visit adherence; (2) acceptability: patient / provider perceptions of study and evaluation of PrEP delivery; (3) proposed modifications (both structural and didactic).<br /><br />Outer setting variables include (1) patient needs and resources; and (2) stigma (likely the Scale of Stigmatizing Attitudes Towards People Living with HIV).<br /><br />Inner setting variables include (1) patient satisfaction (likely with Client Satisfaction Questionnaire); (2) provider satisfaction (likely with Behavioral Interventionist Satisfaction Survey); (3) Structural characteristics (e.g., social structure, age, maturity, and size); (4) culture (likely with Culture Assessment Instrument); (5) readiness for change.<br /><br />Individual characteristics include (1) demographics; (2) intimate partner violence (likely Abuse Assessment Screen); (3) mental health variables (e.g., depression, anxiety, PTSD); (4) sexual risk behavior; (5) substance use (likely ASI Lite); (5) individual stage of change.<br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />My response to this is...<br /><br />Process evaluation measures (feasibility assessments) will include number of participants screened for participation, number of eligible participants who enrolled, and the number of participants who agreed to use PrEP and adhered to their regimen as prescribed. We will also track reasons for declining enrollment and for prematurely leaving the study and/or discontinuing PrEP (permission to contact participants via phone will be obtained during the consent process). Recruitment and scheduling strategies and contacts with participants throughout the study will be documented as well as feasibility to administer survey instruments during assessments (e.g. length of time for conducting assessments). We will also collect data on acceptability measures; all participating patients and providers will be asked to consent to an in-depth phone interview addressing their perceptions of the study as a whole and an evaluation of the PrEP delivery protocol, including location of delivery.<br /><br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />My response to this is: <br /><br />Every effort will be made to identify additional resources and services available to address and mitigate the impact of barriers and challenges identified. The knowledge and expertise of the implementation resource team convened for the adaptation of the intervention will be valuable to the improvement of patient care beyond the traditional clinic setting (e.g. linkage with community services unrelated to HIV care). Should contextual and structural factors that are beyond the scope of the resources in this application be discovered, the study team will communicate this issues with the appropriate local or state health departments to shape an environment supportive of PrEP services for AA women in Alabama.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"a59c9fe6298d015931efc1dd361d99b2";}s:4:"show";b:1;s:3:"cid";s:32:"2feca61bf4801d6f8c47d15f824deb9a";}s:32:"ed486630ca99b2af21ed889adb02fbcc";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537889223;}s:3:"raw";s:1021:"1. I appreciate that you chose a determinants framework, and a commonly used one. Just as an aside, this program does hope that trainees can feel free to comment/ask questions in response to their fellow trainee responses -- and as well, may pick up some tips or hints on how other folks (take a look!) are handling the complex issues that you are grappling with...
2. Such as, how to best "map" frameworks and models to variables and outcomes. And you are right to appraise the framework as a broad canvas to help guide the questions you ask and the variables you select -- and not limiting. you may well uncover/find a factor that doesn't quite neatly fit. Have your mentors at UW pointed you in any direction for potential measures for the constructs you have in mind?
3. Very thoughtful responses re: the measures. It will be interesting to see what (if any) secular trends may also affect intervention implementation. Hard to anticipate everything, but good that your qualitative efforts could shine a light on this.";s:5:"xhtml";s:1046:"1. I appreciate that you chose a determinants framework, and a commonly used one. Just as an aside, this program does hope that trainees can feel free to comment/ask questions in response to their fellow trainee responses -- and as well, may pick up some tips or hints on how other folks (take a look!) are handling the complex issues that you are grappling with...<br />2. Such as, how to best &quot;map&quot; frameworks and models to variables and outcomes. And you are right to appraise the framework as a broad canvas to help guide the questions you ask and the variables you select -- and not limiting. you may well uncover/find a factor that doesn&#039;t quite neatly fit. Have your mentors at UW pointed you in any direction for potential measures for the constructs you have in mind?<br />3. Very thoughtful responses re: the measures. It will be interesting to see what (if any) secular trends may also affect intervention implementation. Hard to anticipate everything, but good that your qualitative efforts could shine a light on this.";s:6:"parent";s:32:"147881660748b960bf8b8068a303b990";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"ed486630ca99b2af21ed889adb02fbcc";}s:32:"c0d939d1413db78fd611ad94791f009d";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537890005;}s:3:"raw";s:731:"v thorough response. just a couple questions as i am thinking about these responses in the context of the methods you propose.
1. I am curious -- in the formative phase of this work, do you anticipate being able to get input/perception/insights about the outer/inner setting from patients, or will that data come primarily from the administrators/clinicians?
2. Do i understand accurately that the evaluation period for intervention implementation will be a 3-month period? Do your previous experiences with devices/this setting suggest this will be enough time to provide you with the info you will need to design the implementation strategy for the R01 to follow? Is the second "cohort" of 30 patients also followed for 3 months?";s:5:"xhtml";s:751:"v thorough response. just a couple questions as i am thinking about these responses in the context of the methods you propose.<br />1. I am curious -- in the formative phase of this work, do you anticipate being able to get input/perception/insights about the outer/inner setting from patients, or will that data come primarily from the administrators/clinicians?<br />2. Do i understand accurately that the evaluation period for intervention implementation will be a 3-month period? Do your previous experiences with devices/this setting suggest this will be enough time to provide you with the info you will need to design the implementation strategy for the R01 to follow? Is the second &quot;cohort&quot; of 30 patients also followed for 3 months?";s:6:"parent";s:32:"0d75ec8fcbe1e469a14b8a94bb43b4fb";s:7:"replies";a:1:{i:0;s:32:"6177df878c4352beff307f0d50e82d92";}s:4:"show";b:1;s:3:"cid";s:32:"c0d939d1413db78fd611ad94791f009d";}s:32:"d6a440511d04dad2d64fad21cc5bc145";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"mmarzan";s:4:"name";s:24:"Melissa Marzan-Rodriguez";s:4:"mail";s:22:"melissa.marzan@upr.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537907013;}s:3:"raw";s:1179:"Marzan-Rodriguez, Assignment 3a

1. Which model or combination of models is most applicable to your proposed study and why?

My response to this is…
SMART program is currently using a RE-AIM framework to guide the implementation research process.  In general, this framework addresses five domains: 1) reach; 2) effectiveness or efficacy; 3) adoption; 4) implementation; and 5) sustainability. Also, the framework model of Consolidated Framework for Implementation Research (CFIR) is expected to use to supplement the evaluation with organization-level measures about the settings in which the SMART Program could be implemented.


2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 

My response to this is:
The CFIR bring the opportunity to understand at the organizational level the facilitators or barriers to implement SMART program.  In order to address the implementation science dimension of “reach the population” thru qualitative semi-structure interviews to Community Based Organizations who work with adolescents and/or adolescents and LGBT issues. 
";s:5:"xhtml";s:1232:"Marzan-Rodriguez, Assignment 3a<br /><br />1. Which model or combination of models is most applicable to your proposed study and why?<br /><br />My response to this is…<br />SMART program is currently using a RE-AIM framework to guide the implementation research process.  In general, this framework addresses five domains: 1) reach; 2) effectiveness or efficacy; 3) adoption; 4) implementation; and 5) sustainability. Also, the framework model of Consolidated Framework for Implementation Research (CFIR) is expected to use to supplement the evaluation with organization-level measures about the settings in which the SMART Program could be implemented.<br /><br /><br />2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br /><br />My response to this is:<br />The CFIR bring the opportunity to understand at the organizational level the facilitators or barriers to implement SMART program.  In order to address the implementation science dimension of “reach the population” thru qualitative semi-structure interviews to Community Based Organizations who work with adolescents and/or adolescents and LGBT issues.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"d6a440511d04dad2d64fad21cc5bc145";}s:32:"fa779879dfd49807f80ae702fbd4bc40";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537974392;}s:3:"raw";s:684:"Thanks, Dennis. Your proposal continues to be unique (to this set of proposals) because you are not going to implement an intervention -- but you are being mindful of the model and process in the research, nonetheless. I agree -- that semi-structured interview protocol for program adoption looks like a good choice to adapt and use. Couple questions -- Will you be inquiring about service providers' intention to integrate some form of e-health delivery in their setting? Also, with respect to co-benefits or unintended consequences, I could imagine that providers' concerns (as you say, perceptions) about any downside to implementation could affect their future use of such tools. ";s:5:"xhtml";s:693:"Thanks, Dennis. Your proposal continues to be unique (to this set of proposals) because you are not going to implement an intervention -- but you are being mindful of the model and process in the research, nonetheless. I agree -- that semi-structured interview protocol for program adoption looks like a good choice to adapt and use. Couple questions -- Will you be inquiring about service providers&#039; intention to integrate some form of e-health delivery in their setting? Also, with respect to co-benefits or unintended consequences, I could imagine that providers&#039; concerns (as you say, perceptions) about any downside to implementation could affect their future use of such tools.";s:6:"parent";s:32:"2ff3b0b3e44ddb0e56dc0c4a7e61a59b";s:7:"replies";a:1:{i:0;s:32:"2d396b0c961851a003d223413f72d442";}s:4:"show";b:1;s:3:"cid";s:32:"fa779879dfd49807f80ae702fbd4bc40";}s:32:"a59c9fe6298d015931efc1dd361d99b2";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537974939;}s:3:"raw";s:341:"Thanks for this response, Christina. I may have missed this -- but i realized that in appraising your evaluation of process and setting-level variables, that i cannot recall how many sites you are proposing for this effort. If a single site, what are the pros/cons of that approach versus the pros/cons of gathering input from several FQHCs?";s:5:"xhtml";s:341:"Thanks for this response, Christina. I may have missed this -- but i realized that in appraising your evaluation of process and setting-level variables, that i cannot recall how many sites you are proposing for this effort. If a single site, what are the pros/cons of that approach versus the pros/cons of gathering input from several FQHCs?";s:6:"parent";s:32:"2feca61bf4801d6f8c47d15f824deb9a";s:7:"replies";a:1:{i:0;s:32:"18fbc312fdd0c9f20b45b01fb894b88e";}s:4:"show";b:1;s:3:"cid";s:32:"a59c9fe6298d015931efc1dd361d99b2";}s:32:"fec54fe4e45428872156809f7c8df56b";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537975156;}s:3:"raw";s:188:"Thank you, Melissa. It would be helpful to know a bit more about how the RE-AIM domains will guide your measurement, and, for example, how you will evaluate the organization-level factors?";s:5:"xhtml";s:188:"Thank you, Melissa. It would be helpful to know a bit more about how the RE-AIM domains will guide your measurement, and, for example, how you will evaluate the organization-level factors?";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"fec54fe4e45428872156809f7c8df56b";}s:32:"18fbc312fdd0c9f20b45b01fb894b88e";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cpsaros";s:4:"name";s:16:"Christina Psaros";s:4:"mail";s:23:"CPSAROS@mgh.harvard.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537985402;}s:3:"raw";s:818:"Thanks, Chris! Funny you should ask... we had originally proposed only one site and that was perhaps the most significant critique of our proposal. I was just in Birmingham, and I think we now have two sites on board... though, another critique was related to having/not having enough formative work to go full on into PrEP delivery mode. So, I think we are changing gears for the resubmission somewhat, and working more on the level of patient provider communication about PrEP at FQHCs, with a plan to refer interested/eligible women to neighborhood PrEP clinics (there are PrEP clinics near the two clinics with whom we would work). This better represents how PrEP is being delivered in AL, and will allow us to collect formative data on what would need to happen in the FQHCs to get PrEP delivery happening there. ";s:5:"xhtml";s:817:"Thanks, Chris! Funny you should ask... we had originally proposed only one site and that was perhaps the most significant critique of our proposal. I was just in Birmingham, and I think we now have two sites on board... though, another critique was related to having/not having enough formative work to go full on into PrEP delivery mode. So, I think we are changing gears for the resubmission somewhat, and working more on the level of patient provider communication about PrEP at FQHCs, with a plan to refer interested/eligible women to neighborhood PrEP clinics (there are PrEP clinics near the two clinics with whom we would work). This better represents how PrEP is being delivered in AL, and will allow us to collect formative data on what would need to happen in the FQHCs to get PrEP delivery happening there.";s:6:"parent";s:32:"a59c9fe6298d015931efc1dd361d99b2";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"18fbc312fdd0c9f20b45b01fb894b88e";}s:32:"1efa4a267f0968cd50e1d3f0edb0570e";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538143972;}s:3:"raw";s:350:"Hi Jessica,  Your fidelity measures look great and should give you a nice way to triangulate the data as you mention.  Waiting to conduct the work on adaptations also makes sense.  I take it you may want to  use observation as you look at fidelity to determine some of the adaptations and then interviews later to understand them better.
best,

Linda";s:5:"xhtml";s:365:"Hi Jessica,  Your fidelity measures look great and should give you a nice way to triangulate the data as you mention.  Waiting to conduct the work on adaptations also makes sense.  I take it you may want to  use observation as you look at fidelity to determine some of the adaptations and then interviews later to understand them better.<br />best,<br /><br />Linda";s:6:"parent";s:32:"dd4fcec4ac82ff427103da63b41206d1";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"1efa4a267f0968cd50e1d3f0edb0570e";}s:32:"250c9bac12041ddab391be66b22ac1c4";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538145624;}s:3:"raw";s:970:"Hi Anjuli,  

To your question on the outcome measures.  A nice paper describing the differences between acceptability appropriateness and feasibility is Weiner et al, Implementation Science (2017) 12:108.  These are still constructs that are being worked out so do not feel bad that you can't tell the difference or that there are not good measures -- that indeed is the state of the science.  However, Weiner does a very good job defining these three constructs and this work will allow you to better determine these outcomes in your project.  In addition, the outcomes toolbox on the Washington University website is very helpful -- Gerke, D. et al.  Implementation Outcomes (internet)., St. Louis, Mo, Washington University; 2017 July. Eight toolkits related to Dissemination and Implementation.  https://sites.wustl.edu/wudandi . The one on D&I outcomes has the specific outcomes measures and references to articles that use them.  
I hope this is helpful.

Linda

";s:5:"xhtml";s:1002:"Hi Anjuli,  <br /><br />To your question on the outcome measures.  A nice paper describing the differences between acceptability appropriateness and feasibility is Weiner et al, Implementation Science (2017) 12:108.  These are still constructs that are being worked out so do not feel bad that you can&#039;t tell the difference or that there are not good measures -- that indeed is the state of the science.  However, Weiner does a very good job defining these three constructs and this work will allow you to better determine these outcomes in your project.  In addition, the outcomes toolbox on the Washington University website is very helpful -- Gerke, D. et al.  Implementation Outcomes (internet)., St. Louis, Mo, Washington University; 2017 July. Eight toolkits related to Dissemination and Implementation.  https://sites.wustl.edu/wudandi . The one on D&amp;I outcomes has the specific outcomes measures and references to articles that use them.  <br />I hope this is helpful.<br /><br />Linda";s:6:"parent";s:32:"13c65b61a7317a2b9b435588a5104a37";s:7:"replies";a:1:{i:0;s:32:"937df76a09beeb152603fd73aa172149";}s:4:"show";b:1;s:3:"cid";s:32:"250c9bac12041ddab391be66b22ac1c4";}s:32:"937df76a09beeb152603fd73aa172149";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"awagner";s:4:"name";s:13:"Anjuli Wagner";s:4:"mail";s:14:"anjuliw@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538154761;}s:3:"raw";s:93:"Dear Linda,

Many thanks for these excellent resources! Much appreciated. 

Thank you,
Anjuli";s:5:"xhtml";s:118:"Dear Linda,<br /><br />Many thanks for these excellent resources! Much appreciated. <br /><br />Thank you,<br />Anjuli";s:6:"parent";s:32:"250c9bac12041ddab391be66b22ac1c4";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"937df76a09beeb152603fd73aa172149";}s:32:"6177df878c4352beff307f0d50e82d92";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jhaberer";s:4:"name";s:15:"Jessica Haberer";s:4:"mail";s:21:"JHABERER@PARTNERS.ORG";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538584861;}s:3:"raw";s:407:"Thanks, Chris. 

1. Yes, we're getting input from patients and admin/clinicians at multiple levels.
2. Yes, I think 3 months is enough for a pilot approach. I'm most worried about the implementation. Long-term use could bring its own challenges, but I'm less concerns about that. Perhaps we could extend the second iteration. We have time for that within the K24 funding mechanism and the cost would be low.";s:5:"xhtml";s:437:"Thanks, Chris. <br /><br />1. Yes, we&#039;re getting input from patients and admin/clinicians at multiple levels.<br />2. Yes, I think 3 months is enough for a pilot approach. I&#039;m most worried about the implementation. Long-term use could bring its own challenges, but I&#039;m less concerns about that. Perhaps we could extend the second iteration. We have time for that within the K24 funding mechanism and the cost would be low.";s:6:"parent";s:32:"c0d939d1413db78fd611ad94791f009d";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"6177df878c4352beff307f0d50e82d92";}s:32:"8bc0b38d1adb30ae94c834a70e9c03a6";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"awagner";s:4:"name";s:13:"Anjuli Wagner";s:4:"mail";s:14:"anjuliw@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538684230;}s:3:"raw";s:4319:"Wagner -- Assignment #4

1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?


(Restating aims as part of response here for ease of reference) 

Aim 1: To identify promising implementation strategies (ISs) to increase penetration and sustainability of PrEP integration into ANC. We will make use of the heterogeneous ISs used in PrIMA and PrIYA to characterize experiences with specific ISs and elucidate opinions on modifications to these specific ISs to further improve penetration and sustainability. 

Design: We will use a mixed-methods approach, combining sequential qualitative and quantitative data. We will first use qualitative focus group discussions (FGDs) with health care workers (HCW), using questions and probes based in the CFIR. This exploratory data will be used to inform quantitative questions administered to a broader sample of HCW. This mixed methods approach follows the qual → QUANT model, where qualitative data is used to generate hypotheses and ideas and quantitative data is used to determine relative frequency, which we will use to prioritize specific ISs to be tested in Aim 2. The qualitative information will also provide information about the details of ISs tested (depth of information that is not feasible to collect in quantitative questionnaires), playing the role of complementarity as well as exploratory data.


Aim 2: To test whether 3 new or modified implementation strategies improve penetration and sustainability of PrEP integration into ANC using a series of experiments. 

Design: We will use pragmatic either rapid-cycle, small-scale cluster randomized controlled trials (cRCTs) or interrupted time series (ITS) studies to test each of these ISs against standard practice; we will use 3 months of baseline and 3 months of intervention time for each experiment. cRCTs with adjustment for baseline temporal trends can improve precision in effect estimation and more appropriately deal with heterogeneity between clusters; ITS with concurrent control groups can increase the robustness of the inference in estimating effect of the intervention. The choice of randomization versus observation for these small RCTs will be based on whether it is deemed ethically and politically feasible to randomize. We considered alternative designs (classical pre-post without a time element, difference in differences, regression discontinuity [not applicable], stepped wedge, adaptive designs [SMART]), but none of the alternatives provided as strong control for concurrent temporal trends or heterogeneity between clinics. We strongly considered an adaptive design, but wanted to isolate the individual, rather than additive or sub-group level, impact of the ISs.

We will additionally evaluate the implementation of each IS using a concurrent mixed-methods approach based in the CFIR, using it as an evaluative framework (and maybe RE-AIM, as it blends implementation and service outcomes). Within CFIR, the two sources of data (qualitative data from FGDs or in-depth interviews [IDIs] and quantitative data from surveys with HCW and PrEP clients) will be considered concurrently to address convergence or corroboration. 


Aim 3: To determine the budget impact of each implementation strategy and assess policymaker understanding and interpretation of budget impact analyses. 

Design: We will conduct a budget impact analysis that assesses the affordability of each strategy within the context of a Kenyan public health care system. The design of this component of Aim 3 will be a traditional health economics approach, involving using effectiveness estimates from the quantitative service data in Aim 2 and doing primary cost collection supplemented with cost estimates from the published literature.

We will conduct semi-structured in-depth interviews to assess policymaker understanding of the budget impact analysis, as well as likelihood of making changes based on these results. This will not be a mixed methods approach, but rather a purely qualitative design.

";s:5:"xhtml";s:4442:"Wagner -- Assignment #4<br /><br />1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br /><br /><br />(Restating aims as part of response here for ease of reference) <br /><br />Aim 1: To identify promising implementation strategies (ISs) to increase penetration and sustainability of PrEP integration into ANC. We will make use of the heterogeneous ISs used in PrIMA and PrIYA to characterize experiences with specific ISs and elucidate opinions on modifications to these specific ISs to further improve penetration and sustainability. <br /><br />Design: We will use a mixed-methods approach, combining sequential qualitative and quantitative data. We will first use qualitative focus group discussions (FGDs) with health care workers (HCW), using questions and probes based in the CFIR. This exploratory data will be used to inform quantitative questions administered to a broader sample of HCW. This mixed methods approach follows the qual → QUANT model, where qualitative data is used to generate hypotheses and ideas and quantitative data is used to determine relative frequency, which we will use to prioritize specific ISs to be tested in Aim 2. The qualitative information will also provide information about the details of ISs tested (depth of information that is not feasible to collect in quantitative questionnaires), playing the role of complementarity as well as exploratory data.<br /><br /><br />Aim 2: To test whether 3 new or modified implementation strategies improve penetration and sustainability of PrEP integration into ANC using a series of experiments. <br /><br />Design: We will use pragmatic either rapid-cycle, small-scale cluster randomized controlled trials (cRCTs) or interrupted time series (ITS) studies to test each of these ISs against standard practice; we will use 3 months of baseline and 3 months of intervention time for each experiment. cRCTs with adjustment for baseline temporal trends can improve precision in effect estimation and more appropriately deal with heterogeneity between clusters; ITS with concurrent control groups can increase the robustness of the inference in estimating effect of the intervention. The choice of randomization versus observation for these small RCTs will be based on whether it is deemed ethically and politically feasible to randomize. We considered alternative designs (classical pre-post without a time element, difference in differences, regression discontinuity [not applicable], stepped wedge, adaptive designs [SMART]), but none of the alternatives provided as strong control for concurrent temporal trends or heterogeneity between clinics. We strongly considered an adaptive design, but wanted to isolate the individual, rather than additive or sub-group level, impact of the ISs.<br /><br />We will additionally evaluate the implementation of each IS using a concurrent mixed-methods approach based in the CFIR, using it as an evaluative framework (and maybe RE-AIM, as it blends implementation and service outcomes). Within CFIR, the two sources of data (qualitative data from FGDs or in-depth interviews [IDIs] and quantitative data from surveys with HCW and PrEP clients) will be considered concurrently to address convergence or corroboration. <br /><br /><br />Aim 3: To determine the budget impact of each implementation strategy and assess policymaker understanding and interpretation of budget impact analyses. <br /><br />Design: We will conduct a budget impact analysis that assesses the affordability of each strategy within the context of a Kenyan public health care system. The design of this component of Aim 3 will be a traditional health economics approach, involving using effectiveness estimates from the quantitative service data in Aim 2 and doing primary cost collection supplemented with cost estimates from the published literature.<br /><br />We will conduct semi-structured in-depth interviews to assess policymaker understanding of the budget impact analysis, as well as likelihood of making changes based on these results. This will not be a mixed methods approach, but rather a purely qualitative design.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"4df4ef471ea07a06db4cfc992c91c689";}s:4:"show";b:1;s:3:"cid";s:32:"8bc0b38d1adb30ae94c834a70e9c03a6";}s:32:"5a3291ce70ebfaaa173e847a6e577c8b";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jhaberer";s:4:"name";s:15:"Jessica Haberer";s:4:"mail";s:21:"JHABERER@PARTNERS.ORG";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538706608;}s:3:"raw";s:2108:"Haberer: Assignment #4 

1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

Response: My proposed study design is observational. In this K24 pilot study (i.e., small scope), we want to learn about ways to implement the real-time adherence monitor plus support in routine clinical practice, exploring facilitators and barriers. In the initial qualitative phase of the study (Aim 1), we are asking patients and clinicians/healthcare administrators for input on the best design for the initial implementation strategy. We will then implement that strategy (Aim 2), observing the degree to which the monitors plus support are used and how they affect clinic flow. We will observe adherence to see how well the monitors are working, but not in manner that comments on effectiveness.

We could alternatively do an effectiveness-implementation trial (type 3) by comparing the effects of the monitor plus support vs. standard care on adherence and viral suppression; however, numerous other studies have already shown effectiveness (at least for adherence; viral suppression would require a very large study). The key question at this point is to understand the facilitators and barriers to implementation. 

Additionally, in the planned second iteration of the implementation phase (Aim 2), we could compare different implementation strategies (i.e., as learned from the first iteration). However, given the small scope and exploratory nature of the K24 award, an iterative pilot approach seems most appropriate.


2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? 

Response: Yes, we are using a sequential mixed methods design. The first phase is qualitative to inform the implementation strategy. We will then collect quantitative metrics about the implementation. We will subsequently do further qualitative work to explain our quantitative findings. 
";s:5:"xhtml";s:2171:"Haberer: Assignment #4 <br /><br />1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />Response: My proposed study design is observational. In this K24 pilot study (i.e., small scope), we want to learn about ways to implement the real-time adherence monitor plus support in routine clinical practice, exploring facilitators and barriers. In the initial qualitative phase of the study (Aim 1), we are asking patients and clinicians/healthcare administrators for input on the best design for the initial implementation strategy. We will then implement that strategy (Aim 2), observing the degree to which the monitors plus support are used and how they affect clinic flow. We will observe adherence to see how well the monitors are working, but not in manner that comments on effectiveness.<br /><br />We could alternatively do an effectiveness-implementation trial (type 3) by comparing the effects of the monitor plus support vs. standard care on adherence and viral suppression; however, numerous other studies have already shown effectiveness (at least for adherence; viral suppression would require a very large study). The key question at this point is to understand the facilitators and barriers to implementation. <br /><br />Additionally, in the planned second iteration of the implementation phase (Aim 2), we could compare different implementation strategies (i.e., as learned from the first iteration). However, given the small scope and exploratory nature of the K24 award, an iterative pilot approach seems most appropriate.<br /><br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? <br /><br />Response: Yes, we are using a sequential mixed methods design. The first phase is qualitative to inform the implementation strategy. We will then collect quantitative metrics about the implementation. We will subsequently do further qualitative work to explain our quantitative findings.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"fe4e3c23a3d114b134a5ca180cb82f7b";}s:4:"show";b:1;s:3:"cid";s:32:"5a3291ce70ebfaaa173e847a6e577c8b";}s:32:"a3784ff031a96dc1da5183bd9395eeae";a:8:{s:4:"user";a:5:{s:2:"id";s:3:"dli";s:4:"name";s:9:"Dennis Li";s:4:"mail";s:23:"dennis@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538745701;}s:3:"raw";s:1814:"Li - Assignment #4:
1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?
2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?

The study design for my proposed study is observational/cross-sectional as I am only assessing service providers’ awareness of and attitudes toward eHealth HIV interventions and their perceptions and/or experiences around barriers and facilitators of implementation. This study design is appropriate because I am only interested in the barriers around adoption and implementation of eHealth as a way to inform the second study aim, which is the selection of appropriate strategies. My procedures—the ERIC protocol—utilizes an expert panel to evaluate the strategies presented. Given their expertise, I see very limited utility in following up with the panel and having them repeat the evaluation to get a second time (longitudinal) measurement; it is unlikely their recommendations would change in such a short amount of time. Additionally, the resources needed to follow the panel over time likely outweighs the benefit of a second wave of data.

My study is certainly a mixed methods study, though. The quantitative portion builds upon the themes outlined in the qualitative portion and is the major focus of the study. Per the taxonomy presented by Palinkas et al. (which overlaps with the Creswell et al. “Best Practices” report and Aarons’ lecture), my study would most likely have a qual  QUAN structure with a development function and a connected process (Aim 1 findings are used to construct measures in Aim 2).";s:5:"xhtml";s:1844:"Li - Assignment #4:<br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br /><br />The study design for my proposed study is observational/cross-sectional as I am only assessing service providers’ awareness of and attitudes toward eHealth HIV interventions and their perceptions and/or experiences around barriers and facilitators of implementation. This study design is appropriate because I am only interested in the barriers around adoption and implementation of eHealth as a way to inform the second study aim, which is the selection of appropriate strategies. My procedures—the ERIC protocol—utilizes an expert panel to evaluate the strategies presented. Given their expertise, I see very limited utility in following up with the panel and having them repeat the evaluation to get a second time (longitudinal) measurement; it is unlikely their recommendations would change in such a short amount of time. Additionally, the resources needed to follow the panel over time likely outweighs the benefit of a second wave of data.<br /><br />My study is certainly a mixed methods study, though. The quantitative portion builds upon the themes outlined in the qualitative portion and is the major focus of the study. Per the taxonomy presented by Palinkas et al. (which overlaps with the Creswell et al. “Best Practices” report and Aarons’ lecture), my study would most likely have a qual  QUAN structure with a development function and a connected process (Aim 1 findings are used to construct measures in Aim 2).";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"9cede267cef6e3a50603a2651070e544";}s:4:"show";b:1;s:3:"cid";s:32:"a3784ff031a96dc1da5183bd9395eeae";}s:32:"e7355e18f3c22e9b56e4dbc6b4cbfeeb";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cpsaros";s:4:"name";s:16:"Christina Psaros";s:4:"mail";s:23:"CPSAROS@mgh.harvard.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538751124;}s:3:"raw";s:2377:"Psaros – Assignment #4
1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

My response to this is…

We propose a mixed-methods pilot demonstration study to understand how to best
implement PrEP services for AA women into FQHC clinics in Alabama using a multidisciplinary collaboration of the research team, and HIV-care providers, as well as patients. In our original proposal, we planned to collect qualitative data from AA women at risk for HIV and providers in the FQHC setting in order to understand how to best implement an existing PrEP delivery protocol (phase 1) and then deliver that protocol (phase II). Based on feedback from the original submission, we now plan to (1) evaluate perceptions of HIV risk and preferences around effective communication about HIV risk and PrEP among AA women and providers by conducting formative research among AA women at-risk for HIV infection and primary care providers to inform the development of a PrEP delivery intervention at two FQHCs in rural Alabama; (2) systematically develop an intervention to increase effective patient-provider communication and prescribing of PrEP at two FQHCs in rural Alabama, and (3) assess the feasibility, acceptability and preliminary efficacy of the PrEP delivery intervention to identify women who are at-risk for HIV and promote prescription/uptake of PrEP among AA women and their providers by implementing an open-label non-randomized pilot trial. 

2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?

My response to this is:

We will used mixed methods. In phase 1, we will conduct formative work, which will involve individual key informant interviews with HIV-uninfected AA women who are potential PrEP candidates based on risk factor assessment and various health care providers (i.e. physicians, physician assistants and nurse practitioners, nurses, medical assistants, etc.). In phase 2, the qualitative data will be used to adapt or develop an intervention to increase PrEP uptake. We will also consider post study qualitative exit interviews to generate additional qualitative data on the experience of the intervention. ";s:5:"xhtml";s:2436:"Psaros – Assignment #4<br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />My response to this is…<br /><br />We propose a mixed-methods pilot demonstration study to understand how to best<br />implement PrEP services for AA women into FQHC clinics in Alabama using a multidisciplinary collaboration of the research team, and HIV-care providers, as well as patients. In our original proposal, we planned to collect qualitative data from AA women at risk for HIV and providers in the FQHC setting in order to understand how to best implement an existing PrEP delivery protocol (phase 1) and then deliver that protocol (phase II). Based on feedback from the original submission, we now plan to (1) evaluate perceptions of HIV risk and preferences around effective communication about HIV risk and PrEP among AA women and providers by conducting formative research among AA women at-risk for HIV infection and primary care providers to inform the development of a PrEP delivery intervention at two FQHCs in rural Alabama; (2) systematically develop an intervention to increase effective patient-provider communication and prescribing of PrEP at two FQHCs in rural Alabama, and (3) assess the feasibility, acceptability and preliminary efficacy of the PrEP delivery intervention to identify women who are at-risk for HIV and promote prescription/uptake of PrEP among AA women and their providers by implementing an open-label non-randomized pilot trial. <br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br /><br />My response to this is:<br /><br />We will used mixed methods. In phase 1, we will conduct formative work, which will involve individual key informant interviews with HIV-uninfected AA women who are potential PrEP candidates based on risk factor assessment and various health care providers (i.e. physicians, physician assistants and nurse practitioners, nurses, medical assistants, etc.). In phase 2, the qualitative data will be used to adapt or develop an intervention to increase PrEP uptake. We will also consider post study qualitative exit interviews to generate additional qualitative data on the experience of the intervention.";s:6:"parent";N;s:7:"replies";a:2:{i:0;s:32:"c97fd51f12909b5af6168e683fd5cb30";i:1;s:32:"0cc890bf6c119a28a7c96af6830b398e";}s:4:"show";b:1;s:3:"cid";s:32:"e7355e18f3c22e9b56e4dbc6b4cbfeeb";}s:32:"05fde554f7240e8619a1126c04613016";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"mmarzan";s:4:"name";s:24:"Melissa Marzan-Rodriguez";s:4:"mail";s:22:"melissa.marzan@upr.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538760145;}s:3:"raw";s:1181:"Marzan-Rodriguez- Assignment #4:
1.What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

An exploratory qualitative approach. The semi-structured in depth interview technique will allow the possibility to explore feasibility of the eHealth implementation process. Is the appropriate approach to contextualized facilitators and barriers of the implementation process.

2.Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?
Is expected to only apply the qualitative techniques, but the interviews can inform next steps in the parent grant such as: development of a satisfaction survey. Also, another way to integrate quantitative method is to do a secondary data analysis (SDA) of the baseline survey that English speakers already did. This SDA can allow to: (1) inform the qualitative interview guide development process or/and (2) strengthen categories identified from the qualitative interviews that also were evaluate in the SDA. 
";s:5:"xhtml";s:1209:"Marzan-Rodriguez- Assignment #4:<br />1.What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />An exploratory qualitative approach. The semi-structured in depth interview technique will allow the possibility to explore feasibility of the eHealth implementation process. Is the appropriate approach to contextualized facilitators and barriers of the implementation process.<br /><br />2.Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br />Is expected to only apply the qualitative techniques, but the interviews can inform next steps in the parent grant such as: development of a satisfaction survey. Also, another way to integrate quantitative method is to do a secondary data analysis (SDA) of the baseline survey that English speakers already did. This SDA can allow to: (1) inform the qualitative interview guide development process or/and (2) strengthen categories identified from the qualitative interviews that also were evaluate in the SDA.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"126d0db99e3bb7ad1b93bb6e5ad59c78";}s:4:"show";b:1;s:3:"cid";s:32:"05fde554f7240e8619a1126c04613016";}s:32:"c97fd51f12909b5af6168e683fd5cb30";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538765721;}s:3:"raw";s:439:"Hi Christina,

Thanks for completing this assignment!  You might want to talk a bit more about the quantative methods you will use and whether they will be used concurrently (which is the way it sounds) or sequentially in phase 1.  How will your phase two open label non-randomized trial be conducted -- will there be a control group?  The more granular you can be, the better we (and the funder! can assess your project.  

Thanks,

Linda";s:5:"xhtml";s:469:"Hi Christina,<br /><br />Thanks for completing this assignment!  You might want to talk a bit more about the quantative methods you will use and whether they will be used concurrently (which is the way it sounds) or sequentially in phase 1.  How will your phase two open label non-randomized trial be conducted -- will there be a control group?  The more granular you can be, the better we (and the funder! can assess your project.  <br /><br />Thanks,<br /><br />Linda";s:6:"parent";s:32:"e7355e18f3c22e9b56e4dbc6b4cbfeeb";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"c97fd51f12909b5af6168e683fd5cb30";}s:32:"f6eb4b3e904df835a67ff3215b8294cb";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"kbeima";s:4:"name";s:19:"Kristin Beima-Sofie";s:4:"mail";s:13:"beimak@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539095778;}s:3:"raw";s:3445:"Beima-Sofie - Assignment #4:

#1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

As I’ve mentioned previously, the additional implementation science objectives I’m proposing to embed in the ongoing ATTACH study will shift the study to be an effectiveness-implementation hybrid design (Curran article). I believe that it falls under the hybrid design 1 category, primarily measuring effectiveness while also observing/measuring implementation outcomes. The original ATTACH study trial is a cluster RCT. Related to Aarons description of the PRECIS tool and pragmatic vs. explanatory studies, it is primarily a pragmatic study, with the goal of evaluating effectiveness (rather than efficacy). We are using routine clinic settings, enrolling all adolescents who attend the clinic, training all HCWs at each intervention facility, and not providing study staff to implement any of the intervention components. While a stepped wedge design would have allowed us to concurrently account for environmental/political/etc changes over time, the length of time required to observe an outcome (transition to adult care or disclosure of HIV diagnosis to an adolescent) was too long, making it impractical. I’m not sure why, but we never considered ITS for design, although that might have been another way to structure the trial. For the cluster RCT, we are still discussing how best to assign clinics to intervention and control arms. Revisiting that fact that randomization does not work well to balance groups for small numbers, which we will have here, is important to consider. We are considering different weighting schemes that might allow us to control for baseline differences in clinics (such as size, model of care, and region). 

#2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? 

Yes, I will be using mixed methods for the study. I think the way the qualitative methods is used will depend on the specific aim, since all aims currently propose mixed methods data collection and integration. For Aim 1, where I’m hoping to describe adaptation and fidelity of the intervention during initial implementation, I’m proposing collecting both qualitative and quantitative data at the same time (concurrent) and will use the information to evaluate convergence between the 2 data sources. I will be conducting interviews with HCWs from the sites at the beginning (1 month after implementation) and again at 6 months after implementation. I also plan to administer surveys at the same time, asking about the same questions, but going deeper into decision-making rationale and processes through the IDIs. Some of the information collected in Aim 1, especially around fidelity, might also be used to inform future question development for testing fidelity again at the end of the implementation period (Aim 2). The second aim will involve collecting some information early (organizational readiness for change) that will be combined with information collected at the end. The third aim, which proposes to use qualitative comparative analysis, would be an example of the deeper dive (expansion) into the data than can be achieved through the use of one data collection method alone. 
";s:5:"xhtml";s:3483:"Beima-Sofie - Assignment #4:<br /><br />#1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />As I’ve mentioned previously, the additional implementation science objectives I’m proposing to embed in the ongoing ATTACH study will shift the study to be an effectiveness-implementation hybrid design (Curran article). I believe that it falls under the hybrid design 1 category, primarily measuring effectiveness while also observing/measuring implementation outcomes. The original ATTACH study trial is a cluster RCT. Related to Aarons description of the PRECIS tool and pragmatic vs. explanatory studies, it is primarily a pragmatic study, with the goal of evaluating effectiveness (rather than efficacy). We are using routine clinic settings, enrolling all adolescents who attend the clinic, training all HCWs at each intervention facility, and not providing study staff to implement any of the intervention components. While a stepped wedge design would have allowed us to concurrently account for environmental/political/etc changes over time, the length of time required to observe an outcome (transition to adult care or disclosure of HIV diagnosis to an adolescent) was too long, making it impractical. I’m not sure why, but we never considered ITS for design, although that might have been another way to structure the trial. For the cluster RCT, we are still discussing how best to assign clinics to intervention and control arms. Revisiting that fact that randomization does not work well to balance groups for small numbers, which we will have here, is important to consider. We are considering different weighting schemes that might allow us to control for baseline differences in clinics (such as size, model of care, and region). <br /><br />#2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? <br /><br />Yes, I will be using mixed methods for the study. I think the way the qualitative methods is used will depend on the specific aim, since all aims currently propose mixed methods data collection and integration. For Aim 1, where I’m hoping to describe adaptation and fidelity of the intervention during initial implementation, I’m proposing collecting both qualitative and quantitative data at the same time (concurrent) and will use the information to evaluate convergence between the 2 data sources. I will be conducting interviews with HCWs from the sites at the beginning (1 month after implementation) and again at 6 months after implementation. I also plan to administer surveys at the same time, asking about the same questions, but going deeper into decision-making rationale and processes through the IDIs. Some of the information collected in Aim 1, especially around fidelity, might also be used to inform future question development for testing fidelity again at the end of the implementation period (Aim 2). The second aim will involve collecting some information early (organizational readiness for change) that will be combined with information collected at the end. The third aim, which proposes to use qualitative comparative analysis, would be an example of the deeper dive (expansion) into the data than can be achieved through the use of one data collection method alone.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"f6eb4b3e904df835a67ff3215b8294cb";}s:32:"2d396b0c961851a003d223413f72d442";a:8:{s:4:"user";a:5:{s:2:"id";s:3:"dli";s:4:"name";s:9:"Dennis Li";s:4:"mail";s:23:"dennis@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539104032;}s:3:"raw";s:1272:"Chris, you bring up a really good point about providers' intentions to get into the eHealth realm. I hadn't thought of inquiring about that as part of the qualitative interview process (which is primarily focused on barriers and facilitators), but now that you mention it, I think it would be a wasted opportunity not to collect some measure of intention. Along those lines, I can also see utility in measuring (hypothetical) readiness for implementation, using something like the Organizational Readiness for Implementing Change, which has decent psychometrics, or the Checklist to Assess Organizational Readiness for EIP Implementation, which is simpler but has gone through less psychometric testing. In fact, moving into the second aim, I could possibly see different implementation strategies being prioritized by providers at different stages of readiness, which would be important to examine.

Providers' concerns or negative attitudes toward implementing eHealth programs would be important to capture. In my mind, I included that with inner setting barriers to implementation / acceptability of the intervention by those delivering it, but to your point, I think stating that I'll also be measuring those negative attitudes is important.

Thanks for the feedback!";s:5:"xhtml";s:1312:"Chris, you bring up a really good point about providers&#039; intentions to get into the eHealth realm. I hadn&#039;t thought of inquiring about that as part of the qualitative interview process (which is primarily focused on barriers and facilitators), but now that you mention it, I think it would be a wasted opportunity not to collect some measure of intention. Along those lines, I can also see utility in measuring (hypothetical) readiness for implementation, using something like the Organizational Readiness for Implementing Change, which has decent psychometrics, or the Checklist to Assess Organizational Readiness for EIP Implementation, which is simpler but has gone through less psychometric testing. In fact, moving into the second aim, I could possibly see different implementation strategies being prioritized by providers at different stages of readiness, which would be important to examine.<br /><br />Providers&#039; concerns or negative attitudes toward implementing eHealth programs would be important to capture. In my mind, I included that with inner setting barriers to implementation / acceptability of the intervention by those delivering it, but to your point, I think stating that I&#039;ll also be measuring those negative attitudes is important.<br /><br />Thanks for the feedback!";s:6:"parent";s:32:"fa779879dfd49807f80ae702fbd4bc40";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"2d396b0c961851a003d223413f72d442";}s:32:"fe4e3c23a3d114b134a5ca180cb82f7b";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539190248;}s:3:"raw";s:495:"First, thanks for your clarifying responses to my last questions... much appreciated.

This design/approach makes sense, esp within the limits of the K24.

On a minor point, because NIH is evolving with respect to clinical trial definitions, I suspect that if such a pilot would be submitted to NIH, that it would be classified as a clinical trial, and not observational -- because the implementation of the monitors would be considered an intervention (a change from standard care practice). 

";s:5:"xhtml";s:512:"First, thanks for your clarifying responses to my last questions... much appreciated.<br /><br />This design/approach makes sense, esp within the limits of the K24.<br /><br />On a minor point, because NIH is evolving with respect to clinical trial definitions, I suspect that if such a pilot would be submitted to NIH, that it would be classified as a clinical trial, and not observational -- because the implementation of the monitors would be considered an intervention (a change from standard care practice).";s:6:"parent";s:32:"5a3291ce70ebfaaa173e847a6e577c8b";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"fe4e3c23a3d114b134a5ca180cb82f7b";}s:32:"4df4ef471ea07a06db4cfc992c91c689";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"lkupfer";s:4:"name";s:12:"Linda Kupfer";s:4:"mail";s:20:"linda.kupfer@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539261153;}s:3:"raw";s:328:"Hi Anjuli,

Your study designs seem extremely well thought out and the options you elucidate give you a great deal of flexibility.  I am sure that you are working with statisticians who can advise you on the numbers you need for each option to get the power you will need to get useful results.  Sounds very good.

Best,

Linda ";s:5:"xhtml";s:357:"Hi Anjuli,<br /><br />Your study designs seem extremely well thought out and the options you elucidate give you a great deal of flexibility.  I am sure that you are working with statisticians who can advise you on the numbers you need for each option to get the power you will need to get useful results.  Sounds very good.<br /><br />Best,<br /><br />Linda";s:6:"parent";s:32:"8bc0b38d1adb30ae94c834a70e9c03a6";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"4df4ef471ea07a06db4cfc992c91c689";}s:32:"9cede267cef6e3a50603a2651070e544";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539275654;}s:3:"raw";s:1046:"As we have discussed before, this does seem like a reasonable, straight-forward approach to this line of work. You know, as you mention the pros/cons of longitudinal, I tend to agree with the merits of the cross-sectional approach to get started. But, perhaps it would be useful to think some about the sustainability of implementation of an eHealth intervention -- given what we know about how these service settings can evolve. And sometimes rapidly. What i am getting at is that you mention a few types of settings to solicit stakeholder input -- health departments, clinics, and community-based organizations (CBOs). For example, we know that leadership and staff turnover (and sources of funding) could be perhaps least predictable in CBOS, followed by health departments and then clinics. As you move forward, it seems important to sort through how best to "institutionalize" the support/integration of eHealth approaches, in whatever means possible. So the process isn't reliant on only one (or a few) individuals/champions in the setting.";s:5:"xhtml";s:1061:"As we have discussed before, this does seem like a reasonable, straight-forward approach to this line of work. You know, as you mention the pros/cons of longitudinal, I tend to agree with the merits of the cross-sectional approach to get started. But, perhaps it would be useful to think some about the sustainability of implementation of an eHealth intervention -- given what we know about how these service settings can evolve. And sometimes rapidly. What i am getting at is that you mention a few types of settings to solicit stakeholder input -- health departments, clinics, and community-based organizations (CBOs). For example, we know that leadership and staff turnover (and sources of funding) could be perhaps least predictable in CBOS, followed by health departments and then clinics. As you move forward, it seems important to sort through how best to &quot;institutionalize&quot; the support/integration of eHealth approaches, in whatever means possible. So the process isn&#039;t reliant on only one (or a few) individuals/champions in the setting.";s:6:"parent";s:32:"a3784ff031a96dc1da5183bd9395eeae";s:7:"replies";a:1:{i:0;s:32:"cf575cd1c3cf5ac3a1ef99f450a2948e";}s:4:"show";b:1;s:3:"cid";s:32:"9cede267cef6e3a50603a2651070e544";}s:32:"cf575cd1c3cf5ac3a1ef99f450a2948e";a:8:{s:4:"user";a:5:{s:2:"id";s:3:"dli";s:4:"name";s:9:"Dennis Li";s:4:"mail";s:23:"dennis@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539276255;}s:3:"raw";s:681:"Once again, Chris, you've given me additional helpful perspective. Just like in my response back to you on Assignment 3, your comment makes me think about the need for different strategies across different time points. Before, it was about different stages of readiness, but here, it does make sense to think about how different implementation strategies would be helpful over time to achieve the institutionalization of eHealth programs (and on that point, also the de-implementation or adaptation process as technology grows old). For the proposed project, perhaps it's just worth exploring qualitatively at first and maybe a small subsection of the quantitative portion as well.";s:5:"xhtml";s:691:"Once again, Chris, you&#039;ve given me additional helpful perspective. Just like in my response back to you on Assignment 3, your comment makes me think about the need for different strategies across different time points. Before, it was about different stages of readiness, but here, it does make sense to think about how different implementation strategies would be helpful over time to achieve the institutionalization of eHealth programs (and on that point, also the de-implementation or adaptation process as technology grows old). For the proposed project, perhaps it&#039;s just worth exploring qualitatively at first and maybe a small subsection of the quantitative portion as well.";s:6:"parent";s:32:"9cede267cef6e3a50603a2651070e544";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"cf575cd1c3cf5ac3a1ef99f450a2948e";}s:32:"0cc890bf6c119a28a7c96af6830b398e";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539277153;}s:3:"raw";s:707:"I don't have significant input on your design responses, they seem clear and sound. However, i was musing about the prospects, in general, of trying to initiate and evaluate a new PrEP "intervention" in one or more of these FQHCs, and i was curious about a couple of things, concretely. Would it be you who did the various trainings and interventions for the staff? (e.g., to enhance communication). And will it be you (or anyone?) who is available in any way, to trouble-shoot any problems that occur as the site(s) begin to deliver PrEP? What about any medical issues that arise? Are there plans, in any fashion, to connect these relatively inexperienced PrEP providers with a more experienced consultant?";s:5:"xhtml";s:722:"I don&#039;t have significant input on your design responses, they seem clear and sound. However, i was musing about the prospects, in general, of trying to initiate and evaluate a new PrEP &quot;intervention&quot; in one or more of these FQHCs, and i was curious about a couple of things, concretely. Would it be you who did the various trainings and interventions for the staff? (e.g., to enhance communication). And will it be you (or anyone?) who is available in any way, to trouble-shoot any problems that occur as the site(s) begin to deliver PrEP? What about any medical issues that arise? Are there plans, in any fashion, to connect these relatively inexperienced PrEP providers with a more experienced consultant?";s:6:"parent";s:32:"e7355e18f3c22e9b56e4dbc6b4cbfeeb";s:7:"replies";a:1:{i:0;s:32:"2b94690f77cf7517af377756cf51d4fa";}s:4:"show";b:1;s:3:"cid";s:32:"0cc890bf6c119a28a7c96af6830b398e";}s:32:"126d0db99e3bb7ad1b93bb6e5ad59c78";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539278079;}s:3:"raw";s:353:"It will be good to know more specifics about your planned methods, soon. Have you developed an interview guide (yet) for the potential patients and providers (a separate one?). Is there anyone doing work in this area that may have an existing interview guide for you to use as a model, if it was used effectively, to not re-invent that particular wheel?";s:5:"xhtml";s:353:"It will be good to know more specifics about your planned methods, soon. Have you developed an interview guide (yet) for the potential patients and providers (a separate one?). Is there anyone doing work in this area that may have an existing interview guide for you to use as a model, if it was used effectively, to not re-invent that particular wheel?";s:6:"parent";s:32:"05fde554f7240e8619a1126c04613016";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"126d0db99e3bb7ad1b93bb6e5ad59c78";}s:32:"38d7122e7ba0e6bc04c9e6e1c7e1b921";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539279653;}s:3:"raw";s:292:"I had not thought of the interrupted time series as an alternative for you. It does seem worthwhile to talk about with your team and mentors, given what may be challenges in identifying the most critical factors to consider in randomization... and the small sample size you are talking about.";s:5:"xhtml";s:292:"I had not thought of the interrupted time series as an alternative for you. It does seem worthwhile to talk about with your team and mentors, given what may be challenges in identifying the most critical factors to consider in randomization... and the small sample size you are talking about.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"38d7122e7ba0e6bc04c9e6e1c7e1b921";}s:32:"3a3fa4747f782beb770ff17ee4a714a5";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"mmarzan";s:4:"name";s:24:"Melissa Marzan-Rodriguez";s:4:"mail";s:22:"melissa.marzan@upr.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539787063;}s:3:"raw";s:2178:"Marzan-Rodriguez, Assignment #5:

1.If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

They are 3 of implementation strategies that must strengthen the specific aims. The first one is the “evaluation and iterative strategies”, the main reason to select this strategy is because is a suitable way to identify potential facilitators and barriers of the SMART program in AMSM Latino Spanish-speakers in the mainland versus those outside. The second strategy is the “adapting and tailoring to context”. This is an excellent tool to apply the field experts in each possible implementation settings (AMSM Latino Spanish-speakers in the mainland versus those in Puerto Rico and/or AMSM Latino Spanish-speakers in the mainland versus those in Latin-America). Finally, the “develop stakeholder relationships” strategy can provide the opportunity to identify current services to the study population and to building-up coalitions and/or community based organizations who can expand their services to AMSM Latino Spanish-speakers in HIV prevention area. 

2.How might you link specific implementation strategies to the context in which your work is set?

The 3 proposed implementation strategies can be linked to the context of SMART program and promote a better understanding of the implementation settings (US Mainland vs. Puerto Rico). To assure a successfully implementation process in each setting is important to use dynamic strategies who brings to the research team the tools to manage any possible barriers. In specific, because SMART program is expected to be used in different cultural contexts, the “adapting and tailoring to context strategy” will be provided the capacity to suggest or make any field modification. 

3.If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?

Not apply. 
";s:5:"xhtml";s:2236:"Marzan-Rodriguez, Assignment #5:<br /><br />1.If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />They are 3 of implementation strategies that must strengthen the specific aims. The first one is the “evaluation and iterative strategies”, the main reason to select this strategy is because is a suitable way to identify potential facilitators and barriers of the SMART program in AMSM Latino Spanish-speakers in the mainland versus those outside. The second strategy is the “adapting and tailoring to context”. This is an excellent tool to apply the field experts in each possible implementation settings (AMSM Latino Spanish-speakers in the mainland versus those in Puerto Rico and/or AMSM Latino Spanish-speakers in the mainland versus those in Latin-America). Finally, the “develop stakeholder relationships” strategy can provide the opportunity to identify current services to the study population and to building-up coalitions and/or community based organizations who can expand their services to AMSM Latino Spanish-speakers in HIV prevention area. <br /><br />2.How might you link specific implementation strategies to the context in which your work is set?<br /><br />The 3 proposed implementation strategies can be linked to the context of SMART program and promote a better understanding of the implementation settings (US Mainland vs. Puerto Rico). To assure a successfully implementation process in each setting is important to use dynamic strategies who brings to the research team the tools to manage any possible barriers. In specific, because SMART program is expected to be used in different cultural contexts, the “adapting and tailoring to context strategy” will be provided the capacity to suggest or make any field modification. <br /><br />3.If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /><br />Not apply.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"1a9f56542a2060e6bca6e714da10747f";}s:4:"show";b:1;s:3:"cid";s:32:"3a3fa4747f782beb770ff17ee4a714a5";}s:32:"1832a922715a8838db07693d0d0723e2";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cpsaros";s:4:"name";s:16:"Christina Psaros";s:4:"mail";s:23:"CPSAROS@mgh.harvard.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539967572;}s:3:"raw";s:2699:"Psaros – Assignment #5

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

My response to this is…

We plan to develop an implementation blueprint as part of our revised design; we think this is particularly important as our revised design includes additional sites with additional roles (e.g., potential PrEP candidates will be introduced to PrEP at their FQHC, but then referred to a local PrEP clinic for all PrEP services). Several units within each site will require modification to standard operating procedures, and this seems a reasonable way to improve organization and to keep each clinic, unit, and individual apprised as to progress. We will also develop educational materials for providers at the FQHCs around effective communication with women around PrEP as we are proposing a new communication tool. As part of these materials, we will consider manuals as well as training videos to complement in-person training efforts. We will focus on e-distribution and storage of these materials to facilitate access. Lastly, we will also convene an implementation resource team comprised of experts in implementation science and PrEP delivery, representatives of the clinic administration, potential PrEP candidates, providers and members of the study team to support the FQHCs in ultimate PrEP delivery, if desired as an outcome of this project. 


2.	How might you link specific implementation strategies to the context in which your work is set?

My response to this is:

Of note, much (though not all) of the study team’s expertise with respect to provider training and PrEP communication are based in Boston. Thus, as noted above, implementation strategies around the design and availability of training materials must keep this in mind. While we will conduct in person trainings, we will also develop a repository of training and educational resources housed in a centralized, e-location for ease of access. To Chris’ point from Assignment #4, we will also have to address how providers at the FQHC can easily reach expertise as challenges arise. To this end, we can consider weekly (or as needed) webinars or phone calls with study team members. These can be recorded and also saved in our educational repository. 

3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?

My response to this is: N/A
";s:5:"xhtml";s:2783:"Psaros – Assignment #5<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />My response to this is…<br /><br />We plan to develop an implementation blueprint as part of our revised design; we think this is particularly important as our revised design includes additional sites with additional roles (e.g., potential PrEP candidates will be introduced to PrEP at their FQHC, but then referred to a local PrEP clinic for all PrEP services). Several units within each site will require modification to standard operating procedures, and this seems a reasonable way to improve organization and to keep each clinic, unit, and individual apprised as to progress. We will also develop educational materials for providers at the FQHCs around effective communication with women around PrEP as we are proposing a new communication tool. As part of these materials, we will consider manuals as well as training videos to complement in-person training efforts. We will focus on e-distribution and storage of these materials to facilitate access. Lastly, we will also convene an implementation resource team comprised of experts in implementation science and PrEP delivery, representatives of the clinic administration, potential PrEP candidates, providers and members of the study team to support the FQHCs in ultimate PrEP delivery, if desired as an outcome of this project. <br /><br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />My response to this is:<br /><br />Of note, much (though not all) of the study team’s expertise with respect to provider training and PrEP communication are based in Boston. Thus, as noted above, implementation strategies around the design and availability of training materials must keep this in mind. While we will conduct in person trainings, we will also develop a repository of training and educational resources housed in a centralized, e-location for ease of access. To Chris’ point from Assignment #4, we will also have to address how providers at the FQHC can easily reach expertise as challenges arise. To this end, we can consider weekly (or as needed) webinars or phone calls with study team members. These can be recorded and also saved in our educational repository. <br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /><br />My response to this is: N/A";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"572fa4a2df87437ba9756e532f553fb2";}s:4:"show";b:1;s:3:"cid";s:32:"1832a922715a8838db07693d0d0723e2";}s:32:"2b94690f77cf7517af377756cf51d4fa";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cpsaros";s:4:"name";s:16:"Christina Psaros";s:4:"mail";s:23:"CPSAROS@mgh.harvard.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539967812;}s:3:"raw";s:675:"Thanks, Chris! I address some of this in the Week 5 assignment. I probably wasn't clear, but the FQHC providers will be responsible only for communicating with their patients about PrEP. Interested persons will then be referred to an established PrEP clinic (there are such clinics very proximate to our FQHC collaborators). That being said, our study team (which includes expertise on pro0vider education around PrEP) will conduct some in person trainings for providers, and we will have to think of means by which we are available for consultation as needs come up (e.g., either weekly calls or on an on-call basis). However, the local PrEP clinic will also have expertise.";s:5:"xhtml";s:680:"Thanks, Chris! I address some of this in the Week 5 assignment. I probably wasn&#039;t clear, but the FQHC providers will be responsible only for communicating with their patients about PrEP. Interested persons will then be referred to an established PrEP clinic (there are such clinics very proximate to our FQHC collaborators). That being said, our study team (which includes expertise on pro0vider education around PrEP) will conduct some in person trainings for providers, and we will have to think of means by which we are available for consultation as needs come up (e.g., either weekly calls or on an on-call basis). However, the local PrEP clinic will also have expertise.";s:6:"parent";s:32:"0cc890bf6c119a28a7c96af6830b398e";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"2b94690f77cf7517af377756cf51d4fa";}s:32:"d1befca0bc71b0a266ad696a1a9139e9";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"kbeima";s:4:"name";s:19:"Kristin Beima-Sofie";s:4:"mail";s:13:"beimak@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539974178;}s:3:"raw";s:3955:"Beima-Sofie - Assignment #5:

#1. If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

From the 9 categories of implementation science strategies described in the lecture (citing Waltz 2015), I believe that my proposed project incorporates the following 3 categories of IS strategies:
	Aim 1: Interactive Assistance; Adapting and Tailoring to Context 
Aim 2: Evaluation and Iterative Strategies
	
For Aim 1, during the first 6 months of intervention implementation, we will use our study staff to facilitate continuous quality improvement and track and monitor adaptations over time. Clinic staff implementing the intervention will participate in bi-monthly meetings to discuss changes in acceptability, feasibility, and fidelity (and adaptation). These meetings will be used to identify barriers to implementation, and small changes in the intervention or implementation process to overcome these barriers. I believe that these activities would encompass all of the strategies listed under interactive assistance (facilitation, technical assistance and clinical supervision) as well as most of the strategies listed in adapting and tailoring to context (tailor strategies, promote adaptability). The third strategy (use data experts) from adapting and tailoring to context would be captured prior to intervention implementation. We recently hosted a 2-day intervention adaptation and development workshop with 40 key experts in adolescent care and policy from throughout Kenya to adapt the interventions from UW and Namibian contexts. 

Aim 2, which includes evaluation of implementation at the end of the study, would focus more on the strategies from the Evaluation and Iterative Strategies category. For the evaluation in Aim 1, we will be looking at the outcomes of adoption, penetration, and fidelity (adaptation) and assessing organizational readiness for change and identifying barriers and facilitators. We will not be doing audit and feedback at this stage (that falls more in line with the proposed research in Aim 1). 

#2. How might you link specific implementation strategies to the context in which your work is set?

CFIR is being used as the guiding framework for identifying implementation determinants. From within the CFIR, I’ve identified constructs from each domain  that appear most likely to influence implementation outcomes based on qualitative data collection with patients, providers and policy makers and our intervention development workshop with key stakeholders. These specific constructs will be used as guides to help identify the strategies we are employing for each aim. For example, the cultural and healthcare system differences have been identified as potential barriers to direct translation of the interventions to the Kenyan context. Therefore, using IS strategies that employ collaborative and facilitated adaption of intervention processes and tools during early implementation is an important strategy to ensure the intervention fits within the new cultural context. Although we’ve identified that most healthcare providers want an intervention similar to what we’re proposing, some feel overburdened by already existing HIV clinical care requirements and may be more resistant to implementing new or additional processes and tools. Strategies that allow clinics to adapt the tools and processes to fit within their existing clinical infrastructure may facilitate improved readiness for change, therefore improving intervention adoption and penetration. 

#3. If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?

Not applicable. 

";s:5:"xhtml";s:4042:"Beima-Sofie - Assignment #5:<br /><br />#1. If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />From the 9 categories of implementation science strategies described in the lecture (citing Waltz 2015), I believe that my proposed project incorporates the following 3 categories of IS strategies:<br />	Aim 1: Interactive Assistance; Adapting and Tailoring to Context <br />Aim 2: Evaluation and Iterative Strategies<br />	<br />For Aim 1, during the first 6 months of intervention implementation, we will use our study staff to facilitate continuous quality improvement and track and monitor adaptations over time. Clinic staff implementing the intervention will participate in bi-monthly meetings to discuss changes in acceptability, feasibility, and fidelity (and adaptation). These meetings will be used to identify barriers to implementation, and small changes in the intervention or implementation process to overcome these barriers. I believe that these activities would encompass all of the strategies listed under interactive assistance (facilitation, technical assistance and clinical supervision) as well as most of the strategies listed in adapting and tailoring to context (tailor strategies, promote adaptability). The third strategy (use data experts) from adapting and tailoring to context would be captured prior to intervention implementation. We recently hosted a 2-day intervention adaptation and development workshop with 40 key experts in adolescent care and policy from throughout Kenya to adapt the interventions from UW and Namibian contexts. <br /><br />Aim 2, which includes evaluation of implementation at the end of the study, would focus more on the strategies from the Evaluation and Iterative Strategies category. For the evaluation in Aim 1, we will be looking at the outcomes of adoption, penetration, and fidelity (adaptation) and assessing organizational readiness for change and identifying barriers and facilitators. We will not be doing audit and feedback at this stage (that falls more in line with the proposed research in Aim 1). <br /><br />#2. How might you link specific implementation strategies to the context in which your work is set?<br /><br />CFIR is being used as the guiding framework for identifying implementation determinants. From within the CFIR, I’ve identified constructs from each domain  that appear most likely to influence implementation outcomes based on qualitative data collection with patients, providers and policy makers and our intervention development workshop with key stakeholders. These specific constructs will be used as guides to help identify the strategies we are employing for each aim. For example, the cultural and healthcare system differences have been identified as potential barriers to direct translation of the interventions to the Kenyan context. Therefore, using IS strategies that employ collaborative and facilitated adaption of intervention processes and tools during early implementation is an important strategy to ensure the intervention fits within the new cultural context. Although we’ve identified that most healthcare providers want an intervention similar to what we’re proposing, some feel overburdened by already existing HIV clinical care requirements and may be more resistant to implementing new or additional processes and tools. Strategies that allow clinics to adapt the tools and processes to fit within their existing clinical infrastructure may facilitate improved readiness for change, therefore improving intervention adoption and penetration. <br /><br />#3. If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /><br />Not applicable.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"19aa704e667a84664edffbac8f4441ff";}s:4:"show";b:1;s:3:"cid";s:32:"d1befca0bc71b0a266ad696a1a9139e9";}s:32:"2ec33d09b7935679e63ecc72018c078b";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"jhaberer";s:4:"name";s:15:"Jessica Haberer";s:4:"mail";s:21:"JHABERER@PARTNERS.ORG";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539975475;}s:3:"raw";s:2257:"Jessica Haberer- Assignment #5

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

My response is: My research is utilizing two primary implementation strategies. First, in my initial qualitative work, I am adapting and tailoring an evidence-based intervention (real-time electronic adherence monitoring and associated SMS/tailored counseling) to the Ugandan context. I am interviewing various stakeholders or potential “actors” (ranging from the Ministry of Health through patients) to understand multiple relevant aspects of the context that could influence uptake. For example, we are assessing priorities, desirable outcomes, etc.

I am also using an evaluation/iterative strategy in the second phase of the study. I am specifically focusing on readiness for using the electronic adherence monitors and associated SMS/tailored counseling, as well as identifying barriers and facilitators for implementation. I will be using metrics (as previously noted) in an initial implementation that I can then improve upon in a second iteration.


2.	How might you link specific implementation strategies to the context in which your work is set?

My response is: We are working in just one site given the limited scope of this exploratory, pilot study; however, by interviewing stakeholders at multiple levels of government, we are trying to link our strategy with the larger Ugandan context. Future studies could look at other sites (e.g., other resource-limited settings with high HIV burden). 


3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? 

My response is: As noted, I will only be studying one site. However, I am looking for variation in the types of patients using the intervention (i.e., early versus late in their use of ART, rural versus urban residence). In a fully powered study, I would like to study implementation in rural versus urban clinics as well. 

";s:5:"xhtml";s:2334:"Jessica Haberer- Assignment #5<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />My response is: My research is utilizing two primary implementation strategies. First, in my initial qualitative work, I am adapting and tailoring an evidence-based intervention (real-time electronic adherence monitoring and associated SMS/tailored counseling) to the Ugandan context. I am interviewing various stakeholders or potential “actors” (ranging from the Ministry of Health through patients) to understand multiple relevant aspects of the context that could influence uptake. For example, we are assessing priorities, desirable outcomes, etc.<br /><br />I am also using an evaluation/iterative strategy in the second phase of the study. I am specifically focusing on readiness for using the electronic adherence monitors and associated SMS/tailored counseling, as well as identifying barriers and facilitators for implementation. I will be using metrics (as previously noted) in an initial implementation that I can then improve upon in a second iteration.<br /><br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />My response is: We are working in just one site given the limited scope of this exploratory, pilot study; however, by interviewing stakeholders at multiple levels of government, we are trying to link our strategy with the larger Ugandan context. Future studies could look at other sites (e.g., other resource-limited settings with high HIV burden). <br /><br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? <br /><br />My response is: As noted, I will only be studying one site. However, I am looking for variation in the types of patients using the intervention (i.e., early versus late in their use of ART, rural versus urban residence). In a fully powered study, I would like to study implementation in rural versus urban clinics as well.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"a78a2449dc2a0413f04e2db23ddb53bc";}s:4:"show";b:1;s:3:"cid";s:32:"2ec33d09b7935679e63ecc72018c078b";}s:32:"439ca4476ac50e12c78a0aaebee7a120";a:8:{s:4:"user";a:5:{s:2:"id";s:3:"dli";s:4:"name";s:9:"Dennis Li";s:4:"mail";s:23:"dennis@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539996835;}s:3:"raw";s:2652:"Li - Assignment #5
1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?
2.	How might you link specific implementation strategies to the context in which your work is set?

The questions posed in this assignment are actually the research questions that I play to answer with my proposed study. In Aim 1, I will be identifying barriers and facilitators of adopting, implementing, and sustaining eHealth programs for HIV, and in Aim 2, I will be systematically identifying and selecting the most appropriate implementation strategies to address those barriers and facilitators. For the second aim, I proposed to adapt the ERIC protocol used by Powell et al. (2015), which was a QUALQUANT mixed-methods approach that included a Delphi process and concept mapping exercise. From that approach, I plan to get quantitative rankings of which of the 73 strategies are best suited to help implement eHealth HIV EBIs. As suggested by Dr. Gordon in response to an earlier assignment, I can try to capture whether or not those rankings vary by stage of implementation or by readiness.

Based off of some experiences I have had with HIV service settings, my understanding of the HIV prevention funding landscape, and my knowledge of the technical requirements of some eHealth interventions, I can imagine some strategies that are bound to emerge include centralizing technical assistance (because the technology might be hosted centrally); developing academic partnerships (because eHealth EBIs are primarily developed in academic settings); funds/contracts for the clinical innovation (which is how the CDC currently funds in-person EBIs and HIV testing and counseling); change physical structure and equipment (if some service providers don’t have the proper technology); and promote adaptability (because CBOs like to have customizability in their interventions).

In reading over the methods to improve selection and tailoring of strategies (Powell et al., 2015), I also wonder if there is utility in comparing two selection methods. A large part of my graduate training was specifically in the use of Intervention Mapping, and I’ve used it enough in various research projects that I would consider myself fairly well-versed in its application. It would not be difficult for me to use IM to match up strategies to the barriers, with the input of some other experts and stakeholders. I’d be curious if the mentors think it would add anything to my proposal to say I want to compare methods, or if it’s something I should just do on the side (or not do).
";s:5:"xhtml";s:2691:"Li - Assignment #5<br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />The questions posed in this assignment are actually the research questions that I play to answer with my proposed study. In Aim 1, I will be identifying barriers and facilitators of adopting, implementing, and sustaining eHealth programs for HIV, and in Aim 2, I will be systematically identifying and selecting the most appropriate implementation strategies to address those barriers and facilitators. For the second aim, I proposed to adapt the ERIC protocol used by Powell et al. (2015), which was a QUALQUANT mixed-methods approach that included a Delphi process and concept mapping exercise. From that approach, I plan to get quantitative rankings of which of the 73 strategies are best suited to help implement eHealth HIV EBIs. As suggested by Dr. Gordon in response to an earlier assignment, I can try to capture whether or not those rankings vary by stage of implementation or by readiness.<br /><br />Based off of some experiences I have had with HIV service settings, my understanding of the HIV prevention funding landscape, and my knowledge of the technical requirements of some eHealth interventions, I can imagine some strategies that are bound to emerge include centralizing technical assistance (because the technology might be hosted centrally); developing academic partnerships (because eHealth EBIs are primarily developed in academic settings); funds/contracts for the clinical innovation (which is how the CDC currently funds in-person EBIs and HIV testing and counseling); change physical structure and equipment (if some service providers don’t have the proper technology); and promote adaptability (because CBOs like to have customizability in their interventions).<br /><br />In reading over the methods to improve selection and tailoring of strategies (Powell et al., 2015), I also wonder if there is utility in comparing two selection methods. A large part of my graduate training was specifically in the use of Intervention Mapping, and I’ve used it enough in various research projects that I would consider myself fairly well-versed in its application. It would not be difficult for me to use IM to match up strategies to the barriers, with the input of some other experts and stakeholders. I’d be curious if the mentors think it would add anything to my proposal to say I want to compare methods, or if it’s something I should just do on the side (or not do).";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"369338ed69498ca9fd9e7d9f9d36c30d";}s:4:"show";b:1;s:3:"cid";s:32:"439ca4476ac50e12c78a0aaebee7a120";}s:32:"c5bace34a60aca023f0c99d5bee26644";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"awagner";s:4:"name";s:13:"Anjuli Wagner";s:4:"mail";s:14:"anjuliw@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540325538;}s:3:"raw";s:2512:"Wagner -- Assignment #5 

My sincere apologies for the late submission. 

1. If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

In this project, we will be testing strategies in Aim 2 that are informed by Aim 1. In Aim 1, we will identify determinants of implementation using CFIR (assessing breadth and depth during qualitative work, and frequency during quantitative questionnaires) and also ask health care workers to describe strategies that they have tried organically in the process of trying to deliver PrEP during pregnancy in their clinics. Based on the identified determinants and the preliminary success of organically tried strategies, we will attempt to map specific determinants to specific strategies, such as those outlined in the Powell IS 2015 ERIC paper. 

However, based on experience to date, we have a few strategies in mind that we anticipate may be relevant. These include task shifting and reorganizing certain responsibilities. These include 1) self-administered PrEP risk assessment, 2) self-administered oral HIV test, 3) risk assessment performed by the HIV testing provider instead of antenatal care nurse, and 4) group-based PrEP counseling. I am working now to see how to incorporate these possible ISs into the grant without undermining the scientific process of using Aim 1 to inform Aim 2.


2. How might you link specific implementation strategies to the context in which your work is set?

I predict that the strategies that could be most relevant in a Kenyan public health system will be different from the types of strategies that might be most relevant in a US-based system. I expect that the determinants that we identify in Aim 1 will identify more issues related to operationalization of PrEP delivery during pregnancy (e.g. time taken, client volumes, patient flows) and less related to individual providers not being interested in offering PrEP during pregnancy. If this is the case, I would expect that ISs that target system operations would be selected instead of ISs that target individual knowledge and behavior. 


3. If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?

Not applicable
";s:5:"xhtml";s:2601:"Wagner -- Assignment #5 <br /><br />My sincere apologies for the late submission. <br /><br />1. If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />In this project, we will be testing strategies in Aim 2 that are informed by Aim 1. In Aim 1, we will identify determinants of implementation using CFIR (assessing breadth and depth during qualitative work, and frequency during quantitative questionnaires) and also ask health care workers to describe strategies that they have tried organically in the process of trying to deliver PrEP during pregnancy in their clinics. Based on the identified determinants and the preliminary success of organically tried strategies, we will attempt to map specific determinants to specific strategies, such as those outlined in the Powell IS 2015 ERIC paper. <br /><br />However, based on experience to date, we have a few strategies in mind that we anticipate may be relevant. These include task shifting and reorganizing certain responsibilities. These include 1) self-administered PrEP risk assessment, 2) self-administered oral HIV test, 3) risk assessment performed by the HIV testing provider instead of antenatal care nurse, and 4) group-based PrEP counseling. I am working now to see how to incorporate these possible ISs into the grant without undermining the scientific process of using Aim 1 to inform Aim 2.<br /><br /><br />2. How might you link specific implementation strategies to the context in which your work is set?<br /><br />I predict that the strategies that could be most relevant in a Kenyan public health system will be different from the types of strategies that might be most relevant in a US-based system. I expect that the determinants that we identify in Aim 1 will identify more issues related to operationalization of PrEP delivery during pregnancy (e.g. time taken, client volumes, patient flows) and less related to individual providers not being interested in offering PrEP during pregnancy. If this is the case, I would expect that ISs that target system operations would be selected instead of ISs that target individual knowledge and behavior. <br /><br /><br />3. If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /><br />Not applicable";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"e5f7f9ae1722e9eb5d1d3ff5712908ad";}s:4:"show";b:1;s:3:"cid";s:32:"c5bace34a60aca023f0c99d5bee26644";}s:32:"1a9f56542a2060e6bca6e714da10747f";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540485396;}s:3:"raw";s:180:"Hello Melissa. Thank you for this response. I look forward to seeing more of how you will operationalize these strategies in the revised research concept that you will submit soon.";s:5:"xhtml";s:180:"Hello Melissa. Thank you for this response. I look forward to seeing more of how you will operationalize these strategies in the revised research concept that you will submit soon.";s:6:"parent";s:32:"3a3fa4747f782beb770ff17ee4a714a5";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"1a9f56542a2060e6bca6e714da10747f";}s:32:"572fa4a2df87437ba9756e532f553fb2";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540486253;}s:3:"raw";s:1222:"Christina, thanks for this response.

It is interesting to me -- as i have become more familiar with the syntax used in implementation science circles over the years, that the expert processes that have been used to try to come up with a taxonomy of implementation strategies are actually (!) useful ways to think about the practical strategies that are used to change organizational, system, and provider behaviors. In your case, as I went back to look at the Waltz et al paper about the ERIC study, the activities that you describe fall nicely into most of the strategies at your disposal, including Change of Infrastructure (SOPs), Interactive assistance, and Supporting clinicians, for example. As this study becomes more concrete and less abstract, for me one piece that will really be informed as an iterative process is the degree to which stakeholders/staff will have the capacity (and interest?) to access any "permanent" forms of assistance. With a change in practice, with all of the already competing demands, will there be time to access videos? Even e-versions of training materials. Might there be any ways for the leadership/system to incentivize use of those materials -- if needed? Just food for thought.";s:5:"xhtml";s:1242:"Christina, thanks for this response.<br /><br />It is interesting to me -- as i have become more familiar with the syntax used in implementation science circles over the years, that the expert processes that have been used to try to come up with a taxonomy of implementation strategies are actually (!) useful ways to think about the practical strategies that are used to change organizational, system, and provider behaviors. In your case, as I went back to look at the Waltz et al paper about the ERIC study, the activities that you describe fall nicely into most of the strategies at your disposal, including Change of Infrastructure (SOPs), Interactive assistance, and Supporting clinicians, for example. As this study becomes more concrete and less abstract, for me one piece that will really be informed as an iterative process is the degree to which stakeholders/staff will have the capacity (and interest?) to access any &quot;permanent&quot; forms of assistance. With a change in practice, with all of the already competing demands, will there be time to access videos? Even e-versions of training materials. Might there be any ways for the leadership/system to incentivize use of those materials -- if needed? Just food for thought.";s:6:"parent";s:32:"1832a922715a8838db07693d0d0723e2";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"572fa4a2df87437ba9756e532f553fb2";}s:32:"19aa704e667a84664edffbac8f4441ff";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540487606;}s:3:"raw";s:1064:"Thank you, Kristin.

With each passing week, and re-reading of the original concept (and communicating), I feel like I am putting the pieces together for how you will implement such a study. A couple questions occur to me based upon your responses this week.

1. I cannot recall a couple of processes regarding the adapting and tailoring. Clearly, the providers will be a huge part of this puzzle as you seek to cooperatively adapt the intervention(s) to fit the setting. In this process, will you (a) be soliciting input from the patients? With respect to their perceptions of the experience and acceptability (and effectiveness), but perhaps more importantly, also (b) reflect that input in some way back to the providers? This can be a delicate process, but often can be valuable for the providers to understand. Especially given their competing priorities, they often may not have time to inquire/appreciate how they are perceived by consumers.

2. Is the clinical supervision done through observation and feedback? Weekly? Monthly? Who does the supervision?

";s:5:"xhtml";s:1092:"Thank you, Kristin.<br /><br />With each passing week, and re-reading of the original concept (and communicating), I feel like I am putting the pieces together for how you will implement such a study. A couple questions occur to me based upon your responses this week.<br /><br />1. I cannot recall a couple of processes regarding the adapting and tailoring. Clearly, the providers will be a huge part of this puzzle as you seek to cooperatively adapt the intervention(s) to fit the setting. In this process, will you (a) be soliciting input from the patients? With respect to their perceptions of the experience and acceptability (and effectiveness), but perhaps more importantly, also (b) reflect that input in some way back to the providers? This can be a delicate process, but often can be valuable for the providers to understand. Especially given their competing priorities, they often may not have time to inquire/appreciate how they are perceived by consumers.<br /><br />2. Is the clinical supervision done through observation and feedback? Weekly? Monthly? Who does the supervision?";s:6:"parent";s:32:"d1befca0bc71b0a266ad696a1a9139e9";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"19aa704e667a84664edffbac8f4441ff";}s:32:"a78a2449dc2a0413f04e2db23ddb53bc";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540490114;}s:3:"raw";s:29:"Thank you for your responses.";s:5:"xhtml";s:29:"Thank you for your responses.";s:6:"parent";s:32:"2ec33d09b7935679e63ecc72018c078b";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"a78a2449dc2a0413f04e2db23ddb53bc";}s:32:"369338ed69498ca9fd9e7d9f9d36c30d";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540490145;}s:3:"raw";s:105:"It does seem like method comparison may be a question for inquiry down the road -- but not at this stage.";s:5:"xhtml";s:105:"It does seem like method comparison may be a question for inquiry down the road -- but not at this stage.";s:6:"parent";s:32:"439ca4476ac50e12c78a0aaebee7a120";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"369338ed69498ca9fd9e7d9f9d36c30d";}s:32:"e5f7f9ae1722e9eb5d1d3ff5712908ad";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"cgordon";s:4:"name";s:12:"Chris Gordon";s:4:"mail";s:21:"cgordon1@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540490309;}s:3:"raw";s:315:"1. It is indeed tricky to hedge your bets on strategies that are likely versus allowing Aim 1 to completely inform Aim 2. The language you use suggests that you understand this balance.
2. I might also predict that stigma will play a significant role in patient decision-making (and sustained behavior) around PrEP.";s:5:"xhtml";s:320:"1. It is indeed tricky to hedge your bets on strategies that are likely versus allowing Aim 1 to completely inform Aim 2. The language you use suggests that you understand this balance.<br />2. I might also predict that stigma will play a significant role in patient decision-making (and sustained behavior) around PrEP.";s:6:"parent";s:32:"c5bace34a60aca023f0c99d5bee26644";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"e5f7f9ae1722e9eb5d1d3ff5712908ad";}}s:11:"subscribers";N;}