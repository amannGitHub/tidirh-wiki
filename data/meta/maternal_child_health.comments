a:5:{s:5:"title";N;s:6:"status";i:1;s:6:"number";i:82;s:8:"comments";a:82:{s:32:"7600a8ddee4343dda59bf3d276f3cae1";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"rgross";s:4:"name";s:12:"Rachel Gross";s:4:"mail";s:22:"Rachel.Gross@nyumc.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1534944340;s:8:"modified";i:1534957960;}s:3:"raw";s:9032:"GROSS - Assignment #1a

1. Draft a specific aims page of your proposed study.

My response to this is... 

Title: Randomized Controlled Trial of a Primary Care-Based Child Obesity Prevention Intervention:       Effectiveness and Implementation

Childhood obesity in the United States is a serious public health concern: Obese children are likely to stay obese into adulthood and to develop early co-morbid conditions, including diabetes and cardiovascular diseases. Growing evidence has documented that pregnancy, infancy and the toddler years represent critical time periods in which the establishment of obesity-promoting behaviors significantly increase the risk of obesity throughout the life-course. Excess weight, even in infancy, greatly increases the risk of lifelong obesity. Economic and racial/ethnic disparities in child obesity may be determined by factors operating during pregnancy, infancy and early childhood. Early prevention strategies are crucial in altering early-life systems that promote the intergenerational transmission of obesity in high-risk populations.

Ecological model of child obesity: Obesity is a complex problem involving contributing factors at multiple levels, including the child, family, and societal forces. Prevention efforts focusing on multiple levels are needed to influence the feeding patterns of infants and toddlers and improve weight outcomes. During pregnancy, modifiable factors include excessive gestational weight gain, gestational diabetes, antenatal depression, and stress. During infancy and early childhood, they include infant feeding practices, non-responsive feeding styles, sleep, diet, physical activity and sedentary behaviors. 

Early obesity prevention interventions: Trials of child obesity prevention interventions during pregnancy and infancy have shown positive impacts on infant feeding and weight using community or nurse home visiting models. These programs were conducted either outside of the United States (US), or in middle-income US communities, limiting their generalizability to low-income US communities. While home visiting programs for high-risk children have resulted in improved outcomes, their current reach is limited, and alternative methods are needed to obtain population level outcomes in low-income communities. Integration of early obesity prevention into the framework of frequent primary health care visits in pregnancy and infancy may increase the ability to reach high-risk families, with potential for population-wide application. The benefits of utilizing primary care include: 1) the high frequency of and adherence to prenatal and pediatric primary care visits, which provide access to high-risk families on a population scale; 2) building on pre-existing provider relationships; 3) using existing infrastructure to lower cost; and 4) decreased need for additional transportation. 

Evidence-based program: Prior to our work, no comprehensive obesity prevention intervention studies had begun prenatally and used the existing framework of frequent prenatal and infant primary care visits to reach high-risk families for child obesity prevention. During the last 6 years, I have been a Co-Project Director for the Starting Early Trial funded by the US Department of Agriculture (AFRI # 2011-68001-30207). The Starting Early Program was a family-centered early child obesity prevention intervention specifically designed for the Hispanic community beginning in pregnancy. The main Starting Early Program components included individual prenatal and peri-partum nutrition counseling and nutrition and parenting support groups coordinated with all well child visits in the first three years of life. Program efficacy has been documented in a randomized controlled trial (n=533) which found that intervention mother-infant pairs had higher rates of exclusive breastfeeding, less early introduction of complementary foods, decreased juice consumption and greater infant physical activity, than controls receiving standard primary care. Starting Early demonstrated significant impacts on child growth, with lower mean weight for age z-scores in the intervention group compared to controls at 2 years old (0.62 vs. 0.85; p=.046). Within the intervention group, high attendance further reduced mean weight for age z-scores (0.37 vs. 0.72; p=038) and obesity (14.8% vs. 35.9%; p=.002). Although a formal cost analysis has not been performed, we estimated that program costs are about $100-150 per participant/year, which is less than 1/10th that of interdisciplinary obesity treatment programs for school-aged children. 

Gaps in the literature: While the Starting Early Program has been studied in controlled research settings and in a high risk Hispanic sample, to date no implementation science studies have focused on early child obesity prevention during pregnancy and infancy in real-world primary care-based settings serving racially and ethnically diverse populations. Studies of implementing group models of prenatal care found that the substantial paradigm shift from traditional individual visits required overcoming barriers related to difficulties with space, scheduling, recruitment, staffing, organizational motivation and leadership buy-in in order to maintain adequate fidelity to the originally designed intervention. However, there has been sparse research on contextual factors (individual and organizational factors) surrounding early child obesity prevention interventions that will move these programs from research to practice.

Specific aims: 
Given this gap in the literature, we will conduct an effectiveness-implementation hybrid trial to test the effectiveness of the Starting Early Program compared to standard prenatal and pediatric care in four primary care-based settings serving a racially and ethnically diverse low-income population. Four sites will be chosen to specifically obtain racially/ethnically diverse patient populations and a mix of academic and community-based primary care settings. Our assessment of implementation will be guided by the RE-AIM framework (Reach, Effectiveness, and Implementation; Adoption and Maintenance are beyond the scope of this project).

Aim 1: To determine if the Starting Early Program implemented in four primary care-based clinics, serving a racially and ethnically diverse population, is effective at decreasing overweight status and weight for length z-score at child age 3 years old compared to standard prenatal and pediatric care.
Hypothesis 1: The prevalence and degree of obesity at child age three years old will be lower in the intervention group receiving the Starting Early Program than in the control group receiving standard care.
We will conduct a randomized controlled trial to determine whether the Starting Early Program implemented at four diverse primary care sites within the New York City Health and Hospitals System, one of the nation’s largest providers of health care for low-income children, will work in reducing obesity in the real world setting compared to standard care. Primary outcome of child growth parameters will be obtained through medical record review. Baseline and follow-up assessments (will occur at child age 12, 24 and 36 months old) will consist of quantitative survey questionnaires with the whole sample and semi-structured qualitative interviews with a subset of individual patients (pregnant women; mothers with infants) to assess views of pregnant women and mothers about intervention acceptability, appropriateness, feasibility and perceived barriers to engagement. 

Aim 2: To explore contextual factors (individual, organizational) that may influence Starting Early Program implementation to inform future modifications. 
We will use a mixed-methods quantitative and qualitative approach using the Consolidated Framework for Implementation Research (CFIR) to examine the individual and organizational factors that may influence implementation of this program broadly within primary care. To assess individual factors, we will explore interventionist and prenatal and pediatric primary care provider knowledge and beliefs about the intervention, self-efficacy in executing the implementation goals, and readiness to change. To assess organizational factors, we will interview clinic leadership to assess structural characteristics, leadership engagement, culture, climate and resource allocation.

Significance: Targeting prenatal and pediatric primary care settings is a novel platform for reaching minority and low-income populations who are at greatest risk of early obesity beginning in infancy. The use of a multi-level, theoretical framework for examining contextual factors at the individual and organizational level will aid in identifying needed implementation strategies for future adaptations that will promote greater effectiveness and broader implementation in real-world settings. This evaluation is critical for bringing the Starting Early Program to scale within diverse high-risk communities. 
";s:5:"xhtml";s:9170:"GROSS - Assignment #1a<br /><br />1. Draft a specific aims page of your proposed study.<br /><br />My response to this is... <br /><br />Title: Randomized Controlled Trial of a Primary Care-Based Child Obesity Prevention Intervention:       Effectiveness and Implementation<br /><br />Childhood obesity in the United States is a serious public health concern: Obese children are likely to stay obese into adulthood and to develop early co-morbid conditions, including diabetes and cardiovascular diseases. Growing evidence has documented that pregnancy, infancy and the toddler years represent critical time periods in which the establishment of obesity-promoting behaviors significantly increase the risk of obesity throughout the life-course. Excess weight, even in infancy, greatly increases the risk of lifelong obesity. Economic and racial/ethnic disparities in child obesity may be determined by factors operating during pregnancy, infancy and early childhood. Early prevention strategies are crucial in altering early-life systems that promote the intergenerational transmission of obesity in high-risk populations.<br /><br />Ecological model of child obesity: Obesity is a complex problem involving contributing factors at multiple levels, including the child, family, and societal forces. Prevention efforts focusing on multiple levels are needed to influence the feeding patterns of infants and toddlers and improve weight outcomes. During pregnancy, modifiable factors include excessive gestational weight gain, gestational diabetes, antenatal depression, and stress. During infancy and early childhood, they include infant feeding practices, non-responsive feeding styles, sleep, diet, physical activity and sedentary behaviors. <br /><br />Early obesity prevention interventions: Trials of child obesity prevention interventions during pregnancy and infancy have shown positive impacts on infant feeding and weight using community or nurse home visiting models. These programs were conducted either outside of the United States (US), or in middle-income US communities, limiting their generalizability to low-income US communities. While home visiting programs for high-risk children have resulted in improved outcomes, their current reach is limited, and alternative methods are needed to obtain population level outcomes in low-income communities. Integration of early obesity prevention into the framework of frequent primary health care visits in pregnancy and infancy may increase the ability to reach high-risk families, with potential for population-wide application. The benefits of utilizing primary care include: 1) the high frequency of and adherence to prenatal and pediatric primary care visits, which provide access to high-risk families on a population scale; 2) building on pre-existing provider relationships; 3) using existing infrastructure to lower cost; and 4) decreased need for additional transportation. <br /><br />Evidence-based program: Prior to our work, no comprehensive obesity prevention intervention studies had begun prenatally and used the existing framework of frequent prenatal and infant primary care visits to reach high-risk families for child obesity prevention. During the last 6 years, I have been a Co-Project Director for the Starting Early Trial funded by the US Department of Agriculture (AFRI # 2011-68001-30207). The Starting Early Program was a family-centered early child obesity prevention intervention specifically designed for the Hispanic community beginning in pregnancy. The main Starting Early Program components included individual prenatal and peri-partum nutrition counseling and nutrition and parenting support groups coordinated with all well child visits in the first three years of life. Program efficacy has been documented in a randomized controlled trial (n=533) which found that intervention mother-infant pairs had higher rates of exclusive breastfeeding, less early introduction of complementary foods, decreased juice consumption and greater infant physical activity, than controls receiving standard primary care. Starting Early demonstrated significant impacts on child growth, with lower mean weight for age z-scores in the intervention group compared to controls at 2 years old (0.62 vs. 0.85; p=.046). Within the intervention group, high attendance further reduced mean weight for age z-scores (0.37 vs. 0.72; p=038) and obesity (14.8% vs. 35.9%; p=.002). Although a formal cost analysis has not been performed, we estimated that program costs are about $100-150 per participant/year, which is less than 1/10th that of interdisciplinary obesity treatment programs for school-aged children. <br /><br />Gaps in the literature: While the Starting Early Program has been studied in controlled research settings and in a high risk Hispanic sample, to date no implementation science studies have focused on early child obesity prevention during pregnancy and infancy in real-world primary care-based settings serving racially and ethnically diverse populations. Studies of implementing group models of prenatal care found that the substantial paradigm shift from traditional individual visits required overcoming barriers related to difficulties with space, scheduling, recruitment, staffing, organizational motivation and leadership buy-in in order to maintain adequate fidelity to the originally designed intervention. However, there has been sparse research on contextual factors (individual and organizational factors) surrounding early child obesity prevention interventions that will move these programs from research to practice.<br /><br />Specific aims: <br />Given this gap in the literature, we will conduct an effectiveness-implementation hybrid trial to test the effectiveness of the Starting Early Program compared to standard prenatal and pediatric care in four primary care-based settings serving a racially and ethnically diverse low-income population. Four sites will be chosen to specifically obtain racially/ethnically diverse patient populations and a mix of academic and community-based primary care settings. Our assessment of implementation will be guided by the RE-AIM framework (Reach, Effectiveness, and Implementation; Adoption and Maintenance are beyond the scope of this project).<br /><br />Aim 1: To determine if the Starting Early Program implemented in four primary care-based clinics, serving a racially and ethnically diverse population, is effective at decreasing overweight status and weight for length z-score at child age 3 years old compared to standard prenatal and pediatric care.<br />Hypothesis 1: The prevalence and degree of obesity at child age three years old will be lower in the intervention group receiving the Starting Early Program than in the control group receiving standard care.<br />We will conduct a randomized controlled trial to determine whether the Starting Early Program implemented at four diverse primary care sites within the New York City Health and Hospitals System, one of the nation’s largest providers of health care for low-income children, will work in reducing obesity in the real world setting compared to standard care. Primary outcome of child growth parameters will be obtained through medical record review. Baseline and follow-up assessments (will occur at child age 12, 24 and 36 months old) will consist of quantitative survey questionnaires with the whole sample and semi-structured qualitative interviews with a subset of individual patients (pregnant women; mothers with infants) to assess views of pregnant women and mothers about intervention acceptability, appropriateness, feasibility and perceived barriers to engagement. <br /><br />Aim 2: To explore contextual factors (individual, organizational) that may influence Starting Early Program implementation to inform future modifications. <br />We will use a mixed-methods quantitative and qualitative approach using the Consolidated Framework for Implementation Research (CFIR) to examine the individual and organizational factors that may influence implementation of this program broadly within primary care. To assess individual factors, we will explore interventionist and prenatal and pediatric primary care provider knowledge and beliefs about the intervention, self-efficacy in executing the implementation goals, and readiness to change. To assess organizational factors, we will interview clinic leadership to assess structural characteristics, leadership engagement, culture, climate and resource allocation.<br /><br />Significance: Targeting prenatal and pediatric primary care settings is a novel platform for reaching minority and low-income populations who are at greatest risk of early obesity beginning in infancy. The use of a multi-level, theoretical framework for examining contextual factors at the individual and organizational level will aid in identifying needed implementation strategies for future adaptations that will promote greater effectiveness and broader implementation in real-world settings. This evaluation is critical for bringing the Starting Early Program to scale within diverse high-risk communities.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"7eed598861a1026b1ea32d63e9933538";}s:4:"show";b:1;s:3:"cid";s:32:"7600a8ddee4343dda59bf3d276f3cae1";}s:32:"d2441dff19c4863903b873c12bb8e2aa";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"jpatterson";s:4:"name";s:19:"Jacquelyn Patterson";s:4:"mail";s:28:"jackie_patterson@med.unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1535026912;s:8:"modified";i:1536327852;}s:3:"raw";s:4659:"PATTERSON – Assignment #1a

1.	Draft a specific aims page of your proposed study.

My response to this is…

An Innovative Audit-Feedback Mobile Health Application
to Improve Newborn Resuscitation in Low-Resource Settings

One million infants die on their day of birth each year; nearly all of these deaths occur in low and lower-middle income countries (LMICs). Many of these deaths result from failure to initiate and sustain breathing at birth, a condition referred to as birth asphyxia. Mortality from birth asphyxia is reduced by tactile stimulation to breathe and bag mask ventilation. A common approach to increasing the use of stimulation and ventilation is training in these practices. However, even with adequate knowledge and skill, many providers in LMICs use stimulation and ventilation ineffectively. This is often due to inaccurate assessment of the newborn’s condition and provider anxiety during resuscitation. Innovative strategies to enhance the translation of knowledge and skill into practice are needed to improve resuscitation care and reduce newborn mortality in LMICs.

Bedside audit and feedback is an effective strategy to enhance knowledge translation. In high-income countries, audit and feedback of newborn resuscitations is performed using video recording of bedside care. Expert clinician-educators review recorded events, identify deficiencies in care and provide feedback. Despite its effectiveness, this strategy is infrequently used in LMICs due to a lack of local expertise in identifying deficiencies in care and delivering feedback. Mobile health (mHealth) approaches could automate audit and feedback, enabling the use of this strategy to improve resuscitation care in LMICs. 
   
We have developed a digital tool to collect observational data during newborn resuscitations for an ongoing study in the Democratic Republic of Congo. Observers use this tool to document the newborn’s condition and provider actions during resuscitation. For this proposal, we will adapt our research tool for use as an audit-feedback mHealth application as part of an integrated implementation intervention to improve resuscitation. 

Our goal is to reduce newborn mortality in LMICs by improving newborn resuscitation using an innovative mHealth application with audit-feedback capacity. To achieve this goal, we propose the following aims:

Aim 1: Develop an audit-feedback mHealth application called NeoWatch. 
Using a Delphi panel, we will define a prioritized list of evidence-based resuscitation practices for comparison to observed care. Using our research tool as a foundation, we will create NeoWatch. NeoWatch will automatically compare observed care to the practices on the Delphi panel’s prioritized list, and deliver automated real-time and post hoc feedback. NeoWatch will also identify common deficiencies in care by aggregating data from many births.

Aim 2: Assess usability of NeoWatch in a beta test. 
In two rounds of beta testing, midwives will observe a minimum of 20 births using NeoWatch, including at least five resuscitations requiring bag mask ventilation. We will test an integrated implementation intervention consisting of recording of care using NeoWatch, automated real-time and post-hoc feedback to individuals, and supervisor-led feedback to groups of providers based on common deficiencies in care. To assess usability, we will directly observe providers interacting with NeoWatch and conduct focus group discussions. After each round, we will refine NeoWatch and the intervention.

Aim 3: Evaluate effectiveness and feasibility of NeoWatch in a pilot implementation study.
In two hospitals in the Democratic Republic of Congo, midwives will implement NeoWatch to assess 1600 births over 12 months. We will deliver feedback using the integrated implementation intervention developed in Aim 2. We will measure the use of evidence-based stimulation and ventilation, as well as time to effective breathing of the newborn. Using baseline data from our current research study (ongoing at our two proposed sites), we will assess changes in stimulation, ventilation and effective breathing over time using an interrupted time series approach implemented through a linear mixed model. We will define a priori criteria for effectiveness to justify definitive testing in a larger randomized trial. In addition, we will assess feasibility, acceptability and appropriateness using a mixed-methods approach of quantitative surveys and qualitative focus groups discussions. 

This study will generate the pilot data to support a randomized trial testing the impact of NeoWatch on newborn mortality in LMICs.
";s:5:"xhtml";s:4788:"PATTERSON – Assignment #1a<br /><br />1.	Draft a specific aims page of your proposed study.<br /><br />My response to this is…<br /><br />An Innovative Audit-Feedback Mobile Health Application<br />to Improve Newborn Resuscitation in Low-Resource Settings<br /><br />One million infants die on their day of birth each year; nearly all of these deaths occur in low and lower-middle income countries (LMICs). Many of these deaths result from failure to initiate and sustain breathing at birth, a condition referred to as birth asphyxia. Mortality from birth asphyxia is reduced by tactile stimulation to breathe and bag mask ventilation. A common approach to increasing the use of stimulation and ventilation is training in these practices. However, even with adequate knowledge and skill, many providers in LMICs use stimulation and ventilation ineffectively. This is often due to inaccurate assessment of the newborn’s condition and provider anxiety during resuscitation. Innovative strategies to enhance the translation of knowledge and skill into practice are needed to improve resuscitation care and reduce newborn mortality in LMICs.<br /><br />Bedside audit and feedback is an effective strategy to enhance knowledge translation. In high-income countries, audit and feedback of newborn resuscitations is performed using video recording of bedside care. Expert clinician-educators review recorded events, identify deficiencies in care and provide feedback. Despite its effectiveness, this strategy is infrequently used in LMICs due to a lack of local expertise in identifying deficiencies in care and delivering feedback. Mobile health (mHealth) approaches could automate audit and feedback, enabling the use of this strategy to improve resuscitation care in LMICs. <br />   <br />We have developed a digital tool to collect observational data during newborn resuscitations for an ongoing study in the Democratic Republic of Congo. Observers use this tool to document the newborn’s condition and provider actions during resuscitation. For this proposal, we will adapt our research tool for use as an audit-feedback mHealth application as part of an integrated implementation intervention to improve resuscitation. <br /><br />Our goal is to reduce newborn mortality in LMICs by improving newborn resuscitation using an innovative mHealth application with audit-feedback capacity. To achieve this goal, we propose the following aims:<br /><br />Aim 1: Develop an audit-feedback mHealth application called NeoWatch. <br />Using a Delphi panel, we will define a prioritized list of evidence-based resuscitation practices for comparison to observed care. Using our research tool as a foundation, we will create NeoWatch. NeoWatch will automatically compare observed care to the practices on the Delphi panel’s prioritized list, and deliver automated real-time and post hoc feedback. NeoWatch will also identify common deficiencies in care by aggregating data from many births.<br /><br />Aim 2: Assess usability of NeoWatch in a beta test. <br />In two rounds of beta testing, midwives will observe a minimum of 20 births using NeoWatch, including at least five resuscitations requiring bag mask ventilation. We will test an integrated implementation intervention consisting of recording of care using NeoWatch, automated real-time and post-hoc feedback to individuals, and supervisor-led feedback to groups of providers based on common deficiencies in care. To assess usability, we will directly observe providers interacting with NeoWatch and conduct focus group discussions. After each round, we will refine NeoWatch and the intervention.<br /><br />Aim 3: Evaluate effectiveness and feasibility of NeoWatch in a pilot implementation study.<br />In two hospitals in the Democratic Republic of Congo, midwives will implement NeoWatch to assess 1600 births over 12 months. We will deliver feedback using the integrated implementation intervention developed in Aim 2. We will measure the use of evidence-based stimulation and ventilation, as well as time to effective breathing of the newborn. Using baseline data from our current research study (ongoing at our two proposed sites), we will assess changes in stimulation, ventilation and effective breathing over time using an interrupted time series approach implemented through a linear mixed model. We will define a priori criteria for effectiveness to justify definitive testing in a larger randomized trial. In addition, we will assess feasibility, acceptability and appropriateness using a mixed-methods approach of quantitative surveys and qualitative focus groups discussions. <br /><br />This study will generate the pilot data to support a randomized trial testing the impact of NeoWatch on newborn mortality in LMICs.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"9c93c3e30b3cc19e07d945444ebbedd9";}s:4:"show";b:1;s:3:"cid";s:32:"d2441dff19c4863903b873c12bb8e2aa";}s:32:"984b8a2ecf57327488db61c93b89f124";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"mroberts";s:4:"name";s:13:"Megan Roberts";s:4:"mail";s:32:"megan.y.roberts@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535074903;}s:3:"raw";s:9218:"
ROBERTS – Assignment #1a

1.	Draft a specific aims page of your proposed study. 

My response to this is….

Improving Parental Involvement in Community-Based Early Language Intervention

The number of children with developmental disabilities has increased by over 15% in the last 12 years and it is estimated that approximately 1 in 6 children in the United States has a developmental delay. Developmental delays include learning disabilities, autism, attention deficit hyperactivity disorder, intellectual disability, or language impairment. When a delay that impacts communication, social skills, motor or cognitive skills occurs prior to 36 months of age, children are eligible to receive home-based early intervention services through Part C of the Individuals with Disabilities Education Act. These state-sponsored early intervention services occur in the child’s natural environment, which is most often the child’s home.  In 2016, 372,896 children received early intervention services, which is 3.12% of the population. The most common early intervention therapy service is speech-language therapy, with over 90% of children enrolled in Early Intervention receiving speech-language therapy.

Effective early speech-language intervention is essential for improving the long-term language outcomes for children with developmental disabilities. However, the long-term outcomes for children with developmental disabilities are variable. For example, as many as 30-40% of children with autism remain minimally verbal even with early intervention. High dosage of early intervention is critical to language skill development in young children and including parents is a cost-effective approach to maximize intervention dosage. In addition, parent-child interactions are an important context for early language development. Parents and children develop nuanced patterns of interaction from the first weeks of life, which form the basis for how children learn language. Several parent behaviors (e.g., amount of engagement, number of words spoken, use of language facilitation strategies) are associated with better language outcomes in children with developmental delays. However, parental responsiveness (i.e., responding to child communication and focus of attention) is the parent behavior that is the most consistently correlated with concurrent and prospective child language skills and also the most widely included parent behavior in studies of parent-mediated language interventions. These findings indicate that teaching parents responsiveness should be a critical target of early intervention. 

In additional to correlational studies, several efficacy trials of parent-mediated interventions have been conducted with positive effects on child language outcomes. Parent-mediated responsiveness interventions has been shown to be effective for infants and toddlers with autism, hearing loss, developmental language disorder and Down syndrome. In fact, meta-analytic results across studies indicate that on average children whose parents learn to be responsive to their communicative attempts say, on average, 53 more words than those children whose parents did not receive any instruction. Methods used to teach parents have typically involved a combination of instruction, modeling, coaching and discussion. 

Despite these positive associations and the demonstrated effects of parent-mediated interventions for infants and toddlers with developmental delays, less than 10% of families enrolled in two ongoing early intervention efficacy trials (1R01DC014709, R324A150094), report that they are included in early intervention speech-language therapy sessions. Furthermore, over 60% of these families express dissatisfaction with their current early intervention speech-language therapy. This lack of instruction may be due to the fact that the majority of graduate programs in speech-language therapy (60%) provide little or no training in early intervention and 68% of speech-language therapists report low-levels of competence in working with infants and toddlers. Furthermore, the current standard of bringing toys into the home and working directly with the child is likely easier for therapists to implement. Teaching parents to be responsive to their child is a complex process that involves a triadic intervention model. A skilled therapist must not only know how to implement responsive strategies with the child but also how to teach the parent to use these strategies within the context of their everyday activities. As such, it is important to understand effective ways to teach therapists how to implement both child (e.g., responsiveness) and parent (e.g., modeling, coaching) intervention strategies. 

While teaching parents to be responsive has strong empirical support in highly controlled efficacy trials across separate and distinct populations of children, little is known about the effectiveness of parent-mediated responsiveness when implemented by an early intervention speech-language therapist in a real world setting for all children on their caseload regardless of diagnosis. Thus, the objective of the proposed study is to develop and test implementation strategies for teaching early intervention speech-language pathologists to teach parents how to be responsive to their child’s communicative attempts. The target population for the proposed study is individual early intervention speech-language therapists who currently provide language therapy to at least four children between 12 and 36 months of age in the home when a parent is present. Using both qualitative methods and a sequential multiple assignment randomized trial (SMART) design, the aims of the proposed study are to:

Aim 1. Understand current early intervention practices regarding, beliefs of and barriers to implementation of parent-mediated interventions. Focus groups, direct observation of therapy sessions and surveys will be used with 20 speech-language therapists from different early intervention agencies across the greater Chicagoland area.

Aim 2. Develop implementation strategies to teach therapists to use parent-mediated responsiveness. In collaboration with 10 Illinois early intervention speech-language therapists, implementation strategies will be developed to address the barriers identified in aim 1. Potential strategies include direct instruction (in-person workshop, online modules) and feedback (immediate in-vivo coaching, delayed feedback). However, it is expected that the development process will likely be iterative, as such, the approach may change during the project. 

Aim 3. Evaluate the process and outcomes of the implementation approach for teaching speech-language therapists to use parent-mediated responsiveness. A hybrid type 2 effectiveness-implementation SMART design will be used to evaluate the ideal combination and sequence of implementation strategies for teaching speech-language therapists to use parent-mediated responsiveness. Participants will include 120 early intervention speech-language therapists in the greater Chicagoland area.

Research Question 3a: What are the effects of the different implementation strategy sequences on therapists’ fidelity of implementation? What is the best implementation approach based on baseline therapist characteristics and therapist progress during intervention?
Research Question 3b: What are barriers and facilitators to therapists’ implementation of the intervention?
Research Question 3c: How do therapists rate their satisfaction, buy-in and opinion of the intervention?

Aim 4. Evaluate the effectiveness of parent-mediated responsiveness when implemented by speech-language therapists as part of the statewide early intervention program. A hybrid type 2 effectiveness-implementation SMART design will be used to test the intervention effects on parent use of responsiveness and their relation to implementation factors such as buy-in and fidelity. Participants are the same as Aim 3.

Research Question 4a: What are the effects of the parent-mediated responsiveness intervention on parents’ rate of responsiveness with their child?
Research Question 4b: How do parents rate their satisfaction of the parent-mediated responsiveness intervention?

The aims of the proposed study relate to PAR-18-007 (Dissemination and Implementation Research in Health) and are consistent with NIDCD’s Strategic Plan Priority Area 4 of improving outcomes for human communication by bridging the gap between research and practice through effective dissemination and implementation strategies. The results of this study are expected to have an important impact because widespread use of parent training to improve responsiveness to child communicative is likely to improve language outcomes for toddlers with developmental delays. In addition, understanding the ways in which to sequence implementation strategies, from least to most supportive based on therapist baseline characteristics and therapist progress during intervention, is expected to advance of the field of implementation science in home-based early intervention settings, where there is currently a striking paucity of implementation research. 

";s:5:"xhtml";s:9379:"ROBERTS – Assignment #1a<br /><br />1.	Draft a specific aims page of your proposed study. <br /><br />My response to this is….<br /><br />Improving Parental Involvement in Community-Based Early Language Intervention<br /><br />The number of children with developmental disabilities has increased by over 15% in the last 12 years and it is estimated that approximately 1 in 6 children in the United States has a developmental delay. Developmental delays include learning disabilities, autism, attention deficit hyperactivity disorder, intellectual disability, or language impairment. When a delay that impacts communication, social skills, motor or cognitive skills occurs prior to 36 months of age, children are eligible to receive home-based early intervention services through Part C of the Individuals with Disabilities Education Act. These state-sponsored early intervention services occur in the child’s natural environment, which is most often the child’s home.  In 2016, 372,896 children received early intervention services, which is 3.12% of the population. The most common early intervention therapy service is speech-language therapy, with over 90% of children enrolled in Early Intervention receiving speech-language therapy.<br /><br />Effective early speech-language intervention is essential for improving the long-term language outcomes for children with developmental disabilities. However, the long-term outcomes for children with developmental disabilities are variable. For example, as many as 30-40% of children with autism remain minimally verbal even with early intervention. High dosage of early intervention is critical to language skill development in young children and including parents is a cost-effective approach to maximize intervention dosage. In addition, parent-child interactions are an important context for early language development. Parents and children develop nuanced patterns of interaction from the first weeks of life, which form the basis for how children learn language. Several parent behaviors (e.g., amount of engagement, number of words spoken, use of language facilitation strategies) are associated with better language outcomes in children with developmental delays. However, parental responsiveness (i.e., responding to child communication and focus of attention) is the parent behavior that is the most consistently correlated with concurrent and prospective child language skills and also the most widely included parent behavior in studies of parent-mediated language interventions. These findings indicate that teaching parents responsiveness should be a critical target of early intervention. <br /><br />In additional to correlational studies, several efficacy trials of parent-mediated interventions have been conducted with positive effects on child language outcomes. Parent-mediated responsiveness interventions has been shown to be effective for infants and toddlers with autism, hearing loss, developmental language disorder and Down syndrome. In fact, meta-analytic results across studies indicate that on average children whose parents learn to be responsive to their communicative attempts say, on average, 53 more words than those children whose parents did not receive any instruction. Methods used to teach parents have typically involved a combination of instruction, modeling, coaching and discussion. <br /><br />Despite these positive associations and the demonstrated effects of parent-mediated interventions for infants and toddlers with developmental delays, less than 10% of families enrolled in two ongoing early intervention efficacy trials (1R01DC014709, R324A150094), report that they are included in early intervention speech-language therapy sessions. Furthermore, over 60% of these families express dissatisfaction with their current early intervention speech-language therapy. This lack of instruction may be due to the fact that the majority of graduate programs in speech-language therapy (60%) provide little or no training in early intervention and 68% of speech-language therapists report low-levels of competence in working with infants and toddlers. Furthermore, the current standard of bringing toys into the home and working directly with the child is likely easier for therapists to implement. Teaching parents to be responsive to their child is a complex process that involves a triadic intervention model. A skilled therapist must not only know how to implement responsive strategies with the child but also how to teach the parent to use these strategies within the context of their everyday activities. As such, it is important to understand effective ways to teach therapists how to implement both child (e.g., responsiveness) and parent (e.g., modeling, coaching) intervention strategies. <br /><br />While teaching parents to be responsive has strong empirical support in highly controlled efficacy trials across separate and distinct populations of children, little is known about the effectiveness of parent-mediated responsiveness when implemented by an early intervention speech-language therapist in a real world setting for all children on their caseload regardless of diagnosis. Thus, the objective of the proposed study is to develop and test implementation strategies for teaching early intervention speech-language pathologists to teach parents how to be responsive to their child’s communicative attempts. The target population for the proposed study is individual early intervention speech-language therapists who currently provide language therapy to at least four children between 12 and 36 months of age in the home when a parent is present. Using both qualitative methods and a sequential multiple assignment randomized trial (SMART) design, the aims of the proposed study are to:<br /><br />Aim 1. Understand current early intervention practices regarding, beliefs of and barriers to implementation of parent-mediated interventions. Focus groups, direct observation of therapy sessions and surveys will be used with 20 speech-language therapists from different early intervention agencies across the greater Chicagoland area.<br /><br />Aim 2. Develop implementation strategies to teach therapists to use parent-mediated responsiveness. In collaboration with 10 Illinois early intervention speech-language therapists, implementation strategies will be developed to address the barriers identified in aim 1. Potential strategies include direct instruction (in-person workshop, online modules) and feedback (immediate in-vivo coaching, delayed feedback). However, it is expected that the development process will likely be iterative, as such, the approach may change during the project. <br /><br />Aim 3. Evaluate the process and outcomes of the implementation approach for teaching speech-language therapists to use parent-mediated responsiveness. A hybrid type 2 effectiveness-implementation SMART design will be used to evaluate the ideal combination and sequence of implementation strategies for teaching speech-language therapists to use parent-mediated responsiveness. Participants will include 120 early intervention speech-language therapists in the greater Chicagoland area.<br /><br />Research Question 3a: What are the effects of the different implementation strategy sequences on therapists’ fidelity of implementation? What is the best implementation approach based on baseline therapist characteristics and therapist progress during intervention?<br />Research Question 3b: What are barriers and facilitators to therapists’ implementation of the intervention?<br />Research Question 3c: How do therapists rate their satisfaction, buy-in and opinion of the intervention?<br /><br />Aim 4. Evaluate the effectiveness of parent-mediated responsiveness when implemented by speech-language therapists as part of the statewide early intervention program. A hybrid type 2 effectiveness-implementation SMART design will be used to test the intervention effects on parent use of responsiveness and their relation to implementation factors such as buy-in and fidelity. Participants are the same as Aim 3.<br /><br />Research Question 4a: What are the effects of the parent-mediated responsiveness intervention on parents’ rate of responsiveness with their child?<br />Research Question 4b: How do parents rate their satisfaction of the parent-mediated responsiveness intervention?<br /><br />The aims of the proposed study relate to PAR-18-007 (Dissemination and Implementation Research in Health) and are consistent with NIDCD’s Strategic Plan Priority Area 4 of improving outcomes for human communication by bridging the gap between research and practice through effective dissemination and implementation strategies. The results of this study are expected to have an important impact because widespread use of parent training to improve responsiveness to child communicative is likely to improve language outcomes for toddlers with developmental delays. In addition, understanding the ways in which to sequence implementation strategies, from least to most supportive based on therapist baseline characteristics and therapist progress during intervention, is expected to advance of the field of implementation science in home-based early intervention settings, where there is currently a striking paucity of implementation research.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"0a909ad86ae1c3f8a46639ff4087829d";}s:4:"show";b:1;s:3:"cid";s:32:"984b8a2ecf57327488db61c93b89f124";}s:32:"990a84f6e316a30198576886651836a4";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"mmoniz";s:4:"name";s:14:"Michelle Moniz";s:4:"mail";s:20:"mmoniz@med.umich.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535124352;}s:3:"raw";s:5598:"MONIZ - Assignment #1a

Draft a specific aims page of your proposed study.

My response to this is... 

SPECIFIC AIMS
     Access to contraception is essential after childbirth. Contraception prevents both unintended and closely spaced pregnancies, thereby improving maternal-child health outcomes and reducing public expenditures. Although 75% of recently pregnant women want to prevent childbearing, many women lack access to the most effective contraceptive methods (i.e. the intrauterine device and contraceptive implant). Currently, these so-called long-acting reversible contraceptives (LARC) are usually placed during an office visit 6-8 weeks after delivery. This approach is suboptimal, because many women lose health insurance coverage or conceive prior to this visit, or do not attend this visit at all. Consequently, although up to 40% of pregnant women report wanting to use LARC methods postpartum, only 6% of postpartum women do so.

     One promising approach to improve contraceptive care and health outcomes is to provide LARC before women leave the hospital after childbirth. Compared to waiting for outpatient insertion, immediate postpartum LARC (IPLARC) in the hospital is associated with high patient satisfaction, fewer short interval pregnancies, and cost-savings for payers. Despite these advantages, IPLARC is not available at most US hospitals. Barriers to IPLARC service delivery include provider knowledge and skill gaps, inadequate prenatal counseling about contraception, LARC device supply chain challenges, and barriers to inpatient LARC billing. There are no proven interventions to help hospitals overcome these impediments to IPLARC care.
Well-designed toolkits – a package of tools and strategies to facilitate clinical practice change – have potential to address these barriers. Toolkits offer advantages of flexibility (sites select items based on local needs) and scalability (toolkit items address barriers common to multiple sites) and a recent systematic review demonstrated their effectiveness at changing clinical practice. However, this same study identified the need to rigorously evaluate the acceptability, utility, and impact of specific toolkit components using mixed methods.

     The overall project goal is to develop and evaluate a toolkit-based implementation strategy to sup- port hospitals and maternity providers in providing immediate postpartum contraceptive care. In ongoing work, we are creating a prototype implementation toolkit that includes tools and activities used at 11 hospitals across the US that implement IPLARC care. However, there remain important knowledge gaps about how new sites should select tools and activities from a toolkit. Developing systematic approaches to selecting and utilizing toolkit resources based on local needs is critical to replicating successful IPLARC implementation in diverse health system settings nationally. It is also currently unknown whether a toolkit-based implementation strategy can support IPLARC services in a way that is acceptable to patients and providers and financially viable for hospitals. In a single setting with low baseline IPLARC use (Michigan Medicine), we have already formed a multidisciplinary Stakeholder Panel and interviewed its members to identify local barriers/facilitators to IPLARC provision. We now propose to use state-of-the-art implementation science methods to develop and test a toolkit-based, multicomponent implementation strategy to drive successful IPLARC provision at one academic center. This work is essential for future scale up of high-quality postpartum contraceptive care nationally.
     
Aim 1: Develop a toolkit-based implementation strategy to support IPLARC provision. Informed by our recently completed barriers/facilitators assessment at the study site, we will next use systematic rating activities with Stakeholder Panel members to assess the predicted acceptability and appropriateness of each item in the prototype toolkit. At Stakeholder Panel meetings, we will elicit reactions to ratings and develop final recommendations for what the core components of the implementation strategy should be.

Aim 2: Evaluate the feasibility of a toolkit-based implementation strategy for IPLARC at one site. In partnership with key stakeholders, we will initiate the toolkit-based implementation strategy developed in Aim 1 at one academic center. We will evaluate feasibility using mixed methods (primary outcomes: acceptability, appropriateness; secondary outcomes: fidelity; real-time intervention modifications; direct medical costs associated with IPLARC implementation). We will also examine preliminary measures of clinical effectiveness (prenatal contraceptive counseling rate; IPLARC utilization rate). Findings will be used to revise prototype toolkit materials and the rating process used in Aim 1.

Impact: This work will generate new knowledge about how to create and evaluate customized interventions for clinical practice change. This proposal will lead to several expected outcomes including a) a refined IPLARC implementation toolkit, b) a systematic approach for designing customized, multicomponent implementation strategies, and c) the first available estimates of IPLARC implementation costs. Our feasibility study will yield key preliminary data to support a competitive NICHD R01 application to conduct a multi-site trial of IPLARC implementation. If effective, this approach could be integrated into maternity hospitals across the United States and scaled to large populations at risk for unintended pregnancy.

";s:5:"xhtml";s:5686:"MONIZ - Assignment #1a<br /><br />Draft a specific aims page of your proposed study.<br /><br />My response to this is... <br /><br />SPECIFIC AIMS<br />     Access to contraception is essential after childbirth. Contraception prevents both unintended and closely spaced pregnancies, thereby improving maternal-child health outcomes and reducing public expenditures. Although 75% of recently pregnant women want to prevent childbearing, many women lack access to the most effective contraceptive methods (i.e. the intrauterine device and contraceptive implant). Currently, these so-called long-acting reversible contraceptives (LARC) are usually placed during an office visit 6-8 weeks after delivery. This approach is suboptimal, because many women lose health insurance coverage or conceive prior to this visit, or do not attend this visit at all. Consequently, although up to 40% of pregnant women report wanting to use LARC methods postpartum, only 6% of postpartum women do so.<br /><br />     One promising approach to improve contraceptive care and health outcomes is to provide LARC before women leave the hospital after childbirth. Compared to waiting for outpatient insertion, immediate postpartum LARC (IPLARC) in the hospital is associated with high patient satisfaction, fewer short interval pregnancies, and cost-savings for payers. Despite these advantages, IPLARC is not available at most US hospitals. Barriers to IPLARC service delivery include provider knowledge and skill gaps, inadequate prenatal counseling about contraception, LARC device supply chain challenges, and barriers to inpatient LARC billing. There are no proven interventions to help hospitals overcome these impediments to IPLARC care.<br />Well-designed toolkits – a package of tools and strategies to facilitate clinical practice change – have potential to address these barriers. Toolkits offer advantages of flexibility (sites select items based on local needs) and scalability (toolkit items address barriers common to multiple sites) and a recent systematic review demonstrated their effectiveness at changing clinical practice. However, this same study identified the need to rigorously evaluate the acceptability, utility, and impact of specific toolkit components using mixed methods.<br /><br />     The overall project goal is to develop and evaluate a toolkit-based implementation strategy to sup- port hospitals and maternity providers in providing immediate postpartum contraceptive care. In ongoing work, we are creating a prototype implementation toolkit that includes tools and activities used at 11 hospitals across the US that implement IPLARC care. However, there remain important knowledge gaps about how new sites should select tools and activities from a toolkit. Developing systematic approaches to selecting and utilizing toolkit resources based on local needs is critical to replicating successful IPLARC implementation in diverse health system settings nationally. It is also currently unknown whether a toolkit-based implementation strategy can support IPLARC services in a way that is acceptable to patients and providers and financially viable for hospitals. In a single setting with low baseline IPLARC use (Michigan Medicine), we have already formed a multidisciplinary Stakeholder Panel and interviewed its members to identify local barriers/facilitators to IPLARC provision. We now propose to use state-of-the-art implementation science methods to develop and test a toolkit-based, multicomponent implementation strategy to drive successful IPLARC provision at one academic center. This work is essential for future scale up of high-quality postpartum contraceptive care nationally.<br />     <br />Aim 1: Develop a toolkit-based implementation strategy to support IPLARC provision. Informed by our recently completed barriers/facilitators assessment at the study site, we will next use systematic rating activities with Stakeholder Panel members to assess the predicted acceptability and appropriateness of each item in the prototype toolkit. At Stakeholder Panel meetings, we will elicit reactions to ratings and develop final recommendations for what the core components of the implementation strategy should be.<br /><br />Aim 2: Evaluate the feasibility of a toolkit-based implementation strategy for IPLARC at one site. In partnership with key stakeholders, we will initiate the toolkit-based implementation strategy developed in Aim 1 at one academic center. We will evaluate feasibility using mixed methods (primary outcomes: acceptability, appropriateness; secondary outcomes: fidelity; real-time intervention modifications; direct medical costs associated with IPLARC implementation). We will also examine preliminary measures of clinical effectiveness (prenatal contraceptive counseling rate; IPLARC utilization rate). Findings will be used to revise prototype toolkit materials and the rating process used in Aim 1.<br /><br />Impact: This work will generate new knowledge about how to create and evaluate customized interventions for clinical practice change. This proposal will lead to several expected outcomes including a) a refined IPLARC implementation toolkit, b) a systematic approach for designing customized, multicomponent implementation strategies, and c) the first available estimates of IPLARC implementation costs. Our feasibility study will yield key preliminary data to support a competitive NICHD R01 application to conduct a multi-site trial of IPLARC implementation. If effective, this approach could be integrated into maternity hospitals across the United States and scaled to large populations at risk for unintended pregnancy.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"47aa22141500718561a9ce0af9c660ec";}s:4:"show";b:1;s:3:"cid";s:32:"990a84f6e316a30198576886651836a4";}s:32:"655c229753261f71b54189234d3b11cb";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"anahmias";s:4:"name";s:15:"Allison Nahmias";s:4:"mail";s:21:"asnahmias@ucdavis.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535143652;}s:3:"raw";s:6428:"NAHMIAS – Assignment #1a

1. Draft a specific aims page of your proposed study

My response to this is…

Health problem:

The growing population of children with autism spectrum disorder (ASD), with current Center Disease Control (CDC) estimates at 1 in 59 children, and an annual cost in the US estimated to be over $236 billion, represents a significant public health challenge. Social deficits are a core component of ASD, and social attention is hypothesized as a possible malleable factor with broad effects, including increasing learning opportunities. Developing interventions that promote social and peer relationships for children with ASD is critical to improving outcomes for children with this disorder.
 
Evidence-based practice, program, intervention or guideline & Dissemination or Implementation gap:

The educational system is the primary service delivery system for children with ASD, and an ideal environment to target social skills as peers also participate in classrooms. Early intervention, beginning at or before preschool, is associated with long term benefits.  Evidence Based Interventions (EBIs) for children with ASD exist but are often branded, comprehensive, complex intervention packages that require extensive training. School providers (such as teachers) report using practices that do not align with EBIs. Even when community providers are trained to implement EBIs they face challenges to reach fidelity, do not sustain use of the EBI after training, and children do not make as large of gains as when the interventions are implemented in university settings. Providers often combine interventions and make adaptations to fit their context and it is unknown how these modifications are associated with outcomes. In addition, in studies comparing branded interventions, while the interventions are superior to usual care, there are seldom differences in child outcomes across interventions, indicating the potential to identify key active ingredients that are common to these interventions that may facilitate implementation in community settings. 
Our preliminary research of usual care for children with ASD in community preschools found that better implementation of strategies to support the development of social and peer relationships was associated with larger cognitive gains in preschoolers with ASD, over and above other setting or child characteristics. Therefore, it is possible that teaching community providers to facilitate these relationships is a key ingredient of effective intervention in classrooms. 

Specific Aims:

The proposed study addresses a critical need to improve community practices for preschool children with ASD by using the Collaborative for Model for Knowledge Translation between Research and Practice (CMKTRP) to build a collaborative, participatory relationship between researchers and community stakeholders aiming at the development of effective, feasible practice guidelines. The project seeks to understand practices currently used by community providers that are associated with positive child outcomes.  We will then use a knowledge translation cycle to integrate this practice-based evidence with research evidence to provide information meaningful for practitioners and support successful implementation. 

Aim 1: Build an equitable community-research partnership (collaborative group) in Sacramento that includes community providers, school administration, parents of preschool children with and without ASD, and researchers, with the goals of (a) developing practices guidelines for working with preschoolers with ASD and (2) knowledge translation initiatives for the implementation of the practice guidelines. This aim will provide a generalizable model for partnership-building and knowledge transfer strategy development in ASD. 

Aim 2: Collaboratively develop practice guidelines to promote the development of social and peer relationships for students with ASD (“ASD social practice guidelines”) by identifying (a) promising community practices associated with improved child outcomes through observational methods, and, (b) areas of overlap with evidence-based strategies. Videotaped classroom observations will be coded for specific strategies used by preschool teachers associated with child improvements. Consistent with the CMKTRP model, emerging findings from this data analysis and synthesis will be shared in “real time” with the collaborative group. These observed promising community practices use will then be compared with those used by comprehensive EBIs for ASD. The collaborative will use the results of this comparison to develop preliminary practice guidelines to support social and peer relationships in preschool settings. 

Aim 3: Collaboratively develop, pilot and refine knowledge translation initiatives for implementation of ASD social practice guidelines that fit with the community context. Preliminary knowledge translation initiatives to train providers on the preliminary practice guidelines from Aim 2 will be developed by the collaborative by comparing the evidence based on adult learning strategies and community partner preferences and current training practices. The preliminary knowledge translation initiatives will then be implemented. Live observation of provider use of the practices, just in time teaching, and responsive dialogue with the providers will be used to further refine and adapt the practice guidelines and knowledge translation initiatives. 

Aim 4: Conduct feasibility testing of the implementation of the ASD social practice guidelines. A small randomized control trial utilizing an interrupted time series design will be used to test the acceptability and feasibility of the practice guidelines and knowledge translation strategies developed in Aims 2 & 3. 

Importance to field:

This proposed study will develop practice guidelines to support social and peer relationships for educators working with children with ASD that will be more effective and sustainable in community settings than previously established EBIs because they were designed for dissemination and in collaboration with stakeholders.  This study will also develop a generalizable model for partnership-building and knowledge transfer strategy development in ASD, which will be a valuable resource to the wider scientific community trying to reduce the research-to-practice gap for ASD interventions.
";s:5:"xhtml";s:6580:"NAHMIAS – Assignment #1a<br /><br />1. Draft a specific aims page of your proposed study<br /><br />My response to this is…<br /><br />Health problem:<br /><br />The growing population of children with autism spectrum disorder (ASD), with current Center Disease Control (CDC) estimates at 1 in 59 children, and an annual cost in the US estimated to be over $236 billion, represents a significant public health challenge. Social deficits are a core component of ASD, and social attention is hypothesized as a possible malleable factor with broad effects, including increasing learning opportunities. Developing interventions that promote social and peer relationships for children with ASD is critical to improving outcomes for children with this disorder.<br /> <br />Evidence-based practice, program, intervention or guideline &amp; Dissemination or Implementation gap:<br /><br />The educational system is the primary service delivery system for children with ASD, and an ideal environment to target social skills as peers also participate in classrooms. Early intervention, beginning at or before preschool, is associated with long term benefits.  Evidence Based Interventions (EBIs) for children with ASD exist but are often branded, comprehensive, complex intervention packages that require extensive training. School providers (such as teachers) report using practices that do not align with EBIs. Even when community providers are trained to implement EBIs they face challenges to reach fidelity, do not sustain use of the EBI after training, and children do not make as large of gains as when the interventions are implemented in university settings. Providers often combine interventions and make adaptations to fit their context and it is unknown how these modifications are associated with outcomes. In addition, in studies comparing branded interventions, while the interventions are superior to usual care, there are seldom differences in child outcomes across interventions, indicating the potential to identify key active ingredients that are common to these interventions that may facilitate implementation in community settings. <br />Our preliminary research of usual care for children with ASD in community preschools found that better implementation of strategies to support the development of social and peer relationships was associated with larger cognitive gains in preschoolers with ASD, over and above other setting or child characteristics. Therefore, it is possible that teaching community providers to facilitate these relationships is a key ingredient of effective intervention in classrooms. <br /><br />Specific Aims:<br /><br />The proposed study addresses a critical need to improve community practices for preschool children with ASD by using the Collaborative for Model for Knowledge Translation between Research and Practice (CMKTRP) to build a collaborative, participatory relationship between researchers and community stakeholders aiming at the development of effective, feasible practice guidelines. The project seeks to understand practices currently used by community providers that are associated with positive child outcomes.  We will then use a knowledge translation cycle to integrate this practice-based evidence with research evidence to provide information meaningful for practitioners and support successful implementation. <br /><br />Aim 1: Build an equitable community-research partnership (collaborative group) in Sacramento that includes community providers, school administration, parents of preschool children with and without ASD, and researchers, with the goals of (a) developing practices guidelines for working with preschoolers with ASD and (2) knowledge translation initiatives for the implementation of the practice guidelines. This aim will provide a generalizable model for partnership-building and knowledge transfer strategy development in ASD. <br /><br />Aim 2: Collaboratively develop practice guidelines to promote the development of social and peer relationships for students with ASD (“ASD social practice guidelines”) by identifying (a) promising community practices associated with improved child outcomes through observational methods, and, (b) areas of overlap with evidence-based strategies. Videotaped classroom observations will be coded for specific strategies used by preschool teachers associated with child improvements. Consistent with the CMKTRP model, emerging findings from this data analysis and synthesis will be shared in “real time” with the collaborative group. These observed promising community practices use will then be compared with those used by comprehensive EBIs for ASD. The collaborative will use the results of this comparison to develop preliminary practice guidelines to support social and peer relationships in preschool settings. <br /><br />Aim 3: Collaboratively develop, pilot and refine knowledge translation initiatives for implementation of ASD social practice guidelines that fit with the community context. Preliminary knowledge translation initiatives to train providers on the preliminary practice guidelines from Aim 2 will be developed by the collaborative by comparing the evidence based on adult learning strategies and community partner preferences and current training practices. The preliminary knowledge translation initiatives will then be implemented. Live observation of provider use of the practices, just in time teaching, and responsive dialogue with the providers will be used to further refine and adapt the practice guidelines and knowledge translation initiatives. <br /><br />Aim 4: Conduct feasibility testing of the implementation of the ASD social practice guidelines. A small randomized control trial utilizing an interrupted time series design will be used to test the acceptability and feasibility of the practice guidelines and knowledge translation strategies developed in Aims 2 &amp; 3. <br /><br />Importance to field:<br /><br />This proposed study will develop practice guidelines to support social and peer relationships for educators working with children with ASD that will be more effective and sustainable in community settings than previously established EBIs because they were designed for dissemination and in collaboration with stakeholders.  This study will also develop a generalizable model for partnership-building and knowledge transfer strategy development in ASD, which will be a valuable resource to the wider scientific community trying to reduce the research-to-practice gap for ASD interventions.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"b6fddfb762c53f61d6544aacc26f17fc";}s:4:"show";b:1;s:3:"cid";s:32:"655c229753261f71b54189234d3b11cb";}s:32:"c70c102b4efeee2f2f0fa87bd66e279e";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"cvamos";s:4:"name";s:12:"Cheryl Vamos";s:4:"mail";s:21:"cvamos@health.usf.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535155943;}s:3:"raw";s:9774:"VAMOS - Assignment #1a

This proposal is in response to PAR-18-007: Dissemination and Implementation Research in Health and aligns with the call to identify and develop innovative approaches to address barriers and facilitators across multi-levels (patient, provider, organization) that influence the implementation of the prenatal oral health guidelines into clinical practice. This proposal also aligns with the NIDCR’s strategic plan objectives: 2-2, 3-1, and 3-2.

Poor oral health is a silent epidemic due to its prevalence and missed prevention opportunities,1 with oral-systemic health conditions and diseases affecting individuals across the lifecourse.2-10 Specifically, evidence has established connections between oral-systemic etiologies, such as periodontal disease and a range of systemic health outcomes and co-morbidities across the lifecourse (e.g., adverse pregnancy and birth outcomes; early childhood caries; chronic conditions such as cardiovascular disease and diabetes).3-16 Pregnancy represents a vulnerable period of increased risk with approximately 40% of women experiencing periodontal disease.11 Nonetheless, only 49% of women visited a dentist during their last pregnancy12 (compared to 66% of the general population who report seeing a dentist in the past year)13,14, and women of color and of lower income are more likely to need a dentist during pregnancy, but less likely to receive care.12  

In response to the significance of prenatal oral health on maternal and child outcomes and this unique window of opportunity for intervention, the American College of Obstetricians and Gynecologists (ACOG) and the American Dental Association (ADA) co-endorsed the Oral Health Care during Pregnancy: A National Consensus Statement which provides prenatal and oral health providers guidance for oral health promotion.15 However, translation of these guidelines into practice specifically among prenatal providers remains a critically gap, with noted barriers identified, including those from studies led the PI, such as: unawareness of the guidelines; lack of oral health knowledge and training; lack of time; lack of motivation; and lack of behavioral skills to assess, advise and refer patients on oral health issues.16 This may result from the lack of training or resources, or due to other implementation barriers that are currently being explored in ongoing NIH-funded study of the PI.17-25 Furthermore, a paucity of research has been conducted exploring other stakeholders’ roles in guideline implementation; although, previous research from the PI has identified potential roles of other clinical team members (e.g., office staff, medical assistants, nurses) in collectively assessing, advising, documenting and referring patients on oral health issues, including engaging patients in reporting oral health history and current concerns and in self-directed oral health education, such as through the use of an eHealth application.19,22,24,25 

Although studying the most appropriate and effective implementation strategies with regards to these guidelines is important for both provider types, this research will initially focus on PPs as a first step to capitalize on a critical period and optimal time to improve oral-systemic health across the lifecourse. For example, more than 70% of U.S. women have reported receiving prenatal care in the first trimester;26 provider recommendation is the most trusted source for information and lack of referral from a PP to an OHP can influence pregnant women’s decisions to seek oral healthcare;27 and PPs are often the “first line”15 in assessing, reinforcing and referring with regards to oral health issues.15,26,28-30 Although 98% of women receive at least one prenatal visit31, most report not receiving oral health information during a prenatal visit,32 and thus lack oral health knoweldge16,32-35 and remain unengaged in such care.36 Moreover, a systematic review led by the PI found that existing prenatal oral health interventions are limited, not theory or evidence-based, and lack rigorous evaluation.37 Those focused on prenatal providers are often limited to passive information;38-41 and of the patient-centered interventions, few concentrate on maternal behaviors and outcomes,42-45 and primarily examine children’s oral health.46-50 Given the significant gaps in oral health promotion among providers and patients, multilevel interventions that implement oral health promotion into the prenatal clinical system by addressing patient, provider and organizational level processes and outcomes are needed to facilitate guidelines implementation, engage patients, and improve quality of care.  

Multilevel interventions that include providers and organizations in a participatory nature in devising strategies to integrate evidence into practice have been successful in improving clinical practice.51-53 Although results from systematic literature reviews of implementation interventions provide models for multidisciplinary and multi-site team-based care, few have linked such interventions to improved patient outcomes or focused on oral health.51-53 One theory-led systematic review concluded that guideline implementation was effective when education was combined with routine monitoring and feedback for clinicians.51 Limitations to previous multilevel interventions that focused on quality improvement were a lack of patient input, limited use of theory, poor quality design and minimal use of technology and innovative solutionsMoreover, evidence-based strategies on guideline implementation have determined that effective interventions directly involve end-users in the development and as targets of the intervention;54,55 and prospectively identified barriers to change prior to intervention development and adaptation into practice,55 which this proposal accomplishes.

Our long-term goal is to increase oral-systemic prevention efforts during pregnancy, thus improving the health and well-being among women and children and decreasing risk for adverse health outcomes across the lifespan. The objective of this proposal is to determine promising implementation strategies to facilitate the translation of the prenatal oral health guidelines into routine prenatal care clinical practice. The objective will be achieved through the following specific aims:

Aim 1: Assess current implementation conditions (i.e., evidence, context and facilitation) and preferred intervention characteristics needed to increase adoption and integration of the prenatal oral health guidelines into the healthcare delivery system among clinic stakeholders (e.g., prenatal provider, nurse, medical assistants, office staff, administrators). Methods include interviews and focus groups with a convenience sample of prenatal care clinic stakeholders (e.g., prenatal provider, nurse, medical assistants, office staff/administrators; n=45 staff across 3 clinical sites) and clinical observations. Interview and focus group guide instruments will be developed based on the PARIHS (e.g., evidence; context; facilitation) and DOI (e.g., intervention characteristics) frameworks to understand the current implementation conditions and preferred intervention characteristics.

Aim 2: Identify promising implementation strategies to facilitate the adoption and integration of the prenatal oral health guidelines into the healthcare delivery system among clinic stakeholders (e.g., prenatal provider, nurse, medical assistants, office staff, administrators). Methods used in this phase will include a modified Delphi technique to identify and prioritize promising implementation strategies given Aim 1 findings, existing literature/known strategies (ERIC)47 and consultation with the Scientific and Practice Advisory Boards.

Aim 3: Develop and evaluate the potential fit and impact of the implementation strategies with regards to the integration of prenatal oral health that considers the complex healthcare delivery contexts on system-level (i.e., patient, provider, clinic) outcomes. Guided by three implementation frameworks (PARIHS; DOI; CFIR), methods include development and evaluation of intervention strategies (e.g., provider training, decision supports, checklists, audit and feedback) with regards to usability, feasibility and preliminary impacts on patient, provider and clinic outcomes (e.g., staff practice behaviors; clinic team communication, documentation and referral processes; patient-provider communication; patients’ overall satisfaction with oral health promotion during prenatal care visit) via surveys and follow-up interviews with patients (n=15) and clinic staff (n=45). 

This innovative, timely, multidisciplinary, and theory-driven project will enhance existing knowledge of implementation science and will guide future interventions that aim to implement evidence-based and multidisciplinary practice guidelines. Findings can also serve as a feedback loop to inform effective dissemination modalities, while accounting for the complex systems influencing the dissemination and implementation process. This study is significant as the Institute of Medicine has recognized the importance of guideline implementation as a strategy to improve health services in the U.S.56 Moreover, improving prenatal oral-systemic health has been identified as a priority area by the NIH Office of Research on Women’s Health and is reflected in the cross-cutting Healthy People 2020 topics (oral health; maternal and child health).57 58 This line of inquiry has the potential to significantly improve maternal and child oral-systemic health across the lifecourse and can be applied to other health issues that necessitate effective dissemination and implementation strategies.
";s:5:"xhtml";s:9873:"VAMOS - Assignment #1a<br /><br />This proposal is in response to PAR-18-007: Dissemination and Implementation Research in Health and aligns with the call to identify and develop innovative approaches to address barriers and facilitators across multi-levels (patient, provider, organization) that influence the implementation of the prenatal oral health guidelines into clinical practice. This proposal also aligns with the NIDCR’s strategic plan objectives: 2-2, 3-1, and 3-2.<br /><br />Poor oral health is a silent epidemic due to its prevalence and missed prevention opportunities,1 with oral-systemic health conditions and diseases affecting individuals across the lifecourse.2-10 Specifically, evidence has established connections between oral-systemic etiologies, such as periodontal disease and a range of systemic health outcomes and co-morbidities across the lifecourse (e.g., adverse pregnancy and birth outcomes; early childhood caries; chronic conditions such as cardiovascular disease and diabetes).3-16 Pregnancy represents a vulnerable period of increased risk with approximately 40% of women experiencing periodontal disease.11 Nonetheless, only 49% of women visited a dentist during their last pregnancy12 (compared to 66% of the general population who report seeing a dentist in the past year)13,14, and women of color and of lower income are more likely to need a dentist during pregnancy, but less likely to receive care.12  <br /><br />In response to the significance of prenatal oral health on maternal and child outcomes and this unique window of opportunity for intervention, the American College of Obstetricians and Gynecologists (ACOG) and the American Dental Association (ADA) co-endorsed the Oral Health Care during Pregnancy: A National Consensus Statement which provides prenatal and oral health providers guidance for oral health promotion.15 However, translation of these guidelines into practice specifically among prenatal providers remains a critically gap, with noted barriers identified, including those from studies led the PI, such as: unawareness of the guidelines; lack of oral health knowledge and training; lack of time; lack of motivation; and lack of behavioral skills to assess, advise and refer patients on oral health issues.16 This may result from the lack of training or resources, or due to other implementation barriers that are currently being explored in ongoing NIH-funded study of the PI.17-25 Furthermore, a paucity of research has been conducted exploring other stakeholders’ roles in guideline implementation; although, previous research from the PI has identified potential roles of other clinical team members (e.g., office staff, medical assistants, nurses) in collectively assessing, advising, documenting and referring patients on oral health issues, including engaging patients in reporting oral health history and current concerns and in self-directed oral health education, such as through the use of an eHealth application.19,22,24,25 <br /><br />Although studying the most appropriate and effective implementation strategies with regards to these guidelines is important for both provider types, this research will initially focus on PPs as a first step to capitalize on a critical period and optimal time to improve oral-systemic health across the lifecourse. For example, more than 70% of U.S. women have reported receiving prenatal care in the first trimester;26 provider recommendation is the most trusted source for information and lack of referral from a PP to an OHP can influence pregnant women’s decisions to seek oral healthcare;27 and PPs are often the “first line”15 in assessing, reinforcing and referring with regards to oral health issues.15,26,28-30 Although 98% of women receive at least one prenatal visit31, most report not receiving oral health information during a prenatal visit,32 and thus lack oral health knoweldge16,32-35 and remain unengaged in such care.36 Moreover, a systematic review led by the PI found that existing prenatal oral health interventions are limited, not theory or evidence-based, and lack rigorous evaluation.37 Those focused on prenatal providers are often limited to passive information;38-41 and of the patient-centered interventions, few concentrate on maternal behaviors and outcomes,42-45 and primarily examine children’s oral health.46-50 Given the significant gaps in oral health promotion among providers and patients, multilevel interventions that implement oral health promotion into the prenatal clinical system by addressing patient, provider and organizational level processes and outcomes are needed to facilitate guidelines implementation, engage patients, and improve quality of care.  <br /><br />Multilevel interventions that include providers and organizations in a participatory nature in devising strategies to integrate evidence into practice have been successful in improving clinical practice.51-53 Although results from systematic literature reviews of implementation interventions provide models for multidisciplinary and multi-site team-based care, few have linked such interventions to improved patient outcomes or focused on oral health.51-53 One theory-led systematic review concluded that guideline implementation was effective when education was combined with routine monitoring and feedback for clinicians.51 Limitations to previous multilevel interventions that focused on quality improvement were a lack of patient input, limited use of theory, poor quality design and minimal use of technology and innovative solutionsMoreover, evidence-based strategies on guideline implementation have determined that effective interventions directly involve end-users in the development and as targets of the intervention;54,55 and prospectively identified barriers to change prior to intervention development and adaptation into practice,55 which this proposal accomplishes.<br /><br />Our long-term goal is to increase oral-systemic prevention efforts during pregnancy, thus improving the health and well-being among women and children and decreasing risk for adverse health outcomes across the lifespan. The objective of this proposal is to determine promising implementation strategies to facilitate the translation of the prenatal oral health guidelines into routine prenatal care clinical practice. The objective will be achieved through the following specific aims:<br /><br />Aim 1: Assess current implementation conditions (i.e., evidence, context and facilitation) and preferred intervention characteristics needed to increase adoption and integration of the prenatal oral health guidelines into the healthcare delivery system among clinic stakeholders (e.g., prenatal provider, nurse, medical assistants, office staff, administrators). Methods include interviews and focus groups with a convenience sample of prenatal care clinic stakeholders (e.g., prenatal provider, nurse, medical assistants, office staff/administrators; n=45 staff across 3 clinical sites) and clinical observations. Interview and focus group guide instruments will be developed based on the PARIHS (e.g., evidence; context; facilitation) and DOI (e.g., intervention characteristics) frameworks to understand the current implementation conditions and preferred intervention characteristics.<br /><br />Aim 2: Identify promising implementation strategies to facilitate the adoption and integration of the prenatal oral health guidelines into the healthcare delivery system among clinic stakeholders (e.g., prenatal provider, nurse, medical assistants, office staff, administrators). Methods used in this phase will include a modified Delphi technique to identify and prioritize promising implementation strategies given Aim 1 findings, existing literature/known strategies (ERIC)47 and consultation with the Scientific and Practice Advisory Boards.<br /><br />Aim 3: Develop and evaluate the potential fit and impact of the implementation strategies with regards to the integration of prenatal oral health that considers the complex healthcare delivery contexts on system-level (i.e., patient, provider, clinic) outcomes. Guided by three implementation frameworks (PARIHS; DOI; CFIR), methods include development and evaluation of intervention strategies (e.g., provider training, decision supports, checklists, audit and feedback) with regards to usability, feasibility and preliminary impacts on patient, provider and clinic outcomes (e.g., staff practice behaviors; clinic team communication, documentation and referral processes; patient-provider communication; patients’ overall satisfaction with oral health promotion during prenatal care visit) via surveys and follow-up interviews with patients (n=15) and clinic staff (n=45). <br /><br />This innovative, timely, multidisciplinary, and theory-driven project will enhance existing knowledge of implementation science and will guide future interventions that aim to implement evidence-based and multidisciplinary practice guidelines. Findings can also serve as a feedback loop to inform effective dissemination modalities, while accounting for the complex systems influencing the dissemination and implementation process. This study is significant as the Institute of Medicine has recognized the importance of guideline implementation as a strategy to improve health services in the U.S.56 Moreover, improving prenatal oral-systemic health has been identified as a priority area by the NIH Office of Research on Women’s Health and is reflected in the cross-cutting Healthy People 2020 topics (oral health; maternal and child health).57 58 This line of inquiry has the potential to significantly improve maternal and child oral-systemic health across the lifecourse and can be applied to other health issues that necessitate effective dissemination and implementation strategies.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"c70c102b4efeee2f2f0fa87bd66e279e";}s:32:"0a909ad86ae1c3f8a46639ff4087829d";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535695497;}s:3:"raw";s:2383:"This is an interesting proposal targeting a provider group (i.e. SLPs) who have not typically been the focus of services or implementation research.  Characterizing routine care delivered by SLPs in the context of Part C services should help map the practice gap to identify care improvement targets. Please see below for a summary of questions/recommendations to further develop the logic and methods.
1.	Further specify of the target population – It seems that the target population is very broad – children age 3 with DD and receiving Part C services.   I recommend considering a more specific group so a more well-defined intervention model could be specified.  Consider selecting a subgroup for whom there are strong efficacy data on parent mediated interventions (e.g., diagnosed with/at risk for autism (or social communication deficits); speech/language delay).  
2.	What is the clinical intervention you will teach SLPs to deliver?  What is the evidence?  It is important to demonstrate that it is ready for scale up and focus on implementation.
3.	More explicitly describe practice or implementation gap – although there is strong efficacy for parent mediated interventions, extent data indicate that parents are not routinely included in SL therapy.  
4.	What is the implementation framework that will guides the rationale and methods?
5.	Rationale for Aim 1 – although data suggest that parents aren’t included, there is limited knowledge about what routine care actually looks like.  This is important to identify care improvement targets. 
6.	Aim 2 – It seems like this aim is about developing a training model.  There are a lot of data on effective training models already and training is just one type of implementation strategy (See Byron Powell’s work on different implementation strategies).  Could Aim 1 provided data on the content of training (beyond the intervention training) and what implementation strategies are relevant and feasible? 
7.	Aims 3 & 4 study seems really ambitious and potentially premature. What is the rationale for a SMART design?  Is that feasible?  Furthermore, what is the rationale for a Type 2 trial? Consider whether there are existing effectiveness data to suggest that the next step is one where effectiveness and implementation outcomes are given equal weight.  
I look forward to seeing how this proposal develops.
";s:5:"xhtml";s:2426:"This is an interesting proposal targeting a provider group (i.e. SLPs) who have not typically been the focus of services or implementation research.  Characterizing routine care delivered by SLPs in the context of Part C services should help map the practice gap to identify care improvement targets. Please see below for a summary of questions/recommendations to further develop the logic and methods.<br />1.	Further specify of the target population – It seems that the target population is very broad – children age 3 with DD and receiving Part C services.   I recommend considering a more specific group so a more well-defined intervention model could be specified.  Consider selecting a subgroup for whom there are strong efficacy data on parent mediated interventions (e.g., diagnosed with/at risk for autism (or social communication deficits); speech/language delay).  <br />2.	What is the clinical intervention you will teach SLPs to deliver?  What is the evidence?  It is important to demonstrate that it is ready for scale up and focus on implementation.<br />3.	More explicitly describe practice or implementation gap – although there is strong efficacy for parent mediated interventions, extent data indicate that parents are not routinely included in SL therapy.  <br />4.	What is the implementation framework that will guides the rationale and methods?<br />5.	Rationale for Aim 1 – although data suggest that parents aren’t included, there is limited knowledge about what routine care actually looks like.  This is important to identify care improvement targets. <br />6.	Aim 2 – It seems like this aim is about developing a training model.  There are a lot of data on effective training models already and training is just one type of implementation strategy (See Byron Powell’s work on different implementation strategies).  Could Aim 1 provided data on the content of training (beyond the intervention training) and what implementation strategies are relevant and feasible? <br />7.	Aims 3 &amp; 4 study seems really ambitious and potentially premature. What is the rationale for a SMART design?  Is that feasible?  Furthermore, what is the rationale for a Type 2 trial? Consider whether there are existing effectiveness data to suggest that the next step is one where effectiveness and implementation outcomes are given equal weight.  <br />I look forward to seeing how this proposal develops.";s:6:"parent";s:32:"984b8a2ecf57327488db61c93b89f124";s:7:"replies";a:1:{i:0;s:32:"44d7d1dcd6142cb056b89d46d0b926cc";}s:4:"show";b:1;s:3:"cid";s:32:"0a909ad86ae1c3f8a46639ff4087829d";}s:32:"b6fddfb762c53f61d6544aacc26f17fc";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535695558;}s:3:"raw";s:1562:"This is an interesting proposal aimed to improve school services for preschool children with autism.  This targets a priority population and service setting. Overall, increased specificity will strengthen the significance and impact of the study. I recommend considering the following questions/comments as you further develop the proposal.
1.	As currently written, Aim 1 seems like a method or approach rather that a scientific aim.  It is not clear how a generalizable model of partnership will be generated and how that would contribute to the literature.  It seems that there are existing partnership models that have already been developed that could be applied in this proposal and don’t require a separate aim. 
2.	Further specificity in the design and approach for each aim would be helpful.  As written, I’m a bit unclear on what the research activities will be and what design is being used. 
3.	In aim 2a, how will you identify promising practices?  Does that involve collecting outcome data on the students? Or is aim really focused on identifying overlap between routine care and EBI?  Could this be characterized as conducting an observational study to examine the impact of current practices (which strategies are associated with improved child outcomes) and consistencies and inconsistencies with EB peer interaction interventions?
4.	How are Aim 3 and Aim 4 distinct?
5.	What is the rationale for practice guidelines vs an intervention package?  Are guidelines enough to change practice?
I look forward to seeing how this proposal develops.
";s:5:"xhtml";s:1591:"This is an interesting proposal aimed to improve school services for preschool children with autism.  This targets a priority population and service setting. Overall, increased specificity will strengthen the significance and impact of the study. I recommend considering the following questions/comments as you further develop the proposal.<br />1.	As currently written, Aim 1 seems like a method or approach rather that a scientific aim.  It is not clear how a generalizable model of partnership will be generated and how that would contribute to the literature.  It seems that there are existing partnership models that have already been developed that could be applied in this proposal and don’t require a separate aim. <br />2.	Further specificity in the design and approach for each aim would be helpful.  As written, I’m a bit unclear on what the research activities will be and what design is being used. <br />3.	In aim 2a, how will you identify promising practices?  Does that involve collecting outcome data on the students? Or is aim really focused on identifying overlap between routine care and EBI?  Could this be characterized as conducting an observational study to examine the impact of current practices (which strategies are associated with improved child outcomes) and consistencies and inconsistencies with EB peer interaction interventions?<br />4.	How are Aim 3 and Aim 4 distinct?<br />5.	What is the rationale for practice guidelines vs an intervention package?  Are guidelines enough to change practice?<br />I look forward to seeing how this proposal develops.";s:6:"parent";s:32:"655c229753261f71b54189234d3b11cb";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"b6fddfb762c53f61d6544aacc26f17fc";}s:32:"44d7d1dcd6142cb056b89d46d0b926cc";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"mroberts";s:4:"name";s:13:"Megan Roberts";s:4:"mail";s:32:"megan.y.roberts@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535740609;}s:3:"raw";s:234:"This is really great feedback - thanks so much! Lots to think about. 

What is the best mechanism for asking additional questions? Would it be in this forum or should I save my list of questions for the in-person meeting in December? ";s:5:"xhtml";s:243:"This is really great feedback - thanks so much! Lots to think about. <br /><br />What is the best mechanism for asking additional questions? Would it be in this forum or should I save my list of questions for the in-person meeting in December?";s:6:"parent";s:32:"0a909ad86ae1c3f8a46639ff4087829d";s:7:"replies";a:1:{i:0;s:32:"4eeedb8b7e4cfbaada2c2eb40ad980dd";}s:4:"show";b:1;s:3:"cid";s:32:"44d7d1dcd6142cb056b89d46d0b926cc";}s:32:"4eeedb8b7e4cfbaada2c2eb40ad980dd";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535754445;}s:3:"raw";s:168:"I'm glad you found this feedback helpful.  You are welcome to ask questions on this forum and we should should also have some time during in person meeting and calls.

";s:5:"xhtml";s:171:"I&#039;m glad you found this feedback helpful.  You are welcome to ask questions on this forum and we should should also have some time during in person meeting and calls.";s:6:"parent";s:32:"44d7d1dcd6142cb056b89d46d0b926cc";s:7:"replies";a:1:{i:0;s:32:"dd31c5cc984ebd9b2bb961143ab2fd0e";}s:4:"show";b:1;s:3:"cid";s:32:"4eeedb8b7e4cfbaada2c2eb40ad980dd";}s:32:"dd31c5cc984ebd9b2bb961143ab2fd0e";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"dbdemner";s:4:"name";s:20:"Dara Blachman-Demner";s:4:"mail";s:28:"dara.blachman-demner@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536088372;}s:3:"raw";s:512:"Thanks, Megan and Lauren. Yes, please consider this forum the main place for your dialogue. I will take this opportunity to encourage trainees to respond to each other as well. We find that is where much of the interesting learning can happen. 

And, yes you will have a few calls and the in person meeting as well. If you have specific questions that you would like to be "banked" for the all trainee/faculty call on October 9th please feel free to email them to me directly at TIDIRH@nih.gov 

hope this helps!";s:5:"xhtml";s:542:"Thanks, Megan and Lauren. Yes, please consider this forum the main place for your dialogue. I will take this opportunity to encourage trainees to respond to each other as well. We find that is where much of the interesting learning can happen. <br /><br />And, yes you will have a few calls and the in person meeting as well. If you have specific questions that you would like to be &quot;banked&quot; for the all trainee/faculty call on October 9th please feel free to email them to me directly at TIDIRH@nih.gov <br /><br />hope this helps!";s:6:"parent";s:32:"4eeedb8b7e4cfbaada2c2eb40ad980dd";s:7:"replies";a:1:{i:0;s:32:"a0589967bfbb2fe5afc844d6041f689a";}s:4:"show";b:1;s:3:"cid";s:32:"dd31c5cc984ebd9b2bb961143ab2fd0e";}s:32:"a0589967bfbb2fe5afc844d6041f689a";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536088856;}s:3:"raw";s:13:"Thanks, Dara.";s:5:"xhtml";s:13:"Thanks, Dara.";s:6:"parent";s:32:"dd31c5cc984ebd9b2bb961143ab2fd0e";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"a0589967bfbb2fe5afc844d6041f689a";}s:32:"9c93c3e30b3cc19e07d945444ebbedd9";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"rsturke";s:4:"name";s:13:"Rachel Sturke";s:4:"mail";s:25:"sturkerachel@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536351870;}s:3:"raw";s:440:"This is a really interesting proposal.  I think it would be useful to think about a framework that would guide your methods.  I also think it will be helpful to think through the implementation outcomes you are most interested in looking at - I think this will help anchor the study as an implementation science study as well as hone your specific research questions. But great job!  And I look forward to seeing how this proposal develops!";s:5:"xhtml";s:440:"This is a really interesting proposal.  I think it would be useful to think about a framework that would guide your methods.  I also think it will be helpful to think through the implementation outcomes you are most interested in looking at - I think this will help anchor the study as an implementation science study as well as hone your specific research questions. But great job!  And I look forward to seeing how this proposal develops!";s:6:"parent";s:32:"d2441dff19c4863903b873c12bb8e2aa";s:7:"replies";a:1:{i:0;s:32:"acec9ad2efa27e0e0d721068c4190bfe";}s:4:"show";b:1;s:3:"cid";s:32:"9c93c3e30b3cc19e07d945444ebbedd9";}s:32:"75afb62e51a6f9d766a154d1c3cdca9e";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"rsturke";s:4:"name";s:13:"Rachel Sturke";s:4:"mail";s:25:"sturkerachel@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536352272;}s:3:"raw";s:659:"This is a really thoughtful, thorough, and interesting proposal.  Great job!  Given the multi-level complexity, I think it'll be really useful to think carefully about the implementation outcomes you are most interested in.  I'm interested in hearing your thoughts on whether there's evidence that guidelines will change practice and/or if you think there are other interventions that could be necessary.  Finally, I do think what you are proposing is a hefty lift and complex and will require you to think carefully about how findings from each aim relate to each other.  Overall, great job and I look forward to seeing how your thinking and proposal evolve!";s:5:"xhtml";s:674:"This is a really thoughtful, thorough, and interesting proposal.  Great job!  Given the multi-level complexity, I think it&#039;ll be really useful to think carefully about the implementation outcomes you are most interested in.  I&#039;m interested in hearing your thoughts on whether there&#039;s evidence that guidelines will change practice and/or if you think there are other interventions that could be necessary.  Finally, I do think what you are proposing is a hefty lift and complex and will require you to think carefully about how findings from each aim relate to each other.  Overall, great job and I look forward to seeing how your thinking and proposal evolve!";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"56200e868482f48b1e50bf30e518a926";}s:4:"show";b:1;s:3:"cid";s:32:"75afb62e51a6f9d766a154d1c3cdca9e";}s:32:"1dbedbfc039b3a3500a4fa392c609437";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"cvamos";s:4:"name";s:12:"Cheryl Vamos";s:4:"mail";s:21:"cvamos@health.usf.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536375287;}s:3:"raw";s:2781:"VAMOS – Assignment #2

1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

No, I will not be measuring or monitoring fidelity with which the evidence-based intervention is delivered for this particular proposal. First, this proposal focuses on identifying and evaluating which implementation strategies show promise in being effective in facilitating providers’ adherence to the prenatal oral health guidelines. Second, currently there are no precise protocol(s) with which the criterion-specific practice behaviors are to be delivered (e.g., no specific prescription/instructions for providers with how best to (1) assess, (2) advise, and (3) refer pregnant patients on oral health issues).  Hopefully, this line of ongoing research will provide the evidence that certain levels of fidelity with identified intervention strategies are associated with patient and provider outcomes.


2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

Yes, most likely adaptions will need to be made to the evidence-based intervention. In this proposal, the intervention in a general involves facilitating providers in implementing the prenatal oral health guidelines into routine prenatal care visits (assess, advise and refer patients on oral health issues). Within these 3 broad practice behaviors, there are sub-practice behaviors (e.g., assess – take an oral health history; assess – visually look in patients mouth for potential signs of an oral health problem; advise – patient on good oral hygiene behaviors; advise – patient on importance of seeing a dentist during pregnancy). Thus, there could be variations in how this oral health promotion (assess, advise, refer) is implemented within the prenatal care practice setting. This could include (1) by whom (e.g., nurse, MA, eHealth app; front desk staff); (2) by what (e.g., content; context); (3) level of delivery (e.g., patient; provider; clinic system); (4) context (e.g., setting – private prenatal care practice; federally qualified health center); and (5) nature of content modifications (e.g., tailoring; re-ordering; shortening). During intervention development, careful consideration to potential adaptions needed given patient, provider, clinic, and other contextual factors will need to be made. It is unknown what process would be used to guide those adaptions at this time (to be determined!).
";s:5:"xhtml";s:2825:"VAMOS – Assignment #2<br /><br />1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />No, I will not be measuring or monitoring fidelity with which the evidence-based intervention is delivered for this particular proposal. First, this proposal focuses on identifying and evaluating which implementation strategies show promise in being effective in facilitating providers’ adherence to the prenatal oral health guidelines. Second, currently there are no precise protocol(s) with which the criterion-specific practice behaviors are to be delivered (e.g., no specific prescription/instructions for providers with how best to (1) assess, (2) advise, and (3) refer pregnant patients on oral health issues).  Hopefully, this line of ongoing research will provide the evidence that certain levels of fidelity with identified intervention strategies are associated with patient and provider outcomes.<br /><br /><br />2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />Yes, most likely adaptions will need to be made to the evidence-based intervention. In this proposal, the intervention in a general involves facilitating providers in implementing the prenatal oral health guidelines into routine prenatal care visits (assess, advise and refer patients on oral health issues). Within these 3 broad practice behaviors, there are sub-practice behaviors (e.g., assess – take an oral health history; assess – visually look in patients mouth for potential signs of an oral health problem; advise – patient on good oral hygiene behaviors; advise – patient on importance of seeing a dentist during pregnancy). Thus, there could be variations in how this oral health promotion (assess, advise, refer) is implemented within the prenatal care practice setting. This could include (1) by whom (e.g., nurse, MA, eHealth app; front desk staff); (2) by what (e.g., content; context); (3) level of delivery (e.g., patient; provider; clinic system); (4) context (e.g., setting – private prenatal care practice; federally qualified health center); and (5) nature of content modifications (e.g., tailoring; re-ordering; shortening). During intervention development, careful consideration to potential adaptions needed given patient, provider, clinic, and other contextual factors will need to be made. It is unknown what process would be used to guide those adaptions at this time (to be determined!).";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"82a7e9425c6e748253e82db0a2885b42";}s:4:"show";b:1;s:3:"cid";s:32:"1dbedbfc039b3a3500a4fa392c609437";}s:32:"4aa5081e5bf9c731fc7b2ec40a0f1b79";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"mroberts";s:4:"name";s:13:"Megan Roberts";s:4:"mail";s:32:"megan.y.roberts@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536428246;}s:3:"raw";s:4206:"ROBERTS – Assignment #2

1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

Given the complexity of evaluating the quantity and quality of caregiver-therapist interactions during home visits, fidelity will be assessed using multiple methods (observational coding from video, therapist self-report). First, we will complete the Routines and Instructional Coding Protocol (Friedman, Woods, & Salisburg, 2012) from video recordings of home-based speech-language therapy sessions to assess the how much time and in what context the caregiver is included in intervention sessions. Each video will be coded in 30-second intervals for the presence of 10 caregiver coaching strategies (direct teaching, demonstration, guided practice with feedback, caregiver practice with feedback, problem solving and/or reflection, conversation, information sharing, observation, joint interaction, and modeling). If more than one rating can be applied during the 30-second interval, the behavior that occurred for the majority of time will be rated. Second, we will complete the Triadic Implementation Scale (Basu, Salisbury, & Woods, 2007) from videos to assess the quality of interactions between the therapist and the caregiver. This empirically validated 33-item global rating scale measures the collaborative and transactional nature of caregiver-therapist interactions. Each item (e.g., did the therapist create/maintain opportunities for caregiver and the child to interact?) is rated on a 3-point likert scale (always, sometimes, never). Measuring the quality of therapist-caregiver interactions is critical given that specific interaction patterns such as encouraging and praising is positively associated with parent participation during intervention sessions (Brady et al., 2007). Lastly, we therapists will complete a two self-report checklists that are parallel to the observational methods above. Therapists will complete the Coaching Checklist (Salisbury, Cambray-Engstrom, & Woods, 2012) in order to evaluate their use of the 10 caregiver coaching behaviors from the Routines and Instructional Coding Protocol. Therapists mark if they used the strategy at any point during the session. Therapists will also complete the Relationship Building, Observation and Opportunities, Learning, and Evaluation implementation checklist (ROLE, Kashinath, Coston, Woods, 2014) to assess the quality of their interactions with caregivers. This 16-item checklist is presence or absence of specific interactional practices (e.g., encourage the caregiver to identify successes and challenges through self-assessment). These measures have been used with culturally, ethnically, and linguistically diverse families. Measuring fidelity of caregiver coaching is essential, given that higher levels of coaching behaviors are associated with greater use of responsive communication strategies by caregivers. In addition, this increased use of responsive communication strategies by caregivers is subsequently associated with higher rates of child communication (Brown & Woods, 2016). 


2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

Given that responsive interaction strategies and collaborative coaching practices are inherently adapted to meet the individual needs of children and families, we don’t anticipate the need to much adaptation. Nevertheless, we will monitor adaptations continuously using real-time tracking of therapists’ session notes. After each session note, therapists will complete a short checklist documenting any adaptations they made during the sessions and the reason for the adaption. In addition, at the end of the study, we will complete interviews with all therapists midway through and at the end of the intervention using the RE-AIM adaptation of the Stirman et al framework. 
";s:5:"xhtml";s:4265:"ROBERTS – Assignment #2<br /><br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />Given the complexity of evaluating the quantity and quality of caregiver-therapist interactions during home visits, fidelity will be assessed using multiple methods (observational coding from video, therapist self-report). First, we will complete the Routines and Instructional Coding Protocol (Friedman, Woods, &amp; Salisburg, 2012) from video recordings of home-based speech-language therapy sessions to assess the how much time and in what context the caregiver is included in intervention sessions. Each video will be coded in 30-second intervals for the presence of 10 caregiver coaching strategies (direct teaching, demonstration, guided practice with feedback, caregiver practice with feedback, problem solving and/or reflection, conversation, information sharing, observation, joint interaction, and modeling). If more than one rating can be applied during the 30-second interval, the behavior that occurred for the majority of time will be rated. Second, we will complete the Triadic Implementation Scale (Basu, Salisbury, &amp; Woods, 2007) from videos to assess the quality of interactions between the therapist and the caregiver. This empirically validated 33-item global rating scale measures the collaborative and transactional nature of caregiver-therapist interactions. Each item (e.g., did the therapist create/maintain opportunities for caregiver and the child to interact?) is rated on a 3-point likert scale (always, sometimes, never). Measuring the quality of therapist-caregiver interactions is critical given that specific interaction patterns such as encouraging and praising is positively associated with parent participation during intervention sessions (Brady et al., 2007). Lastly, we therapists will complete a two self-report checklists that are parallel to the observational methods above. Therapists will complete the Coaching Checklist (Salisbury, Cambray-Engstrom, &amp; Woods, 2012) in order to evaluate their use of the 10 caregiver coaching behaviors from the Routines and Instructional Coding Protocol. Therapists mark if they used the strategy at any point during the session. Therapists will also complete the Relationship Building, Observation and Opportunities, Learning, and Evaluation implementation checklist (ROLE, Kashinath, Coston, Woods, 2014) to assess the quality of their interactions with caregivers. This 16-item checklist is presence or absence of specific interactional practices (e.g., encourage the caregiver to identify successes and challenges through self-assessment). These measures have been used with culturally, ethnically, and linguistically diverse families. Measuring fidelity of caregiver coaching is essential, given that higher levels of coaching behaviors are associated with greater use of responsive communication strategies by caregivers. In addition, this increased use of responsive communication strategies by caregivers is subsequently associated with higher rates of child communication (Brown &amp; Woods, 2016). <br /><br /><br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />Given that responsive interaction strategies and collaborative coaching practices are inherently adapted to meet the individual needs of children and families, we don’t anticipate the need to much adaptation. Nevertheless, we will monitor adaptations continuously using real-time tracking of therapists’ session notes. After each session note, therapists will complete a short checklist documenting any adaptations they made during the sessions and the reason for the adaption. In addition, at the end of the study, we will complete interviews with all therapists midway through and at the end of the intervention using the RE-AIM adaptation of the Stirman et al framework.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"f34a30e4b718a8a4e58f66d231745561";}s:4:"show";b:1;s:3:"cid";s:32:"4aa5081e5bf9c731fc7b2ec40a0f1b79";}s:32:"56200e868482f48b1e50bf30e518a926";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"cvamos";s:4:"name";s:12:"Cheryl Vamos";s:4:"mail";s:21:"cvamos@health.usf.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536454627;}s:3:"raw";s:1136:"Hello Rachel: 
Thank you for your comments! I am definitely looking forward to learning more about implementation outcomes in this training to help guide me in identifying which one(s) may be best suited for this proposal. I'm also eager to further discuss ways to best craft the argument in the proposal about the potential of guidelines in changing providers' practice behaviors (and patients' behaviors). This is something I'm struggling with in terms of how to write this up in grant proposals. There is evidence about the importance and significance of the guidelines (oral health promotion during pregnancy), but I'm not sure how best to focus on the utility and effectiveness of the guidelines on various outcomes. For instance, is this something I can focus on in a proposal/future research study (whether implementation strategies that facilitate guideline implementation leads to behavior change); or is this something that needs to come first before I submit a to the NIH "D&I" mechanism? Thank you for the positive remarks and the encouragement. I'm greatly looking forward to learning more about D&I in this course!  Cheryl";s:5:"xhtml";s:1189:"Hello Rachel: <br />Thank you for your comments! I am definitely looking forward to learning more about implementation outcomes in this training to help guide me in identifying which one(s) may be best suited for this proposal. I&#039;m also eager to further discuss ways to best craft the argument in the proposal about the potential of guidelines in changing providers&#039; practice behaviors (and patients&#039; behaviors). This is something I&#039;m struggling with in terms of how to write this up in grant proposals. There is evidence about the importance and significance of the guidelines (oral health promotion during pregnancy), but I&#039;m not sure how best to focus on the utility and effectiveness of the guidelines on various outcomes. For instance, is this something I can focus on in a proposal/future research study (whether implementation strategies that facilitate guideline implementation leads to behavior change); or is this something that needs to come first before I submit a to the NIH &quot;D&amp;I&quot; mechanism? Thank you for the positive remarks and the encouragement. I&#039;m greatly looking forward to learning more about D&amp;I in this course!  Cheryl";s:6:"parent";s:32:"75afb62e51a6f9d766a154d1c3cdca9e";s:7:"replies";a:1:{i:0;s:32:"caa57fe3c705c833211132f76d08332a";}s:4:"show";b:1;s:3:"cid";s:32:"56200e868482f48b1e50bf30e518a926";}s:32:"d5d96757e6164d5fcd6f72891629dbba";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"jpatterson";s:4:"name";s:19:"Jacquelyn Patterson";s:4:"mail";s:28:"jackie_patterson@med.unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1536610338;s:8:"modified";i:1536610610;}s:3:"raw";s:4202:"PATTERSON-ASSIGNMENT #2

1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

This proposal seeks to improve compliance to an evidence-based newborn resuscitation algorithm designed for low-resource settings entitled Helping Babies Breathe. There is strong evidence to support the evidence-based practices emphasized in the Helping Babies Breathe algorithm. For example, the routine use of tactile stimulation reduces newborn mortality from birth asphyxia up to 10%. Timely bag mask ventilation reduces mortality by an additional 30%.

We will achieve improved compliance with Helping Babies Breathe through implementation of the evidence-based intervention of audit and feedback. Audit and feedback will be facilitated by our purpose-designed mobile health application called NeoWatch. NeoWatch will allow a delivery room observer to document the actions of a provider during a resuscitation and the respiratory status of the newborn. Following documentation of the resuscitation, our intervention will deliver three types of feedback: 1) real-time feedback to providers during a resuscitation given by NeoWatch; 2) post-hoc feedback to providers following a resuscitation given by NeoWatch; 3) feedback to groups of providers led by the head nurse midwife.

For this proposal, there are two aspects of fidelity that we will measure. First, we will measure fidelity to the evidence-based resuscitation algorithm Helping Babies Breathe. We will use NeoWatch to collect this data. For example, we will measure the time to initiation of bag mask ventilation as recorded by NeoWatch during a resuscitation. We will also measure percent of newborns who receive stimulation to breathe, percent of cord clamping episodes that occur following one minute of life and percent of newborns placed skin to skin. Second, we will measure fidelity to the evidence-based behavioral intervention of audit and feedback. We will review the medical charts to document the number of total births occurring at the hospital each month. Then, we will interrogate NeoWatch to document all episodes in which real-time and individual post-hoc feedback was provided. We will also have the head nurse midwife record all occurrences of group feedback.

2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

We do not anticipate making any adaptations to the Helping Babies Breathe resuscitation algorithm. This algorithm is evidence-based, and has been proven to reduce perinatal mortality. However, we do anticipate making adaptations to the audit and feedback strategy we have developed to improve compliance with this algorithm. During aim two, usability testing, we will elicit recommendations from the midwives for modifications to the intervention such as changes to when and who observes births, and when and how feedback is provided. We will refine the intervention accordingly. Additionally, as part of aim three, we will evaluate the feasibility of our implementation strategy at the end of our pilot test. We will administer quantitative questionnaires to all study subjects consisting of previously validated five-item scales for feasibility, acceptability and appropriateness (Weiner et al 2017). We will also administer an abbreviated questionnaire on acceptability to a convenience sample of mothers. Finally, we will conduct qualitative focus group discussions to explore these three implementation outcomes as well as facilitators and barriers to implementation of NeoWatch, modifications to the intervention, and opportunities for sustainability. If preliminary effectiveness of this intervention is established during the pilot test, this information will be used to make any additional modifications to the audit-feedback implementation strategy before more definitive testing in a randomized trial. 
";s:5:"xhtml";s:4260:"PATTERSON-ASSIGNMENT #2<br /><br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />This proposal seeks to improve compliance to an evidence-based newborn resuscitation algorithm designed for low-resource settings entitled Helping Babies Breathe. There is strong evidence to support the evidence-based practices emphasized in the Helping Babies Breathe algorithm. For example, the routine use of tactile stimulation reduces newborn mortality from birth asphyxia up to 10%. Timely bag mask ventilation reduces mortality by an additional 30%.<br /><br />We will achieve improved compliance with Helping Babies Breathe through implementation of the evidence-based intervention of audit and feedback. Audit and feedback will be facilitated by our purpose-designed mobile health application called NeoWatch. NeoWatch will allow a delivery room observer to document the actions of a provider during a resuscitation and the respiratory status of the newborn. Following documentation of the resuscitation, our intervention will deliver three types of feedback: 1) real-time feedback to providers during a resuscitation given by NeoWatch; 2) post-hoc feedback to providers following a resuscitation given by NeoWatch; 3) feedback to groups of providers led by the head nurse midwife.<br /><br />For this proposal, there are two aspects of fidelity that we will measure. First, we will measure fidelity to the evidence-based resuscitation algorithm Helping Babies Breathe. We will use NeoWatch to collect this data. For example, we will measure the time to initiation of bag mask ventilation as recorded by NeoWatch during a resuscitation. We will also measure percent of newborns who receive stimulation to breathe, percent of cord clamping episodes that occur following one minute of life and percent of newborns placed skin to skin. Second, we will measure fidelity to the evidence-based behavioral intervention of audit and feedback. We will review the medical charts to document the number of total births occurring at the hospital each month. Then, we will interrogate NeoWatch to document all episodes in which real-time and individual post-hoc feedback was provided. We will also have the head nurse midwife record all occurrences of group feedback.<br /><br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />We do not anticipate making any adaptations to the Helping Babies Breathe resuscitation algorithm. This algorithm is evidence-based, and has been proven to reduce perinatal mortality. However, we do anticipate making adaptations to the audit and feedback strategy we have developed to improve compliance with this algorithm. During aim two, usability testing, we will elicit recommendations from the midwives for modifications to the intervention such as changes to when and who observes births, and when and how feedback is provided. We will refine the intervention accordingly. Additionally, as part of aim three, we will evaluate the feasibility of our implementation strategy at the end of our pilot test. We will administer quantitative questionnaires to all study subjects consisting of previously validated five-item scales for feasibility, acceptability and appropriateness (Weiner et al 2017). We will also administer an abbreviated questionnaire on acceptability to a convenience sample of mothers. Finally, we will conduct qualitative focus group discussions to explore these three implementation outcomes as well as facilitators and barriers to implementation of NeoWatch, modifications to the intervention, and opportunities for sustainability. If preliminary effectiveness of this intervention is established during the pilot test, this information will be used to make any additional modifications to the audit-feedback implementation strategy before more definitive testing in a randomized trial.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"1b92d707df4b4445adeec45e8a213055";}s:4:"show";b:1;s:3:"cid";s:32:"d5d96757e6164d5fcd6f72891629dbba";}s:32:"acec9ad2efa27e0e0d721068c4190bfe";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"jpatterson";s:4:"name";s:19:"Jacquelyn Patterson";s:4:"mail";s:28:"jackie_patterson@med.unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1536611776;s:8:"modified";i:1536611794;}s:3:"raw";s:1020:"Rachel,

Thanks for these comments. I have tried to be more specific about the implementation outcomes--and previously validated measures--I would like to measure in assignment #2. See if this communicates!

I am still struggling with a framework to guide my methods. Perhaps Proctor's conceptual model of implementation research would fit? The intervention strategy is helping babies breathe, and the implementation strategy at the individual provider level (for the most part!). Then I am focusing on a proximal service outcome of effectiveness (breathing in the newborn) and a few implementation outcomes (feasibility, acceptability, appropriateness). I realize I'm articulating it after selecting the outcomes I would measure(!), but is this what you are talking about with selecting a framework to guide my methods? Are there other frameworks that stand out to you that would be relevant? The other ones I am familiar with such as CFIR, Re-AIM etc don't seem to apply to the concept I have crafted above...

Jackie ";s:5:"xhtml";s:1064:"Rachel,<br /><br />Thanks for these comments. I have tried to be more specific about the implementation outcomes--and previously validated measures--I would like to measure in assignment #2. See if this communicates!<br /><br />I am still struggling with a framework to guide my methods. Perhaps Proctor&#039;s conceptual model of implementation research would fit? The intervention strategy is helping babies breathe, and the implementation strategy at the individual provider level (for the most part!). Then I am focusing on a proximal service outcome of effectiveness (breathing in the newborn) and a few implementation outcomes (feasibility, acceptability, appropriateness). I realize I&#039;m articulating it after selecting the outcomes I would measure(!), but is this what you are talking about with selecting a framework to guide my methods? Are there other frameworks that stand out to you that would be relevant? The other ones I am familiar with such as CFIR, Re-AIM etc don&#039;t seem to apply to the concept I have crafted above...<br /><br />Jackie";s:6:"parent";s:32:"9c93c3e30b3cc19e07d945444ebbedd9";s:7:"replies";a:1:{i:0;s:32:"a0bcce854de5d30f054587a200cd37c0";}s:4:"show";b:1;s:3:"cid";s:32:"acec9ad2efa27e0e0d721068c4190bfe";}s:32:"7eed598861a1026b1ea32d63e9933538";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"dbdemner";s:4:"name";s:20:"Dara Blachman-Demner";s:4:"mail";s:28:"dara.blachman-demner@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536694402;}s:3:"raw";s:1559:"Rachel,

Thank you for this well-thought out proposal on an important and interesting topic and for your patience in getting written feedback. I am offering some thoughts here to complement the oral feedback from your faculty.

The proposal seems to be for primarily an effectiveness trial with some preliminary data being collected about implementation barriers and opportunities. This is fine, but I would encourage you to think about some implementation focused questions that you might be poised to answer either in this trial or in the next step. Based on your data from the efficacy trial do you have any insight or ideas about some of the key individual or organizational factors that might be at play here ir any implementation strategies that might work best?  Is the population you are serving here similar to the population in the efficacy study (e.g., low income Latino) or will it be more diverse? how might that matter in your consideration of implementation strategies? 

From an analysis perspective, you have a number of variables (prenatal or pediatric, academic or community, patient population). given your sample size of 4 clinics do you anticipate being able to answer any questions about differences among these settings? What will be the level of randomization? It is unclear how if at all the prenatal and pediatric clinics are linked in terms of the intervention model. 

Do you have an existing commitment/relationship with the settings you plan to implement in? 

I look forward to hearing how this progresses. Great work!

Dara 

";s:5:"xhtml";s:1616:"Rachel,<br /><br />Thank you for this well-thought out proposal on an important and interesting topic and for your patience in getting written feedback. I am offering some thoughts here to complement the oral feedback from your faculty.<br /><br />The proposal seems to be for primarily an effectiveness trial with some preliminary data being collected about implementation barriers and opportunities. This is fine, but I would encourage you to think about some implementation focused questions that you might be poised to answer either in this trial or in the next step. Based on your data from the efficacy trial do you have any insight or ideas about some of the key individual or organizational factors that might be at play here ir any implementation strategies that might work best?  Is the population you are serving here similar to the population in the efficacy study (e.g., low income Latino) or will it be more diverse? how might that matter in your consideration of implementation strategies? <br /><br />From an analysis perspective, you have a number of variables (prenatal or pediatric, academic or community, patient population). given your sample size of 4 clinics do you anticipate being able to answer any questions about differences among these settings? What will be the level of randomization? It is unclear how if at all the prenatal and pediatric clinics are linked in terms of the intervention model. <br /><br />Do you have an existing commitment/relationship with the settings you plan to implement in? <br /><br />I look forward to hearing how this progresses. Great work!<br /><br />Dara";s:6:"parent";s:32:"7600a8ddee4343dda59bf3d276f3cae1";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"7eed598861a1026b1ea32d63e9933538";}s:32:"47aa22141500718561a9ce0af9c660ec";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"dbdemner";s:4:"name";s:20:"Dara Blachman-Demner";s:4:"mail";s:28:"dara.blachman-demner@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536695340;}s:3:"raw";s:1097:"Michelle,

Thank your for your proposal on this interesting topic and for your patience in receiving feedback. I am providing some written thoughts below that should complement your faculty feedback on the call.

 It seems you have already begun work with the community stakeholders which is obviously going to be key moving forward. I also like that you are considering cost though I didn't see explicit measure of it. It was unclear to me if the goal is to develop the toolkit based strategy in one community and then test it in another or develop it and test it in both. it would also be helpful to have some idea of the types of implementation strategies that are being considered for the toolkit. Who are the stakeholders? Do they include patients? I would be curious to see some of the initial evidence on the outcomes focused on patient preferences. I can imagine that there may be challenges around acceptability and vulnerability during this sensitive time period, particularly among some marginalized communities. 

I will look forward to hearing how this develops. Great start!

Dara 

";s:5:"xhtml";s:1139:"Michelle,<br /><br />Thank your for your proposal on this interesting topic and for your patience in receiving feedback. I am providing some written thoughts below that should complement your faculty feedback on the call.<br /><br /> It seems you have already begun work with the community stakeholders which is obviously going to be key moving forward. I also like that you are considering cost though I didn&#039;t see explicit measure of it. It was unclear to me if the goal is to develop the toolkit based strategy in one community and then test it in another or develop it and test it in both. it would also be helpful to have some idea of the types of implementation strategies that are being considered for the toolkit. Who are the stakeholders? Do they include patients? I would be curious to see some of the initial evidence on the outcomes focused on patient preferences. I can imagine that there may be challenges around acceptability and vulnerability during this sensitive time period, particularly among some marginalized communities. <br /><br />I will look forward to hearing how this develops. Great start!<br /><br />Dara";s:6:"parent";s:32:"990a84f6e316a30198576886651836a4";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"47aa22141500718561a9ce0af9c660ec";}s:32:"9493d48e9321b8dbd491d4eba86c841a";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"anahmias";s:4:"name";s:15:"Allison Nahmias";s:4:"mail";s:21:"asnahmias@ucdavis.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536770225;}s:3:"raw";s:2873:"Nahmias: Assignment #2:
Based on the feedback on the specific aims and from the group call, I have begun to further refine my proposal. Rather than developing ASD social practice guidelines, I am now thinking of developing an intervention package of evidence-based strategies to support social and peer relationships that combines research and practice-based evidence. 
1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?
Yes, I am planning on measuring and monitoring the fidelity to the intervention package using observational coding of teacher use of the evidence-based strategies in classrooms. As far as I know, there is no evidence that associates the level of fidelity of specific intervention strategies with individual level outcomes, rather there is evidence of the effects of full intervention packages.  Part of my goal with this project to establish the core elements and strategies necessary for positive individual level outcomes. 
2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?
Adaptations will be considered and monitored during the development of my intervention package, first by reviewing videotapes of teachers (recorded from a prior study) who were rated as having good implementation of general recommended practices to support social and peer relationships and whose students had positive outcomes. As many of the peer-mediated EBIs have been designed and tested in environments where students with autism interact with typically developing peers, one adaptation of interest is how the evidence-based strategies apply in more restrictive educational settings (e.g., autism support classrooms, mixed-disability special education classrooms) in which opportunities to interact with typically developing peers are not as available.  Then once an initial draft of my intervention package is developed, just-in-time teaching will be used to train teachers and monitor their adaptations (via observational coding and teacher interviews) in order to further refine my intervention package of evidence-based strategies and build flexibility into the intervention package for beneficial adaptations while preserving the core evidence-based strategies.  In the initial pilot project testing my intervention package I was not planning on monitoring further adaptations, as my primary aim is to test the effectiveness of the intervention package (but I am open to suggestions if others think it would still be beneficial to monitor).       


";s:5:"xhtml";s:2888:"Nahmias: Assignment #2:<br />Based on the feedback on the specific aims and from the group call, I have begun to further refine my proposal. Rather than developing ASD social practice guidelines, I am now thinking of developing an intervention package of evidence-based strategies to support social and peer relationships that combines research and practice-based evidence. <br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br />Yes, I am planning on measuring and monitoring the fidelity to the intervention package using observational coding of teacher use of the evidence-based strategies in classrooms. As far as I know, there is no evidence that associates the level of fidelity of specific intervention strategies with individual level outcomes, rather there is evidence of the effects of full intervention packages.  Part of my goal with this project to establish the core elements and strategies necessary for positive individual level outcomes. <br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br />Adaptations will be considered and monitored during the development of my intervention package, first by reviewing videotapes of teachers (recorded from a prior study) who were rated as having good implementation of general recommended practices to support social and peer relationships and whose students had positive outcomes. As many of the peer-mediated EBIs have been designed and tested in environments where students with autism interact with typically developing peers, one adaptation of interest is how the evidence-based strategies apply in more restrictive educational settings (e.g., autism support classrooms, mixed-disability special education classrooms) in which opportunities to interact with typically developing peers are not as available.  Then once an initial draft of my intervention package is developed, just-in-time teaching will be used to train teachers and monitor their adaptations (via observational coding and teacher interviews) in order to further refine my intervention package of evidence-based strategies and build flexibility into the intervention package for beneficial adaptations while preserving the core evidence-based strategies.  In the initial pilot project testing my intervention package I was not planning on monitoring further adaptations, as my primary aim is to test the effectiveness of the intervention package (but I am open to suggestions if others think it would still be beneficial to monitor).";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"d1ee52719fd04a155a1639e5e136a3bb";}s:4:"show";b:1;s:3:"cid";s:32:"9493d48e9321b8dbd491d4eba86c841a";}s:32:"a0bcce854de5d30f054587a200cd37c0";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"rsturke";s:4:"name";s:13:"Rachel Sturke";s:4:"mail";s:25:"sturkerachel@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536776873;}s:3:"raw";s:267:"Hi Jackie,

Proctor's conceptual model could work.  Take a look here: http://dissemination-implementation.org/index.aspx.  This is a website that helps you pic out a framework or model.  You might find it useful to explore.  Let me know if it's helpful.

Best,
Rachel";s:5:"xhtml";s:302:"Hi Jackie,<br /><br />Proctor&#039;s conceptual model could work.  Take a look here: http://dissemination-implementation.org/index.aspx.  This is a website that helps you pic out a framework or model.  You might find it useful to explore.  Let me know if it&#039;s helpful.<br /><br />Best,<br />Rachel";s:6:"parent";s:32:"acec9ad2efa27e0e0d721068c4190bfe";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"a0bcce854de5d30f054587a200cd37c0";}s:32:"caa57fe3c705c833211132f76d08332a";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"rsturke";s:4:"name";s:13:"Rachel Sturke";s:4:"mail";s:25:"sturkerachel@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536777074;}s:3:"raw";s:546:"Hi Cheryl,

The struggle you raise is complicated.  I think the answer is kind of both - that you need to be able to make the case that guidelines influence behavior and that what you're looking at is implementation of guidelines and barriers/facilitators there.  But also think you could focus on more operationally in future research.  I would encourage you to engage some of the program staff at the in-person part of the training as they will be able to give you more practical advice on what they have seen in grant proposals.

Best,
Rachel
";s:5:"xhtml";s:575:"Hi Cheryl,<br /><br />The struggle you raise is complicated.  I think the answer is kind of both - that you need to be able to make the case that guidelines influence behavior and that what you&#039;re looking at is implementation of guidelines and barriers/facilitators there.  But also think you could focus on more operationally in future research.  I would encourage you to engage some of the program staff at the in-person part of the training as they will be able to give you more practical advice on what they have seen in grant proposals.<br /><br />Best,<br />Rachel";s:6:"parent";s:32:"56200e868482f48b1e50bf30e518a926";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"caa57fe3c705c833211132f76d08332a";}s:32:"82a7e9425c6e748253e82db0a2885b42";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"rsturke";s:4:"name";s:13:"Rachel Sturke";s:4:"mail";s:25:"sturkerachel@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536777526;}s:3:"raw";s:871:"Cheryl,

Great job on this assignment.  I think it would be useful for you to be more specific and unpack what the implementation strategies you refer to above.  Do you have a sense of what the range of these strategies might be?  I think thinking this through will also help you think about what adaptations might need to be made and why.  This would also force you to think more carefully about what barriers and facilitators there might be.  Stepping back, it might also be helpful for you to map out the "mechanism" or "pathway" between the guidelines and their implementation in routine prenatal care visits.  You state that the intervention is facilitating providers to implement the guidelines, but what does this facilitation consist of?  Or are you just collecting data on barriers and facilitators.  Would be helpful if you unpack this a bit more.

Best,
Rachel";s:5:"xhtml";s:916:"Cheryl,<br /><br />Great job on this assignment.  I think it would be useful for you to be more specific and unpack what the implementation strategies you refer to above.  Do you have a sense of what the range of these strategies might be?  I think thinking this through will also help you think about what adaptations might need to be made and why.  This would also force you to think more carefully about what barriers and facilitators there might be.  Stepping back, it might also be helpful for you to map out the &quot;mechanism&quot; or &quot;pathway&quot; between the guidelines and their implementation in routine prenatal care visits.  You state that the intervention is facilitating providers to implement the guidelines, but what does this facilitation consist of?  Or are you just collecting data on barriers and facilitators.  Would be helpful if you unpack this a bit more.<br /><br />Best,<br />Rachel";s:6:"parent";s:32:"1dbedbfc039b3a3500a4fa392c609437";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"82a7e9425c6e748253e82db0a2885b42";}s:32:"1b92d707df4b4445adeec45e8a213055";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"rsturke";s:4:"name";s:13:"Rachel Sturke";s:4:"mail";s:25:"sturkerachel@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536777732;}s:3:"raw";s:509:"Great job Jackie!  I think your approach to measuring fidelity is spot on, though important to keep not of the complexity of measuring the actual intervention as well as the audit and feedback piece.  Would be interesting to hear more about what adaptations you might anticipate for the audit and feedback strategy. Do you already have a sense of what barriers or facilitators there might be in implementing this strategy.  Perhaps useful for you to unpack that a bit more in an actual proposal.

Best,
Rachel";s:5:"xhtml";s:524:"Great job Jackie!  I think your approach to measuring fidelity is spot on, though important to keep not of the complexity of measuring the actual intervention as well as the audit and feedback piece.  Would be interesting to hear more about what adaptations you might anticipate for the audit and feedback strategy. Do you already have a sense of what barriers or facilitators there might be in implementing this strategy.  Perhaps useful for you to unpack that a bit more in an actual proposal.<br /><br />Best,<br />Rachel";s:6:"parent";s:32:"d5d96757e6164d5fcd6f72891629dbba";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"1b92d707df4b4445adeec45e8a213055";}s:32:"8e418ba8e58af4b72003dda91c395d1b";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"mmoniz";s:4:"name";s:14:"Michelle Moniz";s:4:"mail";s:20:"mmoniz@med.umich.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536782899;}s:3:"raw";s:2129:"MONIZ – Assignment #2

1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

Our “evidence-based practice” is new clinical care process – offering immediate postpartum LARC to interested, eligible postpartum women.  Across multiple randomized, controlled clinical trials of immediate vs. delayed LARC provision, there were some variations in how this process of care was delivered. In our preliminary work studying this care process at early adopter sites, we have documented “on the ground” variation in how care is being delivered. 

Therefore, for our pilot study, we are prospectively defining a workflow map with key stakeholders.  We will then measure fidelity to this workflow in 2 ways: 1) key stakeholder observations, collected at monthly meetings during implementation, and 2) provider and patient interviews 6 months after implementation. We will secondarily collect measures of fidelity to the workflow process via metrics in the EMR (e.g., rate of prenatal counseling about postpartum LARC). Due to time and cost constraints and concerns about patient privacy, we will NOT collect measures of fidelity via direct observation. 

2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

Over the past year, a Stakeholder Panel of about 20 individuals (including staff and patients) has met every 1-2 months to design/tailor the evidence-based intervention to our local setting. We hope this will minimize the need for ongoing adaptations. However, we will collect data on any adaptations that naturally occur using the processes described above for measuring fidelity.  We will also regularly check in with staff to see if there are workflow processes that need to be more efficient or effective. 
";s:5:"xhtml";s:2177:"MONIZ – Assignment #2<br /><br />1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />Our “evidence-based practice” is new clinical care process – offering immediate postpartum LARC to interested, eligible postpartum women.  Across multiple randomized, controlled clinical trials of immediate vs. delayed LARC provision, there were some variations in how this process of care was delivered. In our preliminary work studying this care process at early adopter sites, we have documented “on the ground” variation in how care is being delivered. <br /><br />Therefore, for our pilot study, we are prospectively defining a workflow map with key stakeholders.  We will then measure fidelity to this workflow in 2 ways: 1) key stakeholder observations, collected at monthly meetings during implementation, and 2) provider and patient interviews 6 months after implementation. We will secondarily collect measures of fidelity to the workflow process via metrics in the EMR (e.g., rate of prenatal counseling about postpartum LARC). Due to time and cost constraints and concerns about patient privacy, we will NOT collect measures of fidelity via direct observation. <br /><br />2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />Over the past year, a Stakeholder Panel of about 20 individuals (including staff and patients) has met every 1-2 months to design/tailor the evidence-based intervention to our local setting. We hope this will minimize the need for ongoing adaptations. However, we will collect data on any adaptations that naturally occur using the processes described above for measuring fidelity.  We will also regularly check in with staff to see if there are workflow processes that need to be more efficient or effective.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"0f53a41346dc3100005c9dfdee16b2d8";}s:4:"show";b:1;s:3:"cid";s:32:"8e418ba8e58af4b72003dda91c395d1b";}s:32:"9ce118d5866750013de8e81cfcaebe4a";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"rgross";s:4:"name";s:12:"Rachel Gross";s:4:"mail";s:22:"Rachel.Gross@nyumc.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536858504;}s:3:"raw";s:4915:"Gross - Assignment 2

1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

I plan to measure and monitor fidelity of our Starting Early Program (StEP) across the four implementation sites to determine that the program is being implemented as intended by the designers. StEP is a comprehensive prenatal and pediatric primary care-based early child obesity prevention program, with demonstrated impacts on infant feeding, activity and growth trajectories. Currently we do not have specific evidence that fidelity to the Starting Early Program relates to individual level outcomes. However, we do know that in the initial RCT, there was significant variation in participant intervention engagement and that intervention impact depended highly on participant attendance, suggesting the critical role of participant engagement. Studying fidelity as well as participant engagement may potentially reveal whether certain intervention components affect outcomes more than others do. 

The main intervention components include: 1) individual nutrition counseling during the prenatal and post-partum periods; and 2) Nutrition and Parenting Support Groups (NPSG) coordinated with all primary care well-child visits. The curriculum, which was ecologically informed and included elements from the health beliefs model and social learning theory, promoted behavior change by addressing perceived barriers to healthy behaviors and using interactive demonstrations and active practicing of skills. StEP specifically addressed the needs of high-risk populations by being sensitive to poverty-associated challenges, acknowledging and supporting cultural values, using a family-centered approached, creating a safe supportive environment by establishing trusting relationships between program staff and families and empowering parents by recognizing and building upon their strengths. 

Fidelity will be recorded by the nutritionist (interventionist) using a fidelity checklist after each individual session or Nutrition and Parenting Support Group. These checklists will record whether each component of the session curriculum was conducted. To provide external validity to our fidelity assessment, we will also conduct observational assessments of a subsample of intervention sessions at each of the four sites using trained observers to monitor that each intervention component was delivered. This will also allow for assessment of the context, environment, group interactions and non-verbal communications. 

2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

I plan to also monitor for adaptations that relate to the needs of each site and their patient populations. We will characterize the specific types of adaptations that are both purposefully designed and that occur throughout the study unintentionally in order to determine if adaptations affect intervention effectiveness. We will utilizing the comprehensive framework of classifying intervention adaptations (Stirman et al., 2013) to determine whether they target the context or the content of the intervention. We will also characterize when, how and why the adaptations occurred. 

Context adaptations could include changes in: 1) format (the addition of a confidential texting service and electronic medical record system communication with providers); 2) personnel (program originally delivered by registered dietitians is now being delivered by a health educator or a psychologist); and 3) population (program specifically developed for low-income Hispanic families is now being delivered to more racially and ethnically diverse patients). Content adaptations could include: 1) directly screening for psychosocial stressors and providing services as needed; and 2) adding curriculum about general parenting skills and early child development. Adaptations will be measured using real-time tracking sheets that will be assessed throughout the entire study and using qualitative semi-structured adaptations interviews with the implementation team These interviews will be planned at two time points in the study, shortly after enrollment and at the end of the outcomes data collection. 

One challenge to understanding how adaptations enhance or hinder intervention impacts is the lack of attention to characterizing the different types of modifications that are planned. Exploring the relationships between context and content adaptations and later infant feeding, activity and growth outcomes will be critical to the scaling up of this intervention. 
";s:5:"xhtml";s:4993:"Gross - Assignment 2<br /><br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />I plan to measure and monitor fidelity of our Starting Early Program (StEP) across the four implementation sites to determine that the program is being implemented as intended by the designers. StEP is a comprehensive prenatal and pediatric primary care-based early child obesity prevention program, with demonstrated impacts on infant feeding, activity and growth trajectories. Currently we do not have specific evidence that fidelity to the Starting Early Program relates to individual level outcomes. However, we do know that in the initial RCT, there was significant variation in participant intervention engagement and that intervention impact depended highly on participant attendance, suggesting the critical role of participant engagement. Studying fidelity as well as participant engagement may potentially reveal whether certain intervention components affect outcomes more than others do. <br /><br />The main intervention components include: 1) individual nutrition counseling during the prenatal and post-partum periods; and 2) Nutrition and Parenting Support Groups (NPSG) coordinated with all primary care well-child visits. The curriculum, which was ecologically informed and included elements from the health beliefs model and social learning theory, promoted behavior change by addressing perceived barriers to healthy behaviors and using interactive demonstrations and active practicing of skills. StEP specifically addressed the needs of high-risk populations by being sensitive to poverty-associated challenges, acknowledging and supporting cultural values, using a family-centered approached, creating a safe supportive environment by establishing trusting relationships between program staff and families and empowering parents by recognizing and building upon their strengths. <br /><br />Fidelity will be recorded by the nutritionist (interventionist) using a fidelity checklist after each individual session or Nutrition and Parenting Support Group. These checklists will record whether each component of the session curriculum was conducted. To provide external validity to our fidelity assessment, we will also conduct observational assessments of a subsample of intervention sessions at each of the four sites using trained observers to monitor that each intervention component was delivered. This will also allow for assessment of the context, environment, group interactions and non-verbal communications. <br /><br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />I plan to also monitor for adaptations that relate to the needs of each site and their patient populations. We will characterize the specific types of adaptations that are both purposefully designed and that occur throughout the study unintentionally in order to determine if adaptations affect intervention effectiveness. We will utilizing the comprehensive framework of classifying intervention adaptations (Stirman et al., 2013) to determine whether they target the context or the content of the intervention. We will also characterize when, how and why the adaptations occurred. <br /><br />Context adaptations could include changes in: 1) format (the addition of a confidential texting service and electronic medical record system communication with providers); 2) personnel (program originally delivered by registered dietitians is now being delivered by a health educator or a psychologist); and 3) population (program specifically developed for low-income Hispanic families is now being delivered to more racially and ethnically diverse patients). Content adaptations could include: 1) directly screening for psychosocial stressors and providing services as needed; and 2) adding curriculum about general parenting skills and early child development. Adaptations will be measured using real-time tracking sheets that will be assessed throughout the entire study and using qualitative semi-structured adaptations interviews with the implementation team These interviews will be planned at two time points in the study, shortly after enrollment and at the end of the outcomes data collection. <br /><br />One challenge to understanding how adaptations enhance or hinder intervention impacts is the lack of attention to characterizing the different types of modifications that are planned. Exploring the relationships between context and content adaptations and later infant feeding, activity and growth outcomes will be critical to the scaling up of this intervention.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"c0277b00e27e3a11985486986f9b67af";}s:4:"show";b:1;s:3:"cid";s:32:"9ce118d5866750013de8e81cfcaebe4a";}s:32:"f34a30e4b718a8a4e58f66d231745561";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536950561;}s:3:"raw";s:1487:"Thanks for your detailed responses to these questions.  I really like that you are using multiple measures and multiple informants of fidelity.  This is really a strength.  It looks the like measures you have selected have some evidence of association with parent engagement outcomes.  I have a few thoughts for you to consider that relate to both of your responses.
1)	Consider which measures are fidelity (interventionist delivery of specific strategies) vs. outcomes (impact of strategy delivery on process or parent engagement).  I’m wondering if the ROLE measure is actually an outcome. 
2)	Defining the intervention – I know I mentioned this in the first assignment and still am unclear on what the intervention model is.   Are the fidelity measures linked to a specific intervention model that has evidence of efficacy and effectiveness?  What is the model? How were the fidelity measures used in efficacy/effectiveness studies? Without that information, I think it is hard to define whether adaptations are needed.  
3)	Adaptations.  Consider whether the intervention model you use may need to be adapted for the services or specific targeted provider.  Was the model developed for SLPs? Is there anything unique about IDEA part C services that would impact delivery of the intervention? Overall, I recommend considering the fit of the intervention model to the service context. This all hinges on what the intervention is and in what contexts it’s been previously tested.
";s:5:"xhtml";s:1501:"Thanks for your detailed responses to these questions.  I really like that you are using multiple measures and multiple informants of fidelity.  This is really a strength.  It looks the like measures you have selected have some evidence of association with parent engagement outcomes.  I have a few thoughts for you to consider that relate to both of your responses.<br />1)	Consider which measures are fidelity (interventionist delivery of specific strategies) vs. outcomes (impact of strategy delivery on process or parent engagement).  I’m wondering if the ROLE measure is actually an outcome. <br />2)	Defining the intervention – I know I mentioned this in the first assignment and still am unclear on what the intervention model is.   Are the fidelity measures linked to a specific intervention model that has evidence of efficacy and effectiveness?  What is the model? How were the fidelity measures used in efficacy/effectiveness studies? Without that information, I think it is hard to define whether adaptations are needed.  <br />3)	Adaptations.  Consider whether the intervention model you use may need to be adapted for the services or specific targeted provider.  Was the model developed for SLPs? Is there anything unique about IDEA part C services that would impact delivery of the intervention? Overall, I recommend considering the fit of the intervention model to the service context. This all hinges on what the intervention is and in what contexts it’s been previously tested.";s:6:"parent";s:32:"4aa5081e5bf9c731fc7b2ec40a0f1b79";s:7:"replies";a:1:{i:0;s:32:"594aff141377c570d2f00fb547b92406";}s:4:"show";b:1;s:3:"cid";s:32:"f34a30e4b718a8a4e58f66d231745561";}s:32:"d1ee52719fd04a155a1639e5e136a3bb";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536950610;}s:3:"raw";s:1181:"I appreciate how this proposal is developing based on prior feedback.  I like that you move away from guidelines.  If I understanding correctly, you are proposing to use observational fidelity measures to characterize routine care which will then inform the selection of EBP strategies. I wonder if your whole proposal could be focused on adaptations. Could you use an adaptation framework to select EBP strategies through practice based evidence regarding implementation outcomes (fit, acceptability, appropriateness etc, from Proctor et al. Implementation outcome framework) and the constraints of the unique classroom context?  Basically, if teachers are observed to be using strategies that are included in comprehensive packages, that provides some evidence of fit within the context and utility of use. In response to your question, I recommend monitoring adaptations.  If teachers are adapting to make the strategies apply, you may adapt the intervention to improve fit. 
Also, I am still unclear if you are proposing to collection child outcomes and consider associations between strategy delivery and child outcomes. That is a different type of practice based evidence.  
";s:5:"xhtml";s:1183:"I appreciate how this proposal is developing based on prior feedback.  I like that you move away from guidelines.  If I understanding correctly, you are proposing to use observational fidelity measures to characterize routine care which will then inform the selection of EBP strategies. I wonder if your whole proposal could be focused on adaptations. Could you use an adaptation framework to select EBP strategies through practice based evidence regarding implementation outcomes (fit, acceptability, appropriateness etc, from Proctor et al. Implementation outcome framework) and the constraints of the unique classroom context?  Basically, if teachers are observed to be using strategies that are included in comprehensive packages, that provides some evidence of fit within the context and utility of use. In response to your question, I recommend monitoring adaptations.  If teachers are adapting to make the strategies apply, you may adapt the intervention to improve fit. <br />Also, I am still unclear if you are proposing to collection child outcomes and consider associations between strategy delivery and child outcomes. That is a different type of practice based evidence.";s:6:"parent";s:32:"9493d48e9321b8dbd491d4eba86c841a";s:7:"replies";a:1:{i:0;s:32:"bac98385a60666a5b51e49afbad9106c";}s:4:"show";b:1;s:3:"cid";s:32:"d1ee52719fd04a155a1639e5e136a3bb";}s:32:"bac98385a60666a5b51e49afbad9106c";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"anahmias";s:4:"name";s:15:"Allison Nahmias";s:4:"mail";s:21:"asnahmias@ucdavis.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537294214;}s:3:"raw";s:646:"Thank you Lauren for this helpful feedback! I really like your idea of "if teachers are observed to be using strategies that are included in comprehensive packages, that provides some evidence of fit within the context and utility of use."  After this module I had been considering an adaption framework, however, in my preliminary sample, no teachers reported having received any training in peer-mediated interventions. Therefore I didn't feel justified in arguing that teachers were adapting the comprehensive peer-mediated packages, but I'd appreciate any suggestions if you think I can still take an adaptation framework approach. Thank you!";s:5:"xhtml";s:666:"Thank you Lauren for this helpful feedback! I really like your idea of &quot;if teachers are observed to be using strategies that are included in comprehensive packages, that provides some evidence of fit within the context and utility of use.&quot;  After this module I had been considering an adaption framework, however, in my preliminary sample, no teachers reported having received any training in peer-mediated interventions. Therefore I didn&#039;t feel justified in arguing that teachers were adapting the comprehensive peer-mediated packages, but I&#039;d appreciate any suggestions if you think I can still take an adaptation framework approach. Thank you!";s:6:"parent";s:32:"d1ee52719fd04a155a1639e5e136a3bb";s:7:"replies";a:1:{i:0;s:32:"e4edc68acd054406ab4dfecf31c810d3";}s:4:"show";b:1;s:3:"cid";s:32:"bac98385a60666a5b51e49afbad9106c";}s:32:"e4edc68acd054406ab4dfecf31c810d3";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537294996;}s:3:"raw";s:322:"I'm glad you find this helpful.  I don't think that the teachers not being trained in peer mediated practices needs to stop you from applying an adaptation framework.  Instead, you could be doing the systematical adaptation in order to fit the context based on what you find that teachers are doing.  Does that make sense?";s:5:"xhtml";s:332:"I&#039;m glad you find this helpful.  I don&#039;t think that the teachers not being trained in peer mediated practices needs to stop you from applying an adaptation framework.  Instead, you could be doing the systematical adaptation in order to fit the context based on what you find that teachers are doing.  Does that make sense?";s:6:"parent";s:32:"bac98385a60666a5b51e49afbad9106c";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"e4edc68acd054406ab4dfecf31c810d3";}s:32:"c0277b00e27e3a11985486986f9b67af";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"chunter";s:4:"name";s:16:"Christine Hunter";s:4:"mail";s:24:"christine.hunter@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537475313;}s:3:"raw";s:2187:"Rachel,
I am “pinch hitting” as a facilitator this week.  Your project is very interesting and addressing an important issue. 

Checklists and observations of a subset are a good way to assess intervention fidelity.   You might also consider assessing individual level indicators of fidelity to the treatment model given that in the initial RCT, participant intervention engagement and attendance related to outcomes. With individual (nutritional counseling) and groups sessions (parenting support) conducted over time, it might be helpful to think of specific aspects of fidelity that you want to assess. For example, at the individual level, you can assess dose (how many of each type of session they attend) as well other forms of engagement. For example, if there are between session assignments, how many do they complete or for the sessions observed, how much to they actively versus passively participate in the session?

In terms of adaptation, I really like the specificity of the factors you are considering assessing. As Dara noted in an earlier comment, is there information from the original RCT that might point to the most important aspects of adaption or fidelity to assess. The real-time tracking sheets make sense but if there are lots of dimensions of variability you are trying to capture, I wonder if they will become burdensome or influence the clinics’ perception of freedom to adapt if they are monitored on these items regularly. It may not be a concern but is something to consider.  Are there aspects of the adaptation that you might be able to capture more passively thorough the EHR?

Finally, you might look at the published papers form the Life-Moms consortium as a means to identify some aspects related to implementation other obesity related trials in this type of populations: https://lifemoms.bsc.gwu.edu/web/lifemoms/clinical-trials-summaries. Also see: Pooled Findings from the LIFE-Moms Randomized Trials: Where Do We Go from Here? Herring SJ. Obesity (Silver Spring). 2018 Sep;26(9):1391. doi: 10.1002/oby.22285. PMID: 30226006.

I hope some of this feedback is useful as you move forward with shaping your proposal!

Best,
Christine Hunter
";s:5:"xhtml";s:2246:"Rachel,<br />I am “pinch hitting” as a facilitator this week.  Your project is very interesting and addressing an important issue. <br /><br />Checklists and observations of a subset are a good way to assess intervention fidelity.   You might also consider assessing individual level indicators of fidelity to the treatment model given that in the initial RCT, participant intervention engagement and attendance related to outcomes. With individual (nutritional counseling) and groups sessions (parenting support) conducted over time, it might be helpful to think of specific aspects of fidelity that you want to assess. For example, at the individual level, you can assess dose (how many of each type of session they attend) as well other forms of engagement. For example, if there are between session assignments, how many do they complete or for the sessions observed, how much to they actively versus passively participate in the session?<br /><br />In terms of adaptation, I really like the specificity of the factors you are considering assessing. As Dara noted in an earlier comment, is there information from the original RCT that might point to the most important aspects of adaption or fidelity to assess. The real-time tracking sheets make sense but if there are lots of dimensions of variability you are trying to capture, I wonder if they will become burdensome or influence the clinics’ perception of freedom to adapt if they are monitored on these items regularly. It may not be a concern but is something to consider.  Are there aspects of the adaptation that you might be able to capture more passively thorough the EHR?<br /><br />Finally, you might look at the published papers form the Life-Moms consortium as a means to identify some aspects related to implementation other obesity related trials in this type of populations: https://lifemoms.bsc.gwu.edu/web/lifemoms/clinical-trials-summaries. Also see: Pooled Findings from the LIFE-Moms Randomized Trials: Where Do We Go from Here? Herring SJ. Obesity (Silver Spring). 2018 Sep;26(9):1391. doi: 10.1002/oby.22285. PMID: 30226006.<br /><br />I hope some of this feedback is useful as you move forward with shaping your proposal!<br /><br />Best,<br />Christine Hunter";s:6:"parent";s:32:"9ce118d5866750013de8e81cfcaebe4a";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"c0277b00e27e3a11985486986f9b67af";}s:32:"0f53a41346dc3100005c9dfdee16b2d8";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"chunter";s:4:"name";s:16:"Christine Hunter";s:4:"mail";s:24:"christine.hunter@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537477022;}s:3:"raw";s:1409:"Michelle,

I am filling in as a facilitator this week.  Your thoughtful approach to various stakeholder needs (patients, providers and payors) and all the ground level stakeholder involvement in the developing the workflow are commendable.

I think measuring fidelity to the workflow map through stakeholder observations as well as provider and patient interviews are important steps. Through the EMR you will also be able to assess rate of prenatal counseling about postpartum LARC. To me this seems like a critical indicator of fidelity rather than a secondary measure.

In terms of adaptation, I know you are hoping your work to design/tailor the evidence-based intervention to your local setting will minimize the need for ongoing adaptations, but I wonder if there are aspects of what is implemented in the toolkit that may vary by healthcare team/patient pairing or characteristics of the patient.  For example, although the toolkit is the primary implementation strategy, are there various ways of introducing, emphasizing, supporting, prompting use that might also be assessed as adaptations in implementation—all using the toolkit, but does how they use it vary?  When it comes to scale up in a future trial, any information you can gather than can inform adaption to other settings/models of care could be very valuable.

I look forward to seeing how this important project progresses.

Christine
";s:5:"xhtml";s:1458:"Michelle,<br /><br />I am filling in as a facilitator this week.  Your thoughtful approach to various stakeholder needs (patients, providers and payors) and all the ground level stakeholder involvement in the developing the workflow are commendable.<br /><br />I think measuring fidelity to the workflow map through stakeholder observations as well as provider and patient interviews are important steps. Through the EMR you will also be able to assess rate of prenatal counseling about postpartum LARC. To me this seems like a critical indicator of fidelity rather than a secondary measure.<br /><br />In terms of adaptation, I know you are hoping your work to design/tailor the evidence-based intervention to your local setting will minimize the need for ongoing adaptations, but I wonder if there are aspects of what is implemented in the toolkit that may vary by healthcare team/patient pairing or characteristics of the patient.  For example, although the toolkit is the primary implementation strategy, are there various ways of introducing, emphasizing, supporting, prompting use that might also be assessed as adaptations in implementation—all using the toolkit, but does how they use it vary?  When it comes to scale up in a future trial, any information you can gather than can inform adaption to other settings/models of care could be very valuable.<br /><br />I look forward to seeing how this important project progresses.<br /><br />Christine";s:6:"parent";s:32:"8e418ba8e58af4b72003dda91c395d1b";s:7:"replies";a:1:{i:0;s:32:"c86a783740ec7fb214679f8beb59e171";}s:4:"show";b:1;s:3:"cid";s:32:"0f53a41346dc3100005c9dfdee16b2d8";}s:32:"8b1c89b34b5c54354a4e468a7d476eeb";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"jpatterson";s:4:"name";s:19:"Jacquelyn Patterson";s:4:"mail";s:28:"jackie_patterson@med.unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537496771;}s:3:"raw";s:6191:"PATTERSON
Assignment #3a

1.	Which model or combination of models is most applicable to your proposed study and why?

An implementation model targeting the organizational level with a focus on feasibility and a patient-oriented health outcome would be most applicable to my study for the following reasons:

•	The study proposes evaluating a strategy to improve the performance of evidence-based newborn resuscitation, thus is focused on implementation rather than dissemination. 
•	While the intervention is targeted at individual providers, it is a systems intervention that involves cooperation amongst a group of providers, and thus ultimately targets care provided at an organizational level. 
•	Feasibility is an important construct to focus on for this study for two reasons: 1) given limited staffing in these settings, strategies to facilitate observation of resuscitations by a staff member using NeoWatch are critical, and testing feasibility is critical; 2) real-time feedback during a newborn resuscitation is an infrequently used strategy that deserves initial pilot testing to determine feasibility (Is automated, audio feedback during a resuscitation possible? Is it distracting to the provider or helpful?). 
•	It is unknown whether using a mobile health application such as NeoWatch could effectively change provider behavior and thus improve a patient-oriented outcome. Investigating the effect of NeoWatch on a patient outcome is critical before considering dissemination of the strategy. 

Given the above outlined criteria, I think that Proctor’s conceptual model of implementation research is most applicable to this proposed study.

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

Proctor’s model suggests that intervention strategies that are tested in implementation research should be evidence-based. This study proposes an implementation strategy to increase fidelity to an evidence-based resuscitation algorithm, Helping Babies Breathe (HBB). HBB has been demonstrated to reduce perinatal mortality. However, for the purposes of this study, the implementation strategy of delivering feedback requires defining specific provider actions within the resuscitation algorithm that can be identified for feedback. As discrete actions of providers within the algorithm have not been independently tested in trials, I will use a Delphi panel of experts in neonatal resuscitation to define the priority evidence-based provider actions within the algorithm by expert consensus. For example, these actions might include stimulating all newborns who are not breathing at birth, and initiating bag mask ventilation prior to 60 seconds for all newborns who do not respond to stimulation. 

In keeping with Proctor’s model, and per my explanation to the above question #1, it will be important to measure both implementation outcomes (particularly feasibility) and service outcomes (particularly effectiveness) given the context of the study. 

**Of note, while Proctor’s model seemed to fit best with my study given my argument above, I am struggling with how it might inform various aspects of my study. The model does not really establish theoretical underpinnings for studying particular outcomes given particular strategies. Rather, it depicts relationships between intervention and implementation strategies, and delineates the variety of outcomes that can be measured in implementation research. 

Assignment #3b

1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

We will measure fidelity of providers to the standard newborn resuscitation algorithm by measuring discrete actions such as stimulation of all newborns to breathe, and initiation of bag mask ventilation by 60 seconds of life in non-breathing newborns. These two actions are critical to reduce mortality from birth asphyxia as they are the two central interventions to prompt breathing of a newborn in a low-resource settings. These actions will be measured by NeoWatch through observations documented by an observer. I will also measure feasibility of the intervention using a previously validated 5-item scale (Weiner et al, 2017). Feasibility is important to measure given the arguments I provided to #3a, question 1. I will measure feasibility through surveys of providers using the 5-item scale, and then explore barriers and facilitators to the intervention with providers in a focus group discussion.

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

We will measure the number of births that are observed using NeoWatch, the number of feedback episodes (including both real-time and post-hoc) and the number of supervisor-led feedback sessions conducted. The number of births at the facility in a given month will be obtained from the birth register; the feedback episodes provided by NeoWatch (both real-time and post hoc) will be obtained directly from the application. We will ask supervisors to document the time spent and feedback provided during group sessions. These measures will ensure that we have a good understanding of the reach of the implementation strategy.

3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

Although a remote possibility, it is important to consider adverse consequences to real-time feedback during a resuscitation. For example, it is remotely possible that a provider might be confused or distracted by real-time feedback to the point that their performance is actually impaired. We will need to closely follow the fidelity measures of stimulation and timely bag mask ventilation to ensure that they are not trending in the opposite direction. Statistical process control charts could be used to monitor changes over time and prompt further evaluation if there is a concern for worsening performance.
";s:5:"xhtml";s:6360:"PATTERSON<br />Assignment #3a<br /><br />1.	Which model or combination of models is most applicable to your proposed study and why?<br /><br />An implementation model targeting the organizational level with a focus on feasibility and a patient-oriented health outcome would be most applicable to my study for the following reasons:<br /><br />•	The study proposes evaluating a strategy to improve the performance of evidence-based newborn resuscitation, thus is focused on implementation rather than dissemination. <br />•	While the intervention is targeted at individual providers, it is a systems intervention that involves cooperation amongst a group of providers, and thus ultimately targets care provided at an organizational level. <br />•	Feasibility is an important construct to focus on for this study for two reasons: 1) given limited staffing in these settings, strategies to facilitate observation of resuscitations by a staff member using NeoWatch are critical, and testing feasibility is critical; 2) real-time feedback during a newborn resuscitation is an infrequently used strategy that deserves initial pilot testing to determine feasibility (Is automated, audio feedback during a resuscitation possible? Is it distracting to the provider or helpful?). <br />•	It is unknown whether using a mobile health application such as NeoWatch could effectively change provider behavior and thus improve a patient-oriented outcome. Investigating the effect of NeoWatch on a patient outcome is critical before considering dissemination of the strategy. <br /><br />Given the above outlined criteria, I think that Proctor’s conceptual model of implementation research is most applicable to this proposed study.<br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />Proctor’s model suggests that intervention strategies that are tested in implementation research should be evidence-based. This study proposes an implementation strategy to increase fidelity to an evidence-based resuscitation algorithm, Helping Babies Breathe (HBB). HBB has been demonstrated to reduce perinatal mortality. However, for the purposes of this study, the implementation strategy of delivering feedback requires defining specific provider actions within the resuscitation algorithm that can be identified for feedback. As discrete actions of providers within the algorithm have not been independently tested in trials, I will use a Delphi panel of experts in neonatal resuscitation to define the priority evidence-based provider actions within the algorithm by expert consensus. For example, these actions might include stimulating all newborns who are not breathing at birth, and initiating bag mask ventilation prior to 60 seconds for all newborns who do not respond to stimulation. <br /><br />In keeping with Proctor’s model, and per my explanation to the above question #1, it will be important to measure both implementation outcomes (particularly feasibility) and service outcomes (particularly effectiveness) given the context of the study. <br /><br />**Of note, while Proctor’s model seemed to fit best with my study given my argument above, I am struggling with how it might inform various aspects of my study. The model does not really establish theoretical underpinnings for studying particular outcomes given particular strategies. Rather, it depicts relationships between intervention and implementation strategies, and delineates the variety of outcomes that can be measured in implementation research. <br /><br />Assignment #3b<br /><br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />We will measure fidelity of providers to the standard newborn resuscitation algorithm by measuring discrete actions such as stimulation of all newborns to breathe, and initiation of bag mask ventilation by 60 seconds of life in non-breathing newborns. These two actions are critical to reduce mortality from birth asphyxia as they are the two central interventions to prompt breathing of a newborn in a low-resource settings. These actions will be measured by NeoWatch through observations documented by an observer. I will also measure feasibility of the intervention using a previously validated 5-item scale (Weiner et al, 2017). Feasibility is important to measure given the arguments I provided to #3a, question 1. I will measure feasibility through surveys of providers using the 5-item scale, and then explore barriers and facilitators to the intervention with providers in a focus group discussion.<br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />We will measure the number of births that are observed using NeoWatch, the number of feedback episodes (including both real-time and post-hoc) and the number of supervisor-led feedback sessions conducted. The number of births at the facility in a given month will be obtained from the birth register; the feedback episodes provided by NeoWatch (both real-time and post hoc) will be obtained directly from the application. We will ask supervisors to document the time spent and feedback provided during group sessions. These measures will ensure that we have a good understanding of the reach of the implementation strategy.<br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />Although a remote possibility, it is important to consider adverse consequences to real-time feedback during a resuscitation. For example, it is remotely possible that a provider might be confused or distracted by real-time feedback to the point that their performance is actually impaired. We will need to closely follow the fidelity measures of stimulation and timely bag mask ventilation to ensure that they are not trending in the opposite direction. Statistical process control charts could be used to monitor changes over time and prompt further evaluation if there is a concern for worsening performance.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"a1d01ad8feafbfebb837bafb622d8e35";}s:4:"show";b:1;s:3:"cid";s:32:"8b1c89b34b5c54354a4e468a7d476eeb";}s:32:"c86a783740ec7fb214679f8beb59e171";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"mmoniz";s:4:"name";s:14:"Michelle Moniz";s:4:"mail";s:20:"mmoniz@med.umich.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537537743;}s:3:"raw";s:191:"Christine, 
Thanks for these fantastic insights!  I appreciate your feedback, and I'll think a little more about your comments about adaptations and the impact on future scale up. 
Michelle 
";s:5:"xhtml";s:204:"Christine, <br />Thanks for these fantastic insights!  I appreciate your feedback, and I&#039;ll think a little more about your comments about adaptations and the impact on future scale up. <br />Michelle";s:6:"parent";s:32:"0f53a41346dc3100005c9dfdee16b2d8";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"c86a783740ec7fb214679f8beb59e171";}s:32:"6fe35d97978e741ebbf5abc049b71f13";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"mmoniz";s:4:"name";s:14:"Michelle Moniz";s:4:"mail";s:20:"mmoniz@med.umich.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537541277;}s:3:"raw";s:4046:"MONIZ
Assignment #3a - Models:
1.	Which model or combination of models is most applicable to your proposed study and why?
We will use CFIR as the determinants framework that guides our study.  We have selected this model because our initial focus is at the organizational level.  We may later need to assess individual provider behaviors, in which case a model like TDF (or individual constructs from TICD) might be more appropriate. 
Enola Proctor’s Conceptual Model of Implementation Research is guiding the evaluation outcomes we have selected. 
2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?
Formative, CFIR-based assessment of determinants of practice at our study site is helping to guide hypotheses about likely mediators/moderators of implementation.  It has also guided our selection of activities to bundle up into our implementation strategy.   
CFIR and the Proctor outcomes paper are guiding our measure selection process.  We will use a mixed-method evaluation, measuring outcomes via both surveys and interviews with both providers and patients.  

Assignment #3b - Measures & Evaluations:
1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?
We are measuring appropriateness and acceptability of various components of the implementation strategy. These outcomes will help us assess our process for selecting implementation strategy components. It will also help us revise items in the toolkit.  It will also help us explain observed preliminary effectiveness of the implementation intervention. 
We will measure fidelity to various components of the implementation strategy and the a priori specific PP LARC workflow map. We will measure real-time implementation strategy modifications and adaptations to the workflow process map. We will assess these via provider interviews, as well at through stakeholder panel meetings. These outcomes are critical for 
We are also measuring implementation costs, which have never been reported for immediate postpartum LARC service implementation. This is a critical knowledge gap for hospitals deciding whether or not to implement this care. 
Finally, we are collecting preliminary measures of effectiveness. These include: 1) % of women counseled prenatally about PP contraceptive options (ascertained via standardized data element in the EMR and 2) rates of LARC utilization in the inpatient and outpatient postpartum settings (ascertained via EMR and institutional claims data). We Will also collect a “balancing” measure of the patient experience of care.  The rationale for this measure is to ensure that we preserve patient-centeredness and that efforts to increase LARC access are not leading to any coercive or non-patient-centered counseling about contraceptive options. We will collect this data via patient surveys in the 3rd trimester of pregnancy and postpartum at two time periods – just prior to implementation, and 6-9 months after initial implementation. 

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?
See comments above about appropriateness, acceptability, fidelity, adaptation. 
3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?
We haven’t planned any yet… but it sounds like a good idea   We have used a robust process for designing the implementation, which has involved meetings every 2 months with a Stakeholder Panel over the past year.  I wonder if this has had downstream effects on teamwork and folks’ sense of belonging/community.  
We will inadvertently be monitoring for unintended consequences via regular Stakeholder Panel meetings throughout the implementation process and subsequent 12 months. 
";s:5:"xhtml";s:4148:"MONIZ<br />Assignment #3a - Models:<br />1.	Which model or combination of models is most applicable to your proposed study and why?<br />We will use CFIR as the determinants framework that guides our study.  We have selected this model because our initial focus is at the organizational level.  We may later need to assess individual provider behaviors, in which case a model like TDF (or individual constructs from TICD) might be more appropriate. <br />Enola Proctor’s Conceptual Model of Implementation Research is guiding the evaluation outcomes we have selected. <br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br />Formative, CFIR-based assessment of determinants of practice at our study site is helping to guide hypotheses about likely mediators/moderators of implementation.  It has also guided our selection of activities to bundle up into our implementation strategy.   <br />CFIR and the Proctor outcomes paper are guiding our measure selection process.  We will use a mixed-method evaluation, measuring outcomes via both surveys and interviews with both providers and patients.  <br /><br />Assignment #3b - Measures &amp; Evaluations:<br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br />We are measuring appropriateness and acceptability of various components of the implementation strategy. These outcomes will help us assess our process for selecting implementation strategy components. It will also help us revise items in the toolkit.  It will also help us explain observed preliminary effectiveness of the implementation intervention. <br />We will measure fidelity to various components of the implementation strategy and the a priori specific PP LARC workflow map. We will measure real-time implementation strategy modifications and adaptations to the workflow process map. We will assess these via provider interviews, as well at through stakeholder panel meetings. These outcomes are critical for <br />We are also measuring implementation costs, which have never been reported for immediate postpartum LARC service implementation. This is a critical knowledge gap for hospitals deciding whether or not to implement this care. <br />Finally, we are collecting preliminary measures of effectiveness. These include: 1) % of women counseled prenatally about PP contraceptive options (ascertained via standardized data element in the EMR and 2) rates of LARC utilization in the inpatient and outpatient postpartum settings (ascertained via EMR and institutional claims data). We Will also collect a “balancing” measure of the patient experience of care.  The rationale for this measure is to ensure that we preserve patient-centeredness and that efforts to increase LARC access are not leading to any coercive or non-patient-centered counseling about contraceptive options. We will collect this data via patient surveys in the 3rd trimester of pregnancy and postpartum at two time periods – just prior to implementation, and 6-9 months after initial implementation. <br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br />See comments above about appropriateness, acceptability, fidelity, adaptation. <br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br />We haven’t planned any yet… but it sounds like a good idea   We have used a robust process for designing the implementation, which has involved meetings every 2 months with a Stakeholder Panel over the past year.  I wonder if this has had downstream effects on teamwork and folks’ sense of belonging/community.  <br />We will inadvertently be monitoring for unintended consequences via regular Stakeholder Panel meetings throughout the implementation process and subsequent 12 months.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"0049284a6a90b4ec2c2dc8670f873eea";}s:4:"show";b:1;s:3:"cid";s:32:"6fe35d97978e741ebbf5abc049b71f13";}s:32:"54c6f71c8df7168c78c74e50d8fcc9da";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"rgross";s:4:"name";s:12:"Rachel Gross";s:4:"mail";s:22:"Rachel.Gross@nyumc.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537555142;}s:3:"raw";s:11783:"Gross_Assignment #3a - Models:

1.	Which model or combination of models is most applicable to your proposed study and why?
2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 

Several models have guided different aspects of my proposal: 1) intervention development; 2) assessment of implementation outcomes; and 3) examining contextual factors to inform future program modifications. 

During intervention development, the socio-ecological framework, the health belief model and social cognitive theory were utilized. The socio-ecological framework can explain poverty-related disparities in the rates of early child obesity. This model highlights that obesity is a complex problem that has been conceptualized as occurring within a multilevel ecological context related to society, community, family and the child. In the context of poverty, the family level greatly influences the onset of disparities, particularly during infancy, a period when children are entirely dependent on their primary caregiver, usually the mother, for appropriate nutrition. Therefore, the Starting Early Program was designed to directly target the family level, specifically mother-infant interactions, while being sensitive to community and society level factors, such as poverty-related challenges and cultural differences. 

The intervention was also developed based on pathways believed to lead to healthy behaviors in high-risk families. Maternal infant feeding knowledge, attitudes, styles and practices, have been demonstrated to be important elements within pathways linking poverty to early child obesity. Determining the best ways to help high-risk families to engage in healthy behaviors required to prevent obesity is key to reducing disparities in early child obesity. The Health Belief Model is one of the most widely used conceptual frameworks for explaining, predicting, and influencing health-related behavior. The model supports that a parents’ intention to perform a healthy behavior is influenced by whether: 1) they feel that their child is vulnerable to developing a problem (perceived susceptibility); 2) they believe the impact of that problem is highly undesirable (perceived severity); 3) they perceive the healthy behavior to reduce the risk of the problems (perceived benefits); 4) they can overcome barriers that make performing the healthy behavior difficult  (perceived barriers); and 5) they feel confident that they can perform the behavior (perceived self-efficacy). We have adapted this model to be more strength based and to take a primary prevention focus to identify factors related to maternal intention to exhibit healthy feeding styles and practices linked to the prevention of early child obesity. The Starting Early Program uses strategies from social cognitive theory to promote engaging in healthy behaviors, such as addressing perceived barriers to healthy behaviors, and using interactive demonstrations, modeling, positive reinforcement and active practicing of skills. 

For Aim 1, my assessment of program effectiveness and implementation outcomes will be guided by the RE-AIM framework (Reach, Effectiveness, and Implementation; Adoption and Maintenance are beyond the scope of this project). To study reach, we will assess the absolute number, proportion, and representativeness of individuals who are willing to participate in the program. For effectiveness, we will assess the impact of the program on health outcomes, including child weight outcomes and parent feeding styles and practices. For implementation outcomes, we will assess acceptability, appropriateness, feasibility, fidelity and adaptations. 

For aim 2, I will use a mixed-methods quantitative and qualitative approach using the Consolidated Framework for Implementation Research (CFIR) to examine contextual factors related to the inner setting, in particular the individual and organizational factors that may influence implementation of this program broadly within primary care.

Gross_Assignment #3b - Measures & Evaluations:

1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

For Aim 1 (to determine if the Starting Early Program implemented in four primary care-based clinics, serving a racially and ethnically diverse population, is effective at decreasing overweight status and weight for length z-score at child age 3 years old compared to standard prenatal and pediatric care), I will conduct the following assessments:

a.	Reach will be defined as the percentage of participants who were approached that enroll and the extent to which the enrolled participants represent the broader community. This will be measured via self-reported participant demographic characteristics and social determinants of health and compared to census or hospital level data. 

b.	Effectiveness, defined as the impact of the intervention on health outcomes, will be assessed by comparing differences in child weight status between the Starting Early intervention group and the control group receiving standard care. Primary outcomes: Infant weight and length/height will be collected using medical record review of the birth hospitalization and subsequent well child visits. For all time points between birth and 2 years old, infant weight will be represented by weight-for-length z-score (WFLz), as calculated by the World Health Organization (WHO). This continuous score will then be used to categorize infants as underweight (<3%), healthy weight (3%-97%), and overweight (>97%) at the time of each measurement. For all time points between 2 and 3 years old, BMI z-score (BMIz) will be calculated using CDC growth curves. Child weight status will be classified as underweight (<5%), healthy weight (5–84.9%), overweight (85–94.9%) and obese (>95%). Secondary outcomes: Infant feeding practices will be assessed using questions adapted from the Infant Feeding Practices Study II, a national longitudinal study of infant feeding. Maternal infant feeding styles will be assessed using the Infant Feeding Style Questionnaire. 

c.	Implementation will be assessed using qualitative interviews with participants to assess perceived acceptability, appropriateness and feasibility of participating in the intervention. We will also monitor fidelity to the intervention. Fidelity corresponds to the design and intent of the original program and how the program will be adapted to fit local needs. We will track the implementation of the 'core components' (the essential and indispensable elements of the intervention) and the adaptations that are designed and that naturally occur throughout the study period (adaptable elements, structures, and systems related to the intervention and organization into which it is being implemented). Fidelity will be assessed via fidelity checklists and observed intervention sessions. We will also track program attendance; participation rates in different aspects of the program including the number of individual sessions and groups attended; and measure the time and cost of the program for each site.

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

For aim 2 (to explore contextual factors (individual, organizational) that may influence Starting Early Program implementation to inform future modifications), I will conduct semi-structured qualitative interviews with organizational stakeholders (prenatal and pediatric health care providers; clinic leadership) to assess individual and organizational factors that may influence implementation. I will use purposive sampling to recruit stakeholders from each of the four sites. A multicultural interdisciplinary team will work collaboratively to design the interview guide. Each interview will last 30 minutes and will be audio recorded. Audio files will be transcribed verbatim and transcripts will be reviewed and interview questions/probes will be iteratively revised as new themes emerge. Interview transcripts will be reviewed for emerging themes using an iterative process of textual analysis and the constant comparative method will be used to identify and iteratively refine codes and devise a final code structure. Themes and subthemes will be reviewed to confirm consistency, coherence, and distinctiveness before defining and finally deciding on a codebook that reflected the meaning of the data. Consensus will be reached regarding each quotation. Data organization and retrieval will be facilitated by Dedoose software. Interviews will be conducted until thematic saturation was reached.

a.	Individual factors: I will explore interventionist and prenatal and pediatric primary care provider attitudes toward and value placed on the intervention as well as familiarity with facts, truths, and principles related to the intervention. We will evaluate individual self-efficacy or the belief in their own capabilities to execute courses of action to achieve implementation goals. We will also assess the readiness to change of the providers defined as the characterization of the phase an individual is in, as he or she progresses toward skilled, enthusiastic, and sustained interactions with the intervention.

b.	Organizational factors: We will assess facilitators and barriers to equipping families to participate actively in the Starting Early Program and to make behavior change by assessing structural characteristics (social architecture, age, maturity, and size of an organization), leadership engagement (commitment, involvement, and accountability of leaders with the implementation), culture (norms, values, and basic assumptions of a given organization), climate (absorptive capacity for change, shared receptivity of involved individuals to an intervention, and the extent to which use of that intervention will be rewarded, supported, and expected within their organization), and resource allocation (level of resources dedicated for implementation and on-going operations, including money, training, education, physical space, and time).

3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? 

An additional area that I am considering is enhancing the study of participant engagement. There has been significant theoretical work in the field of mental health research that could inform strategies to enhance participant engagement in early child obesity prevention interventions. These models operationalize engagement as either “attitudinal” or “behavioral”. The attitudinal dimension refers to the emotional investment or commitment one has to participate. The behavioral dimension refers to the performance of tasks that are needed to implement recommendations and to ultimately achieve desired outcomes. Factors have been posited to influence these dimensions of engagement: 1) perceived relevance and acceptability of the program; 2) cognitions and beliefs about the programs benefits; 3) daily stressors; 4) external barriers and 5) the quality of the therapeutic alliance. Each component of this model could be assessed. 

A potential area to explore related to unintended consequences may be to assess impacts on other areas of pediatric preventive medicine, such as injury prevention and early child development, to ensure that the clinic focus on obesity prevention does not limit the guidance of providers on other important areas. This will also help to determine if indirect effects on other domains of parenting. 
";s:5:"xhtml";s:12002:"Gross_Assignment #3a - Models:<br /><br />1.	Which model or combination of models is most applicable to your proposed study and why?<br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br /><br />Several models have guided different aspects of my proposal: 1) intervention development; 2) assessment of implementation outcomes; and 3) examining contextual factors to inform future program modifications. <br /><br />During intervention development, the socio-ecological framework, the health belief model and social cognitive theory were utilized. The socio-ecological framework can explain poverty-related disparities in the rates of early child obesity. This model highlights that obesity is a complex problem that has been conceptualized as occurring within a multilevel ecological context related to society, community, family and the child. In the context of poverty, the family level greatly influences the onset of disparities, particularly during infancy, a period when children are entirely dependent on their primary caregiver, usually the mother, for appropriate nutrition. Therefore, the Starting Early Program was designed to directly target the family level, specifically mother-infant interactions, while being sensitive to community and society level factors, such as poverty-related challenges and cultural differences. <br /><br />The intervention was also developed based on pathways believed to lead to healthy behaviors in high-risk families. Maternal infant feeding knowledge, attitudes, styles and practices, have been demonstrated to be important elements within pathways linking poverty to early child obesity. Determining the best ways to help high-risk families to engage in healthy behaviors required to prevent obesity is key to reducing disparities in early child obesity. The Health Belief Model is one of the most widely used conceptual frameworks for explaining, predicting, and influencing health-related behavior. The model supports that a parents’ intention to perform a healthy behavior is influenced by whether: 1) they feel that their child is vulnerable to developing a problem (perceived susceptibility); 2) they believe the impact of that problem is highly undesirable (perceived severity); 3) they perceive the healthy behavior to reduce the risk of the problems (perceived benefits); 4) they can overcome barriers that make performing the healthy behavior difficult  (perceived barriers); and 5) they feel confident that they can perform the behavior (perceived self-efficacy). We have adapted this model to be more strength based and to take a primary prevention focus to identify factors related to maternal intention to exhibit healthy feeding styles and practices linked to the prevention of early child obesity. The Starting Early Program uses strategies from social cognitive theory to promote engaging in healthy behaviors, such as addressing perceived barriers to healthy behaviors, and using interactive demonstrations, modeling, positive reinforcement and active practicing of skills. <br /><br />For Aim 1, my assessment of program effectiveness and implementation outcomes will be guided by the RE-AIM framework (Reach, Effectiveness, and Implementation; Adoption and Maintenance are beyond the scope of this project). To study reach, we will assess the absolute number, proportion, and representativeness of individuals who are willing to participate in the program. For effectiveness, we will assess the impact of the program on health outcomes, including child weight outcomes and parent feeding styles and practices. For implementation outcomes, we will assess acceptability, appropriateness, feasibility, fidelity and adaptations. <br /><br />For aim 2, I will use a mixed-methods quantitative and qualitative approach using the Consolidated Framework for Implementation Research (CFIR) to examine contextual factors related to the inner setting, in particular the individual and organizational factors that may influence implementation of this program broadly within primary care.<br /><br />Gross_Assignment #3b - Measures &amp; Evaluations:<br /><br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />For Aim 1 (to determine if the Starting Early Program implemented in four primary care-based clinics, serving a racially and ethnically diverse population, is effective at decreasing overweight status and weight for length z-score at child age 3 years old compared to standard prenatal and pediatric care), I will conduct the following assessments:<br /><br />a.	Reach will be defined as the percentage of participants who were approached that enroll and the extent to which the enrolled participants represent the broader community. This will be measured via self-reported participant demographic characteristics and social determinants of health and compared to census or hospital level data. <br /><br />b.	Effectiveness, defined as the impact of the intervention on health outcomes, will be assessed by comparing differences in child weight status between the Starting Early intervention group and the control group receiving standard care. Primary outcomes: Infant weight and length/height will be collected using medical record review of the birth hospitalization and subsequent well child visits. For all time points between birth and 2 years old, infant weight will be represented by weight-for-length z-score (WFLz), as calculated by the World Health Organization (WHO). This continuous score will then be used to categorize infants as underweight (&lt;3%), healthy weight (3%-97%), and overweight (&gt;97%) at the time of each measurement. For all time points between 2 and 3 years old, BMI z-score (BMIz) will be calculated using CDC growth curves. Child weight status will be classified as underweight (&lt;5%), healthy weight (5–84.9%), overweight (85–94.9%) and obese (&gt;95%). Secondary outcomes: Infant feeding practices will be assessed using questions adapted from the Infant Feeding Practices Study II, a national longitudinal study of infant feeding. Maternal infant feeding styles will be assessed using the Infant Feeding Style Questionnaire. <br /><br />c.	Implementation will be assessed using qualitative interviews with participants to assess perceived acceptability, appropriateness and feasibility of participating in the intervention. We will also monitor fidelity to the intervention. Fidelity corresponds to the design and intent of the original program and how the program will be adapted to fit local needs. We will track the implementation of the &#039;core components&#039; (the essential and indispensable elements of the intervention) and the adaptations that are designed and that naturally occur throughout the study period (adaptable elements, structures, and systems related to the intervention and organization into which it is being implemented). Fidelity will be assessed via fidelity checklists and observed intervention sessions. We will also track program attendance; participation rates in different aspects of the program including the number of individual sessions and groups attended; and measure the time and cost of the program for each site.<br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />For aim 2 (to explore contextual factors (individual, organizational) that may influence Starting Early Program implementation to inform future modifications), I will conduct semi-structured qualitative interviews with organizational stakeholders (prenatal and pediatric health care providers; clinic leadership) to assess individual and organizational factors that may influence implementation. I will use purposive sampling to recruit stakeholders from each of the four sites. A multicultural interdisciplinary team will work collaboratively to design the interview guide. Each interview will last 30 minutes and will be audio recorded. Audio files will be transcribed verbatim and transcripts will be reviewed and interview questions/probes will be iteratively revised as new themes emerge. Interview transcripts will be reviewed for emerging themes using an iterative process of textual analysis and the constant comparative method will be used to identify and iteratively refine codes and devise a final code structure. Themes and subthemes will be reviewed to confirm consistency, coherence, and distinctiveness before defining and finally deciding on a codebook that reflected the meaning of the data. Consensus will be reached regarding each quotation. Data organization and retrieval will be facilitated by Dedoose software. Interviews will be conducted until thematic saturation was reached.<br /><br />a.	Individual factors: I will explore interventionist and prenatal and pediatric primary care provider attitudes toward and value placed on the intervention as well as familiarity with facts, truths, and principles related to the intervention. We will evaluate individual self-efficacy or the belief in their own capabilities to execute courses of action to achieve implementation goals. We will also assess the readiness to change of the providers defined as the characterization of the phase an individual is in, as he or she progresses toward skilled, enthusiastic, and sustained interactions with the intervention.<br /><br />b.	Organizational factors: We will assess facilitators and barriers to equipping families to participate actively in the Starting Early Program and to make behavior change by assessing structural characteristics (social architecture, age, maturity, and size of an organization), leadership engagement (commitment, involvement, and accountability of leaders with the implementation), culture (norms, values, and basic assumptions of a given organization), climate (absorptive capacity for change, shared receptivity of involved individuals to an intervention, and the extent to which use of that intervention will be rewarded, supported, and expected within their organization), and resource allocation (level of resources dedicated for implementation and on-going operations, including money, training, education, physical space, and time).<br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? <br /><br />An additional area that I am considering is enhancing the study of participant engagement. There has been significant theoretical work in the field of mental health research that could inform strategies to enhance participant engagement in early child obesity prevention interventions. These models operationalize engagement as either “attitudinal” or “behavioral”. The attitudinal dimension refers to the emotional investment or commitment one has to participate. The behavioral dimension refers to the performance of tasks that are needed to implement recommendations and to ultimately achieve desired outcomes. Factors have been posited to influence these dimensions of engagement: 1) perceived relevance and acceptability of the program; 2) cognitions and beliefs about the programs benefits; 3) daily stressors; 4) external barriers and 5) the quality of the therapeutic alliance. Each component of this model could be assessed. <br /><br />A potential area to explore related to unintended consequences may be to assess impacts on other areas of pediatric preventive medicine, such as injury prevention and early child development, to ensure that the clinic focus on obesity prevention does not limit the guidance of providers on other important areas. This will also help to determine if indirect effects on other domains of parenting.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"fc588eef8efa3e798aaf423d1569d98a";}s:4:"show";b:1;s:3:"cid";s:32:"54c6f71c8df7168c78c74e50d8fcc9da";}s:32:"12d7d674f0cb4298f1ad2be6d7e7b33e";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"cvamos";s:4:"name";s:12:"Cheryl Vamos";s:4:"mail";s:21:"cvamos@health.usf.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537580582;}s:3:"raw";s:6052:"VAMOS_3a - Models:

1.	Which model or combination of models is most applicable to your proposed study and why?

The PARIHS, CFIR, & DOI theories/frameworks are most applicable to the proposed study. In Aim 1 (formative research), PARIHS will assist in understanding current implementation conditions of the prenatal care practice (i.e., evidence, context, facilitation) to which the prenatal oral health guidelines will be implemented. Also in Aim 1, DOI will help identify those interventions characteristics that are preferred by stakeholders/end-users (i.e., prenatal care staff) that would help increase adoption and integration of oral health promotion into routine prenatal care practice. Aim 2 involves identifying promising implementation strategies, so no explicit theories/frameworks are being used. However, Aim 2 will be guided by Aim 1 findings, existing literature/known strategies (ERIC), and consultation with the Scientific and Practice Advisory Boards. In Aim 3, all three theories/frameworks (PARIHS; DOI; CFIR) will be used to guide the development and evaluation of intervention strategy prototypes.

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 

All three theories/frameworks (PARIHS, CFIR, & DOI) will guide the instruments used in Aim 1 (formative research – interview/focus group guides) and Aim 3 (evaluation – pre-post tests; follow-up interviews/focus groups), as well as inform the development of the chosen intervention strategies that will be tested in Aim 2. PARIHS, CFIR and DOI will help inform the chosen implementation strategies and the actual intervention that is developed to facilitate prenatal providers’ practice behaviors with regards to the oral health guidelines. For example, my previous research has shown the “strength of the evidence” is important in influencing whether prenatal providers address oral health, thus, the intervention strategies must include messaging that ACOG (American Congress of OB/GYN) recommends this practice. Previous research has also revealed several barriers to integrating oral health promotion into prenatal care (e.g., lack of knowledge among providers; lack of time; competing priorities; etc.), thus, constructs from DOI (intervention characteristic sub-constructs) as well as the CFIR constructs from the domains of 'characteristics of the intervention' and 'characteristics of the individuals' will help inform chosen implementation strategies to account for such barriers (and in some cases maximize known facilitators/assets!). Implementation constructs from these three theories/frameworks may also be aligned/cross-walked with the ERIC strategies (if this information is available).

Assignment #3b - Measures & Evaluations:

1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?
2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

The outcomes that will be measured, along with the rationale and process for measuring are outlined below. All of these outcomes will provide the preliminary evidence for the identified/developed intervention strategies needed to conduct further research and testing via larger evaluation trials (e.g., RCT; different practice settings; etc.). However, please note that I am also still working through/exploring which outcomes to focus on for this proposal and do not have identified measures/scales for these outcomes at this time (sorry!)

Implementation Outcomes: 
(1) Acceptability – Must meet the needs and preferences of prenatal providers
(2) Appropriateness – Must fit into the prenatal care context background of stakeholders/end-users (time limited; providers who are not trained in oral health; etc.)
(3) Feasibility – Must be able to be incorporated into busy prenatal care appointments; and a mechanism for follow-up with patients must occur over multiple visit schedules 

Client/Service Outcomes: 
(1) Patient-centeredness – This is important to know if the intervention meets the needs and preferences of pregnant patients needs (as they too – in addition to prenatal providers - are the targets for behavior change - they must engage in positive oral health hygiene and dietary behaviors and engage in oral healthcare service seeking behaviors). The intervention must address their questions, concerns, and physiological oral-systemic health changes. 
(2) Satisfaction (from prenatal providers’ perspective) – This is important to know whether providers are satisfied and will adopt/integrate/sustain these practice behaviors (oral health promotion) into their routine prenatal care practice over time.
(3) Effectiveness – This is important to establish preliminary evidence that the identified/developed implementation strategy(ies) lead to provider practice behaviors (i.e., assess, advise, refer)

3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? 

This is an interesting question. Given the known barriers for why the guidelines are not implemented into routine prenatal care practice, other co-benefits from doing this practice behavior would help motivate providers and sustain their practice behaviors over time. For instance, maybe this improves their patient-provider communication and overall satisfaction with the visit experience; or perhaps these implementation strategies could be used to incorporate health promotion on other topics (e.g., adherence to physical activity guidelines during pregnancy). Similarly, any unintended consequences (decreases rapport with patients; adds time to clinic workflow; etc.) will be important to capture early so they can be addressed in future iterations/testing of the implementation strategies.
";s:5:"xhtml";s:6228:"VAMOS_3a - Models:<br /><br />1.	Which model or combination of models is most applicable to your proposed study and why?<br /><br />The PARIHS, CFIR, &amp; DOI theories/frameworks are most applicable to the proposed study. In Aim 1 (formative research), PARIHS will assist in understanding current implementation conditions of the prenatal care practice (i.e., evidence, context, facilitation) to which the prenatal oral health guidelines will be implemented. Also in Aim 1, DOI will help identify those interventions characteristics that are preferred by stakeholders/end-users (i.e., prenatal care staff) that would help increase adoption and integration of oral health promotion into routine prenatal care practice. Aim 2 involves identifying promising implementation strategies, so no explicit theories/frameworks are being used. However, Aim 2 will be guided by Aim 1 findings, existing literature/known strategies (ERIC), and consultation with the Scientific and Practice Advisory Boards. In Aim 3, all three theories/frameworks (PARIHS; DOI; CFIR) will be used to guide the development and evaluation of intervention strategy prototypes.<br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br /><br />All three theories/frameworks (PARIHS, CFIR, &amp; DOI) will guide the instruments used in Aim 1 (formative research – interview/focus group guides) and Aim 3 (evaluation – pre-post tests; follow-up interviews/focus groups), as well as inform the development of the chosen intervention strategies that will be tested in Aim 2. PARIHS, CFIR and DOI will help inform the chosen implementation strategies and the actual intervention that is developed to facilitate prenatal providers’ practice behaviors with regards to the oral health guidelines. For example, my previous research has shown the “strength of the evidence” is important in influencing whether prenatal providers address oral health, thus, the intervention strategies must include messaging that ACOG (American Congress of OB/GYN) recommends this practice. Previous research has also revealed several barriers to integrating oral health promotion into prenatal care (e.g., lack of knowledge among providers; lack of time; competing priorities; etc.), thus, constructs from DOI (intervention characteristic sub-constructs) as well as the CFIR constructs from the domains of &#039;characteristics of the intervention&#039; and &#039;characteristics of the individuals&#039; will help inform chosen implementation strategies to account for such barriers (and in some cases maximize known facilitators/assets!). Implementation constructs from these three theories/frameworks may also be aligned/cross-walked with the ERIC strategies (if this information is available).<br /><br />Assignment #3b - Measures &amp; Evaluations:<br /><br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />The outcomes that will be measured, along with the rationale and process for measuring are outlined below. All of these outcomes will provide the preliminary evidence for the identified/developed intervention strategies needed to conduct further research and testing via larger evaluation trials (e.g., RCT; different practice settings; etc.). However, please note that I am also still working through/exploring which outcomes to focus on for this proposal and do not have identified measures/scales for these outcomes at this time (sorry!)<br /><br />Implementation Outcomes: <br />(1) Acceptability – Must meet the needs and preferences of prenatal providers<br />(2) Appropriateness – Must fit into the prenatal care context background of stakeholders/end-users (time limited; providers who are not trained in oral health; etc.)<br />(3) Feasibility – Must be able to be incorporated into busy prenatal care appointments; and a mechanism for follow-up with patients must occur over multiple visit schedules <br /><br />Client/Service Outcomes: <br />(1) Patient-centeredness – This is important to know if the intervention meets the needs and preferences of pregnant patients needs (as they too – in addition to prenatal providers - are the targets for behavior change - they must engage in positive oral health hygiene and dietary behaviors and engage in oral healthcare service seeking behaviors). The intervention must address their questions, concerns, and physiological oral-systemic health changes. <br />(2) Satisfaction (from prenatal providers’ perspective) – This is important to know whether providers are satisfied and will adopt/integrate/sustain these practice behaviors (oral health promotion) into their routine prenatal care practice over time.<br />(3) Effectiveness – This is important to establish preliminary evidence that the identified/developed implementation strategy(ies) lead to provider practice behaviors (i.e., assess, advise, refer)<br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? <br /><br />This is an interesting question. Given the known barriers for why the guidelines are not implemented into routine prenatal care practice, other co-benefits from doing this practice behavior would help motivate providers and sustain their practice behaviors over time. For instance, maybe this improves their patient-provider communication and overall satisfaction with the visit experience; or perhaps these implementation strategies could be used to incorporate health promotion on other topics (e.g., adherence to physical activity guidelines during pregnancy). Similarly, any unintended consequences (decreases rapport with patients; adds time to clinic workflow; etc.) will be important to capture early so they can be addressed in future iterations/testing of the implementation strategies.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"4d7f9785c3ea6d6781b6afd97d1be7ac";}s:4:"show";b:1;s:3:"cid";s:32:"12d7d674f0cb4298f1ad2be6d7e7b33e";}s:32:"594aff141377c570d2f00fb547b92406";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"mroberts";s:4:"name";s:13:"Megan Roberts";s:4:"mail";s:32:"megan.y.roberts@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537586746;}s:3:"raw";s:1845:"Thank you so much for all of the feedback. With regard to your comment about what the intervention is – I am struggling with how to talk about this. Many parent-mediated interventions are very complicated to implement in terms of the strategies taught to parents. Furthermore, the way in which a therapist works with the parent is also considered to be an intervention. Thus, the intervention is a two-tiered intervention (one for the child and one for the parent). Because, considerably less attention has been given to how we include parents in intervention (parent intervention) as compared to the child intervention, I would like the focus of the grant to be on a collaborative coaching model for including parents in intervention. However, coaching occurs within the context of teaching parents a specific behavior. Rather than try to examine all of nuances of parent coaching for various language support strategies a therapist could teach a parent, I have chosen to focus on responsiveness (following the child’s lead and responding to child communication), because it is a common element in all evidence-based parent-mediated interventions and because it is the most robust correlational predictor of child communication outcomes for all populations of children with language delays (autism, hearing loss, language delays). Maybe this would be considered an adaptation in that I am taking the commonly used element across several evidence-based parent-mediated interventions. However the context is the same, parent-mediated interventions are implemented in the home (which is also the context of IDEA part C services). However, there has not been an effectiveness trial of collaborative coaching. But since this course, I am wondering how it is possible to do an effectiveness trial without understanding implementation approaches. ";s:5:"xhtml";s:1844:"Thank you so much for all of the feedback. With regard to your comment about what the intervention is – I am struggling with how to talk about this. Many parent-mediated interventions are very complicated to implement in terms of the strategies taught to parents. Furthermore, the way in which a therapist works with the parent is also considered to be an intervention. Thus, the intervention is a two-tiered intervention (one for the child and one for the parent). Because, considerably less attention has been given to how we include parents in intervention (parent intervention) as compared to the child intervention, I would like the focus of the grant to be on a collaborative coaching model for including parents in intervention. However, coaching occurs within the context of teaching parents a specific behavior. Rather than try to examine all of nuances of parent coaching for various language support strategies a therapist could teach a parent, I have chosen to focus on responsiveness (following the child’s lead and responding to child communication), because it is a common element in all evidence-based parent-mediated interventions and because it is the most robust correlational predictor of child communication outcomes for all populations of children with language delays (autism, hearing loss, language delays). Maybe this would be considered an adaptation in that I am taking the commonly used element across several evidence-based parent-mediated interventions. However the context is the same, parent-mediated interventions are implemented in the home (which is also the context of IDEA part C services). However, there has not been an effectiveness trial of collaborative coaching. But since this course, I am wondering how it is possible to do an effectiveness trial without understanding implementation approaches.";s:6:"parent";s:32:"f34a30e4b718a8a4e58f66d231745561";s:7:"replies";a:1:{i:0;s:32:"1a42d25459f061752665bb414633f870";}s:4:"show";b:1;s:3:"cid";s:32:"594aff141377c570d2f00fb547b92406";}s:32:"be78f8b79bb1e2f6cc89097e3ad11813";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"mroberts";s:4:"name";s:13:"Megan Roberts";s:4:"mail";s:32:"megan.y.roberts@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537590510;}s:3:"raw";s:5216:"Roberts_3a Models:

1. Which model or combination of models is most applicable to your proposed study and why?
2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

A consolidated framework for advancing implementation science (CFIR) will help to identify facilitators and barriers to SLPs implementation of a collaborative coaching model (Aim 1) prior to and after testing a specific implementation approach. For example, understanding specific characteristics of the collaborative coaching approach. Specifically we will measure stakeholder’s (speech-language pathologists, clinical supervisors) perception of the strength of evidence, relative advantage, adaptability, and complexity of the intervention. We will also example outer setting characteristics such as parent needs and resources. Given that the inclusion of the parent is the focus on the intervention, it is important to understand the extent to which the company for whom the SLP works is family- versus child-centered. We will also measure inner setting characteristics such as the structural characteristics of the early intervention practice (e.g., number of SLPs, experience practicing, turn over) and the implementation climate (tension for change, compatibility, leadership engagement, available resources, access to information and knowledge). Given that SLPs are working 1:1 in the individual homes of families, they ultimately have greater autonomy in their clinical decision-making. As such, it is important to measure the characteristics of individuals such as knowledge and beliefs about the collaborative coaching, their own self-efficacy and their openness to learning new ideas. 

The 10-stage model of for planning behavior change (Grol and Wensing 2005) will be used to identify implementation strategies that are the most feasible and relevant to SLPs (Aim 2). The outcomes of the selected implementation strategies will be assessed in Aim 3.  
The RE-AIM model will provide an overarching framework for assessing the outcomes of the implementation strategies (Aim 3) given the need to identify: (a) the number of parents who receive the collaborative coaching model (Reach), (b) the extent to which the collaborative coaching approach is effective at increasing parental responsiveness (Efficacy), (c) the proportion of SLPs who adopt a collaborative coaching approach (Adoption), (d) the extent to which the implementation strategies are effective at increasing collaborative coaching in SLPs (Implementation), and (e) the extent to which SLPs use the collaborative coaching approach over time (Maintenance). 

Roberts_3b Measures and Evaluations

1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

First, we will measure the acceptability and the feasibility of the intervention through survey and qualitative interviews. Second, we will measure the RE-AIM outcomes as described above. The number of parents receiving collaborative coaching will be obtained by a review of session notes. The extent to which the collaborative coaching approach is effective at increasing parental responsiveness will be measured by a video recording of a parent-child interaction in which we will calculate the percentage of child communicative acts to which a parent responds. The proportion of SLPs who adopt a collaborative coaching approach will be obtained by video observation of SLPs interacting with families and by SLP report. The extent to which SLPs implement a collaborative coaching approach will be measured through video observations and SLP self-report as described in week 2. Feasibility and acceptability will also be assessed via interviews. Maintenance will be measured by assessing fidelity of implementation 12 months later.
 
2. What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

Throughout the implementation process, we will also measure how SLPs become engaged in the implementation of a collaborative coaching approach. This will be obtained via a questionnaire that measures buy-in regarding the collaborative coaching and the selected implementation approach. Iterative quantitative (surveys) and qualitative (interviews) will be conducted regularly throughout the course of the study to better understand progress and experience SLPs with the intervention. 

3. Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

We will assess the potential co-benefit of other early intervention providers adopting a collaborative coaching model. This may occur because SLPs often conduct therapy sessions with other providers or parent may request the collaborative coaching approach from their other therapists. However, it may be possible that some families prefer a “hands off approach” and may be less satisfied with their speech-language therapy using a collaborative coaching approach. 
";s:5:"xhtml";s:5324:"Roberts_3a Models:<br /><br />1. Which model or combination of models is most applicable to your proposed study and why?<br />2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />A consolidated framework for advancing implementation science (CFIR) will help to identify facilitators and barriers to SLPs implementation of a collaborative coaching model (Aim 1) prior to and after testing a specific implementation approach. For example, understanding specific characteristics of the collaborative coaching approach. Specifically we will measure stakeholder’s (speech-language pathologists, clinical supervisors) perception of the strength of evidence, relative advantage, adaptability, and complexity of the intervention. We will also example outer setting characteristics such as parent needs and resources. Given that the inclusion of the parent is the focus on the intervention, it is important to understand the extent to which the company for whom the SLP works is family- versus child-centered. We will also measure inner setting characteristics such as the structural characteristics of the early intervention practice (e.g., number of SLPs, experience practicing, turn over) and the implementation climate (tension for change, compatibility, leadership engagement, available resources, access to information and knowledge). Given that SLPs are working 1:1 in the individual homes of families, they ultimately have greater autonomy in their clinical decision-making. As such, it is important to measure the characteristics of individuals such as knowledge and beliefs about the collaborative coaching, their own self-efficacy and their openness to learning new ideas. <br /><br />The 10-stage model of for planning behavior change (Grol and Wensing 2005) will be used to identify implementation strategies that are the most feasible and relevant to SLPs (Aim 2). The outcomes of the selected implementation strategies will be assessed in Aim 3.  <br />The RE-AIM model will provide an overarching framework for assessing the outcomes of the implementation strategies (Aim 3) given the need to identify: (a) the number of parents who receive the collaborative coaching model (Reach), (b) the extent to which the collaborative coaching approach is effective at increasing parental responsiveness (Efficacy), (c) the proportion of SLPs who adopt a collaborative coaching approach (Adoption), (d) the extent to which the implementation strategies are effective at increasing collaborative coaching in SLPs (Implementation), and (e) the extent to which SLPs use the collaborative coaching approach over time (Maintenance). <br /><br />Roberts_3b Measures and Evaluations<br /><br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />First, we will measure the acceptability and the feasibility of the intervention through survey and qualitative interviews. Second, we will measure the RE-AIM outcomes as described above. The number of parents receiving collaborative coaching will be obtained by a review of session notes. The extent to which the collaborative coaching approach is effective at increasing parental responsiveness will be measured by a video recording of a parent-child interaction in which we will calculate the percentage of child communicative acts to which a parent responds. The proportion of SLPs who adopt a collaborative coaching approach will be obtained by video observation of SLPs interacting with families and by SLP report. The extent to which SLPs implement a collaborative coaching approach will be measured through video observations and SLP self-report as described in week 2. Feasibility and acceptability will also be assessed via interviews. Maintenance will be measured by assessing fidelity of implementation 12 months later.<br /> <br />2. What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />Throughout the implementation process, we will also measure how SLPs become engaged in the implementation of a collaborative coaching approach. This will be obtained via a questionnaire that measures buy-in regarding the collaborative coaching and the selected implementation approach. Iterative quantitative (surveys) and qualitative (interviews) will be conducted regularly throughout the course of the study to better understand progress and experience SLPs with the intervention. <br /><br />3. Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />We will assess the potential co-benefit of other early intervention providers adopting a collaborative coaching model. This may occur because SLPs often conduct therapy sessions with other providers or parent may request the collaborative coaching approach from their other therapists. However, it may be possible that some families prefer a “hands off approach” and may be less satisfied with their speech-language therapy using a collaborative coaching approach.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"cd8c769346c0aea5f4e4fe105bd7b0a6";}s:4:"show";b:1;s:3:"cid";s:32:"be78f8b79bb1e2f6cc89097e3ad11813";}s:32:"2817455eff427bea6e23146f4e5068e4";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"anahmias";s:4:"name";s:15:"Allison Nahmias";s:4:"mail";s:21:"asnahmias@ucdavis.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537904698;}s:3:"raw";s:4648:"Nahmias- 
Assignment #3a - Models:
1.	Which model or combination of models is most applicable to your proposed study and why?

I was originally thinking of using the Collaborative Model for Knowledge Translation between Research and Practice in Clinical Settings, due to its emphasis on collaboration with community partners and the dynamic, iterative cycle for developing and evaluating content and knowledge translation strategies.  I thought this would facilitate the adaptation of existing peer-mediated interventions into a package of evidence-based strategies and development of knowledge translation strategies (e.g., teacher training strategies) that were appropriate, acceptable, feasible, sustainable, and fit the community context by having stakeholder input and feedback throughout the entire process and using “just in time teaching” to observe teacher penetration, fidelity, adaptation, as well as student outcomes.
However, after the modules and Lauren’s feedback from the last module, I have honestly been a bit overwhelmed by all of the potential options and exploring if another model (or models) would be more appropriate.  I’m currently considering a combination of pieces of a couple different models/frameworks but recognize that there may be another model that would be a more parsimonious approach that I may have missed, so am definitely open to feedback. 
I am considering using an adapted version of the Replicating Effective Programs framework (focused on the first 3 phases) to guide this project. CFIR would guide the measurement of barriers (and facilitators) to implementation in the Pre-conditions phase. The Stirman et al. framework would be used to guide and document adaptations to the peer-mediated EBPs to develop the new EBP package (as I am envisioning larger adaptations than REP recommends) in the Pre-conditions and Pre-implementation phases.  Implementation outcomes (feasibility, acceptability, fit, appropriateness) in the Implementation phase would be evaluated based on the Proctor et al. framework. 

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

The selection of these models will guide all other aspects of the study and will likely change my hypotheses, outcomes, processes, and measurement. I am now envisioning this project as a hybrid type 1 with aim 1 to adapt peer-mediated EBPs to develop a new EBP package, aim 2 to test effectiveness of the new EBP package on student outcomes, and aim 3 to evaluate implementation outcomes of the new EBP package (fidelity, feasibility, acceptability, and appropriateness).

Assignment #3b - Measures & Evaluations:
1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them? 

Student outcomes will be measured to evaluate the effectiveness of the new EBP package. Student outcomes will be measured utilizing direct testing of developmental/cognitive skills (e.g., MSEL, DAS), parent and teacher report of social behavior, goal attainment scaling of targeted student skills, and observation coding of student attention/engagement.  In regards to implementation outcomes, fidelity to the new EBP package will be measured by direct observation of classroom instruction.   Feasibility, acceptability, and appropriateness will be measured via teacher (and leadership) report using the questionnaires by Weiner et al. 

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

The process of adapting the existing peer-mediated EBPs based on identified barriers and promising strategies in usual care will be measured based on the Stirman et al framework and CFIR interview questions, as this can guide further intervention evaluation, adaptation and refinement.  The processes of engagement and implementation (e.g., teacher training) will also be measured based on CFIR interview questions, as these results are important for informing future implementation trials. 

3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? 

Potential co-benefits or unintended consequences of the implementation will be evaluated via interviews with the community working group as part of the “feedback and refinement” sub-phase of the Implementation phase.  One outcome of interest I am considering assessing is teacher stress/burnout.
";s:5:"xhtml";s:4766:"Nahmias- <br />Assignment #3a - Models:<br />1.	Which model or combination of models is most applicable to your proposed study and why?<br /><br />I was originally thinking of using the Collaborative Model for Knowledge Translation between Research and Practice in Clinical Settings, due to its emphasis on collaboration with community partners and the dynamic, iterative cycle for developing and evaluating content and knowledge translation strategies.  I thought this would facilitate the adaptation of existing peer-mediated interventions into a package of evidence-based strategies and development of knowledge translation strategies (e.g., teacher training strategies) that were appropriate, acceptable, feasible, sustainable, and fit the community context by having stakeholder input and feedback throughout the entire process and using “just in time teaching” to observe teacher penetration, fidelity, adaptation, as well as student outcomes.<br />However, after the modules and Lauren’s feedback from the last module, I have honestly been a bit overwhelmed by all of the potential options and exploring if another model (or models) would be more appropriate.  I’m currently considering a combination of pieces of a couple different models/frameworks but recognize that there may be another model that would be a more parsimonious approach that I may have missed, so am definitely open to feedback. <br />I am considering using an adapted version of the Replicating Effective Programs framework (focused on the first 3 phases) to guide this project. CFIR would guide the measurement of barriers (and facilitators) to implementation in the Pre-conditions phase. The Stirman et al. framework would be used to guide and document adaptations to the peer-mediated EBPs to develop the new EBP package (as I am envisioning larger adaptations than REP recommends) in the Pre-conditions and Pre-implementation phases.  Implementation outcomes (feasibility, acceptability, fit, appropriateness) in the Implementation phase would be evaluated based on the Proctor et al. framework. <br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />The selection of these models will guide all other aspects of the study and will likely change my hypotheses, outcomes, processes, and measurement. I am now envisioning this project as a hybrid type 1 with aim 1 to adapt peer-mediated EBPs to develop a new EBP package, aim 2 to test effectiveness of the new EBP package on student outcomes, and aim 3 to evaluate implementation outcomes of the new EBP package (fidelity, feasibility, acceptability, and appropriateness).<br /><br />Assignment #3b - Measures &amp; Evaluations:<br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them? <br /><br />Student outcomes will be measured to evaluate the effectiveness of the new EBP package. Student outcomes will be measured utilizing direct testing of developmental/cognitive skills (e.g., MSEL, DAS), parent and teacher report of social behavior, goal attainment scaling of targeted student skills, and observation coding of student attention/engagement.  In regards to implementation outcomes, fidelity to the new EBP package will be measured by direct observation of classroom instruction.   Feasibility, acceptability, and appropriateness will be measured via teacher (and leadership) report using the questionnaires by Weiner et al. <br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />The process of adapting the existing peer-mediated EBPs based on identified barriers and promising strategies in usual care will be measured based on the Stirman et al framework and CFIR interview questions, as this can guide further intervention evaluation, adaptation and refinement.  The processes of engagement and implementation (e.g., teacher training) will also be measured based on CFIR interview questions, as these results are important for informing future implementation trials. <br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? <br /><br />Potential co-benefits or unintended consequences of the implementation will be evaluated via interviews with the community working group as part of the “feedback and refinement” sub-phase of the Implementation phase.  One outcome of interest I am considering assessing is teacher stress/burnout.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"387bd85e9068bdf53bb2be1bb32f7ad3";}s:4:"show";b:1;s:3:"cid";s:32:"2817455eff427bea6e23146f4e5068e4";}s:32:"a1d01ad8feafbfebb837bafb622d8e35";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538189083;}s:3:"raw";s:1134:"•	I like how you have applied Proctor’s model to frame your intervention, implementation strategies and differentiate implementation and service outcomes.  You raise a great question on how that will inform other aspects of the study.  Additionally, your comment that an implementation model targeting the organization level makes sense given the target of your intervention.  Based on this good point, I suggest considering an additional model that specifies the multi-level context (e.g., CFIR, EPIS).  It seems that you are focused on the “inner context” in the development of the approach as you target provider groups and individual providers.  That may also guide your measurement approach for determinants of implementation. 
•	The outcomes you list seem appropriate.  I particularly like that you have an observational measure of fidelity and that some of the process data can be obtained from the application.  Consider whether you could link outcomes to different levels – provider, provider teams, site and whether your conceptual model can help guide your qualitative data collection on barriers/facilitators.
";s:5:"xhtml";s:1138:"•	I like how you have applied Proctor’s model to frame your intervention, implementation strategies and differentiate implementation and service outcomes.  You raise a great question on how that will inform other aspects of the study.  Additionally, your comment that an implementation model targeting the organization level makes sense given the target of your intervention.  Based on this good point, I suggest considering an additional model that specifies the multi-level context (e.g., CFIR, EPIS).  It seems that you are focused on the “inner context” in the development of the approach as you target provider groups and individual providers.  That may also guide your measurement approach for determinants of implementation. <br />•	The outcomes you list seem appropriate.  I particularly like that you have an observational measure of fidelity and that some of the process data can be obtained from the application.  Consider whether you could link outcomes to different levels – provider, provider teams, site and whether your conceptual model can help guide your qualitative data collection on barriers/facilitators.";s:6:"parent";s:32:"8b1c89b34b5c54354a4e468a7d476eeb";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"a1d01ad8feafbfebb837bafb622d8e35";}s:32:"0049284a6a90b4ec2c2dc8670f873eea";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538189133;}s:3:"raw";s:763:"•	I appreciate your thoughtful rationale for selecting CFIR and Proctor’s model. It appears that you have an in-depth plan for applying CFIR for determinants and Proctor’s model for conceptualizing outcomes in the proposed formative work. 
•	The evaluation plan sounds well developed and appropriate.  I particularly appreciate the real-time measurement of implementation strategy modifications and cost.  I encourage you to consider how implementation costs will be measured. In your patient experience data collection you could consider applying implementation outcome constructs that you are using for providers for patients. Regarding unintended consequences, the stakeholder panel meetings should be a very appropriate method for identifying them.  
";s:5:"xhtml";s:765:"•	I appreciate your thoughtful rationale for selecting CFIR and Proctor’s model. It appears that you have an in-depth plan for applying CFIR for determinants and Proctor’s model for conceptualizing outcomes in the proposed formative work. <br />•	The evaluation plan sounds well developed and appropriate.  I particularly appreciate the real-time measurement of implementation strategy modifications and cost.  I encourage you to consider how implementation costs will be measured. In your patient experience data collection you could consider applying implementation outcome constructs that you are using for providers for patients. Regarding unintended consequences, the stakeholder panel meetings should be a very appropriate method for identifying them.";s:6:"parent";s:32:"6fe35d97978e741ebbf5abc049b71f13";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"0049284a6a90b4ec2c2dc8670f873eea";}s:32:"fc588eef8efa3e798aaf423d1569d98a";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538189146;}s:3:"raw";s:981:"•	I really like how you differentiate models as they apply to the intervention vs studying implementation. RE-AIM appears appropriate to characterize your implementation outcomes and CFIR to examine influences on these outcomes. I encourage you to be explicit about what models you are using linked which aspects of the implementation data collection.
•	In regards to measures, it seems that aim 1 is really about effectiveness.  I suggest only including your #b effectiveness outcomes linked to that Aim.  If I understand correctly, you are proposing a second aim to implementation process and outcomes and determinants.  It seems that reach and implementation fit better under this aim.  I like how you are looking at multi-level influences on implementation.  Consider whether you have the resources to measure all of the proposed constructs. 
•	Is it possible to include in your qualitative interview a question about impact of program on other aspects of care provided?
";s:5:"xhtml";s:990:"•	I really like how you differentiate models as they apply to the intervention vs studying implementation. RE-AIM appears appropriate to characterize your implementation outcomes and CFIR to examine influences on these outcomes. I encourage you to be explicit about what models you are using linked which aspects of the implementation data collection.<br />•	In regards to measures, it seems that aim 1 is really about effectiveness.  I suggest only including your #b effectiveness outcomes linked to that Aim.  If I understand correctly, you are proposing a second aim to implementation process and outcomes and determinants.  It seems that reach and implementation fit better under this aim.  I like how you are looking at multi-level influences on implementation.  Consider whether you have the resources to measure all of the proposed constructs. <br />•	Is it possible to include in your qualitative interview a question about impact of program on other aspects of care provided?";s:6:"parent";s:32:"54c6f71c8df7168c78c74e50d8fcc9da";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"fc588eef8efa3e798aaf423d1569d98a";}s:32:"4d7f9785c3ea6d6781b6afd97d1be7ac";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538189170;}s:3:"raw";s:942:"•	I appreciate your selection of multiple implementation models to guide different aspects of your study.  I can see why you did this, but do encourage you to identify the minimum key models you can use to achieve your aims.  This is a complex study and reducing the number of different constructs (especially is there is overlap) may be helpful in linking measures/constructs and synthesizing results. 
•	I like how you distinguished implementation from client/service outcomes.  I am a bit unclear on how provider satisfaction and effectiveness of provider practice behaviors fit with client outcomes.  How is provider satisfaction distinct from their perceptions of acceptability, appropriateness and feasibility?  Furthermore, provide behavior change seems like a relevant implementation outcome that may be measured as fidelity. 
•	The potential co-benefits seem plausible.  Could they be incorporated into qualitative interviews?
";s:5:"xhtml";s:951:"•	I appreciate your selection of multiple implementation models to guide different aspects of your study.  I can see why you did this, but do encourage you to identify the minimum key models you can use to achieve your aims.  This is a complex study and reducing the number of different constructs (especially is there is overlap) may be helpful in linking measures/constructs and synthesizing results. <br />•	I like how you distinguished implementation from client/service outcomes.  I am a bit unclear on how provider satisfaction and effectiveness of provider practice behaviors fit with client outcomes.  How is provider satisfaction distinct from their perceptions of acceptability, appropriateness and feasibility?  Furthermore, provide behavior change seems like a relevant implementation outcome that may be measured as fidelity. <br />•	The potential co-benefits seem plausible.  Could they be incorporated into qualitative interviews?";s:6:"parent";s:32:"12d7d674f0cb4298f1ad2be6d7e7b33e";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"4d7f9785c3ea6d6781b6afd97d1be7ac";}s:32:"cd8c769346c0aea5f4e4fe105bd7b0a6";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538189190;}s:3:"raw";s:1355:"•	I agree that CFIR could be a useful framework in conceptualizing barriers/facilitators to implementation of collaborative coaching.  Furthermore, you raise good observations of the potential multi-level context of collaborative coaching delivery. I encourage you to further differentiate the clinical intervention (specific parent mediated intervention) from the implementation strategies (e.g., provider training model, performance feedback – see ERIC compilation). A minor point, but I would consider parent needs and resources inner context factors and the Part C funding requirements as outer context factors. The application of RE-AIM to your outcomes seems appropriate.  It’s a lot to measure, so you may focus more resources on a subset. It seems like fidelity as an indicator of implementation would be a priority.
•	You are proposing a lot of very resource intensive data collection – reviewing session notes, video recording. If there are already strong data on effectiveness of the intervention, consider putting less resources into that data collection and prioritize measurement of implementation (observed fidelity). Is there a way to measure reach without going through all session notes?
•	For co-benefits, consider asking providers whether they have applied the model to other families with outside of early intervention.  
";s:5:"xhtml";s:1362:"•	I agree that CFIR could be a useful framework in conceptualizing barriers/facilitators to implementation of collaborative coaching.  Furthermore, you raise good observations of the potential multi-level context of collaborative coaching delivery. I encourage you to further differentiate the clinical intervention (specific parent mediated intervention) from the implementation strategies (e.g., provider training model, performance feedback – see ERIC compilation). A minor point, but I would consider parent needs and resources inner context factors and the Part C funding requirements as outer context factors. The application of RE-AIM to your outcomes seems appropriate.  It’s a lot to measure, so you may focus more resources on a subset. It seems like fidelity as an indicator of implementation would be a priority.<br />•	You are proposing a lot of very resource intensive data collection – reviewing session notes, video recording. If there are already strong data on effectiveness of the intervention, consider putting less resources into that data collection and prioritize measurement of implementation (observed fidelity). Is there a way to measure reach without going through all session notes?<br />•	For co-benefits, consider asking providers whether they have applied the model to other families with outside of early intervention.";s:6:"parent";s:32:"be78f8b79bb1e2f6cc89097e3ad11813";s:7:"replies";a:1:{i:0;s:32:"f17469e9985c1b86684236e14ba91a58";}s:4:"show";b:1;s:3:"cid";s:32:"cd8c769346c0aea5f4e4fe105bd7b0a6";}s:32:"387bd85e9068bdf53bb2be1bb32f7ad3";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538189222;}s:3:"raw";s:884:"1)	I appreciate your thoughtful approach about selection of models and agree that it can be challenging to select.  If you are focusing on adaptation, including adaptation model definitely makes sense. I suggest interacting with some of tools included in the models for your selection and trying to map on to the constructs in your aims. I agree with you that parsimony is ideal, so selecting the fewest models needed to achieve aims would be beneficial.  Based on your responses to #b, it sounds like Stirman adaptations and CFIR really fit with the project.
2)	As suggested above, I recommend considering how the models link to your aims and measurement. I look forward to seeing you further develop the proposal. 
3)	Your proposed approach to use interviews to measure co-benefits is appropriate and measuring burnout could certainly inform intervention and training development. 
";s:5:"xhtml";s:892:"1)	I appreciate your thoughtful approach about selection of models and agree that it can be challenging to select.  If you are focusing on adaptation, including adaptation model definitely makes sense. I suggest interacting with some of tools included in the models for your selection and trying to map on to the constructs in your aims. I agree with you that parsimony is ideal, so selecting the fewest models needed to achieve aims would be beneficial.  Based on your responses to #b, it sounds like Stirman adaptations and CFIR really fit with the project.<br />2)	As suggested above, I recommend considering how the models link to your aims and measurement. I look forward to seeing you further develop the proposal. <br />3)	Your proposed approach to use interviews to measure co-benefits is appropriate and measuring burnout could certainly inform intervention and training development.";s:6:"parent";s:32:"2817455eff427bea6e23146f4e5068e4";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"387bd85e9068bdf53bb2be1bb32f7ad3";}s:32:"1a42d25459f061752665bb414633f870";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538189542;}s:3:"raw";s:618:"Good questions.  I agree with you that parent mediated intervention are complex as the target is really the parent. I do think there is research on parent intervention (see work by Sally Rogers, Brooke Ingersoll and Aubyn Stahmer) that include the focus on responsiveness.  If you are going to select a common elements approach, I think you'll need to  justify why you aren't using a full package and what evidence there is to support the effectiveness of specific components. The issues you raise about effectiveness/implementation might suggest that a Hybrid 1 is the appropriate for your study.  I hope this helps.
";s:5:"xhtml";s:627:"Good questions.  I agree with you that parent mediated intervention are complex as the target is really the parent. I do think there is research on parent intervention (see work by Sally Rogers, Brooke Ingersoll and Aubyn Stahmer) that include the focus on responsiveness.  If you are going to select a common elements approach, I think you&#039;ll need to  justify why you aren&#039;t using a full package and what evidence there is to support the effectiveness of specific components. The issues you raise about effectiveness/implementation might suggest that a Hybrid 1 is the appropriate for your study.  I hope this helps.";s:6:"parent";s:32:"594aff141377c570d2f00fb547b92406";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"1a42d25459f061752665bb414633f870";}s:32:"f17469e9985c1b86684236e14ba91a58";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"mroberts";s:4:"name";s:13:"Megan Roberts";s:4:"mail";s:32:"megan.y.roberts@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538426034;}s:3:"raw";s:1251:"Thanks so much for your feedback Lauren! I especially appreciate the resource-related questions. I really need to think about what is feasible. 

There are so many pieces to consider that I get overwhelmed and wondering where I should start. 

Here is what I know:
1. Involving parents in early intervention is recommended practice. 
2. Existing observational studies (albeit not that many) indicate that involving parents in early intervention is not widely implemented by SLPs.  
3. The collaborative coaching method, as a way to include parents in early intervention, has been used in many efficacy trials but not an effectiveness trial. 
4. In order to do an effectiveness trial, I think we need to know implementation strategies that work, such that we can teach SLPs in the real life to use this approach. 

I am going around in circles trying to figure out where to start. Should I start by understanding why collaborative coaching isn't being implemented or is the next step an effectiveness trial or maybe a hybrid type 1? Is an effectiveness trial a prerequisite to an implementation study? Or can the first effectiveness trial also include an implementation component? 

I feel like I need to do 15 studies to answer all of my questions! 

";s:5:"xhtml";s:1313:"Thanks so much for your feedback Lauren! I especially appreciate the resource-related questions. I really need to think about what is feasible. <br /><br />There are so many pieces to consider that I get overwhelmed and wondering where I should start. <br /><br />Here is what I know:<br />1. Involving parents in early intervention is recommended practice. <br />2. Existing observational studies (albeit not that many) indicate that involving parents in early intervention is not widely implemented by SLPs.  <br />3. The collaborative coaching method, as a way to include parents in early intervention, has been used in many efficacy trials but not an effectiveness trial. <br />4. In order to do an effectiveness trial, I think we need to know implementation strategies that work, such that we can teach SLPs in the real life to use this approach. <br /><br />I am going around in circles trying to figure out where to start. Should I start by understanding why collaborative coaching isn&#039;t being implemented or is the next step an effectiveness trial or maybe a hybrid type 1? Is an effectiveness trial a prerequisite to an implementation study? Or can the first effectiveness trial also include an implementation component? <br /><br />I feel like I need to do 15 studies to answer all of my questions!";s:6:"parent";s:32:"cd8c769346c0aea5f4e4fe105bd7b0a6";s:7:"replies";a:1:{i:0;s:32:"48e327bcb2143b8601aaeb38e54550b7";}s:4:"show";b:1;s:3:"cid";s:32:"f17469e9985c1b86684236e14ba91a58";}s:32:"48e327bcb2143b8601aaeb38e54550b7";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538432625;}s:3:"raw";s:544:"You are asking really good questions, Meg.  I like how you summarized what is known and not known yet. I do think reviewers will likely want to see some data on effectiveness before moving to studying different implementation approaches. I like your suggestion of a hybrid 1.  Your primary questions could be the effectiveness (impact of collaborative coaching on parent and child outcomes) and secondary aim to collect data on implementation process (determinants of uptake, SLP fidelity, etc) to inform a future implementation-focused study. ";s:5:"xhtml";s:543:"You are asking really good questions, Meg.  I like how you summarized what is known and not known yet. I do think reviewers will likely want to see some data on effectiveness before moving to studying different implementation approaches. I like your suggestion of a hybrid 1.  Your primary questions could be the effectiveness (impact of collaborative coaching on parent and child outcomes) and secondary aim to collect data on implementation process (determinants of uptake, SLP fidelity, etc) to inform a future implementation-focused study.";s:6:"parent";s:32:"f17469e9985c1b86684236e14ba91a58";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"48e327bcb2143b8601aaeb38e54550b7";}s:32:"6847f29f15382f6065e104a8472bf2e4";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"jpatterson";s:4:"name";s:19:"Jacquelyn Patterson";s:4:"mail";s:28:"jackie_patterson@med.unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1538511307;s:8:"modified";i:1538511517;}s:3:"raw";s:1977:"PATTERSON – Assignment #4

1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

For Aim 1 on development of NeoWatch, we will use the Delphi method to achieve expert consensus on provider actions to be prioritized for feedback with NeoWatch. For Aim 2, we will beta test the application through direct observation of users interacting with NeoWatch as well as feedback from end-users. For Aim 3, we will use a pre-post hybrid type 2 design to evaluate effectiveness and feasibility of NeoWatch. 

While a randomized trial would be more definitive for Aim 3, it would require randomization at the facility level (ie cluster randomization) and thus require significant funds. Given the novelty of the audit-feedback approach in these settings, I think pilot testing prior to progressing to a randomized trial is warranted. Additionally, the hybrid type 2 design is particularly appropriate for this intervention given valid concerns about staffing in these settings and the feasibility of an audit-feedback approach that requires an observer to implement. 

2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?

In this pre-post hybrid type 2 study, we will explore feasibility and acceptability of the intervention in addition to effectiveness. We will administer quantitative questionnaires to all study subjects (midwives at the hospitals) consisting of previously validated five-item scales for both of these constructs. Additionally, we will conduct qualitative focus group discussions to explore these constructs as well as facilitators and barriers to implementation of NeoWatch and opportunities for sustainability. We will merge the quantitative and qualitative data and assess fit of integration to inform aim 3. ";s:5:"xhtml";s:2026:"PATTERSON – Assignment #4<br /><br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />For Aim 1 on development of NeoWatch, we will use the Delphi method to achieve expert consensus on provider actions to be prioritized for feedback with NeoWatch. For Aim 2, we will beta test the application through direct observation of users interacting with NeoWatch as well as feedback from end-users. For Aim 3, we will use a pre-post hybrid type 2 design to evaluate effectiveness and feasibility of NeoWatch. <br /><br />While a randomized trial would be more definitive for Aim 3, it would require randomization at the facility level (ie cluster randomization) and thus require significant funds. Given the novelty of the audit-feedback approach in these settings, I think pilot testing prior to progressing to a randomized trial is warranted. Additionally, the hybrid type 2 design is particularly appropriate for this intervention given valid concerns about staffing in these settings and the feasibility of an audit-feedback approach that requires an observer to implement. <br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br /><br />In this pre-post hybrid type 2 study, we will explore feasibility and acceptability of the intervention in addition to effectiveness. We will administer quantitative questionnaires to all study subjects (midwives at the hospitals) consisting of previously validated five-item scales for both of these constructs. Additionally, we will conduct qualitative focus group discussions to explore these constructs as well as facilitators and barriers to implementation of NeoWatch and opportunities for sustainability. We will merge the quantitative and qualitative data and assess fit of integration to inform aim 3.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"2bd128783bfab2e6aca20ce89035cc5f";}s:4:"show";b:1;s:3:"cid";s:32:"6847f29f15382f6065e104a8472bf2e4";}s:32:"1d63607e08cb0eef9d11a69417819e07";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"mroberts";s:4:"name";s:13:"Megan Roberts";s:4:"mail";s:32:"megan.y.roberts@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538666266;}s:3:"raw";s:4275:"Roberts - Assignment #4 

1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses? 

I have re-worked my study aims (again!). 

Aim 1: Evaluate the effectiveness of a collaborative coaching approach when implemented by speech-language pathologists as part of the statewide early intervention program on: (a) parent use of responsive language facilitation strategies, (b) parent satisfaction with early intervention services, (c) parent self-efficacy, and (d) child rate of spontaneous communicative acts,

Aim 2: Examine parent (level of alliance with the speech-language pathologist) and child (type of developmental delay) moderators of outcomes. 

Aim 3: Evaluate SLPs satisfaction with and implementation of the collaborative coaching approach.

Aim 4: Identify implementation strategies that are relevant and feasible to SLPs. 

Given that the effectiveness of the collaborative coaching approach by community-based early intervention speech-language pathologists has not yet been established, we will use an effectiveness-implementation hybrid type 1 design. This will allow us to test the effectiveness of a collaborative coaching approach on parents’ use of responsive language facilitation strategies, parent satisfaction, parent self-efficacy, and child rate of spontaneous communicative acts. This design will allow us to simultaneously examine facilitators and barriers to implementation of a collaborative coaching approach, which will inform a subsequent implementation trial. 

The first two study aims will use a randomized cluster design in which speech-language pathologists will be randomly assigned to the collaborative coaching approach or a business-as-usual control group. Randomization will be stratified by years of experience (clinical fellowship year, not clinical fellowship year). SLPs who provide language therapy to at least 5 home-based children will be included in the study. Given that caseloads vary, the cluster size will like vary among SLPs. A Hybrid 1 design is appropriate given that there are existing efficacy data for the collaborative coaching approach across different populations of children enrolled in early intervention (autism, hearing loss, developmental language delay). Furthermore, collaborative coaching is not only considered to be low-risk, but also the recommended standard of care, despite its lack of use. 

Aims 3 and 4 will use a mixed-methods approach to process evaluation. We will quantitatively examine the extent to which certain characteristics (e.g., attitudes towards evidence-based practices, years of experience, support from early intervention agency) impact fidelity of SLPs use of the collaborative coaching method. In addition, we will use surveys and interviews to assess the acceptability and the feasibility of collaborative coaching approach. This mixed-method approach will us to examine the extent to which hypothesized factors influence implementation (quantitative) and also to identify unknown facilitators of and barriers to implementation (qualitative). Interviews will also allow us to determine potential implementation strategies that are likely to be most feasible and relevant to SLPs. 


2. Will you be incorporating a mixed-methods design into your study? Is so, what approach will you use to incorporate the qualitative date into the study?

We will use a complementary approach to integrate qualitative and quantitative data. For example to examine parent satisfaction, we will use a Likert rating scale in conjuction with open ended questions. Content analysis will be used to analyze responses to open-ended questions in order to identify common themes related to parents’ satisfaction and barriers to their implementation of responsive language facilitation strategies. These satisfaction data will be integrated into tables that display both qualitative and quantitative data. A similar approach will be used to examine SLP satisfaction and implementation of the collaborative coaching approach: quantitative analysis of the effect of hypothesized factors (attitudes towards EBP, years of experience) on SLPs fidelity; qualitative interviews to determine unknown facilitators of and barriers to implementation. 
";s:5:"xhtml";s:4388:"Roberts - Assignment #4 <br /><br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses? <br /><br />I have re-worked my study aims (again!). <br /><br />Aim 1: Evaluate the effectiveness of a collaborative coaching approach when implemented by speech-language pathologists as part of the statewide early intervention program on: (a) parent use of responsive language facilitation strategies, (b) parent satisfaction with early intervention services, (c) parent self-efficacy, and (d) child rate of spontaneous communicative acts,<br /><br />Aim 2: Examine parent (level of alliance with the speech-language pathologist) and child (type of developmental delay) moderators of outcomes. <br /><br />Aim 3: Evaluate SLPs satisfaction with and implementation of the collaborative coaching approach.<br /><br />Aim 4: Identify implementation strategies that are relevant and feasible to SLPs. <br /><br />Given that the effectiveness of the collaborative coaching approach by community-based early intervention speech-language pathologists has not yet been established, we will use an effectiveness-implementation hybrid type 1 design. This will allow us to test the effectiveness of a collaborative coaching approach on parents’ use of responsive language facilitation strategies, parent satisfaction, parent self-efficacy, and child rate of spontaneous communicative acts. This design will allow us to simultaneously examine facilitators and barriers to implementation of a collaborative coaching approach, which will inform a subsequent implementation trial. <br /><br />The first two study aims will use a randomized cluster design in which speech-language pathologists will be randomly assigned to the collaborative coaching approach or a business-as-usual control group. Randomization will be stratified by years of experience (clinical fellowship year, not clinical fellowship year). SLPs who provide language therapy to at least 5 home-based children will be included in the study. Given that caseloads vary, the cluster size will like vary among SLPs. A Hybrid 1 design is appropriate given that there are existing efficacy data for the collaborative coaching approach across different populations of children enrolled in early intervention (autism, hearing loss, developmental language delay). Furthermore, collaborative coaching is not only considered to be low-risk, but also the recommended standard of care, despite its lack of use. <br /><br />Aims 3 and 4 will use a mixed-methods approach to process evaluation. We will quantitatively examine the extent to which certain characteristics (e.g., attitudes towards evidence-based practices, years of experience, support from early intervention agency) impact fidelity of SLPs use of the collaborative coaching method. In addition, we will use surveys and interviews to assess the acceptability and the feasibility of collaborative coaching approach. This mixed-method approach will us to examine the extent to which hypothesized factors influence implementation (quantitative) and also to identify unknown facilitators of and barriers to implementation (qualitative). Interviews will also allow us to determine potential implementation strategies that are likely to be most feasible and relevant to SLPs. <br /><br /><br />2. Will you be incorporating a mixed-methods design into your study? Is so, what approach will you use to incorporate the qualitative date into the study?<br /><br />We will use a complementary approach to integrate qualitative and quantitative data. For example to examine parent satisfaction, we will use a Likert rating scale in conjuction with open ended questions. Content analysis will be used to analyze responses to open-ended questions in order to identify common themes related to parents’ satisfaction and barriers to their implementation of responsive language facilitation strategies. These satisfaction data will be integrated into tables that display both qualitative and quantitative data. A similar approach will be used to examine SLP satisfaction and implementation of the collaborative coaching approach: quantitative analysis of the effect of hypothesized factors (attitudes towards EBP, years of experience) on SLPs fidelity; qualitative interviews to determine unknown facilitators of and barriers to implementation.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"ecdb1419b24f94a1c7bbf0ecd39c4bb1";}s:4:"show";b:1;s:3:"cid";s:32:"1d63607e08cb0eef9d11a69417819e07";}s:32:"a12cce983fcac80180cf96123420b4bd";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"rgross";s:4:"name";s:12:"Rachel Gross";s:4:"mail";s:22:"Rachel.Gross@nyumc.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538777905;}s:3:"raw";s:5546:"Gross_Assignment #4

1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

Given that we completed our efficacy trials of the Starting Early Program (StEP), I plan to use an effectiveness-implementation hybrid type 1 design for this new proposal. For Aim 1, to determine if StEP is effective at decreasing overweight status and weight for length z-scores compared to standard prenatal and pediatric care, we will conduct a randomized controlled trial. This randomization will occur at the participant level not the clinic level. I am currently proposing to randomize participants at the individual level across four primary care sites. I do not believe that we are ready to fund a cluster randomized trial with enough power to detect differences. 


2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?

For Aim 2, to explore contextual factors (individual, organizational) that may influence StEP implementation to inform future modifications, we will use a mixed methods design. 

We will use mixed methods approaches to assess stakeholder perspectives of StEP. Mixed methods research designs encompass collecting, analyzing, and integrating quantitative and qualitative data, and their analyses and interpretations, to provide a richer understanding than either approach could provide alone. We will conduct quantitative surveys and semi-structured qualitative interviews with organizational stakeholders to assess the Inner Setting domain of the Consolidated Framework for Implementation Research (CFIR). Specifically we will assess facilitators and barriers to implementation, such as leadership culture, climate, motivation and alignment of organizational priorities. 

Sampling: We will conduct concurrent data collection of quantitative and qualitative assessments with organizational stakeholders. Surveys and semi-structured interviews will be conducted with stakeholders at the beginning of implementation and near program completion (n=60, ~30 per period facilitating the likelihood of achieving saturation). Purposive sampling will be utilized to obtain maximum variation in clinic involvement (varied clinic staff; e.g., prenatal and pediatric primary care providers, nurses, front desk staff, nutritionists, social workers, healthy steps specialists, and clinic leadership) in order to select information-rich cases for intensive study with divergent perspectives. Interviews will continue until thematic saturation is reached. 

Quantitative data collection: Stakeholders will complete a validated questionnaire to assess constructs within the CFIR Inner Setting domain, including subscales on culture, leadership engagement and resource allocation (Fernandez et al, 2018). Surveys will be sent to organizational stakeholders anonymously using REDcap. 

Qualitative data collection: Interviews will be conducted by a trained interviewer in a private setting. Interviews will last about 30 minutes, and will be audio recorded and professionally transcribed. We will develop and critically review an interview codebook with a team of general and developmental-behavioral pediatricians and nutritionists. Interviews will assess organizational facilitators and barriers to StEP implementation, including structural characteristics (social architecture and size of the organization), leadership engagement (commitment, involvement, and leaders’ accountability with the implementation), culture (norms, values, and basic assumptions of a given organization), climate (capacity for change, shared receptivity of involved individuals to an intervention, and the extent to which use of that intervention will be rewarded, supported, and expected within their organization), and resource allocation (resources dedicated for implementation, including training, space, and time). 

Qualitative analyses: Interview transcripts will be reviewed for emerging themes using an iterative process of textual analysis. The constant comparative method will be used to identify and iteratively refine codes and devise a final code structure. Transcripts will initially be read in their entirety to gain insight into broad themes. The group will note repeating themes. Themes and subthemes will be reviewed to confirm consistency, coherence, and distinctiveness before defining and finally deciding on a codebook that reflects the meaning of the data. Each reader will do multiple passes to get the full picture of interview flow. Consensus will be reached regarding each quotation. Data organization and retrieval will be facilitated by Dedoose software. 

Mixed methods analyses: Through triangulation, a process for strategically utilizing both methods together, we will examine convergence and expansion. Convergence is a strategy used to determine whether the quantitative and qualitative results provide the same answer to the same question (e.g., does the survey of leadership engagement in general clinic procedures concur with qualitative interview data regarding perceived leadership support of the StEP program?). Expansion is a strategy to determine whether the qualitative data can explain unanticipated findings produced by the quantitative data (e.g., can the quantitative data that suggests reasons for low resources availability be further explained by the qualitative data?). 



";s:5:"xhtml";s:5646:"Gross_Assignment #4<br /><br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />Given that we completed our efficacy trials of the Starting Early Program (StEP), I plan to use an effectiveness-implementation hybrid type 1 design for this new proposal. For Aim 1, to determine if StEP is effective at decreasing overweight status and weight for length z-scores compared to standard prenatal and pediatric care, we will conduct a randomized controlled trial. This randomization will occur at the participant level not the clinic level. I am currently proposing to randomize participants at the individual level across four primary care sites. I do not believe that we are ready to fund a cluster randomized trial with enough power to detect differences. <br /><br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br /><br />For Aim 2, to explore contextual factors (individual, organizational) that may influence StEP implementation to inform future modifications, we will use a mixed methods design. <br /><br />We will use mixed methods approaches to assess stakeholder perspectives of StEP. Mixed methods research designs encompass collecting, analyzing, and integrating quantitative and qualitative data, and their analyses and interpretations, to provide a richer understanding than either approach could provide alone. We will conduct quantitative surveys and semi-structured qualitative interviews with organizational stakeholders to assess the Inner Setting domain of the Consolidated Framework for Implementation Research (CFIR). Specifically we will assess facilitators and barriers to implementation, such as leadership culture, climate, motivation and alignment of organizational priorities. <br /><br />Sampling: We will conduct concurrent data collection of quantitative and qualitative assessments with organizational stakeholders. Surveys and semi-structured interviews will be conducted with stakeholders at the beginning of implementation and near program completion (n=60, ~30 per period facilitating the likelihood of achieving saturation). Purposive sampling will be utilized to obtain maximum variation in clinic involvement (varied clinic staff; e.g., prenatal and pediatric primary care providers, nurses, front desk staff, nutritionists, social workers, healthy steps specialists, and clinic leadership) in order to select information-rich cases for intensive study with divergent perspectives. Interviews will continue until thematic saturation is reached. <br /><br />Quantitative data collection: Stakeholders will complete a validated questionnaire to assess constructs within the CFIR Inner Setting domain, including subscales on culture, leadership engagement and resource allocation (Fernandez et al, 2018). Surveys will be sent to organizational stakeholders anonymously using REDcap. <br /><br />Qualitative data collection: Interviews will be conducted by a trained interviewer in a private setting. Interviews will last about 30 minutes, and will be audio recorded and professionally transcribed. We will develop and critically review an interview codebook with a team of general and developmental-behavioral pediatricians and nutritionists. Interviews will assess organizational facilitators and barriers to StEP implementation, including structural characteristics (social architecture and size of the organization), leadership engagement (commitment, involvement, and leaders’ accountability with the implementation), culture (norms, values, and basic assumptions of a given organization), climate (capacity for change, shared receptivity of involved individuals to an intervention, and the extent to which use of that intervention will be rewarded, supported, and expected within their organization), and resource allocation (resources dedicated for implementation, including training, space, and time). <br /><br />Qualitative analyses: Interview transcripts will be reviewed for emerging themes using an iterative process of textual analysis. The constant comparative method will be used to identify and iteratively refine codes and devise a final code structure. Transcripts will initially be read in their entirety to gain insight into broad themes. The group will note repeating themes. Themes and subthemes will be reviewed to confirm consistency, coherence, and distinctiveness before defining and finally deciding on a codebook that reflects the meaning of the data. Each reader will do multiple passes to get the full picture of interview flow. Consensus will be reached regarding each quotation. Data organization and retrieval will be facilitated by Dedoose software. <br /><br />Mixed methods analyses: Through triangulation, a process for strategically utilizing both methods together, we will examine convergence and expansion. Convergence is a strategy used to determine whether the quantitative and qualitative results provide the same answer to the same question (e.g., does the survey of leadership engagement in general clinic procedures concur with qualitative interview data regarding perceived leadership support of the StEP program?). Expansion is a strategy to determine whether the qualitative data can explain unanticipated findings produced by the quantitative data (e.g., can the quantitative data that suggests reasons for low resources availability be further explained by the qualitative data?).";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"316e5a1ce2c21964b5897881f30f26c2";}s:4:"show";b:1;s:3:"cid";s:32:"a12cce983fcac80180cf96123420b4bd";}s:32:"1a913ecf6fefadbc9487aee62f11d45e";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"cvamos";s:4:"name";s:12:"Cheryl Vamos";s:4:"mail";s:21:"cvamos@health.usf.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538785403;}s:3:"raw";s:3212:"VAMOS_Assignment #4

1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

Aim 1: Assess current implementation conditions (i.e., evidence, context and facilitation) and preferred intervention characteristics needed to increase adoption and integration of the prenatal oral health guidelines into the healthcare delivery system among clinic stakeholders (e.g., prenatal provider, nurse, medical assistants, office staff, administrators).

An observational study design will be employed in Aim 1. Specifically, this will be a cross-sectional study to assess implementation conditions and elicit preferred intervention characteristics. Methods include clinic observation, and in-depth interviews and focus groups with providers and clinic staff and administration. This is the best design for this aim, as it will collect important formative data to inform the identification of potential intervention strategies to facilitate the translation of the prenatal oral health guidelines into routine prenatal care visits.

Aim 2: Identify promising implementation strategies to facilitate the adoption and integration of the prenatal oral health guidelines into the healthcare delivery system among clinic stakeholders (e.g., prenatal provider, nurse, medical assistants, office staff, administrators).

To be honest, I am not sure what to call the study design for this aim, as methods used in this phase will include a modified Delphi technique to identify and prioritize promising implementation strategies given Aim 1 findings, existing literature/known strategies (ERIC) and consultation with the Scientific and Practice Advisory Boards. 

Aim 3: Develop and evaluate the potential fit and impact of the implementation strategies with regards to the integration of prenatal oral health that considers the complex healthcare delivery contexts on system-level (i.e., patient, provider, clinic) outcomes.

An effectiveness hybrid Type 2 study design will be employed in Aim 3. This aim will both test identified implementation strategies and evaluate their impact on system-level (i.e., patient, provider, clinic) outcomes. Implementation outcomes include (1) acceptability; (2) appropriateness; and (3) feasibility (measured by established/validated instruments). Clinic/service outcomes include (1) patient-centeredness (patient perspective); (2) satisfaction (provider/clinic perspective); and (3) effectiveness (provider/clinic perspective). 


2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? 

Yes, a mixed method design will be utilized for this study. Specifically, an embedded-sequential mixed methods design will be employed. Quantitative (observations, in-depth interviews, focus groups) and qualitative (e.g., modified Delphi technique, pre-post test surveys) methods will be used across the three aims. These methods will provide a comprehensive understanding of the context, process, and potential impact of identified implementation strategies.
";s:5:"xhtml";s:3306:"VAMOS_Assignment #4<br /><br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />Aim 1: Assess current implementation conditions (i.e., evidence, context and facilitation) and preferred intervention characteristics needed to increase adoption and integration of the prenatal oral health guidelines into the healthcare delivery system among clinic stakeholders (e.g., prenatal provider, nurse, medical assistants, office staff, administrators).<br /><br />An observational study design will be employed in Aim 1. Specifically, this will be a cross-sectional study to assess implementation conditions and elicit preferred intervention characteristics. Methods include clinic observation, and in-depth interviews and focus groups with providers and clinic staff and administration. This is the best design for this aim, as it will collect important formative data to inform the identification of potential intervention strategies to facilitate the translation of the prenatal oral health guidelines into routine prenatal care visits.<br /><br />Aim 2: Identify promising implementation strategies to facilitate the adoption and integration of the prenatal oral health guidelines into the healthcare delivery system among clinic stakeholders (e.g., prenatal provider, nurse, medical assistants, office staff, administrators).<br /><br />To be honest, I am not sure what to call the study design for this aim, as methods used in this phase will include a modified Delphi technique to identify and prioritize promising implementation strategies given Aim 1 findings, existing literature/known strategies (ERIC) and consultation with the Scientific and Practice Advisory Boards. <br /><br />Aim 3: Develop and evaluate the potential fit and impact of the implementation strategies with regards to the integration of prenatal oral health that considers the complex healthcare delivery contexts on system-level (i.e., patient, provider, clinic) outcomes.<br /><br />An effectiveness hybrid Type 2 study design will be employed in Aim 3. This aim will both test identified implementation strategies and evaluate their impact on system-level (i.e., patient, provider, clinic) outcomes. Implementation outcomes include (1) acceptability; (2) appropriateness; and (3) feasibility (measured by established/validated instruments). Clinic/service outcomes include (1) patient-centeredness (patient perspective); (2) satisfaction (provider/clinic perspective); and (3) effectiveness (provider/clinic perspective). <br /><br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? <br /><br />Yes, a mixed method design will be utilized for this study. Specifically, an embedded-sequential mixed methods design will be employed. Quantitative (observations, in-depth interviews, focus groups) and qualitative (e.g., modified Delphi technique, pre-post test surveys) methods will be used across the three aims. These methods will provide a comprehensive understanding of the context, process, and potential impact of identified implementation strategies.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"2c2789c58e72142d416822822d1bd831";}s:4:"show";b:1;s:3:"cid";s:32:"1a913ecf6fefadbc9487aee62f11d45e";}s:32:"e410b99715913d8380ab7b620942229c";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"anahmias";s:4:"name";s:15:"Allison Nahmias";s:4:"mail";s:21:"asnahmias@ucdavis.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538960258;}s:3:"raw";s:2893:"Nahmias_Assignment #4

1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?
I am thinking that I need to break my proposed project up into 2 pieces, as I am feeling that I’ll need more evidence of the efficacy the adapted EBP package before I can move on to the hybrid type 1 effectiveness implementation trial I had been planning. So the first project would be to adapt peer-mediated evidence-based interventions for students with autism to fit the school context by simplifying existing comprehensive EBI packages down to key ingredients.  This efficacy trial would be conducted using a community partnered approach, obtaining stakeholder needs prior to the intervention adaptation.  Ideally a SMART design would be used to identify active intervention ingredients, although due to sample & power concerns a multiple baseline approach is more likely.  
The second project would be a hybrid type 1 effectiveness implementation trial, which would test the effectiveness on student outcomes of the adapted EBP package to support social & peer relationships in students with ASD developed in the first project and collect information about implementation processes and outcomes.  Aim 1 (testing the effectiveness of the adapted EBP package) would be a wait-list control randomized control trial, randomized at the level of the school to prevent contamination.  Aim 2 would assess teacher fidelity to the adapted EBP package via observational coding of teaching strategies.  Further adaptations to the intervention made by teachers would be documented based on observational coding and interviews.  Aim 3 would assess implementation processes and outcomes from the CFIR framework related to intervention characteristics (e.g., complexity), inner setting (e.g., compatibility, implementation climate), and characteristics of individuals (e.g., individual stage of change) using mixed methods. Teachers and important stakeholders would complete brief questionnaires prior to and following the intervention training regarding intervention characteristics, inner setting, and characteristics.  A subset of teachers that did and did not reach fidelity in the intervention (and leaders from their schools/districts) will participate for focus groups to collect qualitative data on the same topics using the CFIR intervention questions.

2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?
Yes.  I will be using a complementary QUAN-> qual approach as described above for Aims 2 & 3 of the second project.  The qualitative data will serve to enhance the understanding of the quantitative data related to adaptations and implementation outcomes.  
";s:5:"xhtml";s:2940:"Nahmias_Assignment #4<br /><br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br />I am thinking that I need to break my proposed project up into 2 pieces, as I am feeling that I’ll need more evidence of the efficacy the adapted EBP package before I can move on to the hybrid type 1 effectiveness implementation trial I had been planning. So the first project would be to adapt peer-mediated evidence-based interventions for students with autism to fit the school context by simplifying existing comprehensive EBI packages down to key ingredients.  This efficacy trial would be conducted using a community partnered approach, obtaining stakeholder needs prior to the intervention adaptation.  Ideally a SMART design would be used to identify active intervention ingredients, although due to sample &amp; power concerns a multiple baseline approach is more likely.  <br />The second project would be a hybrid type 1 effectiveness implementation trial, which would test the effectiveness on student outcomes of the adapted EBP package to support social &amp; peer relationships in students with ASD developed in the first project and collect information about implementation processes and outcomes.  Aim 1 (testing the effectiveness of the adapted EBP package) would be a wait-list control randomized control trial, randomized at the level of the school to prevent contamination.  Aim 2 would assess teacher fidelity to the adapted EBP package via observational coding of teaching strategies.  Further adaptations to the intervention made by teachers would be documented based on observational coding and interviews.  Aim 3 would assess implementation processes and outcomes from the CFIR framework related to intervention characteristics (e.g., complexity), inner setting (e.g., compatibility, implementation climate), and characteristics of individuals (e.g., individual stage of change) using mixed methods. Teachers and important stakeholders would complete brief questionnaires prior to and following the intervention training regarding intervention characteristics, inner setting, and characteristics.  A subset of teachers that did and did not reach fidelity in the intervention (and leaders from their schools/districts) will participate for focus groups to collect qualitative data on the same topics using the CFIR intervention questions.<br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br />Yes.  I will be using a complementary QUAN-&gt; qual approach as described above for Aims 2 &amp; 3 of the second project.  The qualitative data will serve to enhance the understanding of the quantitative data related to adaptations and implementation outcomes.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"20b40a7146f691a4cf584fdd8c38c254";}s:4:"show";b:1;s:3:"cid";s:32:"e410b99715913d8380ab7b620942229c";}s:32:"ab18b21f1ef7395e748af62e84fcc776";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"mmoniz";s:4:"name";s:14:"Michelle Moniz";s:4:"mail";s:20:"mmoniz@med.umich.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539002455;}s:3:"raw";s:2263:"Moniz_Assignment #4:
1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?
We will conduct an explanatory sequential mixed methods study to pilot evaluate a toolkit-based, multicomponent implementation strategy to implement immediate postpartum contraceptive services in a single academic medical center (primary outcomes: acceptability and appropriateness of individual components of the implementation strategy among healthcare providers and patients; secondary outcomes: real-time intervention modifications, costs of implementation, patient experience of care, utilization of contraception among postpartum women).
 
2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?
Yes, we will use mixed methods.  After implementation and a 6 month wash-in period, we will survey of all maternity care providers at our pilot study site (approximately 150 attendings, 60 residents, and 250 nurses). Survey items will measure respondent perceptions of the appropriateness and acceptability of implementation strategy components. We will then purposively sample very positive and very negative respondents for semi-structured interviews, with a goal of better understanding providers’ implementation experience as well as refining components of the implementation toolkit.  We will also complete a mixed methods study of patients.  We will complete surveys of third trimester and 4-8weeks postpartum women (n=90 in each group) pre-implementation and at about 6 months post-implementation to measure the patient experience of care.  Post-implementation surveys will also include items measuring the appropriateness and acceptability of patient-facing implementation strategy components (e.g. patient counseling tools).  We will then purposively sample very positive and very negative respondents for semi-structured interviews. These activities will help us refine items in our implementation toolkit, refine our data collection tools, and collect the necessary pilot data for a future multi-site implementation study. 
";s:5:"xhtml";s:2286:"Moniz_Assignment #4:<br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br />We will conduct an explanatory sequential mixed methods study to pilot evaluate a toolkit-based, multicomponent implementation strategy to implement immediate postpartum contraceptive services in a single academic medical center (primary outcomes: acceptability and appropriateness of individual components of the implementation strategy among healthcare providers and patients; secondary outcomes: real-time intervention modifications, costs of implementation, patient experience of care, utilization of contraception among postpartum women).<br /> <br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br />Yes, we will use mixed methods.  After implementation and a 6 month wash-in period, we will survey of all maternity care providers at our pilot study site (approximately 150 attendings, 60 residents, and 250 nurses). Survey items will measure respondent perceptions of the appropriateness and acceptability of implementation strategy components. We will then purposively sample very positive and very negative respondents for semi-structured interviews, with a goal of better understanding providers’ implementation experience as well as refining components of the implementation toolkit.  We will also complete a mixed methods study of patients.  We will complete surveys of third trimester and 4-8weeks postpartum women (n=90 in each group) pre-implementation and at about 6 months post-implementation to measure the patient experience of care.  Post-implementation surveys will also include items measuring the appropriateness and acceptability of patient-facing implementation strategy components (e.g. patient counseling tools).  We will then purposively sample very positive and very negative respondents for semi-structured interviews. These activities will help us refine items in our implementation toolkit, refine our data collection tools, and collect the necessary pilot data for a future multi-site implementation study.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"1eca0a5e9aa2bfd99ef649bde4f47a49";}s:4:"show";b:1;s:3:"cid";s:32:"ab18b21f1ef7395e748af62e84fcc776";}s:32:"2bd128783bfab2e6aca20ce89035cc5f";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"rsturke";s:4:"name";s:13:"Rachel Sturke";s:4:"mail";s:25:"sturkerachel@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539428273;}s:3:"raw";s:298:"Really thoughtful approach to your study design.  And think it's especially appropriate to use the hybrid type 2 design and agree with the practicalities of pilot testing prior to a randomized trial.  Am interested to hear more about how you will structure the focus groups.  But overall great job!";s:5:"xhtml";s:303:"Really thoughtful approach to your study design.  And think it&#039;s especially appropriate to use the hybrid type 2 design and agree with the practicalities of pilot testing prior to a randomized trial.  Am interested to hear more about how you will structure the focus groups.  But overall great job!";s:6:"parent";s:32:"6847f29f15382f6065e104a8472bf2e4";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"2bd128783bfab2e6aca20ce89035cc5f";}s:32:"ecdb1419b24f94a1c7bbf0ecd39c4bb1";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"rsturke";s:4:"name";s:13:"Rachel Sturke";s:4:"mail";s:25:"sturkerachel@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539429021;}s:3:"raw";s:516:"thoughtful reworking of your aims! Would be good to unpack your cluster randomized approach to the first two aims a bit and clarify/provide more detail on how your approach is cluster randomized given randomization at the individual level.  I would suggest providing more detail on your mixed-method approach to aims 3 and 4 and the value of survey and interview data - specifically what you will gain from each of these approaches and in the context of your study how the data will compliment each other.  Good job!";s:5:"xhtml";s:516:"thoughtful reworking of your aims! Would be good to unpack your cluster randomized approach to the first two aims a bit and clarify/provide more detail on how your approach is cluster randomized given randomization at the individual level.  I would suggest providing more detail on your mixed-method approach to aims 3 and 4 and the value of survey and interview data - specifically what you will gain from each of these approaches and in the context of your study how the data will compliment each other.  Good job!";s:6:"parent";s:32:"1d63607e08cb0eef9d11a69417819e07";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"ecdb1419b24f94a1c7bbf0ecd39c4bb1";}s:32:"2c2789c58e72142d416822822d1bd831";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"rsturke";s:4:"name";s:13:"Rachel Sturke";s:4:"mail";s:25:"sturkerachel@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539432866;}s:3:"raw";s:383:"Nice job.  In reading your assignment, it seems that it might be worth considering merging aim 1 and aim 2 and the delphi method would also provide cross sectional data to inform your further aims.  It would be useful for you to unpack your study design for aim 3 a bit more.  How and where are you planning to test the implementation strategies and how many of them will you test?  ";s:5:"xhtml";s:381:"Nice job.  In reading your assignment, it seems that it might be worth considering merging aim 1 and aim 2 and the delphi method would also provide cross sectional data to inform your further aims.  It would be useful for you to unpack your study design for aim 3 a bit more.  How and where are you planning to test the implementation strategies and how many of them will you test?";s:6:"parent";s:32:"1a913ecf6fefadbc9487aee62f11d45e";s:7:"replies";a:1:{i:0;s:32:"c7ee3427ad20c026e6267b39c6610559";}s:4:"show";b:1;s:3:"cid";s:32:"2c2789c58e72142d416822822d1bd831";}s:32:"20b40a7146f691a4cf584fdd8c38c254";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"rsturke";s:4:"name";s:13:"Rachel Sturke";s:4:"mail";s:25:"sturkerachel@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539434570;}s:3:"raw";s:390:"I agree that it makes sense to partition the project into two in the way that you propose.  You are proposing a lot, so in thinking about this practically, it'll be important to figure out funding realities, timeline, etc.  For the second project, can you anticipate what further adaptations might be necessary?  And will be important to define fidelity for your project.  Overall nice job.";s:5:"xhtml";s:395:"I agree that it makes sense to partition the project into two in the way that you propose.  You are proposing a lot, so in thinking about this practically, it&#039;ll be important to figure out funding realities, timeline, etc.  For the second project, can you anticipate what further adaptations might be necessary?  And will be important to define fidelity for your project.  Overall nice job.";s:6:"parent";s:32:"e410b99715913d8380ab7b620942229c";s:7:"replies";a:1:{i:0;s:32:"7858e599a2739220cb4a5b359b7949b6";}s:4:"show";b:1;s:3:"cid";s:32:"20b40a7146f691a4cf584fdd8c38c254";}s:32:"1eca0a5e9aa2bfd99ef649bde4f47a49";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"rsturke";s:4:"name";s:13:"Rachel Sturke";s:4:"mail";s:25:"sturkerachel@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539435095;}s:3:"raw";s:237:"Nice job and very thoughtful approach. Would be important to very carefully describe the individual components of the implementation strategy and also think ahead of time about what adaptations might be necessary.  But overall great job!";s:5:"xhtml";s:237:"Nice job and very thoughtful approach. Would be important to very carefully describe the individual components of the implementation strategy and also think ahead of time about what adaptations might be necessary.  But overall great job!";s:6:"parent";s:32:"ab18b21f1ef7395e748af62e84fcc776";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"1eca0a5e9aa2bfd99ef649bde4f47a49";}s:32:"7858e599a2739220cb4a5b359b7949b6";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"anahmias";s:4:"name";s:15:"Allison Nahmias";s:4:"mail";s:21:"asnahmias@ucdavis.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539563682;}s:3:"raw";s:501:"Thanks Rachel! In response to your point about funding realities, I was thinking of applying for an F32 for the first project, but have been struggling with how to methods for the efficacy testing that would be feasible given the small budget.  It seems like the most feasible options would be a single-subject multiple baseline, an under-powered RCT, or a quasi-experimental approach (like an interrupted time series), but I'm not sure if others have thoughts as to what would be the most fundable.  ";s:5:"xhtml";s:504:"Thanks Rachel! In response to your point about funding realities, I was thinking of applying for an F32 for the first project, but have been struggling with how to methods for the efficacy testing that would be feasible given the small budget.  It seems like the most feasible options would be a single-subject multiple baseline, an under-powered RCT, or a quasi-experimental approach (like an interrupted time series), but I&#039;m not sure if others have thoughts as to what would be the most fundable.";s:6:"parent";s:32:"20b40a7146f691a4cf584fdd8c38c254";s:7:"replies";a:1:{i:0;s:32:"fbdd2a0459989592c4f30c7a678b69bc";}s:4:"show";b:1;s:3:"cid";s:32:"7858e599a2739220cb4a5b359b7949b6";}s:32:"fbdd2a0459989592c4f30c7a678b69bc";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"anahmias";s:4:"name";s:15:"Allison Nahmias";s:4:"mail";s:21:"asnahmias@ucdavis.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1539807971;s:8:"modified";i:1539831594;}s:3:"raw";s:134:"Actually I just realized that clinical trials aren't allowed under an F32, so I need to rethink my funding mechanism or the project.  ";s:5:"xhtml";s:137:"Actually I just realized that clinical trials aren&#039;t allowed under an F32, so I need to rethink my funding mechanism or the project.";s:6:"parent";s:32:"7858e599a2739220cb4a5b359b7949b6";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"fbdd2a0459989592c4f30c7a678b69bc";}s:32:"5da2fb7adf25b35fc1f3e7c3606f8e94";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"jpatterson";s:4:"name";s:19:"Jacquelyn Patterson";s:4:"mail";s:28:"jackie_patterson@med.unc.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539951020;}s:3:"raw";s:2443:"PATTERSON – Assignment #5

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

My proposed study is testing the implementation strategy of audit and feedback. The evidence-based practice is the standard resuscitation algorithm, Helping Babies Breathe. The implementation strategy of audit and feedback is delivered via a combination of the mobile health application, NeoWatch, and feedback from a supervisor. 

Audit and feedback is a particularly fitting implementation strategy to change resuscitation practice. In life-threatening, time-pressured situations such as resuscitation, decision making is driven by past exposure to similar situations (a concept referred to as recognition-primed decision making). This theory suggests that expertise in resuscitation care does not come from formal training alone, but requires complementary and repetitive bedside decision making. This repetition builds a critical mass of experience to identify newborns in need of resuscitation and to develop habitual patterns of effective care. Developing these patterns occurs following an evaluation, typically a self-evaluation, of the outcome of events. However, self-evaluation is flawed by a tendency to invoke competence after success, but not question it after failure. This tendency can be tempered by an objective evaluation of effectiveness, a strategy called audit and feedback. 

2.	How might you link specific implementation strategies to the context in which your work is set?

Audit and feedback of resuscitations in high-income countries (HICs) by video recording care followed by debriefing improves provider performance. This strategy requires debriefing by an expert clinician-educator who identifies deficiencies in care and delivers feedback. Unfortunately, local expertise is infrequently available in health facilities in LMICs. mHealth approaches could automate audit and feedback to improve adherence to evidence-based practices in LMICs where it is needed most.

3.	If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? 

Non-applicable. See above answers.";s:5:"xhtml";s:2513:"PATTERSON – Assignment #5<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />My proposed study is testing the implementation strategy of audit and feedback. The evidence-based practice is the standard resuscitation algorithm, Helping Babies Breathe. The implementation strategy of audit and feedback is delivered via a combination of the mobile health application, NeoWatch, and feedback from a supervisor. <br /><br />Audit and feedback is a particularly fitting implementation strategy to change resuscitation practice. In life-threatening, time-pressured situations such as resuscitation, decision making is driven by past exposure to similar situations (a concept referred to as recognition-primed decision making). This theory suggests that expertise in resuscitation care does not come from formal training alone, but requires complementary and repetitive bedside decision making. This repetition builds a critical mass of experience to identify newborns in need of resuscitation and to develop habitual patterns of effective care. Developing these patterns occurs following an evaluation, typically a self-evaluation, of the outcome of events. However, self-evaluation is flawed by a tendency to invoke competence after success, but not question it after failure. This tendency can be tempered by an objective evaluation of effectiveness, a strategy called audit and feedback. <br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />Audit and feedback of resuscitations in high-income countries (HICs) by video recording care followed by debriefing improves provider performance. This strategy requires debriefing by an expert clinician-educator who identifies deficiencies in care and delivers feedback. Unfortunately, local expertise is infrequently available in health facilities in LMICs. mHealth approaches could automate audit and feedback to improve adherence to evidence-based practices in LMICs where it is needed most.<br /><br />3.	If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? <br /><br />Non-applicable. See above answers.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"7239598ddb9d18692476ccfe0f383d42";}s:4:"show";b:1;s:3:"cid";s:32:"5da2fb7adf25b35fc1f3e7c3606f8e94";}s:32:"caf043fdb4b6ed9d1f118f844bf2f6ca";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"mroberts";s:4:"name";s:13:"Megan Roberts";s:4:"mail";s:32:"megan.y.roberts@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539976952;}s:3:"raw";s:1974:"ROBERTS – Assignment #5

1. If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

Given that my proposed research project is an effectiveness-implementation hybrid type 1, I will not be testing the effects of specific implementation strategies but rather I will use the results of the trial to: (a) examine facilitators and barriers to implementation and (b) select implementation strategies that are relevant and feasible to speech-language pathologists. I will match identified determinants (facilitators and barriers) to specific implementation strategies. For example, I hypothesize that lack of knowledge as to what to teach parents may be one barrier to implementation. An implementation strategy to address this barrier might be education sessions. However, to determine the ideal components of the education session I will use conjoint analysis (discrete choice experiments) to determine the features of implementation components that speech-language pathologists prefer the most. 

2. How might you link specific implementation strategies to the context in which your work is set?

The context in which this research occurs is the homes of families. Given that this context is likely to vary considerable between homes, I hope that by identifying determinants and choosing implementation strategies using the preferences of speech-language pathologists that the implementation strategies will more readily fit the context in which collaborative coaching occurs (i.e., the homes of families).

3. If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? 

This question does not apply to my study. 
";s:5:"xhtml";s:2032:"ROBERTS – Assignment #5<br /><br />1. If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />Given that my proposed research project is an effectiveness-implementation hybrid type 1, I will not be testing the effects of specific implementation strategies but rather I will use the results of the trial to: (a) examine facilitators and barriers to implementation and (b) select implementation strategies that are relevant and feasible to speech-language pathologists. I will match identified determinants (facilitators and barriers) to specific implementation strategies. For example, I hypothesize that lack of knowledge as to what to teach parents may be one barrier to implementation. An implementation strategy to address this barrier might be education sessions. However, to determine the ideal components of the education session I will use conjoint analysis (discrete choice experiments) to determine the features of implementation components that speech-language pathologists prefer the most. <br /><br />2. How might you link specific implementation strategies to the context in which your work is set?<br /><br />The context in which this research occurs is the homes of families. Given that this context is likely to vary considerable between homes, I hope that by identifying determinants and choosing implementation strategies using the preferences of speech-language pathologists that the implementation strategies will more readily fit the context in which collaborative coaching occurs (i.e., the homes of families).<br /><br />3. If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? <br /><br />This question does not apply to my study.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"d0c04ea55d35abd3f1f196b84318fb1f";}s:4:"show";b:1;s:3:"cid";s:32:"caf043fdb4b6ed9d1f118f844bf2f6ca";}s:32:"c7ee3427ad20c026e6267b39c6610559";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"cvamos";s:4:"name";s:12:"Cheryl Vamos";s:4:"mail";s:21:"cvamos@health.usf.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539994364;}s:3:"raw";s:142:"Hello Rachel:
Thank you for your comments - all excellent and important points that I need to consider and still flesh out. Thank you!

Cheryl";s:5:"xhtml";s:157:"Hello Rachel:<br />Thank you for your comments - all excellent and important points that I need to consider and still flesh out. Thank you!<br /><br />Cheryl";s:6:"parent";s:32:"2c2789c58e72142d416822822d1bd831";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"c7ee3427ad20c026e6267b39c6610559";}s:32:"94613ae730393dda80f0807234685e23";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"anahmias";s:4:"name";s:15:"Allison Nahmias";s:4:"mail";s:21:"asnahmias@ucdavis.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539996780;}s:3:"raw";s:1804:"Nahmias Assignment #5:
1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?
My proposed project does not involve selecting specific implementation strategies.  For the first project I will be asking key stakeholders about barriers and facilitators to the implementation of peer-mediated interventions.  These will be coded based on the CFIR constructs and the CFIR-ERIC Matching Tool will be used to identify potential implementation strategies to address the identified CFIR barriers. The second project is a hybrid type 1 model, so I am planning to continue to monitor facilitators and barriers to implementation and sustainment of the adapted EBP package, and explore how they align with the implementation strategies identified from project 1.  
2.	How might you link specific implementation strategies to the context in which your work is set?
My work is set in schools, so I am thinking about ways that implementation strategies can be linked to existing training and support mechanisms in schools (e.g., Professional Development days, Professional Learning Communities), but I will be mainly learning about what implementation strategies are relevant for this context via my proposed projects.  

3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?
This is not explicitly relevant to my proposal, but I am planning on using focus groups and questionnaires to monitor participants’ use of implementation strategies.  
";s:5:"xhtml";s:1836:"Nahmias Assignment #5:<br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br />My proposed project does not involve selecting specific implementation strategies.  For the first project I will be asking key stakeholders about barriers and facilitators to the implementation of peer-mediated interventions.  These will be coded based on the CFIR constructs and the CFIR-ERIC Matching Tool will be used to identify potential implementation strategies to address the identified CFIR barriers. The second project is a hybrid type 1 model, so I am planning to continue to monitor facilitators and barriers to implementation and sustainment of the adapted EBP package, and explore how they align with the implementation strategies identified from project 1.  <br />2.	How might you link specific implementation strategies to the context in which your work is set?<br />My work is set in schools, so I am thinking about ways that implementation strategies can be linked to existing training and support mechanisms in schools (e.g., Professional Development days, Professional Learning Communities), but I will be mainly learning about what implementation strategies are relevant for this context via my proposed projects.  <br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br />This is not explicitly relevant to my proposal, but I am planning on using focus groups and questionnaires to monitor participants’ use of implementation strategies.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"286ad670cbe1cb3a38d812509a865269";}s:4:"show";b:1;s:3:"cid";s:32:"94613ae730393dda80f0807234685e23";}s:32:"0f49c1daa87e12c91ec4443956914962";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"cvamos";s:4:"name";s:12:"Cheryl Vamos";s:4:"mail";s:21:"cvamos@health.usf.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539996982;}s:3:"raw";s:4053:"VAMOS - Assignment #5

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

Aim 1 involves assessing current implementation conditions and preferred intervention characteristics, and Aim 2 involves identifying/prioritizing promising implementation strategies. Given the (1) previously identified barriers to integrating the oral health guidelines into prenatal care practice from existing research as well as the studies conducted by the PI, (2) different barriers and facilitators needed to be addressed across the three main practice behaviors (assess, advise, refer/coordinate care), and (3) different clinic stakeholders who could play a role in guideline implementation throughout the clinical visit (e.g., prenatal provider, nurse, medical assistance, front office staff, administrators, IT, etc.), it is expected that multiple implementation strategies will need to be identified and tested.  Of the 9 categories of implementation strategies, it is assumed that the following would emerge from Aims 1 and 2 findings: Strategy #2 (Interactive Assistance); Strategy #3 (Adapting and Tailoring to Context); Strategy #5 (Train/Educate Stakeholder); Strategy #6 (Supporting Clinicians); and Strategy #7 (Engage Consumers). Specifically, the following implementation strategies as outlined in Expert Recommendations for Implementing Change (ERIC) are would be most promising:
Conduct educational meetings
Conduct educational outreach visits
Develop and implement tools for quality monitoring
Develop educational materials
Distribute educational materials
Facilitation
Involve patients/consumers
Prepare patients/consumers to be active participants
Remind clinicians
Tailor strategies

2.	How might you link specific implementation strategies to the context in which your work is set?

It is hypothesized that the implementation strategies needed to facilitate integration of the oral health guidelines into the prenatal care visit would depend on the prenatal care setting (e.g., private clinic; federally qualified health center) and prenatal clinic staff involved at the individual clinic site given the clinical workflow (e.g., prenatal provider; nurse; midwife; etc.). Also, depending on the extent to which the prenatal care clinic (organization) would like to engage patients through education, self-assessment and other patient activation activities prior to their interaction with the main provider (e.g., while pregnant patients are still in the waiting room), some intervention strategies may also be targeted towards pregnant women (e.g., provide patients with an eHealth app that self-directs them in the education and some assessment questions).

3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? 

An effectiveness hybrid Type 2 study design will be employed in Aim 3. This aim will both test identified implementation strategies and evaluate their impact on system-level (i.e., patient, provider, clinic) outcomes. Implementation outcomes include (1) acceptability; (2) appropriateness; and (3) feasibility (measured by established/validated instruments). Clinic/service outcomes include (1) patient-centeredness (patient perspective); (2) satisfaction (provider/clinic perspective); and (3) effectiveness (provider/clinic perspective). Thus, fidelity will need to be measured and will be self-reported via pre/post-tests and follow-up interviews with clinic staff. At this time, fidelity is more important for clinic stakeholders vs. pregnant patients as the guidelines specify practice behaviors for prenatal providers and thus they serve as the primary audience (even though patients may be engaged and active participants in an intervention strategy).
";s:5:"xhtml";s:4162:"VAMOS - Assignment #5<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />Aim 1 involves assessing current implementation conditions and preferred intervention characteristics, and Aim 2 involves identifying/prioritizing promising implementation strategies. Given the (1) previously identified barriers to integrating the oral health guidelines into prenatal care practice from existing research as well as the studies conducted by the PI, (2) different barriers and facilitators needed to be addressed across the three main practice behaviors (assess, advise, refer/coordinate care), and (3) different clinic stakeholders who could play a role in guideline implementation throughout the clinical visit (e.g., prenatal provider, nurse, medical assistance, front office staff, administrators, IT, etc.), it is expected that multiple implementation strategies will need to be identified and tested.  Of the 9 categories of implementation strategies, it is assumed that the following would emerge from Aims 1 and 2 findings: Strategy #2 (Interactive Assistance); Strategy #3 (Adapting and Tailoring to Context); Strategy #5 (Train/Educate Stakeholder); Strategy #6 (Supporting Clinicians); and Strategy #7 (Engage Consumers). Specifically, the following implementation strategies as outlined in Expert Recommendations for Implementing Change (ERIC) are would be most promising:<br />Conduct educational meetings<br />Conduct educational outreach visits<br />Develop and implement tools for quality monitoring<br />Develop educational materials<br />Distribute educational materials<br />Facilitation<br />Involve patients/consumers<br />Prepare patients/consumers to be active participants<br />Remind clinicians<br />Tailor strategies<br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />It is hypothesized that the implementation strategies needed to facilitate integration of the oral health guidelines into the prenatal care visit would depend on the prenatal care setting (e.g., private clinic; federally qualified health center) and prenatal clinic staff involved at the individual clinic site given the clinical workflow (e.g., prenatal provider; nurse; midwife; etc.). Also, depending on the extent to which the prenatal care clinic (organization) would like to engage patients through education, self-assessment and other patient activation activities prior to their interaction with the main provider (e.g., while pregnant patients are still in the waiting room), some intervention strategies may also be targeted towards pregnant women (e.g., provide patients with an eHealth app that self-directs them in the education and some assessment questions).<br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? <br /><br />An effectiveness hybrid Type 2 study design will be employed in Aim 3. This aim will both test identified implementation strategies and evaluate their impact on system-level (i.e., patient, provider, clinic) outcomes. Implementation outcomes include (1) acceptability; (2) appropriateness; and (3) feasibility (measured by established/validated instruments). Clinic/service outcomes include (1) patient-centeredness (patient perspective); (2) satisfaction (provider/clinic perspective); and (3) effectiveness (provider/clinic perspective). Thus, fidelity will need to be measured and will be self-reported via pre/post-tests and follow-up interviews with clinic staff. At this time, fidelity is more important for clinic stakeholders vs. pregnant patients as the guidelines specify practice behaviors for prenatal providers and thus they serve as the primary audience (even though patients may be engaged and active participants in an intervention strategy).";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"69f3262d77a3bb866671bb8cdea71e0b";}s:4:"show";b:1;s:3:"cid";s:32:"0f49c1daa87e12c91ec4443956914962";}s:32:"8bba0dec479521074b3fb96f592f478c";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"rgross";s:4:"name";s:12:"Rachel Gross";s:4:"mail";s:22:"Rachel.Gross@nyumc.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540219993;}s:3:"raw";s:3222:"Gross_Assignment #5

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

Given that my proposal is an effectiveness-implementation hybrid type 1 study, I will not be directly testing the effects of specific implementation strategies. However, when I think about where my study fits into the Proctor 2009 Model, this current proposal aims to focus on implementation and health outcomes of the Starting Early Program (our evidence-based program for the prevention of early child obesity). It also aims to build evidence for what implementation strategies would be needed to enhance our implementation outcomes in order to improve adoption, implementation and sustainability of the program going forward. Through my second aim to study organizational level contextual factors, I will be preparing to study the following potential implementation strategies: 
a) Evaluation and iterative strategies (e.g., assessing for readiness; identifying barriers and facilitators); 
b) Adapting and tailoring to context (e.g., tailoring strategies; promoting adaptability) 
c) Developing stakeholder relationships (e.g., identifying and preparing champions; informing local opinion leaders; building coalitions)
d) Training/educating stakeholders (e.g., conducting ongoing training; developing educational materials; creating learning collaborative)
e) Engaging participants (e.g., involving participants and family members; intervening to enhance uptake and adherence) 
f) Using financial strategies (e.g., accessing new funding). 

2.	How might you link specific implementation strategies to the context in which your work is set?

The context in which this research will take place is in the primary care setting. Advantages include: 1) frequent prenatal and pediatric visits, which are widely attended, even among high-risk families; 2) use of existing infrastructure to lower cost and decrease need for additional transportation; and 3) ability to build on preexisting provider relationships. We will work to match the identified contextual barriers with the appropriate implementation strategies. Given variability in the primary care settings, we will work to identify contextual factors/barriers that may influence implementation across the different sites. For example, if we find lack of knowledge to be a barrier then we would consider adding educational sessions. However, if beliefs/attitudes are a barrier, then we would consider adding strategies to enhance peer influences and opinion leaders. If participant engagement is a barrier, we would add strategies to enhance engagement such as motivational interviewing. If the barrier to sustainability involves the need for sustainable funding, we will test different billing strategies. 

3.	If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?

I do not think that this question applies to my study.
";s:5:"xhtml";s:3311:"Gross_Assignment #5<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />Given that my proposal is an effectiveness-implementation hybrid type 1 study, I will not be directly testing the effects of specific implementation strategies. However, when I think about where my study fits into the Proctor 2009 Model, this current proposal aims to focus on implementation and health outcomes of the Starting Early Program (our evidence-based program for the prevention of early child obesity). It also aims to build evidence for what implementation strategies would be needed to enhance our implementation outcomes in order to improve adoption, implementation and sustainability of the program going forward. Through my second aim to study organizational level contextual factors, I will be preparing to study the following potential implementation strategies: <br />a) Evaluation and iterative strategies (e.g., assessing for readiness; identifying barriers and facilitators); <br />b) Adapting and tailoring to context (e.g., tailoring strategies; promoting adaptability) <br />c) Developing stakeholder relationships (e.g., identifying and preparing champions; informing local opinion leaders; building coalitions)<br />d) Training/educating stakeholders (e.g., conducting ongoing training; developing educational materials; creating learning collaborative)<br />e) Engaging participants (e.g., involving participants and family members; intervening to enhance uptake and adherence) <br />f) Using financial strategies (e.g., accessing new funding). <br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />The context in which this research will take place is in the primary care setting. Advantages include: 1) frequent prenatal and pediatric visits, which are widely attended, even among high-risk families; 2) use of existing infrastructure to lower cost and decrease need for additional transportation; and 3) ability to build on preexisting provider relationships. We will work to match the identified contextual barriers with the appropriate implementation strategies. Given variability in the primary care settings, we will work to identify contextual factors/barriers that may influence implementation across the different sites. For example, if we find lack of knowledge to be a barrier then we would consider adding educational sessions. However, if beliefs/attitudes are a barrier, then we would consider adding strategies to enhance peer influences and opinion leaders. If participant engagement is a barrier, we would add strategies to enhance engagement such as motivational interviewing. If the barrier to sustainability involves the need for sustainable funding, we will test different billing strategies. <br /><br />3.	If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /><br />I do not think that this question applies to my study.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"e3019f7203b50a6e0a7820a2faad9d8f";}s:4:"show";b:1;s:3:"cid";s:32:"8bba0dec479521074b3fb96f592f478c";}s:32:"316e5a1ce2c21964b5897881f30f26c2";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"rsturke";s:4:"name";s:13:"Rachel Sturke";s:4:"mail";s:25:"sturkerachel@mail.nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540320866;}s:3:"raw";s:589:"Nice job and very thoughtful responses.  Apologies as well for my delay here -- was a complete oversight on my part.  I think your approach is very well thought out. A couple of questions - do you anticipate any response rate issues with the validated questionnaire?  Might be worth considering as you design the study.  And how many respondents do you anticipate interviewing?  I think the CFIR makes a lot of sense for your study and I also think your approach of triangulation will add strength to your study as it is a powerful tool to ensure robust study results.  Overall, great job.";s:5:"xhtml";s:589:"Nice job and very thoughtful responses.  Apologies as well for my delay here -- was a complete oversight on my part.  I think your approach is very well thought out. A couple of questions - do you anticipate any response rate issues with the validated questionnaire?  Might be worth considering as you design the study.  And how many respondents do you anticipate interviewing?  I think the CFIR makes a lot of sense for your study and I also think your approach of triangulation will add strength to your study as it is a powerful tool to ensure robust study results.  Overall, great job.";s:6:"parent";s:32:"a12cce983fcac80180cf96123420b4bd";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"316e5a1ce2c21964b5897881f30f26c2";}s:32:"e385b29c67e042ea526e118bcd7f9db6";a:8:{s:4:"user";a:5:{s:2:"id";s:6:"mmoniz";s:4:"name";s:14:"Michelle Moniz";s:4:"mail";s:20:"mmoniz@med.umich.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540325500;}s:3:"raw";s:1598:"Moniz_Assignment #5:
1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?
Based on a multiple case study of 11 hospitals, we compiled a list of over 25 commonly used ERIC strategies to assist with immediate postpartum LARC implementation. We also added suggested strategies and tools, based on interviewees assessment of what could have made the implementation process more effective.In our feasibility study, we will have our study site stakeholder Panel select from these strategies. 
2.	How might you link specific implementation strategies to the context in which your work is set?
We are using a rapid assessment approach with CFIR-based semi-structured interviews with key informants to identify determinants of practice at the feasibility study site.  We are then mapping determinants to ERIC strategies. We are using Stakeholder Panel meetings to prioritize determinants and select final strategies/behavior change techniques to bundle into a multi-component implementation strategy. 
3.	If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?
We will evaluate key differences in outcome by type of maternity care provider (Ob, midwife, family physician, resident). We will also monitor trends in key outcomes over time.
";s:5:"xhtml";s:1627:"Moniz_Assignment #5:<br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br />Based on a multiple case study of 11 hospitals, we compiled a list of over 25 commonly used ERIC strategies to assist with immediate postpartum LARC implementation. We also added suggested strategies and tools, based on interviewees assessment of what could have made the implementation process more effective.In our feasibility study, we will have our study site stakeholder Panel select from these strategies. <br />2.	How might you link specific implementation strategies to the context in which your work is set?<br />We are using a rapid assessment approach with CFIR-based semi-structured interviews with key informants to identify determinants of practice at the feasibility study site.  We are then mapping determinants to ERIC strategies. We are using Stakeholder Panel meetings to prioritize determinants and select final strategies/behavior change techniques to bundle into a multi-component implementation strategy. <br />3.	If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br />We will evaluate key differences in outcome by type of maternity care provider (Ob, midwife, family physician, resident). We will also monitor trends in key outcomes over time.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"56439f58ea200f4fad10e3667309aec0";}s:4:"show";b:1;s:3:"cid";s:32:"e385b29c67e042ea526e118bcd7f9db6";}s:32:"7239598ddb9d18692476ccfe0f383d42";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540514139;}s:3:"raw";s:262:"You have a well-developed rationale for selection of the implementation strategy and for pairing audit and feedback with mHealth. It may be helpful to monitor whether other implementation strategies end up also being used in combination with audit and feedback. ";s:5:"xhtml";s:261:"You have a well-developed rationale for selection of the implementation strategy and for pairing audit and feedback with mHealth. It may be helpful to monitor whether other implementation strategies end up also being used in combination with audit and feedback.";s:6:"parent";s:32:"5da2fb7adf25b35fc1f3e7c3606f8e94";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"7239598ddb9d18692476ccfe0f383d42";}s:32:"d0c04ea55d35abd3f1f196b84318fb1f";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540514197;}s:3:"raw";s:1125:"1.	 This sounds like a well-developed approach to selection of implementation trial.  I expect you will learn a lot from the trial about implementation process to inform selection of implementations strategies.  It also sounds like you already have hypotheses about potential barriers/strategies. 
2.	I agree that the setting of the intervention will largely be in the home, but it seems that the implementation context is Part C funded Early Intervention Speech and language services. It is possible that this context is even broader and that organizations included additional disciplines beyond SLPs. I expect that you’ll learn a lot more about the context as you test effectiveness.  I encourage you to consider what information you’ll collect about the context (org size, training structures, etc) during the effectiveness test to set you up for an implementation study. 
3.	I do think this question could apply to your study as you may find significant variability in how the intervention is delivered.  Understanding factors that explain variability should be helpful in the selection of implementation strategies.
";s:5:"xhtml";s:1134:"1.	 This sounds like a well-developed approach to selection of implementation trial.  I expect you will learn a lot from the trial about implementation process to inform selection of implementations strategies.  It also sounds like you already have hypotheses about potential barriers/strategies. <br />2.	I agree that the setting of the intervention will largely be in the home, but it seems that the implementation context is Part C funded Early Intervention Speech and language services. It is possible that this context is even broader and that organizations included additional disciplines beyond SLPs. I expect that you’ll learn a lot more about the context as you test effectiveness.  I encourage you to consider what information you’ll collect about the context (org size, training structures, etc) during the effectiveness test to set you up for an implementation study. <br />3.	I do think this question could apply to your study as you may find significant variability in how the intervention is delivered.  Understanding factors that explain variability should be helpful in the selection of implementation strategies.";s:6:"parent";s:32:"caf043fdb4b6ed9d1f118f844bf2f6ca";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"d0c04ea55d35abd3f1f196b84318fb1f";}s:32:"286ad670cbe1cb3a38d812509a865269";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540514220;}s:3:"raw";s:522:"1.	 Your approach to selecting strategies appears systematic and appropriate. To further expand beyond barriers and facilitator, it may be helpful to also ask stakeholders about which implementation strategies they are already using or find helpful. 
2.	I expect that you will learn a lot from your first step about the context of schools and the types of implementation strategies that will be most useful in this context.
3.	Your use of mixed methods makes sense for your goals of monitoring implementation strategies. 
";s:5:"xhtml";s:530:"1.	 Your approach to selecting strategies appears systematic and appropriate. To further expand beyond barriers and facilitator, it may be helpful to also ask stakeholders about which implementation strategies they are already using or find helpful. <br />2.	I expect that you will learn a lot from your first step about the context of schools and the types of implementation strategies that will be most useful in this context.<br />3.	Your use of mixed methods makes sense for your goals of monitoring implementation strategies.";s:6:"parent";s:32:"94613ae730393dda80f0807234685e23";s:7:"replies";a:1:{i:0;s:32:"3bc2f807557d3bb7fa2a74b2de994824";}s:4:"show";b:1;s:3:"cid";s:32:"286ad670cbe1cb3a38d812509a865269";}s:32:"69f3262d77a3bb866671bb8cdea71e0b";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540514261;}s:3:"raw";s:412:"1.	I like your approach for selection of strategies.  You are anticipating using a number of strategies. As you select, it could be helpful to consider how these strategies will be sequenced and combined. 
2.	I agree with the approach to select strategies based on the characteristics of setting and the audience.
3.	The proposed implementation outcomes seem appropriate as do the mixed methods to collect them.
";s:5:"xhtml";s:421:"1.	I like your approach for selection of strategies.  You are anticipating using a number of strategies. As you select, it could be helpful to consider how these strategies will be sequenced and combined. <br />2.	I agree with the approach to select strategies based on the characteristics of setting and the audience.<br />3.	The proposed implementation outcomes seem appropriate as do the mixed methods to collect them.";s:6:"parent";s:32:"0f49c1daa87e12c91ec4443956914962";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"69f3262d77a3bb866671bb8cdea71e0b";}s:32:"e3019f7203b50a6e0a7820a2faad9d8f";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540514301;}s:3:"raw";s:735:"1.	 It sounds like you are considering a number of different implementation strategies.  Might it be possible to measure or characterize the implementation strategies you use to implement the program during the effectiveness test?  Those could be useful data in informing the selection of strategies you will test in the next step.
2.	Your approach to selection of strategies based on barriers within the context seems appropriate. I also wonder if there are specific strategies that you think are particularly relevant to known barriers/constraints the service setting. 
3.	See my response to #1. You may gain a lot of useful information about helpful or needed implementation strategies within the context of the effectiveness test.
";s:5:"xhtml";s:744:"1.	 It sounds like you are considering a number of different implementation strategies.  Might it be possible to measure or characterize the implementation strategies you use to implement the program during the effectiveness test?  Those could be useful data in informing the selection of strategies you will test in the next step.<br />2.	Your approach to selection of strategies based on barriers within the context seems appropriate. I also wonder if there are specific strategies that you think are particularly relevant to known barriers/constraints the service setting. <br />3.	See my response to #1. You may gain a lot of useful information about helpful or needed implementation strategies within the context of the effectiveness test.";s:6:"parent";s:32:"8bba0dec479521074b3fb96f592f478c";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"e3019f7203b50a6e0a7820a2faad9d8f";}s:32:"56439f58ea200f4fad10e3667309aec0";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540514338;}s:3:"raw";s:457:"1.	It sounds like you used a very systematic approach to characterizing implementation strategies.  I appreciate that you are using data from end users on what they are using and perceive as useful.  Those data should be helpful in informing the stakeholder panel selection of strategies.
2.	I wonder whether any of your data suggest or will suggest strategies that are feasible and perceived as useful in combination. 
3.	This approach sounds appropriate.
";s:5:"xhtml";s:466:"1.	It sounds like you used a very systematic approach to characterizing implementation strategies.  I appreciate that you are using data from end users on what they are using and perceive as useful.  Those data should be helpful in informing the stakeholder panel selection of strategies.<br />2.	I wonder whether any of your data suggest or will suggest strategies that are feasible and perceived as useful in combination. <br />3.	This approach sounds appropriate.";s:6:"parent";s:32:"e385b29c67e042ea526e118bcd7f9db6";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"56439f58ea200f4fad10e3667309aec0";}s:32:"8d0fdc82e0e2a596a2a3caae270bcc34";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540514467;}s:3:"raw";s:220:"Hi All,
Just a quick note to let you know how impressed I am by the development of your projects!  I appreciate how well you have already incorporated so much of what you have learned to your projects.  Nice job!
Lauren
";s:5:"xhtml";s:229:"Hi All,<br />Just a quick note to let you know how impressed I am by the development of your projects!  I appreciate how well you have already incorporated so much of what you have learned to your projects.  Nice job!<br />Lauren";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"8d0fdc82e0e2a596a2a3caae270bcc34";}s:32:"3bc2f807557d3bb7fa2a74b2de994824";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"anahmias";s:4:"name";s:15:"Allison Nahmias";s:4:"mail";s:21:"asnahmias@ucdavis.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540575685;}s:3:"raw";s:1223:"Thanks, Lauren for the helpful feedback, that's a great idea to also ask stakeholders about which implementation strategies they are already using or find helpful. 
As an update, I've had to switch gears a bit as I was planning on using this project to apply for an F32 but learned last week that clinical trials are not allowed.  So now I am thinking of the following aims for the first project:
•	Aim 1: To identify facilitators and barriers, and current implementation strategies for the use of peer-mediated EBIs for ASD in community-settings based on CFIR constructs via mixed methods (focus groups and questionnaires).
•	Aim 2:  To identify adaptations to peer-mediated EBIs made by community-providers and to explore the association of these adaptations with student outcomes via classroom observations.
•	Aim 3: To develop an adapted peer-mediated intervention for ASD and implementation strategies for the adapted intervention based on the results from Aims 1 & 2.
Then the second project would be the efficacy trial of the adapted peer-mediated intervention and then a third project would be a hybrid 1 effectiveness trial.
I'd really appreciate any feedback you have about this adapted plan.   Thank you!
 ";s:5:"xhtml";s:1270:"Thanks, Lauren for the helpful feedback, that&#039;s a great idea to also ask stakeholders about which implementation strategies they are already using or find helpful. <br />As an update, I&#039;ve had to switch gears a bit as I was planning on using this project to apply for an F32 but learned last week that clinical trials are not allowed.  So now I am thinking of the following aims for the first project:<br />•	Aim 1: To identify facilitators and barriers, and current implementation strategies for the use of peer-mediated EBIs for ASD in community-settings based on CFIR constructs via mixed methods (focus groups and questionnaires).<br />•	Aim 2:  To identify adaptations to peer-mediated EBIs made by community-providers and to explore the association of these adaptations with student outcomes via classroom observations.<br />•	Aim 3: To develop an adapted peer-mediated intervention for ASD and implementation strategies for the adapted intervention based on the results from Aims 1 &amp; 2.<br />Then the second project would be the efficacy trial of the adapted peer-mediated intervention and then a third project would be a hybrid 1 effectiveness trial.<br />I&#039;d really appreciate any feedback you have about this adapted plan.   Thank you!";s:6:"parent";s:32:"286ad670cbe1cb3a38d812509a865269";s:7:"replies";a:1:{i:0;s:32:"175d57cf1e6eb668845997b6abc2f197";}s:4:"show";b:1;s:3:"cid";s:32:"3bc2f807557d3bb7fa2a74b2de994824";}s:32:"175d57cf1e6eb668845997b6abc2f197";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"lbrookman";s:4:"name";s:23:"Lauren Brookman-Frazee ";s:4:"mail";s:18:"lbrookman@ucsd.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540577859;}s:3:"raw";s:337:"I'm glad the feedback was helpful.  It sounds like you have a good plan for the constraints of the F32. Since there is evidence for the peer-mediated interventions and you will be making adaptations to facilitate community implementation and fit, you could likely make the case for an effectiveness study (vs efficacy) as your next step.";s:5:"xhtml";s:342:"I&#039;m glad the feedback was helpful.  It sounds like you have a good plan for the constraints of the F32. Since there is evidence for the peer-mediated interventions and you will be making adaptations to facilitate community implementation and fit, you could likely make the case for an effectiveness study (vs efficacy) as your next step.";s:6:"parent";s:32:"3bc2f807557d3bb7fa2a74b2de994824";s:7:"replies";a:1:{i:0;s:32:"31ad7b2e550a860b621be89786d03f08";}s:4:"show";b:1;s:3:"cid";s:32:"175d57cf1e6eb668845997b6abc2f197";}s:32:"31ad7b2e550a860b621be89786d03f08";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"anahmias";s:4:"name";s:15:"Allison Nahmias";s:4:"mail";s:21:"asnahmias@ucdavis.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540585740;}s:3:"raw";s:11:"Thank you!!";s:5:"xhtml";s:11:"Thank you!!";s:6:"parent";s:32:"175d57cf1e6eb668845997b6abc2f197";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"31ad7b2e550a860b621be89786d03f08";}}s:11:"subscribers";N;}