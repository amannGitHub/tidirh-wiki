a:5:{s:5:"title";N;s:6:"status";i:1;s:6:"number";i:100;s:8:"comments";a:100:{s:32:"e484b33b6831a5b28d946fed99c60fc7";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1534353660;}s:3:"raw";s:101:"Hello Everyone!

Welcome to TIDIRH 2018. I look forward to getting started working together.

Lindsey";s:5:"xhtml";s:121:"Hello Everyone!<br /><br />Welcome to TIDIRH 2018. I look forward to getting started working together.<br /><br />Lindsey";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"e484b33b6831a5b28d946fed99c60fc7";}s:32:"7b12768ff77ef6d730490744d29f4357";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"adopp";s:4:"name";s:9:"Alex Dopp";s:4:"mail";s:13:"dopp@uark.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1534783952;}s:3:"raw";s:5800:"DOPP - Assignment #1a

1. Draft a specific aims page (2 pages maximum) of your proposed study. 

Here are my draft specific aims. I am happy to send a formatted Word version of these aims that includes bold/underlining for emphasis, a reference list, etc. to anyone who is interested.

SPECIFIC AIMS

High-quality research has identified a number of evidence-based treatments (EBTs) that are clinically effective and cost-beneficial when implemented by youth mental health service agencies1–6 and taken to scale in service systems.7–9 Yet the availability of EBTs in youth mental health services remains limited10–12 due to critical challenges related to their adoption, such as financing the numerous activities (e.g., training and consultation, monitoring and feedback, case management and coordination) necessary for successful, sustained implementation.13,14 Currently, youth mental health service agencies and their partners (e.g., purveyor and intermediary organizations that support EBT implementation; financial partners who fund implementation efforts) must navigate a complex process of selecting and coordinating various strategies to sustain long-term financing for EBT delivery.13–15 Thus, there is a significant unmet need to provide these agencies and their partners with guidance on how to select the optimal combination of financial strategies for their EBT sustainment efforts. Improved financial sustainability of EBTs will be essential to addressing the considerable public health impact16,17 and associated economic burden18 of mental health problems in youth. 

The overall goal of the proposed project is to work with youth mental health service agencies and their partners to develop a “fiscal mapping process” that will guide the tailored selection and coordination of financing strategies from a comprehensive, well-specified set of options. We define financing strategies as activities direct financial resources to support other activities involved in implementation and/or sustainment of EBTs. The project goal is consistent with the NIMH priority (Strategic Objective 4.2, Priority A) to develop and validate strategies for implementing, sustaining, and continuously improving EBTs in partnership with key stakeholders. Our highly interdisciplinary team will produce innovative extensions of current best-practice approaches to (1) specification19–21 and (2) tailored selection22–24 of implementation strategies. Graduate and undergraduate students will play key roles on the project team. 

Our approach will employ a sequenced set of mixed (i.e., quantitative and qualitative) methods to accomplish two Specific Aims listed below. Throughout the project, we will work with a panel of expert stakeholder participants that will include representatives from EBT intermediaries affiliated with the project team, youth mental health service agencies with whom the intermediaries have worked, and financing partners with whom the service agencies have worked. The expected sample sizes are 10-15 for each stakeholder group (representing a national sample of 6-8 youth mental health service agencies and their partners) for a total of N = 40 members on the panel.

	Aim 1: To develop and characterize a well-specified compilation of financing strategies used in youth mental health service agencies. We will begin by (1a) generating an initial compilation of financing strategies through a hermeneutic literature review,25 which will involve mutually informative cycles of engagement with empirical literature and our expert panel. Then, we will refine the compilation of financing strategies by having the expert panel (1b) specify each strategy in the compilation on seven key dimensions19 through a three-round Delphi26 consensus-building technique and (1c) characterize the importance, feasibility, and contextual influences on use for each specified strategy.

	Aim 2: To develop and characterize a fiscal mapping process that guides tailored selection and coordination of financing strategies. We will begin by (2a) co-developing the fiscal mapping process with key stakeholders, which will involve the expert panel specifying the steps of the process (again through three Delphi rounds) while each youth mental health service agency completes a sample fiscal map with its partners. The fiscal mapping process will be adapted from the intervention mapping process,27,28 an established technique for intervention planning that shows great promise for the tailored selection of implementation strategies.23 Finally, we will (2b) explore key characteristics of the youth mental health services ecosystem that can influence the fiscal mapping process. We will conduct a group model-building exercise,29 in which the expert panel co-constructs a model of how the fiscal mapping process functions within the youth mental health services ecosystem, and follow-up surveys about program characteristics related to financial sustainment.

The proposed research will generate a fiscal mapping process and component financial strategies that will facilitate the ability of youth mental health service agencies to sustain funding for EBTs following initial implementation. Unlike previous efforts to specify and tailor implementation strategies, we will explicitly consider the unique characteristics of financing strategies. Moreover, completion of this R15 project will significantly strengthen the research capacity and infrastructure at the University of Arkansas in preparation for the planned future studies in which we will test the impact of the fiscal mapping process on sustainment outcomes (including identification and testing of candidate mechanisms of action30) through a combination of randomized trials and “systems science” simulation studies.29";s:5:"xhtml";s:5890:"DOPP - Assignment #1a<br /><br />1. Draft a specific aims page (2 pages maximum) of your proposed study. <br /><br />Here are my draft specific aims. I am happy to send a formatted Word version of these aims that includes bold/underlining for emphasis, a reference list, etc. to anyone who is interested.<br /><br />SPECIFIC AIMS<br /><br />High-quality research has identified a number of evidence-based treatments (EBTs) that are clinically effective and cost-beneficial when implemented by youth mental health service agencies1–6 and taken to scale in service systems.7–9 Yet the availability of EBTs in youth mental health services remains limited10–12 due to critical challenges related to their adoption, such as financing the numerous activities (e.g., training and consultation, monitoring and feedback, case management and coordination) necessary for successful, sustained implementation.13,14 Currently, youth mental health service agencies and their partners (e.g., purveyor and intermediary organizations that support EBT implementation; financial partners who fund implementation efforts) must navigate a complex process of selecting and coordinating various strategies to sustain long-term financing for EBT delivery.13–15 Thus, there is a significant unmet need to provide these agencies and their partners with guidance on how to select the optimal combination of financial strategies for their EBT sustainment efforts. Improved financial sustainability of EBTs will be essential to addressing the considerable public health impact16,17 and associated economic burden18 of mental health problems in youth. <br /><br />The overall goal of the proposed project is to work with youth mental health service agencies and their partners to develop a “fiscal mapping process” that will guide the tailored selection and coordination of financing strategies from a comprehensive, well-specified set of options. We define financing strategies as activities direct financial resources to support other activities involved in implementation and/or sustainment of EBTs. The project goal is consistent with the NIMH priority (Strategic Objective 4.2, Priority A) to develop and validate strategies for implementing, sustaining, and continuously improving EBTs in partnership with key stakeholders. Our highly interdisciplinary team will produce innovative extensions of current best-practice approaches to (1) specification19–21 and (2) tailored selection22–24 of implementation strategies. Graduate and undergraduate students will play key roles on the project team. <br /><br />Our approach will employ a sequenced set of mixed (i.e., quantitative and qualitative) methods to accomplish two Specific Aims listed below. Throughout the project, we will work with a panel of expert stakeholder participants that will include representatives from EBT intermediaries affiliated with the project team, youth mental health service agencies with whom the intermediaries have worked, and financing partners with whom the service agencies have worked. The expected sample sizes are 10-15 for each stakeholder group (representing a national sample of 6-8 youth mental health service agencies and their partners) for a total of N = 40 members on the panel.<br /><br />	Aim 1: To develop and characterize a well-specified compilation of financing strategies used in youth mental health service agencies. We will begin by (1a) generating an initial compilation of financing strategies through a hermeneutic literature review,25 which will involve mutually informative cycles of engagement with empirical literature and our expert panel. Then, we will refine the compilation of financing strategies by having the expert panel (1b) specify each strategy in the compilation on seven key dimensions19 through a three-round Delphi26 consensus-building technique and (1c) characterize the importance, feasibility, and contextual influences on use for each specified strategy.<br /><br />	Aim 2: To develop and characterize a fiscal mapping process that guides tailored selection and coordination of financing strategies. We will begin by (2a) co-developing the fiscal mapping process with key stakeholders, which will involve the expert panel specifying the steps of the process (again through three Delphi rounds) while each youth mental health service agency completes a sample fiscal map with its partners. The fiscal mapping process will be adapted from the intervention mapping process,27,28 an established technique for intervention planning that shows great promise for the tailored selection of implementation strategies.23 Finally, we will (2b) explore key characteristics of the youth mental health services ecosystem that can influence the fiscal mapping process. We will conduct a group model-building exercise,29 in which the expert panel co-constructs a model of how the fiscal mapping process functions within the youth mental health services ecosystem, and follow-up surveys about program characteristics related to financial sustainment.<br /><br />The proposed research will generate a fiscal mapping process and component financial strategies that will facilitate the ability of youth mental health service agencies to sustain funding for EBTs following initial implementation. Unlike previous efforts to specify and tailor implementation strategies, we will explicitly consider the unique characteristics of financing strategies. Moreover, completion of this R15 project will significantly strengthen the research capacity and infrastructure at the University of Arkansas in preparation for the planned future studies in which we will test the impact of the fiscal mapping process on sustainment outcomes (including identification and testing of candidate mechanisms of action30) through a combination of randomized trials and “systems science” simulation studies.29";s:6:"parent";N;s:7:"replies";a:2:{i:0;s:32:"c152e46d4696e7abc2084ff28696c874";i:1;s:32:"a10e6c6f679370977234d2ec80bf54ed";}s:4:"show";b:1;s:3:"cid";s:32:"7b12768ff77ef6d730490744d29f4357";}s:32:"c152e46d4696e7abc2084ff28696c874";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1534900426;}s:3:"raw";s:362:"Hi Alex and Everyone,

Thanks, Alex, for getting the ball rolling in our workgroup with your R15 aims. 
I'm excited to see your proposal to engage stakeholders in group modeling, something we've done on our team.

Denny, Nick, and I, are still finalizing our plans for providing feedback and setting up our first call.  More to come soon!

Thanks again,

Lindsey";s:5:"xhtml";s:417:"Hi Alex and Everyone,<br /><br />Thanks, Alex, for getting the ball rolling in our workgroup with your R15 aims. <br />I&#039;m excited to see your proposal to engage stakeholders in group modeling, something we&#039;ve done on our team.<br /><br />Denny, Nick, and I, are still finalizing our plans for providing feedback and setting up our first call.  More to come soon!<br /><br />Thanks again,<br /><br />Lindsey";s:6:"parent";s:32:"7b12768ff77ef6d730490744d29f4357";s:7:"replies";a:1:{i:0;s:32:"b33014d6ff592086b1bd2d2181d06940";}s:4:"show";b:1;s:3:"cid";s:32:"c152e46d4696e7abc2084ff28696c874";}s:32:"0a9049790fdeffd9ec6d64a97575c2a6";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bmaddox";s:4:"name";s:13:"Brenna Maddox";s:4:"mail";s:17:"maddoxb@upenn.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1534965672;}s:3:"raw";s:4990:"MADDOX - Assignment #1a

1. Draft a specific aims page (2 pages maximum) of your proposed study.

Most adults with autism spectrum disorder (ASD) struggle with mental health problems, particularly anxiety and depression.1-6 Despite evidence that cognitive-behavioral treatment (CBT) can reduce co-occurring anxiety and depression in adults with ASD,7 mental health difficulties often are left untreated.8,9 A shortage of trained clinicians is one of the most commonly reported barriers to quality mental health care for adults with ASD.10,11 There is an urgent need to expand the supply of community mental health (CMH) professionals trained to work with adults who have ASD.

Through my recent partnership with six Philadelphia CMH centers, I have learned that clinicians do not feel competent or confident in treating adults with ASD.12 They report limited training, knowledge, and comfort in caring for adults with ASD. These clinicians and their agency leaders have expressed a strong desire for in-service trainings on how to treat adults with ASD and common co-occurring psychiatric disorders. However, training alone is unlikely to change therapist behavior.13 Ongoing consultation provides additional support, but it is often time-limited. It is currently unknown whether other implementation strategies are needed and feasible to improve clinicians’ ability and willingness to deliver CBT to adults with ASD. 

Based on preliminary data from my current F32 study, a major barrier is clinicians’ beliefs and attitudes that (1) CBT is not appropriate or effective for people with ASD, and (2) as mental health providers (separate from the developmental disability system), they are unequipped to treat people with ASD. To address this barrier, I will study the implementation strategy of using opinion leaders.14 Opinion leaders are individuals able to influence other people’s attitudes, beliefs, or overt behavior informally. Opinions leaders are the center of their interpersonal communication networks and share the characteristics of being knowledgeable, good communicators, and caring. The implementation strategy of using opinion leaders has primarily been studied in medicine, not mental healthcare.

Specifically, I propose an effectiveness-implementation hybrid trial, type 2 with the following aims:    

Aim 1: Evaluate the effectiveness of therapist training and ongoing consultation (implementation as usual) versus therapist training, ongoing consultation, and opinion leader involvement (multifaceted implementation strategy) to facilitate delivery of CBT for adults with ASD and co-occurring anxiety/depression in CMH centers. Opinion leaders will be identified using sociometric methods, which involves extensive analyses of leadership nominations within peer group members. I hypothesize that clinicians will have better fidelity to the CBT program and greater penetration among eligible clients in the multifaceted condition, compared to clinicians in the implementation as usual condition. 

Aim 2: Determine the effectiveness of CBT for reducing anxiety and depression in adults with ASD served in CMH centers. Previous studies of CBT for adults with ASD have been conducted in controlled, academic settings.7 I hypothesize that participants will experience improvements in their anxiety and/or depression symptoms, as measured by self-report measures administered at each treatment session.

To accomplish these aims, I will rely on an established academic-community partnership between the University of Pennsylvania’s Center for Mental Health Policy and Services Research and Philadelphia CMH centers. A total of 80 outpatient clinicians who serve adults with ASD and co-occurring anxiety and/or depression will be randomly assigned to receive either implementation as usual or the multifaceted implementation strategy. The targets of the implementation as usual condition are clinicians’ knowledge about the treatment, skills to use the treatment, and confidence to use the treatment effectively. The additional targets of the multifaceted implementation strategy are clinicians’ attitudes and beliefs about their ability to deliver CBT to people with ASD, and their attitudes and beliefs about the appropriateness and effectiveness of using CBT with people with ASD.

The proposed study will translate evidence-based CBT strategies for adults with ASD and co-occurring anxiety/depression to the front lines of clinical settings, in an effort to bridge the gap between research and practice. In addition, this study would advance the field of implementation science by directly testing implementation strategies that could be generalized to many evidence-based practices across multiple settings. The proposed project is well-aligned with recommendations from the Interagency Autism Coordinating Committee (IACC) to address ways to improve community access to high-quality services and supports that improve mental health for adults with ASD.16 ";s:5:"xhtml";s:5079:"MADDOX - Assignment #1a<br /><br />1. Draft a specific aims page (2 pages maximum) of your proposed study.<br /><br />Most adults with autism spectrum disorder (ASD) struggle with mental health problems, particularly anxiety and depression.1-6 Despite evidence that cognitive-behavioral treatment (CBT) can reduce co-occurring anxiety and depression in adults with ASD,7 mental health difficulties often are left untreated.8,9 A shortage of trained clinicians is one of the most commonly reported barriers to quality mental health care for adults with ASD.10,11 There is an urgent need to expand the supply of community mental health (CMH) professionals trained to work with adults who have ASD.<br /><br />Through my recent partnership with six Philadelphia CMH centers, I have learned that clinicians do not feel competent or confident in treating adults with ASD.12 They report limited training, knowledge, and comfort in caring for adults with ASD. These clinicians and their agency leaders have expressed a strong desire for in-service trainings on how to treat adults with ASD and common co-occurring psychiatric disorders. However, training alone is unlikely to change therapist behavior.13 Ongoing consultation provides additional support, but it is often time-limited. It is currently unknown whether other implementation strategies are needed and feasible to improve clinicians’ ability and willingness to deliver CBT to adults with ASD. <br /><br />Based on preliminary data from my current F32 study, a major barrier is clinicians’ beliefs and attitudes that (1) CBT is not appropriate or effective for people with ASD, and (2) as mental health providers (separate from the developmental disability system), they are unequipped to treat people with ASD. To address this barrier, I will study the implementation strategy of using opinion leaders.14 Opinion leaders are individuals able to influence other people’s attitudes, beliefs, or overt behavior informally. Opinions leaders are the center of their interpersonal communication networks and share the characteristics of being knowledgeable, good communicators, and caring. The implementation strategy of using opinion leaders has primarily been studied in medicine, not mental healthcare.<br /><br />Specifically, I propose an effectiveness-implementation hybrid trial, type 2 with the following aims:    <br /><br />Aim 1: Evaluate the effectiveness of therapist training and ongoing consultation (implementation as usual) versus therapist training, ongoing consultation, and opinion leader involvement (multifaceted implementation strategy) to facilitate delivery of CBT for adults with ASD and co-occurring anxiety/depression in CMH centers. Opinion leaders will be identified using sociometric methods, which involves extensive analyses of leadership nominations within peer group members. I hypothesize that clinicians will have better fidelity to the CBT program and greater penetration among eligible clients in the multifaceted condition, compared to clinicians in the implementation as usual condition. <br /><br />Aim 2: Determine the effectiveness of CBT for reducing anxiety and depression in adults with ASD served in CMH centers. Previous studies of CBT for adults with ASD have been conducted in controlled, academic settings.7 I hypothesize that participants will experience improvements in their anxiety and/or depression symptoms, as measured by self-report measures administered at each treatment session.<br /><br />To accomplish these aims, I will rely on an established academic-community partnership between the University of Pennsylvania’s Center for Mental Health Policy and Services Research and Philadelphia CMH centers. A total of 80 outpatient clinicians who serve adults with ASD and co-occurring anxiety and/or depression will be randomly assigned to receive either implementation as usual or the multifaceted implementation strategy. The targets of the implementation as usual condition are clinicians’ knowledge about the treatment, skills to use the treatment, and confidence to use the treatment effectively. The additional targets of the multifaceted implementation strategy are clinicians’ attitudes and beliefs about their ability to deliver CBT to people with ASD, and their attitudes and beliefs about the appropriateness and effectiveness of using CBT with people with ASD.<br /><br />The proposed study will translate evidence-based CBT strategies for adults with ASD and co-occurring anxiety/depression to the front lines of clinical settings, in an effort to bridge the gap between research and practice. In addition, this study would advance the field of implementation science by directly testing implementation strategies that could be generalized to many evidence-based practices across multiple settings. The proposed project is well-aligned with recommendations from the Interagency Autism Coordinating Committee (IACC) to address ways to improve community access to high-quality services and supports that improve mental health for adults with ASD.16";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"b2ff94bb72e4b38045efa4976ff15aa1";}s:4:"show";b:1;s:3:"cid";s:32:"0a9049790fdeffd9ec6d64a97575c2a6";}s:32:"c07bc00a1899f6f501a3f02f667cd8d3";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"kokamura";s:4:"name";s:14:"Kelsie Okamura";s:4:"mail";s:26:"kelsie.h.okamura@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535068096;}s:3:"raw";s:4998:"OKAMURA-Assignment #1a

1. Draft a specific aims page (2 pages maximum) of your proposed study.

One in five youth will suffer from a mental health condition before the age of 181 and few will receive best practice treatments including evidence-based practices (EBPs) in community settings.2 Over the past decade, EBP implementation has been highlighted as a public health concern and a resulting science focused on improving the adoption of quality mental health services delivered to youth has emerged.3 There are many barriers to EBP implementation in community settings that depend on the context: socio-political (e.g., funding priorities), service system (e.g., Department of Health policies), organization (e.g., readiness for change), therapists involved (e.g., beliefs around EBP), and the youth served (e.g., multimorbidity).

A critical component of many EBPs is the use of routine client outcome data to inform treatment decision-making, also known as measurement-based care (MBC).4-6 This distinct skill offers transparency in treatment outcome to consumers and provides valuable data on service effectiveness at the consumer level. Individual-level (e.g., therapist, care coordinator) knowledge measurement developments have revealed a distinction in specific EBP and MBC knowledge within community mental health therapists suggesting that implementation strategies targeted specifically at MBC are warranted.7 Therefore, a critical first step in successful EBP implementation is multicontextual implementation strategies to promote therapist MBC adoption.
	  
The State of Hawaiʻi Department of Health Behavioral Health Administration is currently undergoing multiyear development of a shared online electronic health record (EHR) platform between the Child and Adolescent Mental Health Division and the Developmental Disabilities Division. Both divisions share similar structures with consumers (e.g., clients, youth, caregivers) nested within care coordinators, geographically divided service units, and island specific branches, with contracted provider agencies delivering direct mental and behavioral health services. The EHR development goal is to embed unique division-specific MBC practice guidelines to improve workflow efficiency around clinical events such as service authorization and to ensure clients receive EBPs. The implementation is a multi-part process: (1) MBC workflow dissemination, (2) product champion testing and feedback, (3) internal division-wide rollout, and (4) contracted provider and consumer portal rollout.
	
This study will use a mixed method approach (QUALQUAN) to develop tailored implementation strategies to support the MBC EHR adoption. Qualitative interview data will inform latent class analysis to determine groups of adopters. We will select tailored implementation strategies for each group based on conjoint analysis with relevant stakeholders. Systematic quantitative analysis using standardized measures and hierarchical linear modeling will identify implementation determinants at the individual and organizational level.

Aim 1: Identify existing MBC barriers and facilitators within each division. Implementation frameworks8,9 and strategy taxonomies10 will guide the development of interviews. We will use focus groups and individual interviews to identify barriers and facilitators with respect to role group and geographic region within division. Latent class analysis will reveal distinct implementation target groups which may be based upon individual role or geographic region.

Aim 2: Use conjoint analysis to identify acceptable and feasible implementation strategies.11 We will use implementation strategies brainstormed in Aim 1 to assess stakeholder acceptability and feasibility with respect to role group and geographic region. Discrete choice experiments will be conducted and quantitatively coded to choose efficient and acceptable implementation strategies.

Aim 3: Develop predictive models of implementation success using standardized measures of knowledge, attitudes, self-efficacy, and organizational climate and change. We will assess implementation determinants within individuals at pre-, mid-, and post-implementation to characterize importance of specific determinants in MBC implementation. 
	
Developing a formal MBC system is an important overlooked step for system-wide EBP implementation in community settings. The extant MBC literature is comprised of frameworks, case examples, and literature reviews on identifying psychometrically-sound measures,4-6 and there is a need for more empirical research to identify successful implementation strategies across multiple contexts. This study will examine implementation strategies at the system, organization, and individual level through mixed method design and stakeholder engagement. Resulting implementation will be tailored to specific groups to support an individual’s idiographic need to ensure consumers receive MBC, EBP, and effective care.
";s:5:"xhtml";s:5087:"OKAMURA-Assignment #1a<br /><br />1. Draft a specific aims page (2 pages maximum) of your proposed study.<br /><br />One in five youth will suffer from a mental health condition before the age of 181 and few will receive best practice treatments including evidence-based practices (EBPs) in community settings.2 Over the past decade, EBP implementation has been highlighted as a public health concern and a resulting science focused on improving the adoption of quality mental health services delivered to youth has emerged.3 There are many barriers to EBP implementation in community settings that depend on the context: socio-political (e.g., funding priorities), service system (e.g., Department of Health policies), organization (e.g., readiness for change), therapists involved (e.g., beliefs around EBP), and the youth served (e.g., multimorbidity).<br /><br />A critical component of many EBPs is the use of routine client outcome data to inform treatment decision-making, also known as measurement-based care (MBC).4-6 This distinct skill offers transparency in treatment outcome to consumers and provides valuable data on service effectiveness at the consumer level. Individual-level (e.g., therapist, care coordinator) knowledge measurement developments have revealed a distinction in specific EBP and MBC knowledge within community mental health therapists suggesting that implementation strategies targeted specifically at MBC are warranted.7 Therefore, a critical first step in successful EBP implementation is multicontextual implementation strategies to promote therapist MBC adoption.<br />	  <br />The State of Hawaiʻi Department of Health Behavioral Health Administration is currently undergoing multiyear development of a shared online electronic health record (EHR) platform between the Child and Adolescent Mental Health Division and the Developmental Disabilities Division. Both divisions share similar structures with consumers (e.g., clients, youth, caregivers) nested within care coordinators, geographically divided service units, and island specific branches, with contracted provider agencies delivering direct mental and behavioral health services. The EHR development goal is to embed unique division-specific MBC practice guidelines to improve workflow efficiency around clinical events such as service authorization and to ensure clients receive EBPs. The implementation is a multi-part process: (1) MBC workflow dissemination, (2) product champion testing and feedback, (3) internal division-wide rollout, and (4) contracted provider and consumer portal rollout.<br />	<br />This study will use a mixed method approach (QUALQUAN) to develop tailored implementation strategies to support the MBC EHR adoption. Qualitative interview data will inform latent class analysis to determine groups of adopters. We will select tailored implementation strategies for each group based on conjoint analysis with relevant stakeholders. Systematic quantitative analysis using standardized measures and hierarchical linear modeling will identify implementation determinants at the individual and organizational level.<br /><br />Aim 1: Identify existing MBC barriers and facilitators within each division. Implementation frameworks8,9 and strategy taxonomies10 will guide the development of interviews. We will use focus groups and individual interviews to identify barriers and facilitators with respect to role group and geographic region within division. Latent class analysis will reveal distinct implementation target groups which may be based upon individual role or geographic region.<br /><br />Aim 2: Use conjoint analysis to identify acceptable and feasible implementation strategies.11 We will use implementation strategies brainstormed in Aim 1 to assess stakeholder acceptability and feasibility with respect to role group and geographic region. Discrete choice experiments will be conducted and quantitatively coded to choose efficient and acceptable implementation strategies.<br /><br />Aim 3: Develop predictive models of implementation success using standardized measures of knowledge, attitudes, self-efficacy, and organizational climate and change. We will assess implementation determinants within individuals at pre-, mid-, and post-implementation to characterize importance of specific determinants in MBC implementation. <br />	<br />Developing a formal MBC system is an important overlooked step for system-wide EBP implementation in community settings. The extant MBC literature is comprised of frameworks, case examples, and literature reviews on identifying psychometrically-sound measures,4-6 and there is a need for more empirical research to identify successful implementation strategies across multiple contexts. This study will examine implementation strategies at the system, organization, and individual level through mixed method design and stakeholder engagement. Resulting implementation will be tailored to specific groups to support an individual’s idiographic need to ensure consumers receive MBC, EBP, and effective care.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"9f7899eab6ac54f4b1461e806796ecdf";}s:4:"show";b:1;s:3:"cid";s:32:"c07bc00a1899f6f501a3f02f667cd8d3";}s:32:"b33014d6ff592086b1bd2d2181d06940";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"kokamura";s:4:"name";s:14:"Kelsie Okamura";s:4:"mail";s:26:"kelsie.h.okamura@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535068207;}s:3:"raw";s:369:"Hi everyone,

Very excited to be a part of TIDIRH 2018 and the Mental Health small group. Just an FYI, I am located in Hawaii. We are currently awaiting Hurricane Lane's arrival and expect to have heavy flooding, damage, and power outages. I'm hoping we will be back online next week, but there may be a delay in my responding. 

Thanks for your understanding!

-Kelsie";s:5:"xhtml";s:409:"Hi everyone,<br /><br />Very excited to be a part of TIDIRH 2018 and the Mental Health small group. Just an FYI, I am located in Hawaii. We are currently awaiting Hurricane Lane&#039;s arrival and expect to have heavy flooding, damage, and power outages. I&#039;m hoping we will be back online next week, but there may be a delay in my responding. <br /><br />Thanks for your understanding!<br /><br />-Kelsie";s:6:"parent";s:32:"c152e46d4696e7abc2084ff28696c874";s:7:"replies";a:2:{i:0;s:32:"dcfe57c23aa3eebfcbeae2d5e094e6f2";i:1;s:32:"b54f5ca9d95bc2bdedafb276e123e9a6";}s:4:"show";b:1;s:3:"cid";s:32:"b33014d6ff592086b1bd2d2181d06940";}s:32:"6753b2736bf728fc517656b02ea34734";a:8:{s:4:"user";a:5:{s:2:"id";s:11:"rshepardson";s:4:"name";s:16:"Robyn Shepardson";s:4:"mail";s:23:"Robyn.Shepardson@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535073830;}s:3:"raw";s:8703:"SHEPARDSON - Assignment #1a 

Draft a specific aims page (2 pages maximum) of your proposed study: 

Integrated primary care (IPC) aims to increase access to mental health treatment by embedding mental health care within the primary care setting (1). The Veterans Health Administration (VHA) implemented IPC nationally over a decade ago to improve care for common mental health concerns such as depression and anxiety (2). Anxiety disorders are quite prevalent (15-20%) and burdensome among VHA primary care patients (3-5). Although effective anxiety intervention techniques for anxiety (e.g., exposure therapy, cognitive therapy) are well-established in specialty mental health settings (6-7), our research on provider practices indicates that the evidence-based intervention techniques are infrequently used in real-world VHA IPC (8). Barriers include provider concerns about the feasibility of adequately delivering interventions within the brief treatment format (≤6 30-minute sessions) used in IPC (9). In general, providers face a dearth of brief, evidence-based anxiety interventions that are feasible to deliver in IPC (10), suggesting a critical gap in primary care anxiety treatment quality. The goal of the proposed study is to increase delivery of evidence-based anxiety interventions in VHA IPC settings to ultimately improve Veterans’ mental health and wellbeing. 

We have developed a brief anxiety intervention, consisting of a variety of modules featuring various evidence-based intervention techniques, that was designed specifically for VHA IPC to increase feasibility and acceptability among IPC providers. The intervention was adapted from extant evidence-based anxiety treatment protocols (11-12) to fit the population-health approach and brief treatment format used in IPC (9). In prior formative research with VHA IPC providers, we identified barriers to using evidence-based anxiety intervention techniques and obtained feedback on the intervention manual to inform further refinements. 

This brief anxiety intervention is currently being evaluated in a hybrid I effectiveness-implementation design randomized controlled trial (RCT) comparing the intervention to IPC usual care on the primary outcome of anxiety symptom severity. An effectiveness, rather than efficacy, trial is appropriate because the intervention is not entirely new, but rather consists of existing evidence-based techniques adapted into a new format for IPC. The hybrid type I design (13) was chosen because the intervention is being tested in this particular format for the first time, so the primary aim is to establish effectiveness. At the same time, to reduce the lag between treatment development and translation into clinical practice, we are collecting preliminary implementation data to inform the proposed study. A mixed methods process evaluation (14), informed by the RE-AIM framework (15), of intervention implementation in the hybrid I trial will identify possible patient-, provider-, and clinic-level barriers and facilitators that may affect future intervention adoption, fidelity, and sustainability.

If the brief anxiety intervention proves effective in the hybrid I trial, it would be eligible for wider implementation across VHA IPC to help fill the gap in primary care treatment options for anxiety. Implementation efforts for new interventions being rolled out in VHA emphasize training individual providers (e.g., didactic training via webinar, access to treatment manual and resources) and providing supervision of consultation cases to ensure fidelity in intervention delivery. However, implementation barriers are likely to occur not only at the level of individual providers, but also at the broader clinic and system levels. These clinic- and system-level barriers (e.g., national productivity targets, clinic/system norms, lack of protected time for training) contribute to an implementation gap that individual IPC providers are ill-equipped to solve on their own. Incorporating an external facilitator (16), who is an expert in both evidence-based implementation strategies and the clinical context (i.e., IPC) and program being introduced (i.e., the brief IPC anxiety intervention), into the implementation effort would likely improve implementation outcomes.    

The proposed study design is a hybrid type II effectiveness-implementation trial (13). This multi-site RCT will evaluate the evidence-based brief anxiety intervention as well as an evidence-based implementation strategy. Our research questions are: (1) Does the brief anxiety intervention improve upon patient clinical outcomes obtained in IPC usual care? and (2) Does an evidence-based implementation strategy (external facilitation) improve upon VHA IPC standard implementation support (SIS)? We hypothesize that patient-level clinical outcomes will be better for the brief anxiety intervention (vs. IPC usual care) and that implementation outcomes will be better for SIS plus external facilitation (vs. SIS). This study will be conducted over three years at four VHA primary care clinics in upstate New York. With a hybrid design, the two target populations at the four clinics are: (1) primary care patients experiencing anxiety symptoms (effectiveness aim) and (2) IPC providers (implementation aim).

Specific aim 1 is to evaluate the effectiveness of the brief anxiety intervention by comparing its patient-level clinical outcomes to those of IPC usual care for anxiety. This aim will be accomplished by comparing changes in anxiety symptom severity (measured by the Generalized Anxiety Disorder-7 (17) [GAD-7]) and depressive symptom severity (measured by the Patient Health Questionnaire-9 (18) [PHQ-9]) from baseline to 12 weeks (post) and 24 weeks (follow-up) of patients randomly assigned to either the brief anxiety intervention or IPC usual care. Participants will be adult, Veteran primary care patients with elevated anxiety (GAD-7 ≥8 (19)) but not major cognitive impairment, psychosis, or imminent suicide risk. With the support of local leadership, we will recruit IPC providers to serve as interventionists (randomized to condition to ensure that usual care is not compromised by the intervention).

Specific aim 2 is to examine the added utility of external facilitation combined with SIS by comparing its implementation outcomes to those of SIS alone. This aim will be accomplished by comparing implementation outcomes of sites randomly assigned to either SIS (standard training and provider educational resources with technical assistance) or SIS plus external facilitation. In this approach, the external facilitator partners with local sites—including administrators/leadership and frontline providers—to identify their specific needs, preferences, and barriers and facilitators to implementation; then based upon this assessment, the facilitator selects particular evidence-based implementation strategies that will best meet the site’s unique needs (20,21). Implementation challenges identified in the current hybrid I trial’s process evaluation will help determine the specific implementation strategies that would be used in external facilitation in the proposed hybrid II trial (e.g., program marketing, champion participation, academic detailing, audit and feedback, fidelity monitoring). (Note that if the current hybrid I trial does not identify broader system-level barriers, then an alternative implementation strategy besides external facilitation may be more efficient.) External facilitation was selected because it yielded sustained increases in reach and adoption of IPC (compared to SIS) in the broader implementation of VHA IPC (21), and VHA and our Center’s Implementation Core already use this strategy within existing IPC training efforts; thus, external facilitation has proven effective in the IPC setting and is consistent with current VHA implementation support, which supports its feasibility outside of the research study. Primary implementation outcomes (22) will be adoption of, fidelity to, and sustainability of the brief anxiety intervention, and secondary implementation outcomes will be intervention acceptability and feasibility among IPC providers.

The proposed research is designed to increase uptake of evidence-based anxiety intervention in the IPC setting, which would increase access to quality mental health care for primary care patients. In addition, this study will evaluate the added utility of external facilitation in addressing implementation challenges in the complex and highly variable VHA IPC context, which would identify whether this implementation strategy will yield sustainable changes to clinical practice.
";s:5:"xhtml";s:8792:"SHEPARDSON - Assignment #1a <br /><br />Draft a specific aims page (2 pages maximum) of your proposed study: <br /><br />Integrated primary care (IPC) aims to increase access to mental health treatment by embedding mental health care within the primary care setting (1). The Veterans Health Administration (VHA) implemented IPC nationally over a decade ago to improve care for common mental health concerns such as depression and anxiety (2). Anxiety disorders are quite prevalent (15-20%) and burdensome among VHA primary care patients (3-5). Although effective anxiety intervention techniques for anxiety (e.g., exposure therapy, cognitive therapy) are well-established in specialty mental health settings (6-7), our research on provider practices indicates that the evidence-based intervention techniques are infrequently used in real-world VHA IPC (8). Barriers include provider concerns about the feasibility of adequately delivering interventions within the brief treatment format (≤6 30-minute sessions) used in IPC (9). In general, providers face a dearth of brief, evidence-based anxiety interventions that are feasible to deliver in IPC (10), suggesting a critical gap in primary care anxiety treatment quality. The goal of the proposed study is to increase delivery of evidence-based anxiety interventions in VHA IPC settings to ultimately improve Veterans’ mental health and wellbeing. <br /><br />We have developed a brief anxiety intervention, consisting of a variety of modules featuring various evidence-based intervention techniques, that was designed specifically for VHA IPC to increase feasibility and acceptability among IPC providers. The intervention was adapted from extant evidence-based anxiety treatment protocols (11-12) to fit the population-health approach and brief treatment format used in IPC (9). In prior formative research with VHA IPC providers, we identified barriers to using evidence-based anxiety intervention techniques and obtained feedback on the intervention manual to inform further refinements. <br /><br />This brief anxiety intervention is currently being evaluated in a hybrid I effectiveness-implementation design randomized controlled trial (RCT) comparing the intervention to IPC usual care on the primary outcome of anxiety symptom severity. An effectiveness, rather than efficacy, trial is appropriate because the intervention is not entirely new, but rather consists of existing evidence-based techniques adapted into a new format for IPC. The hybrid type I design (13) was chosen because the intervention is being tested in this particular format for the first time, so the primary aim is to establish effectiveness. At the same time, to reduce the lag between treatment development and translation into clinical practice, we are collecting preliminary implementation data to inform the proposed study. A mixed methods process evaluation (14), informed by the RE-AIM framework (15), of intervention implementation in the hybrid I trial will identify possible patient-, provider-, and clinic-level barriers and facilitators that may affect future intervention adoption, fidelity, and sustainability.<br /><br />If the brief anxiety intervention proves effective in the hybrid I trial, it would be eligible for wider implementation across VHA IPC to help fill the gap in primary care treatment options for anxiety. Implementation efforts for new interventions being rolled out in VHA emphasize training individual providers (e.g., didactic training via webinar, access to treatment manual and resources) and providing supervision of consultation cases to ensure fidelity in intervention delivery. However, implementation barriers are likely to occur not only at the level of individual providers, but also at the broader clinic and system levels. These clinic- and system-level barriers (e.g., national productivity targets, clinic/system norms, lack of protected time for training) contribute to an implementation gap that individual IPC providers are ill-equipped to solve on their own. Incorporating an external facilitator (16), who is an expert in both evidence-based implementation strategies and the clinical context (i.e., IPC) and program being introduced (i.e., the brief IPC anxiety intervention), into the implementation effort would likely improve implementation outcomes.    <br /><br />The proposed study design is a hybrid type II effectiveness-implementation trial (13). This multi-site RCT will evaluate the evidence-based brief anxiety intervention as well as an evidence-based implementation strategy. Our research questions are: (1) Does the brief anxiety intervention improve upon patient clinical outcomes obtained in IPC usual care? and (2) Does an evidence-based implementation strategy (external facilitation) improve upon VHA IPC standard implementation support (SIS)? We hypothesize that patient-level clinical outcomes will be better for the brief anxiety intervention (vs. IPC usual care) and that implementation outcomes will be better for SIS plus external facilitation (vs. SIS). This study will be conducted over three years at four VHA primary care clinics in upstate New York. With a hybrid design, the two target populations at the four clinics are: (1) primary care patients experiencing anxiety symptoms (effectiveness aim) and (2) IPC providers (implementation aim).<br /><br />Specific aim 1 is to evaluate the effectiveness of the brief anxiety intervention by comparing its patient-level clinical outcomes to those of IPC usual care for anxiety. This aim will be accomplished by comparing changes in anxiety symptom severity (measured by the Generalized Anxiety Disorder-7 (17) [GAD-7]) and depressive symptom severity (measured by the Patient Health Questionnaire-9 (18) [PHQ-9]) from baseline to 12 weeks (post) and 24 weeks (follow-up) of patients randomly assigned to either the brief anxiety intervention or IPC usual care. Participants will be adult, Veteran primary care patients with elevated anxiety (GAD-7 ≥8 (19)) but not major cognitive impairment, psychosis, or imminent suicide risk. With the support of local leadership, we will recruit IPC providers to serve as interventionists (randomized to condition to ensure that usual care is not compromised by the intervention).<br /><br />Specific aim 2 is to examine the added utility of external facilitation combined with SIS by comparing its implementation outcomes to those of SIS alone. This aim will be accomplished by comparing implementation outcomes of sites randomly assigned to either SIS (standard training and provider educational resources with technical assistance) or SIS plus external facilitation. In this approach, the external facilitator partners with local sites—including administrators/leadership and frontline providers—to identify their specific needs, preferences, and barriers and facilitators to implementation; then based upon this assessment, the facilitator selects particular evidence-based implementation strategies that will best meet the site’s unique needs (20,21). Implementation challenges identified in the current hybrid I trial’s process evaluation will help determine the specific implementation strategies that would be used in external facilitation in the proposed hybrid II trial (e.g., program marketing, champion participation, academic detailing, audit and feedback, fidelity monitoring). (Note that if the current hybrid I trial does not identify broader system-level barriers, then an alternative implementation strategy besides external facilitation may be more efficient.) External facilitation was selected because it yielded sustained increases in reach and adoption of IPC (compared to SIS) in the broader implementation of VHA IPC (21), and VHA and our Center’s Implementation Core already use this strategy within existing IPC training efforts; thus, external facilitation has proven effective in the IPC setting and is consistent with current VHA implementation support, which supports its feasibility outside of the research study. Primary implementation outcomes (22) will be adoption of, fidelity to, and sustainability of the brief anxiety intervention, and secondary implementation outcomes will be intervention acceptability and feasibility among IPC providers.<br /><br />The proposed research is designed to increase uptake of evidence-based anxiety intervention in the IPC setting, which would increase access to quality mental health care for primary care patients. In addition, this study will evaluate the added utility of external facilitation in addressing implementation challenges in the complex and highly variable VHA IPC context, which would identify whether this implementation strategy will yield sustainable changes to clinical practice.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"e109374c7a10f6b3f321c84129ef4fca";}s:4:"show";b:1;s:3:"cid";s:32:"6753b2736bf728fc517656b02ea34734";}s:32:"75a72eff22b1c58af59220a826911b40";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"aweaver";s:4:"name";s:12:"Addie Weaver";s:4:"mail";s:18:"weaverad@umich.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535112678;}s:3:"raw";s:4067:"Weaver - Assignment #1a

Specific Aims

Postpartum depression (PPD) affects approximately 10-15% of new mothers in the United States.1,2 Systematic and integrative reviews suggest women living in rural America are about twice as likely to experience PPD than their urban counterparts.3,4 This is of concern as PPD is associated with increased maternal mortality,5,6 and children whose mothers have PPD demonstrated cognitive, development, and social impairments compared to children whose mothers are not depressed.6,7 Further, mothers with PPD are less likely to attend their own healthcare visits, or take their children to well child visits and immunization appointments.8-10 Though PPD presents a public health concern, it goes unrecognized in over 50% of patients.11 This is in part due to a lack of screening. Though multiple professional associations recommend PPD screening, it is not mandated in the U.S. and there is a lack of consensus as to where and when screening should occur.12-13 PPD is most commonly screened for at postpartum obstetric and gynecological (OB/GYN) visits and at well child visits; however, as previously noted, women with PPD are less likely to attend these healthcare appointments.8-10 Further, rural women face substantial barriers to accessing services where screening may occur, including a lack of local providers,14-16 associated travel burden with seeking distant care,17,18 cost,19,20 lack of insurance coverage,21,22 and stigma.23,24 Therefore, it is imperative to consider opportunities to screen for PPD in rural community settings where postpartum women seek services. 

The Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) is a public assistance program that provides food, nutrition education and counseling, health care referrals, and breastfeeding support to low-income pregnant, postpartum, and breastfeeding women and children from birth to age 5. In 2016, WIC served over 7 million women and children nationwide, with more than 290,000 of those women and children receiving WIC services in the state of Michigan.25,26 As WIC is a setting where pregnant and postpartum women naturally go for food and nutrition services during a period of time when they are at high risk for PPD, it seems a potentially promising venue for administering PPD screening in rural communities. Further, WIC staff include nurses and nutritionists, who are well-positioned to be trained to administer and interpret PPD screening tools and provide needed referrals.

Although WIC clinics seem like a potentially promising setting to initiate PPD screening in rural communities, it is essential to assess barriers and facilitators to PPD screening in order to understand whether it is feasible and acceptable in this setting, and to inform the design and testing of implementation strategies for initiating screening. 

Specific Aims of this Proposal are to: 
1.	Identify barriers and facilitators to implementing postpartum depression screening in rural Michigan WIC clinics by:
a.	Conducting semi-structured interviews, guided by the Theoretical Domains Framework (TDF), with WIC program staff (e.g., nurses, nutritionists), and
b.	Administering a questionnaire, based upon the TDF, to WIC program staff

2.	Identify and tailor appropriate behavior change techniques to implement PPD screening based upon what was learned about barriers and facilitators in Aim 1.

3.	Implement and assess the impact of behavior change techniques, developed in Aim 2, on PPD screening rates in one rural Michigan WIC clinic by:
a.	Conducting a one group pre-/post- test of behavior change techniques to assess their effect on PPD screening rates over time
b.	Conducting a one group pre-/post test of to assess change in TDF domains among WIC staff, and identify potential mechanisms of change
c.	Conducting qualitative interviews, based the TDF, to explore WIC staff members’ perceptions of barriers and facilitators to PPD in order to better understand the acceptability and sustainability of screening as routine practice
";s:5:"xhtml";s:4166:"Weaver - Assignment #1a<br /><br />Specific Aims<br /><br />Postpartum depression (PPD) affects approximately 10-15% of new mothers in the United States.1,2 Systematic and integrative reviews suggest women living in rural America are about twice as likely to experience PPD than their urban counterparts.3,4 This is of concern as PPD is associated with increased maternal mortality,5,6 and children whose mothers have PPD demonstrated cognitive, development, and social impairments compared to children whose mothers are not depressed.6,7 Further, mothers with PPD are less likely to attend their own healthcare visits, or take their children to well child visits and immunization appointments.8-10 Though PPD presents a public health concern, it goes unrecognized in over 50% of patients.11 This is in part due to a lack of screening. Though multiple professional associations recommend PPD screening, it is not mandated in the U.S. and there is a lack of consensus as to where and when screening should occur.12-13 PPD is most commonly screened for at postpartum obstetric and gynecological (OB/GYN) visits and at well child visits; however, as previously noted, women with PPD are less likely to attend these healthcare appointments.8-10 Further, rural women face substantial barriers to accessing services where screening may occur, including a lack of local providers,14-16 associated travel burden with seeking distant care,17,18 cost,19,20 lack of insurance coverage,21,22 and stigma.23,24 Therefore, it is imperative to consider opportunities to screen for PPD in rural community settings where postpartum women seek services. <br /><br />The Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) is a public assistance program that provides food, nutrition education and counseling, health care referrals, and breastfeeding support to low-income pregnant, postpartum, and breastfeeding women and children from birth to age 5. In 2016, WIC served over 7 million women and children nationwide, with more than 290,000 of those women and children receiving WIC services in the state of Michigan.25,26 As WIC is a setting where pregnant and postpartum women naturally go for food and nutrition services during a period of time when they are at high risk for PPD, it seems a potentially promising venue for administering PPD screening in rural communities. Further, WIC staff include nurses and nutritionists, who are well-positioned to be trained to administer and interpret PPD screening tools and provide needed referrals.<br /><br />Although WIC clinics seem like a potentially promising setting to initiate PPD screening in rural communities, it is essential to assess barriers and facilitators to PPD screening in order to understand whether it is feasible and acceptable in this setting, and to inform the design and testing of implementation strategies for initiating screening. <br /><br />Specific Aims of this Proposal are to: <br />1.	Identify barriers and facilitators to implementing postpartum depression screening in rural Michigan WIC clinics by:<br />a.	Conducting semi-structured interviews, guided by the Theoretical Domains Framework (TDF), with WIC program staff (e.g., nurses, nutritionists), and<br />b.	Administering a questionnaire, based upon the TDF, to WIC program staff<br /><br />2.	Identify and tailor appropriate behavior change techniques to implement PPD screening based upon what was learned about barriers and facilitators in Aim 1.<br /><br />3.	Implement and assess the impact of behavior change techniques, developed in Aim 2, on PPD screening rates in one rural Michigan WIC clinic by:<br />a.	Conducting a one group pre-/post- test of behavior change techniques to assess their effect on PPD screening rates over time<br />b.	Conducting a one group pre-/post test of to assess change in TDF domains among WIC staff, and identify potential mechanisms of change<br />c.	Conducting qualitative interviews, based the TDF, to explore WIC staff members’ perceptions of barriers and facilitators to PPD in order to better understand the acceptability and sustainability of screening as routine practice";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"103d8d2237b2002acc548e47ac287dd1";}s:4:"show";b:1;s:3:"cid";s:32:"75a72eff22b1c58af59220a826911b40";}s:32:"137b30cd1b4c9dac168fdfdffd76afae";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"aprogovac";s:4:"name";s:12:"Ana Progovac";s:4:"mail";s:24:"aprogovac@challiance.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535124276;}s:3:"raw";s:5191:"PROGOVAC - Assignment 1A

Draft Specific Aims Page 

[Comment...wish I knew this was just a textbox! Sorry for the numbers throughout that are Endnote citations...and the lack of figure which communicated a lot as well.]

TITLE: A Three-Phase Mixed-Methods Implementation Study of a real-world Behavioral Health Home (BHH) 

People with serious mental illnesses (SMI) die 20-30 years earlier than their peers, largely due to metabolic side effects of psychotropic drugs, modifiable health behaviors, and less receipt of quality primary care.1 Many receive healthcare primarily in specialty mental health settings, often avoiding primary care where they may experience stigma. The Behavioral Health Home (BHH) model at Cambridge Health Alliance (CHA, an urban safety net health system) “reverse integrates” primary care and medical home services (e.g., care coordination, population health management, health promotion) into a specialty mental health care setting while using system-wide electronic health record (EHR) and active care management to promote integration. This promising “reverse integration” approach has encouraging early evidence of its effectiveness in the literature,2-4 including for reducing emergency room visits and increasing screenings.5
 But real-world models are “still in their infancy”, and SMI care remains fragmented.6 In practice, simply linking data systems like EHRs does not overcome the enormous organizational, coordination, and financing barriers to combat care fragmentation.6 Many existing evaluations of integrated care programs do not delve deep enough into the extent to which program elements are fully integrated into care. This may in part be due to the fact that comprehensive theoretical models of integration (and when it has truly been achieved) have been lacking. Singer et al. (2018) posit in their Comprehensive Theory of Integration7 that we should distinguish and measure integration at five levels: structural and functional integration (organizational), interpersonal and normative integration (social features), and process integration.7 Implementation Science research methods have also been specifically called upon to help understand the nuance underlying mixed results from integrating mental health into primary care.8 Combining a comprehensive theory of integration with implementation science research methods is need to help researchers and clinical stakeholders better study real-world BHH effectiveness, including for sub-groups4,9  as well as barriers/facilitators to sustaining and expanding safety net BHH models, in order to support better program implementation and dissemination. 
CHA (a safety net health system serving ~140,000 Massachusetts residents annually) began transforming its Central Street outpatient mental health program for adults with SMI into a BHH in September 2015 (n=439). The redesign added an on-site primary care provider (PCP) and full-time care manager, improved workflows leveraging EHR data (including follow-up after psychiatric hospitalization), and prioritized coordination with off-site PCPs, patient engagement in primary care, wellness interventions, and metabolic monitoring. 

The specific aims of the current Hybrid Type II study are as follows: 
Aim 1: Assess the long-term effectiveness of the existing BHH relative to usual care for persons with depressive disorders (a) at years 2, 3, and 4 post-implementation, and (b) for sub-groups of patients based on program engagement, sociodemographic factors, and type of disorder (bipolar vs. schizophrenia). 
Aim 2: Assess the primary implementation outcome, degree of integration, using Singer et al.’s five-levels: (1) structural, (2) functional, (3) interpersonal, (4) normative, and (5) process integration. 
Aim 3: Conduct targeted semi-structured qualitative interviews with providers and stakeholders to assess (a) ongoing barriers/facilitators to implementation and sustainability at an existing BHH site, and (b) appropriateness, acceptability, feasibility, and barriers/facilitators at 2 potential BHH expansion sites.

This is the 2nd phase of a 3-phase mixed methods study leveraging a strong research-practice partnership to rigorously study a BHH implementation and its scale-up within CHA. In the 1st phase (Hybrid Type I), we evaluated overall BHH’s effectiveness in its 1st year using a quasi-experimental pre-post propensity score weighted comparison group10 and assessed the implementation context (qualitative manuscript in preparation). In the 2nd phase (Hybrid Type II), we plan to measure long-term effectiveness over implementation years 2-4 and also include sub-group analyses, as well as pinpoint implementation strategies likely to be effective during program expansion to 2 other CHA sites via qualitative interviews with staff and health system administrators. Contingent upon a funded 2nd phase, the 3rd phase (Hybrid Type III) will test the implementation strategies developed in the 2nd Phase. The current implementation of a BHH at CHA presents an excellent opportunity to study real-world BHH effectiveness and leverage existing expansion efforts to study implementation strategies. 
";s:5:"xhtml";s:5274:"PROGOVAC - Assignment 1A<br /><br />Draft Specific Aims Page <br /><br />[Comment...wish I knew this was just a textbox! Sorry for the numbers throughout that are Endnote citations...and the lack of figure which communicated a lot as well.]<br /><br />TITLE: A Three-Phase Mixed-Methods Implementation Study of a real-world Behavioral Health Home (BHH) <br /><br />People with serious mental illnesses (SMI) die 20-30 years earlier than their peers, largely due to metabolic side effects of psychotropic drugs, modifiable health behaviors, and less receipt of quality primary care.1 Many receive healthcare primarily in specialty mental health settings, often avoiding primary care where they may experience stigma. The Behavioral Health Home (BHH) model at Cambridge Health Alliance (CHA, an urban safety net health system) “reverse integrates” primary care and medical home services (e.g., care coordination, population health management, health promotion) into a specialty mental health care setting while using system-wide electronic health record (EHR) and active care management to promote integration. This promising “reverse integration” approach has encouraging early evidence of its effectiveness in the literature,2-4 including for reducing emergency room visits and increasing screenings.5<br /> But real-world models are “still in their infancy”, and SMI care remains fragmented.6 In practice, simply linking data systems like EHRs does not overcome the enormous organizational, coordination, and financing barriers to combat care fragmentation.6 Many existing evaluations of integrated care programs do not delve deep enough into the extent to which program elements are fully integrated into care. This may in part be due to the fact that comprehensive theoretical models of integration (and when it has truly been achieved) have been lacking. Singer et al. (2018) posit in their Comprehensive Theory of Integration7 that we should distinguish and measure integration at five levels: structural and functional integration (organizational), interpersonal and normative integration (social features), and process integration.7 Implementation Science research methods have also been specifically called upon to help understand the nuance underlying mixed results from integrating mental health into primary care.8 Combining a comprehensive theory of integration with implementation science research methods is need to help researchers and clinical stakeholders better study real-world BHH effectiveness, including for sub-groups4,9  as well as barriers/facilitators to sustaining and expanding safety net BHH models, in order to support better program implementation and dissemination. <br />CHA (a safety net health system serving ~140,000 Massachusetts residents annually) began transforming its Central Street outpatient mental health program for adults with SMI into a BHH in September 2015 (n=439). The redesign added an on-site primary care provider (PCP) and full-time care manager, improved workflows leveraging EHR data (including follow-up after psychiatric hospitalization), and prioritized coordination with off-site PCPs, patient engagement in primary care, wellness interventions, and metabolic monitoring. <br /><br />The specific aims of the current Hybrid Type II study are as follows: <br />Aim 1: Assess the long-term effectiveness of the existing BHH relative to usual care for persons with depressive disorders (a) at years 2, 3, and 4 post-implementation, and (b) for sub-groups of patients based on program engagement, sociodemographic factors, and type of disorder (bipolar vs. schizophrenia). <br />Aim 2: Assess the primary implementation outcome, degree of integration, using Singer et al.’s five-levels: (1) structural, (2) functional, (3) interpersonal, (4) normative, and (5) process integration. <br />Aim 3: Conduct targeted semi-structured qualitative interviews with providers and stakeholders to assess (a) ongoing barriers/facilitators to implementation and sustainability at an existing BHH site, and (b) appropriateness, acceptability, feasibility, and barriers/facilitators at 2 potential BHH expansion sites.<br /><br />This is the 2nd phase of a 3-phase mixed methods study leveraging a strong research-practice partnership to rigorously study a BHH implementation and its scale-up within CHA. In the 1st phase (Hybrid Type I), we evaluated overall BHH’s effectiveness in its 1st year using a quasi-experimental pre-post propensity score weighted comparison group10 and assessed the implementation context (qualitative manuscript in preparation). In the 2nd phase (Hybrid Type II), we plan to measure long-term effectiveness over implementation years 2-4 and also include sub-group analyses, as well as pinpoint implementation strategies likely to be effective during program expansion to 2 other CHA sites via qualitative interviews with staff and health system administrators. Contingent upon a funded 2nd phase, the 3rd phase (Hybrid Type III) will test the implementation strategies developed in the 2nd Phase. The current implementation of a BHH at CHA presents an excellent opportunity to study real-world BHH effectiveness and leverage existing expansion efforts to study implementation strategies.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"4d0b2b0280a3ef223cabf6dc01ad3647";}s:4:"show";b:1;s:3:"cid";s:32:"137b30cd1b4c9dac168fdfdffd76afae";}s:32:"dcfe57c23aa3eebfcbeae2d5e094e6f2";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"aprogovac";s:4:"name";s:12:"Ana Progovac";s:4:"mail";s:24:"aprogovac@challiance.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535124323;}s:3:"raw";s:66:"That's really scary, Kelsie! Good luck and please stay safe. 
-Ana";s:5:"xhtml";s:76:"That&#039;s really scary, Kelsie! Good luck and please stay safe. <br />-Ana";s:6:"parent";s:32:"b33014d6ff592086b1bd2d2181d06940";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"dcfe57c23aa3eebfcbeae2d5e094e6f2";}s:32:"52d8f69591ce10d86e339e1f7a948f49";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"ipinsky";s:4:"name";s:12:"Ilana Pinsky";s:4:"mail";s:21:"pinskyilana@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535143690;}s:3:"raw";s:6812:"Pinsky - Assignment #1a

Scale-up PRIDE Mozambique: Fidelity and Adaptation of the Interpersonal Counseling Intervention (IPC) 
Ilana Pinsky

AIMS

Mental health (MH) is one of the leading causes of the burden of disease worldwide. At the same time, there is a huge MH treatment gap. This gap is especially profound in low income countries because most do not have the capacity to provide broad mental health (MH) services using a specialized workforce. To address this situation in low resource contexts, task­sharing strategies, that are considered efficient strategies to facilitate the dissemination of mental health interventions, are increasingly being employed. Determining the most effective delivery pathway is of critical importance to the success of scaling-up. 
 We will examine this research gap in Mozambique, the seventh poorest country in the world and where over 60% of the population lives in rural areas with no access to MH care. Mozambique currently lacks psychiatry training capacity and the mental health care system is in development. Located in sub-Saharan Africa, with a population of nearly 26 million, it has 109 Masters level psychologists and 20 psychiatrists, 13 of them Mozambicans.
Partnering with the Mozambican Ministry of Health (MoH), Scale-up PRIDE Mozambique (a NIMH funded U19 grant study) will use Interpersonal Counseling (IPC) to provide treatment for common mental disorders (including depression, anxiety and uncomplicated – non psychotic – post traumatic stress disorder-PTSD), and produce generalizable knowledge about how best to scale­up IPC in LMICs. IPC has been studied for the last 40 years and have shown to be an effective intervention in the treatment of several MH diagnosis in a number of settings and populations.
Successful scaling up requires not only to identify the best interventions and strategies for health-service delivery, but also to monitor providers’ performance during the implementation and sustainability phases of the identified intervention. However, monitoring the fidelity of intervention delivery is both pivotal and complex. 
Its complexity, especially in low resource settings like Mozambique, is related to the need of adapting it to a new, diverse context, to be delivered using a task-shifting, without losing sight of the intervention core evidence principles. In sum, there is tension between EB intervention delivery with fidelity and the necessity of adapting it to be implemented in a new setting.  Further, choosing the right tools and strategies to determine the fidelity of implementing EB interventions in new contexts adds to this complexity.  Last, but not least, the present study, is, as described above, part of a large implementation process whose main aim is clinical sustainability. In this way, we are interested in assessing intervention adherence and adaptation in a way that supports the evolution of the intervention within a changing context.
Currently, the usual MH delivery pathway in Mozambique depends on one MH specialty clinic per district, staffed by psychiatric technicians (PsyT), supervised by MH specialists (Pathway 1 – Usual Care). Thus, the vast majority of individuals with psychiatric disorders are not served because of scarcity of service providers. The large study will test two alternative delivery pathways using existing human resources including community health workers (CHWs) and primary care providers (PCPs; medicine technicians and nurses). Unlike PCPs, CHWs have wider reach and less training. In Pathway 2 – Screen, Refer, and Treat (SRT), CHWs will refer identified cases to PCPs for treatment in community clinics.  In Pathway 3 – community MH stepped care (COMmhCARE), treatment will be delivered in the community and in the clinic by both CHWs and PCPs, respectively.
The final objective of the large study is to implement a sustainable MH program nationally in Mozambique. In accordance to MoH protocol, a group of local MH providers (psychiatrists, psychologists and PsyTs) will be trained and supervised in IPC. This group (N=25) will be charged with training and supervising all the other professionals providing MH described in the above-mentioned pathways above, using a train-the-trainers model. The training of trainers required for each professional to be considered certified as IPC therapists, includes having 5 complete cases and 75% participation in supervision sessions. In addition, their initial training groups will be observed by experienced IPC experts. 
The proposed methodology contains several steps in a mixed-methods approach (in-depth interviews and surveys). Quantitative tools will measure variables shown to predict sustainability in community settings, including characteristics of the intervention (e.g. fit with the organization, adaptability), perception and engagement with the program among participants, organizational/contextual factors. In addition, qualitative open-ended survey questions will assist in identify other factors to emerge. Both methods will measure the natural variation balancing fidelity to the intervention and adaption. 

Specific aims
The overarching aim of this proposal is to develop and implement a model of continuous monitoring of IPC training, delivery and supervision within the Mozambique low-resource context that ensures intervention adherence while preserving the core principles of evidence of IPC during the intervention delivery, within  a sustainability model. 
As presented, to maximize adherence (including providers’ engagement and satisfaction) and fidelity to the delivery of the EB intervention,  I will analyze baseline data from about 25 Mozambique mental health providers selected by the MoH to train the larger group of providers (CHWs, PCPs) that will deliver the treatment in a national scale. My specific aims are to: 

AIM 1: Investigate, select and test intervention fidelity/competence tools and processes to measure adherence and adaptability within a sustainability model to IPC’s main features that could assist will enhance on the continuous quality improvement (CQI) of the large scale-up study. 

AIM 2: Guide the continual adoption and fidelity of IPC in Mozambique. 


Next Steps
       We will start this fidelity survey using the 25 initial mental health trainers. Once we understand the variations within this group, we will develop similar methods to enhance our goals of sustainability by incorporating all stakeholders involved in the study: PCPs, CWHs, PsyTs, community leaders, clients, clients’ families. Eventually, we intend to explore if and how IPC was adapted across different settings and geographic contexts and whether sites that implement the program with fidelity or sites that have adapted the program have higher sustainability. 

";s:5:"xhtml";s:6939:"Pinsky - Assignment #1a<br /><br />Scale-up PRIDE Mozambique: Fidelity and Adaptation of the Interpersonal Counseling Intervention (IPC) <br />Ilana Pinsky<br /><br />AIMS<br /><br />Mental health (MH) is one of the leading causes of the burden of disease worldwide. At the same time, there is a huge MH treatment gap. This gap is especially profound in low income countries because most do not have the capacity to provide broad mental health (MH) services using a specialized workforce. To address this situation in low resource contexts, task­sharing strategies, that are considered efficient strategies to facilitate the dissemination of mental health interventions, are increasingly being employed. Determining the most effective delivery pathway is of critical importance to the success of scaling-up. <br /> We will examine this research gap in Mozambique, the seventh poorest country in the world and where over 60% of the population lives in rural areas with no access to MH care. Mozambique currently lacks psychiatry training capacity and the mental health care system is in development. Located in sub-Saharan Africa, with a population of nearly 26 million, it has 109 Masters level psychologists and 20 psychiatrists, 13 of them Mozambicans.<br />Partnering with the Mozambican Ministry of Health (MoH), Scale-up PRIDE Mozambique (a NIMH funded U19 grant study) will use Interpersonal Counseling (IPC) to provide treatment for common mental disorders (including depression, anxiety and uncomplicated – non psychotic – post traumatic stress disorder-PTSD), and produce generalizable knowledge about how best to scale­up IPC in LMICs. IPC has been studied for the last 40 years and have shown to be an effective intervention in the treatment of several MH diagnosis in a number of settings and populations.<br />Successful scaling up requires not only to identify the best interventions and strategies for health-service delivery, but also to monitor providers’ performance during the implementation and sustainability phases of the identified intervention. However, monitoring the fidelity of intervention delivery is both pivotal and complex. <br />Its complexity, especially in low resource settings like Mozambique, is related to the need of adapting it to a new, diverse context, to be delivered using a task-shifting, without losing sight of the intervention core evidence principles. In sum, there is tension between EB intervention delivery with fidelity and the necessity of adapting it to be implemented in a new setting.  Further, choosing the right tools and strategies to determine the fidelity of implementing EB interventions in new contexts adds to this complexity.  Last, but not least, the present study, is, as described above, part of a large implementation process whose main aim is clinical sustainability. In this way, we are interested in assessing intervention adherence and adaptation in a way that supports the evolution of the intervention within a changing context.<br />Currently, the usual MH delivery pathway in Mozambique depends on one MH specialty clinic per district, staffed by psychiatric technicians (PsyT), supervised by MH specialists (Pathway 1 – Usual Care). Thus, the vast majority of individuals with psychiatric disorders are not served because of scarcity of service providers. The large study will test two alternative delivery pathways using existing human resources including community health workers (CHWs) and primary care providers (PCPs; medicine technicians and nurses). Unlike PCPs, CHWs have wider reach and less training. In Pathway 2 – Screen, Refer, and Treat (SRT), CHWs will refer identified cases to PCPs for treatment in community clinics.  In Pathway 3 – community MH stepped care (COMmhCARE), treatment will be delivered in the community and in the clinic by both CHWs and PCPs, respectively.<br />The final objective of the large study is to implement a sustainable MH program nationally in Mozambique. In accordance to MoH protocol, a group of local MH providers (psychiatrists, psychologists and PsyTs) will be trained and supervised in IPC. This group (N=25) will be charged with training and supervising all the other professionals providing MH described in the above-mentioned pathways above, using a train-the-trainers model. The training of trainers required for each professional to be considered certified as IPC therapists, includes having 5 complete cases and 75% participation in supervision sessions. In addition, their initial training groups will be observed by experienced IPC experts. <br />The proposed methodology contains several steps in a mixed-methods approach (in-depth interviews and surveys). Quantitative tools will measure variables shown to predict sustainability in community settings, including characteristics of the intervention (e.g. fit with the organization, adaptability), perception and engagement with the program among participants, organizational/contextual factors. In addition, qualitative open-ended survey questions will assist in identify other factors to emerge. Both methods will measure the natural variation balancing fidelity to the intervention and adaption. <br /><br />Specific aims<br />The overarching aim of this proposal is to develop and implement a model of continuous monitoring of IPC training, delivery and supervision within the Mozambique low-resource context that ensures intervention adherence while preserving the core principles of evidence of IPC during the intervention delivery, within  a sustainability model. <br />As presented, to maximize adherence (including providers’ engagement and satisfaction) and fidelity to the delivery of the EB intervention,  I will analyze baseline data from about 25 Mozambique mental health providers selected by the MoH to train the larger group of providers (CHWs, PCPs) that will deliver the treatment in a national scale. My specific aims are to: <br /><br />AIM 1: Investigate, select and test intervention fidelity/competence tools and processes to measure adherence and adaptability within a sustainability model to IPC’s main features that could assist will enhance on the continuous quality improvement (CQI) of the large scale-up study. <br /><br />AIM 2: Guide the continual adoption and fidelity of IPC in Mozambique. <br /><br /><br />Next Steps<br />       We will start this fidelity survey using the 25 initial mental health trainers. Once we understand the variations within this group, we will develop similar methods to enhance our goals of sustainability by incorporating all stakeholders involved in the study: PCPs, CWHs, PsyTs, community leaders, clients, clients’ families. Eventually, we intend to explore if and how IPC was adapted across different settings and geographic contexts and whether sites that implement the program with fidelity or sites that have adapted the program have higher sustainability.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"59ef5da78ed8de28648cb6aadb5a5d8d";}s:4:"show";b:1;s:3:"cid";s:32:"52d8f69591ce10d86e339e1f7a948f49";}s:32:"b54f5ca9d95bc2bdedafb276e123e9a6";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1535161673;s:8:"modified";i:1535640591;}s:3:"raw";s:356:"Kelsie,

Thanks for letting us know.
I'm sure everyone completely understands and just wants you to be safe.
My brother- and sister-in-law, and our two nieces are on Oahu. 

We are watching things very closely and sending our best from the mainland. We are hoping that storm turns West and away from you all and the Islands very soon.

Very best,

Lindsey
";s:5:"xhtml";s:410:"Kelsie,<br /><br />Thanks for letting us know.<br />I&#039;m sure everyone completely understands and just wants you to be safe.<br />My brother- and sister-in-law, and our two nieces are on Oahu. <br /><br />We are watching things very closely and sending our best from the mainland. We are hoping that storm turns West and away from you all and the Islands very soon.<br /><br />Very best,<br /><br />Lindsey";s:6:"parent";s:32:"b33014d6ff592086b1bd2d2181d06940";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"b54f5ca9d95bc2bdedafb276e123e9a6";}s:32:"dd2d1a4f0c00fca7eea99ffbf398923f";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535161710;}s:3:"raw";s:76:"Thanks everyone for posting your aims pages!

Have a great weekend,

Lindsey";s:5:"xhtml";s:96:"Thanks everyone for posting your aims pages!<br /><br />Have a great weekend,<br /><br />Lindsey";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"dd2d1a4f0c00fca7eea99ffbf398923f";}s:32:"e109374c7a10f6b3f321c84129ef4fca";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"nbowersox";s:4:"name";s:13:"Nick Bowersox";s:4:"mail";s:24:"Nicholas.Bowersox@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1535607552;s:8:"modified";i:1535639161;}s:3:"raw";s:4767:"This is a really well-reasoned proposal, which includes a logical expansion of services to address an often overlooked need in VA mental health.  It's great to hear that you have some previous pilot work of this intervention underway focusing on intervention effectiveness that will allow for an updated focus on implementation method assessment as you move toward multi-site spread.  In looking this over, I had a few thoughts - some of which you have likely already considered (but may not have included due to space constraints) while other may be worth exploring a bit.  I expect that we will explore these ideas over the coming months, but wanted to give my first impression of your proposal.

1. Who are the "providers" mentioned in this proposal - PCPs?  RNs? Social Workers or Psychologists?  Perhaps you are allowing this term to broadly apply to multiple interested staff, but it would be good to clarify as different staff will have different levels of pre-project training in anxiety EBPs.  Clarity on targeted providers will also help with considering incentives for project participation as well as discipline-specific barriers to participation.
2. I'd love to hear a bit more about the specifics of the intervention itself.  It sounds like it includes aspects of multiple EBPs for anxiety, and may be a "toolkit" of anxiety intervention techniques?  I wonder about your expectations for EBP fidelity if you are allowing providers to use pieces of multiple anxiety EBP approaches.  
3. Building on the above thought, I am also wondering about your expectations for provider flexibility versus fidelity in delivering this intervention, with regard to number of sessions, specific interventions, etc.  What happens if a Veteran responds well to this intervention after only 6 sessions - will the provider be able to discontinue treatment and consider this a success, or will this be viewed as a "failure" due to a lack of fidelity to treatment delivery expectations?
4. (Apologies, I'm stuck on fidelity).  Also, what role will the external facilitator play in assessing program fidelity - will there be any formal assessment of the intervention delivery?  If so, what sorts of delivery aspects will be prioritized to assess whether the evaluation is being delivered as expected?  Number of sessions?  Specific modules or interventions delivered?
5. One additional thought about fidelity - what are your plans for Veterans with more complicated or severe anxiety?  The natural course of treatment for these Veterans might be to transition them to a higher level of care (mental health clinic) rather than continue them in IPC.  Also, anxiety treatment is often provided along with medication care for anxiety in PC settings - this can make it challenging to distinguish pharmacological from psychological intervention effects.  Have you thought about how to handle situations such as these, where Veterans might be receiving care that deviates from strict IPC anxiety EBP?
6. What is the "carrot" for these providers to learn how to provide this intervention (and set aside time to deliver it)?  I suppose this question is a bit different given the different providers I asked about in question #1, but it is good to think about this a bit beyond the initial early adopter sites that are highly motivated to deliver your intervention.  What will motivate providers to deliver this intervention?
7. Similar to the previous question, what is site leadership's motivation to support implementation of this project?  Are there any plans to try to align project adoption with high-priority motivators like national quality metrics?
8. You mention that the PHQ9 and GAD7 will be used to demonstrate intervention effects.  This seems like a really smart choice given the movement toward measurement based care in VA, so it can be expected that these measures should be somewhat available for control group Veterans.  Are you considering other measures of intervention impact that might be reflected in the administrative data, such as medication trial starts/stops, ER visits, hospitalizations, or transitions to higher levels of MH care?
9. Another thought - your proposal seems to suggest that sites will be randomized to receive facilitation or EBP training as usual for your Type II trial, but I think that I read that you will only be rolling this out to four sites.  Will this be enough sites to randomize implementation method?

I hope my above thoughts reflect my interest in this project rather than concerns with your proposal.  Again, this seems like a really smart idea which could really enhance care for Veterans that are often overlooked with regard to their mental health needs.  I look forward to discussing this further with you!

Best,

- Nick";s:5:"xhtml";s:4907:"This is a really well-reasoned proposal, which includes a logical expansion of services to address an often overlooked need in VA mental health.  It&#039;s great to hear that you have some previous pilot work of this intervention underway focusing on intervention effectiveness that will allow for an updated focus on implementation method assessment as you move toward multi-site spread.  In looking this over, I had a few thoughts - some of which you have likely already considered (but may not have included due to space constraints) while other may be worth exploring a bit.  I expect that we will explore these ideas over the coming months, but wanted to give my first impression of your proposal.<br /><br />1. Who are the &quot;providers&quot; mentioned in this proposal - PCPs?  RNs? Social Workers or Psychologists?  Perhaps you are allowing this term to broadly apply to multiple interested staff, but it would be good to clarify as different staff will have different levels of pre-project training in anxiety EBPs.  Clarity on targeted providers will also help with considering incentives for project participation as well as discipline-specific barriers to participation.<br />2. I&#039;d love to hear a bit more about the specifics of the intervention itself.  It sounds like it includes aspects of multiple EBPs for anxiety, and may be a &quot;toolkit&quot; of anxiety intervention techniques?  I wonder about your expectations for EBP fidelity if you are allowing providers to use pieces of multiple anxiety EBP approaches.  <br />3. Building on the above thought, I am also wondering about your expectations for provider flexibility versus fidelity in delivering this intervention, with regard to number of sessions, specific interventions, etc.  What happens if a Veteran responds well to this intervention after only 6 sessions - will the provider be able to discontinue treatment and consider this a success, or will this be viewed as a &quot;failure&quot; due to a lack of fidelity to treatment delivery expectations?<br />4. (Apologies, I&#039;m stuck on fidelity).  Also, what role will the external facilitator play in assessing program fidelity - will there be any formal assessment of the intervention delivery?  If so, what sorts of delivery aspects will be prioritized to assess whether the evaluation is being delivered as expected?  Number of sessions?  Specific modules or interventions delivered?<br />5. One additional thought about fidelity - what are your plans for Veterans with more complicated or severe anxiety?  The natural course of treatment for these Veterans might be to transition them to a higher level of care (mental health clinic) rather than continue them in IPC.  Also, anxiety treatment is often provided along with medication care for anxiety in PC settings - this can make it challenging to distinguish pharmacological from psychological intervention effects.  Have you thought about how to handle situations such as these, where Veterans might be receiving care that deviates from strict IPC anxiety EBP?<br />6. What is the &quot;carrot&quot; for these providers to learn how to provide this intervention (and set aside time to deliver it)?  I suppose this question is a bit different given the different providers I asked about in question #1, but it is good to think about this a bit beyond the initial early adopter sites that are highly motivated to deliver your intervention.  What will motivate providers to deliver this intervention?<br />7. Similar to the previous question, what is site leadership&#039;s motivation to support implementation of this project?  Are there any plans to try to align project adoption with high-priority motivators like national quality metrics?<br />8. You mention that the PHQ9 and GAD7 will be used to demonstrate intervention effects.  This seems like a really smart choice given the movement toward measurement based care in VA, so it can be expected that these measures should be somewhat available for control group Veterans.  Are you considering other measures of intervention impact that might be reflected in the administrative data, such as medication trial starts/stops, ER visits, hospitalizations, or transitions to higher levels of MH care?<br />9. Another thought - your proposal seems to suggest that sites will be randomized to receive facilitation or EBP training as usual for your Type II trial, but I think that I read that you will only be rolling this out to four sites.  Will this be enough sites to randomize implementation method?<br /><br />I hope my above thoughts reflect my interest in this project rather than concerns with your proposal.  Again, this seems like a really smart idea which could really enhance care for Veterans that are often overlooked with regard to their mental health needs.  I look forward to discussing this further with you!<br /><br />Best,<br /><br />- Nick";s:6:"parent";s:32:"6753b2736bf728fc517656b02ea34734";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"e109374c7a10f6b3f321c84129ef4fca";}s:32:"4d0b2b0280a3ef223cabf6dc01ad3647";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"nbowersox";s:4:"name";s:13:"Nick Bowersox";s:4:"mail";s:24:"Nicholas.Bowersox@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1535636297;s:8:"modified";i:1535639141;}s:3:"raw";s:5535:"This is a really exciting and important project with potential real-life consequences in terms of lifespan and quality of life.  Care fragmentation is a huge problem for persons with chronic mental health conditions and your proposed approach is a sensible one that builds off research and models for care integration in this area.  It's an interesting idea, to build medical care into mental health rather than vice versa, which is the standard for other mental health conditions given the tendency of persons with chronic mental health issues to attach to mental health clinics over Primary Care (as we see in depression/anxiety).  What you are proposing is complex and there are likely some aspects of this project that you have developed which could not be fit into this brief format, so please excuse some questions that you have already considered.  Upon reviewing the above, I had the following questions/thoughts:

1. The description and measurement of Singer's model was a bit unclear - when it is initially introduced, it seemed framed as reflecting 3 areas (organizational, social, process), but your Aim 2 mentions a focus on 5 areas.  Are you planning on sticking to the model as proposed or assessing constructs which may span multiple areas?
2. Building off the above question of assessment, I would love to hear about your plans to collect information related to the areas included in Singer's model - does Singer (or later researchers) propose data collection approaches to assess things like interpersonal or normative integration?  It seems like evaluations of these concepts could be very challenging to standardize in a measure, which makes me think that you are planning on doing qualitative interviews to assess these areas (which is consistent with your Aim 3).  Even so, it would be interesting to hear about some of the benchmarks/characteristics of high/medium/low performance in these areas that you are expecting to find to allow you to compare differences across sites.  Such benchmarks may also need to be able to consider the variations in organizational culture (holistic recovery versus medical model), staffing, patient engagement in care, etc.  Perhaps you already have these things fleshed out, but it seems that you would want to think well ahead of your evaluation in this area.
3. Your description of your cohort of interest and analysis approach was a bit confusing, based on Aim 1.  You mention patients with depressive disorders, but also patients with SMI.  Are you planning on using persons with depressive disorders as a comparison group to measure the response of patients with SMI against?  This feels like a bit of an "apples to oranges" comparison unless you are able to establish group equivalence beyond diagnosis.  Again, I'm guessing that space limitations did not allow a discussion of your full plans here.
4. Building a bit on the previous point, I was wondering what your main outcomes of interest were to demonstrate the effects of this intervention.  You mention years of life lost as a main motivator for the development of this intervention, but that would be challenging to use as a primary outcome for a variety of reasons.  You also mentioned that a Hybrid Type I trial was underway, suggesting that you are already collecting outcome/effectiveness data.  What sorts of measures are you using to demonstrate effect?
5. Continuing the thinking about demonstration of intervention effect, how will this intervention be "sold" to new sites in a way that will motivate them to change their clinical practices?  The proposed intervention is moderately costly, so it will be important to offer some strong outcomes demonstrating that the project is value added from the perspective of multiple stakeholders.  What outcomes/changes will motivate providers to change their clinical practices to collaborate with on-site medical staff?  What will motivate administrators to budget to hire and place these staff in their mental health clinics?
6. You mention the idea of potential collaboration with off-site medical providers as well as embedded medical providers in your model, which seems like a smart way to offer sites the flexibility to implement this program around resource limitations while also further enhancing care collaboration beyond Primary Care and mental health.  However, such coordination/collaboration can be challenging when it involves multiple providers who work with a patient from different perspectives and with different treatment agendas.  Many collaborative care models include the nomination/appointment of one provider who works as a "treatment coordinator" for a patient to support care collaboration and consistent treatment decisions.  Is this an aspect of your model?  It was not clear from the description above.
7. You mention that your evaluation focus is transitioning from a Hybrid Type I to II, with a planned transition to a Type III in the future, but I did not see much discussion of implementation spread models or approaches.  What sorts of implementation spread techniques are you using to support the spread of this program to new sites, assess program fidelity, and allow for corrective adjustment?  

I hope that my above comments reflect enthusiasm for this project rather than major concerns - your proposed project is well-developed and represents a necessary expansion of care that is overdue for this vulnerable population.  I look forward to talking with you a bit more about your project in the near future!

Best,

- Nick";s:5:"xhtml";s:5655:"This is a really exciting and important project with potential real-life consequences in terms of lifespan and quality of life.  Care fragmentation is a huge problem for persons with chronic mental health conditions and your proposed approach is a sensible one that builds off research and models for care integration in this area.  It&#039;s an interesting idea, to build medical care into mental health rather than vice versa, which is the standard for other mental health conditions given the tendency of persons with chronic mental health issues to attach to mental health clinics over Primary Care (as we see in depression/anxiety).  What you are proposing is complex and there are likely some aspects of this project that you have developed which could not be fit into this brief format, so please excuse some questions that you have already considered.  Upon reviewing the above, I had the following questions/thoughts:<br /><br />1. The description and measurement of Singer&#039;s model was a bit unclear - when it is initially introduced, it seemed framed as reflecting 3 areas (organizational, social, process), but your Aim 2 mentions a focus on 5 areas.  Are you planning on sticking to the model as proposed or assessing constructs which may span multiple areas?<br />2. Building off the above question of assessment, I would love to hear about your plans to collect information related to the areas included in Singer&#039;s model - does Singer (or later researchers) propose data collection approaches to assess things like interpersonal or normative integration?  It seems like evaluations of these concepts could be very challenging to standardize in a measure, which makes me think that you are planning on doing qualitative interviews to assess these areas (which is consistent with your Aim 3).  Even so, it would be interesting to hear about some of the benchmarks/characteristics of high/medium/low performance in these areas that you are expecting to find to allow you to compare differences across sites.  Such benchmarks may also need to be able to consider the variations in organizational culture (holistic recovery versus medical model), staffing, patient engagement in care, etc.  Perhaps you already have these things fleshed out, but it seems that you would want to think well ahead of your evaluation in this area.<br />3. Your description of your cohort of interest and analysis approach was a bit confusing, based on Aim 1.  You mention patients with depressive disorders, but also patients with SMI.  Are you planning on using persons with depressive disorders as a comparison group to measure the response of patients with SMI against?  This feels like a bit of an &quot;apples to oranges&quot; comparison unless you are able to establish group equivalence beyond diagnosis.  Again, I&#039;m guessing that space limitations did not allow a discussion of your full plans here.<br />4. Building a bit on the previous point, I was wondering what your main outcomes of interest were to demonstrate the effects of this intervention.  You mention years of life lost as a main motivator for the development of this intervention, but that would be challenging to use as a primary outcome for a variety of reasons.  You also mentioned that a Hybrid Type I trial was underway, suggesting that you are already collecting outcome/effectiveness data.  What sorts of measures are you using to demonstrate effect?<br />5. Continuing the thinking about demonstration of intervention effect, how will this intervention be &quot;sold&quot; to new sites in a way that will motivate them to change their clinical practices?  The proposed intervention is moderately costly, so it will be important to offer some strong outcomes demonstrating that the project is value added from the perspective of multiple stakeholders.  What outcomes/changes will motivate providers to change their clinical practices to collaborate with on-site medical staff?  What will motivate administrators to budget to hire and place these staff in their mental health clinics?<br />6. You mention the idea of potential collaboration with off-site medical providers as well as embedded medical providers in your model, which seems like a smart way to offer sites the flexibility to implement this program around resource limitations while also further enhancing care collaboration beyond Primary Care and mental health.  However, such coordination/collaboration can be challenging when it involves multiple providers who work with a patient from different perspectives and with different treatment agendas.  Many collaborative care models include the nomination/appointment of one provider who works as a &quot;treatment coordinator&quot; for a patient to support care collaboration and consistent treatment decisions.  Is this an aspect of your model?  It was not clear from the description above.<br />7. You mention that your evaluation focus is transitioning from a Hybrid Type I to II, with a planned transition to a Type III in the future, but I did not see much discussion of implementation spread models or approaches.  What sorts of implementation spread techniques are you using to support the spread of this program to new sites, assess program fidelity, and allow for corrective adjustment?  <br /><br />I hope that my above comments reflect enthusiasm for this project rather than major concerns - your proposed project is well-developed and represents a necessary expansion of care that is overdue for this vulnerable population.  I look forward to talking with you a bit more about your project in the near future!<br /><br />Best,<br /><br />- Nick";s:6:"parent";s:32:"137b30cd1b4c9dac168fdfdffd76afae";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"4d0b2b0280a3ef223cabf6dc01ad3647";}s:32:"59ef5da78ed8de28648cb6aadb5a5d8d";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"nbowersox";s:4:"name";s:13:"Nick Bowersox";s:4:"mail";s:24:"Nicholas.Bowersox@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535639111;}s:3:"raw";s:5003:"This is a really interesting proposal developed to address some serious unmet needs in resource-limited settings via creative use of available staff and training.  The proposed approach is straight-forward and sensible, but has a lot of aspects that may benefit from advanced thought and development to support success.  It is likely that you have already developed plans to address these areas and they are not included in your summary due to space limitations, but I wanted to mention my thoughts/questions about your proposed project to support further discussion:

1. You mention the treatment gap, which is a crucial motivator for this work.  Additional specific information on this gap and its potential impact on persons who would benefit from mental health care but do not receive this care would be very useful to help "sell" this intervention to sites with limited enthusiasm.  Information on this gap and its effects can be contrasted with your evaluation results that demonstrate that this intervention reduces/eliminates this gap (and, hopefully, the negative outcomes associated with a lack of access to mental health care) to form a very strong argument in support of program implementation and resource allocation.
2. Building from the above point, I was not clear what your main outcomes of interest would be for this project.  What sorts of measures will allow you to demonstrate the effectiveness of the program?  Based on your description of your evaluation approach, it seems that you will be collecting data to assess organizational fit and other measures related to implementation, but I was not clear about whether you would also be assessing patient outcomes of interest (which could be based on the research on lack of access to mental health care from point #1 above) or provider outcomes of interest (e.g., satisfaction, burnout, etc).  These measures may be a part of your evaluation plan already, but are worth considering.
3. Continuing the discussion on evaluation, I would be interested to hear more information on your plans for data collection.  In more established areas, we tend to rely on electronic medical records to provide us with treatment information and outcomes.  Would this information be available for Mozambique health care settings?  If not, how will you gather information related to patient response to treatment such as number of sessions attended, changes in symptom scores, etc?
4. I am a bit unclear about the various roles that providers to be trained in IPC will play in this program.  You mentioned PsyTs, CHWs, and PCPs, and my understanding is that all of these providers will deliver IPC under supervision of more formally trained mental health providers (psychiatrists and psychologists).  However, I expect that there will be a lot of variability in the amount of previous mental health training, availability to provide IPC, and resources that will allow them to deliver IPC.  More information about these persons and their expected roles would be helpful to better understand how to get them involved in your projet.
5. Building on the previous point, will the PsyTs, CHWs, and PCPs have adequate freedom in their schedules to deliver IPC as intended with patients?  My understanding of IPC is that it requires a much higher level of provider/patient contact than is the norm for Primary Care or standard medical settings.  Will this cause problems for these providers in terms of balancing their other clinical duties and patient caseloads, or place limitations on the number of patients to whom they can offer IPC?
6. You mention the use of a "train the trainer" model to spread IPC skills across providers.  I would be interested to hear more of the specifics of your plans related to this area.  More specifically, how often are trainers interacting with trainees, and for how many training sessions?  How will these assess IPC fidelity - what measures or benchmarks will be the focus of fidelity and trigger remedial discussions?  Will there be formal remedial plans in the case of sustained low fidelity to IPC?  You mention that supervision will be provided until there are "5 completed cases" - how is this defined?
7. It would be helpful to hear a bit more about your expectations for your patient population of interest.  Based on the setting of care, I am assuming that this would be a general mental health population with primarily mood and anxiety disorders as the conditions of focus.  Would there be conditions which would be a poor fit to this approach (e.g., schizophrenia)?  Such information could be helpful to assist with the development of triage guidance materials, given the role that CHWs may play in the treatment process.

I hope that my comments reflect enthusiasm for this project and not major concerns - this project is quite strong and many of my thoughts likely reflect a lack of familiarity with the treatment setting.  I look forward to discussing this more with you in the near future!

Best,

- Nick
";s:5:"xhtml";s:5102:"This is a really interesting proposal developed to address some serious unmet needs in resource-limited settings via creative use of available staff and training.  The proposed approach is straight-forward and sensible, but has a lot of aspects that may benefit from advanced thought and development to support success.  It is likely that you have already developed plans to address these areas and they are not included in your summary due to space limitations, but I wanted to mention my thoughts/questions about your proposed project to support further discussion:<br /><br />1. You mention the treatment gap, which is a crucial motivator for this work.  Additional specific information on this gap and its potential impact on persons who would benefit from mental health care but do not receive this care would be very useful to help &quot;sell&quot; this intervention to sites with limited enthusiasm.  Information on this gap and its effects can be contrasted with your evaluation results that demonstrate that this intervention reduces/eliminates this gap (and, hopefully, the negative outcomes associated with a lack of access to mental health care) to form a very strong argument in support of program implementation and resource allocation.<br />2. Building from the above point, I was not clear what your main outcomes of interest would be for this project.  What sorts of measures will allow you to demonstrate the effectiveness of the program?  Based on your description of your evaluation approach, it seems that you will be collecting data to assess organizational fit and other measures related to implementation, but I was not clear about whether you would also be assessing patient outcomes of interest (which could be based on the research on lack of access to mental health care from point #1 above) or provider outcomes of interest (e.g., satisfaction, burnout, etc).  These measures may be a part of your evaluation plan already, but are worth considering.<br />3. Continuing the discussion on evaluation, I would be interested to hear more information on your plans for data collection.  In more established areas, we tend to rely on electronic medical records to provide us with treatment information and outcomes.  Would this information be available for Mozambique health care settings?  If not, how will you gather information related to patient response to treatment such as number of sessions attended, changes in symptom scores, etc?<br />4. I am a bit unclear about the various roles that providers to be trained in IPC will play in this program.  You mentioned PsyTs, CHWs, and PCPs, and my understanding is that all of these providers will deliver IPC under supervision of more formally trained mental health providers (psychiatrists and psychologists).  However, I expect that there will be a lot of variability in the amount of previous mental health training, availability to provide IPC, and resources that will allow them to deliver IPC.  More information about these persons and their expected roles would be helpful to better understand how to get them involved in your projet.<br />5. Building on the previous point, will the PsyTs, CHWs, and PCPs have adequate freedom in their schedules to deliver IPC as intended with patients?  My understanding of IPC is that it requires a much higher level of provider/patient contact than is the norm for Primary Care or standard medical settings.  Will this cause problems for these providers in terms of balancing their other clinical duties and patient caseloads, or place limitations on the number of patients to whom they can offer IPC?<br />6. You mention the use of a &quot;train the trainer&quot; model to spread IPC skills across providers.  I would be interested to hear more of the specifics of your plans related to this area.  More specifically, how often are trainers interacting with trainees, and for how many training sessions?  How will these assess IPC fidelity - what measures or benchmarks will be the focus of fidelity and trigger remedial discussions?  Will there be formal remedial plans in the case of sustained low fidelity to IPC?  You mention that supervision will be provided until there are &quot;5 completed cases&quot; - how is this defined?<br />7. It would be helpful to hear a bit more about your expectations for your patient population of interest.  Based on the setting of care, I am assuming that this would be a general mental health population with primarily mood and anxiety disorders as the conditions of focus.  Would there be conditions which would be a poor fit to this approach (e.g., schizophrenia)?  Such information could be helpful to assist with the development of triage guidance materials, given the role that CHWs may play in the treatment process.<br /><br />I hope that my comments reflect enthusiasm for this project and not major concerns - this project is quite strong and many of my thoughts likely reflect a lack of familiarity with the treatment setting.  I look forward to discussing this more with you in the near future!<br /><br />Best,<br /><br />- Nick";s:6:"parent";s:32:"52d8f69591ce10d86e339e1f7a948f49";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"59ef5da78ed8de28648cb6aadb5a5d8d";}s:32:"103d8d2237b2002acc548e47ac287dd1";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dpintello";s:4:"name";s:14:"Denny Pintello";s:4:"mail";s:23:"Denise.Pintello@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1535745746;s:8:"modified";i:1535748286;}s:3:"raw";s:2640:"Addie: 

Welcome to TIDIRH and thank you for submitting your specific aims paper in an area of importance to the mental health field. Allow me to share my impressions - and please understand that while it may seem like I am focusing in the not-so-good stuff, my ultimate goal is to be helpful as you develop into a formidable D&I researcher.

I really like how you begin by setting up the background of your focus area. As you shifted to the statement of the problem, I found myself wanting to know more about what research has been done so far in this area, what - if anything, has been successfully implemented, what is the current research gap, and what is the new knowledge that your study would bring to the field of PPD screening (aka: why should anyone fund this :) ). I would also like to see a sentence that starts with: "The purpose of this study is to ...", as well as a brief description of the behavioral change techniques that you are considering, are they evidence-based, what is your sample size and the location/setting, and what is the public health impact that your findings would contribute to the field. 

Then - moving on to your specific aims - these are nicely and sequentially laid out, yet need more clarity. 
Aim 1: You are conducting a mixed methods approach - are you including any mothers (patients) to gain their input? Also - what is the purpose of the survey and which one(s) are you using?
Aim 2: As mentioned above - what beh change techniques are you planning to introduce to the WIC staff? Since this looks exploratory, it may be helpful to include a hypothesis here.  
Aim 3: It is difficult to determine exactly what you are planning to test - are you testing the implementation of a training strategy to modify WIC staff perceptions - that will lead to behavioral changes - that in turn, will increase PPD screening? Keep in mind that usually, Aim 3 is the core component of your study and reviewers will want to be excited about what you are proposing to do. 

I encourage you to check out the NIH RePorter and look at Aaron Lyons BOLT abstract. While he is focusing on a different research question and setting, he clearly outlines his specific aims. Also, Christina Studts is testing Family-Check-up in rural settings -- while she uses a different writing style in her abstract, she nicely conveys the focus and scope of her study. I always find it helpful to look at other researchers successfully funded abstracts - and there are many of them in NIH RePorter.

I think this will be enough to get you started as you work on your next assignment. I look forward to seeing your next effort! 

Denny";s:5:"xhtml";s:2729:"Addie: <br /><br />Welcome to TIDIRH and thank you for submitting your specific aims paper in an area of importance to the mental health field. Allow me to share my impressions - and please understand that while it may seem like I am focusing in the not-so-good stuff, my ultimate goal is to be helpful as you develop into a formidable D&amp;I researcher.<br /><br />I really like how you begin by setting up the background of your focus area. As you shifted to the statement of the problem, I found myself wanting to know more about what research has been done so far in this area, what - if anything, has been successfully implemented, what is the current research gap, and what is the new knowledge that your study would bring to the field of PPD screening (aka: why should anyone fund this :) ). I would also like to see a sentence that starts with: &quot;The purpose of this study is to ...&quot;, as well as a brief description of the behavioral change techniques that you are considering, are they evidence-based, what is your sample size and the location/setting, and what is the public health impact that your findings would contribute to the field. <br /><br />Then - moving on to your specific aims - these are nicely and sequentially laid out, yet need more clarity. <br />Aim 1: You are conducting a mixed methods approach - are you including any mothers (patients) to gain their input? Also - what is the purpose of the survey and which one(s) are you using?<br />Aim 2: As mentioned above - what beh change techniques are you planning to introduce to the WIC staff? Since this looks exploratory, it may be helpful to include a hypothesis here.  <br />Aim 3: It is difficult to determine exactly what you are planning to test - are you testing the implementation of a training strategy to modify WIC staff perceptions - that will lead to behavioral changes - that in turn, will increase PPD screening? Keep in mind that usually, Aim 3 is the core component of your study and reviewers will want to be excited about what you are proposing to do. <br /><br />I encourage you to check out the NIH RePorter and look at Aaron Lyons BOLT abstract. While he is focusing on a different research question and setting, he clearly outlines his specific aims. Also, Christina Studts is testing Family-Check-up in rural settings -- while she uses a different writing style in her abstract, she nicely conveys the focus and scope of her study. I always find it helpful to look at other researchers successfully funded abstracts - and there are many of them in NIH RePorter.<br /><br />I think this will be enough to get you started as you work on your next assignment. I look forward to seeing your next effort! <br /><br />Denny";s:6:"parent";s:32:"75a72eff22b1c58af59220a826911b40";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"103d8d2237b2002acc548e47ac287dd1";}s:32:"b2ff94bb72e4b38045efa4976ff15aa1";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dpintello";s:4:"name";s:14:"Denny Pintello";s:4:"mail";s:23:"Denise.Pintello@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1535747653;s:8:"modified";i:1535748366;}s:3:"raw";s:2458:"Brenna:

Hi there - and welcome to our Mental Health Group in TIDIRH. Allow me to comment on your specific aims page - and please keep in mind that it may feel as though I am zeroing in on what is missing, I hope that you know my ultimate goal is that you learn from Nick, Lindsey and me over the coming months to hopefully develop into a successful D&I researcher. 

Please know that ASD research is a priority at NIMH. You describe an important problem among adults and the research gap regarding co-occurring ASD & problems with MH. As I suspect that you know, Lauren Brookman-Frazee's AIM-HI treatment focuses on this same domain pertaining to kids (and has been found to be effective). She is currently testing various implementation strategies to deliver AIM-HI in MH clinics and there may be an opportunity to look at her work to determine if you can build off of her research. 

You do a nice job setting up the background, statement of the problem and as well as the purpose of your study. I found myself wanting to know more about what you are specifically proposing to do with the opinion leaders (do they receive any training, are they involved in reducing org barriers to delivering the training?, etc.). I also would like to know your hypothesis as to what is it about the opinion leader that will enhance the delivery of CBT with fidelity. 

Regarding your specific aims: 
AIM 1: Since you are manipulating the implementation strategy in Aim 1 and measuring the effectiveness of CBT treatment in AIM 2, I am wondering if this really is more of a hybrid type III. Are you adapting the CBT in some way to test the effectiveness? Usually, in a hybrid II, you are also manipulating the effectiveness in some way. As currently written, it is unclear if you are tinkering with it to boost effectiveness. 

You do a nice job in describing both aims and provide important details. I was also pleased to see that you referred to 'targets' - which demonstrates that you are familiar with the NIMH experimental therapeutics paradigm - and also to the IACC - as well as the potential public health impact of the study. If you haven't done so already, please visit NIH RePorter and look at Lauren B-Fs latest publications from her collaborative R01 with Aubyn Stahmer to explore how your work can nicely complement their efforts. 

I do hope you find this feedback helpful and look forward to seeing how you incorporate it into your next assignment.

Denny";s:5:"xhtml";s:2561:"Brenna:<br /><br />Hi there - and welcome to our Mental Health Group in TIDIRH. Allow me to comment on your specific aims page - and please keep in mind that it may feel as though I am zeroing in on what is missing, I hope that you know my ultimate goal is that you learn from Nick, Lindsey and me over the coming months to hopefully develop into a successful D&amp;I researcher. <br /><br />Please know that ASD research is a priority at NIMH. You describe an important problem among adults and the research gap regarding co-occurring ASD &amp; problems with MH. As I suspect that you know, Lauren Brookman-Frazee&#039;s AIM-HI treatment focuses on this same domain pertaining to kids (and has been found to be effective). She is currently testing various implementation strategies to deliver AIM-HI in MH clinics and there may be an opportunity to look at her work to determine if you can build off of her research. <br /><br />You do a nice job setting up the background, statement of the problem and as well as the purpose of your study. I found myself wanting to know more about what you are specifically proposing to do with the opinion leaders (do they receive any training, are they involved in reducing org barriers to delivering the training?, etc.). I also would like to know your hypothesis as to what is it about the opinion leader that will enhance the delivery of CBT with fidelity. <br /><br />Regarding your specific aims: <br />AIM 1: Since you are manipulating the implementation strategy in Aim 1 and measuring the effectiveness of CBT treatment in AIM 2, I am wondering if this really is more of a hybrid type III. Are you adapting the CBT in some way to test the effectiveness? Usually, in a hybrid II, you are also manipulating the effectiveness in some way. As currently written, it is unclear if you are tinkering with it to boost effectiveness. <br /><br />You do a nice job in describing both aims and provide important details. I was also pleased to see that you referred to &#039;targets&#039; - which demonstrates that you are familiar with the NIMH experimental therapeutics paradigm - and also to the IACC - as well as the potential public health impact of the study. If you haven&#039;t done so already, please visit NIH RePorter and look at Lauren B-Fs latest publications from her collaborative R01 with Aubyn Stahmer to explore how your work can nicely complement their efforts. <br /><br />I do hope you find this feedback helpful and look forward to seeing how you incorporate it into your next assignment.<br /><br />Denny";s:6:"parent";s:32:"0a9049790fdeffd9ec6d64a97575c2a6";s:7:"replies";a:1:{i:0;s:32:"1ec2a86c5dcf9c94c585ac96380b2ed5";}s:4:"show";b:1;s:3:"cid";s:32:"b2ff94bb72e4b38045efa4976ff15aa1";}s:32:"9f7899eab6ac54f4b1461e806796ecdf";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1535852231;s:8:"modified";i:1535852322;}s:3:"raw";s:9528:"Hi Kelsie,

Great work on your aims! I’m going to offer my feedback in the form of questions. Take my questions with a grain of salt. Other readers may not seek the exact same details.  However, since aims are supposed to be specific, I hope sharing the questions that I still had after reading your aims will help you with your next iteration of refinement.

Reviewers typically form quick gestalt impressions of the study based on the aims. My takeaway was that your implementation problem statement could be summarized as,
“Measurement-based care is the first step to EBP implementation, but hindered by provider adoption.”  Is that what you intended?  Are you arguing that the challenges with provider adoption of MBC limits the reach of EBPs among youth served in community settings?  If so, I think you may be on to something important!  It may also be that you are trying to do quite a lot in this study. Which funding mechanism (budget and timeline) are you targeting? Knowing this helps readers understand whether you are proposing an appropriate scope for your questions.

1. Beef up your justification for the link between MBC as a barrier in EBP adoption overall. And, justify your rationale for focusing specifically on the EBP implementation outcome of “adoption” when describing your EBP implementation problem in the first paragraph (see Proctor et al., 2011 or https://cpb-us-w2.wpmucdn.com/sites.wustl.edu/dist/6/786/files/2017/08/DIRC-implementation-outcomes-tool-dg_7-27-17_ab-27xbrka.pdf).  Try to instill great confidence in your readers that it is provider adoption of MBC that limits the EBP availability for youth.

2. My favorite sentence in all of your aims, leads the last paragraph: “Developing a formal MBC system is an important overlooked step for system-wide EBP implementation in community settings.” I consider this your most compelling thesis, and it would pack a lot of punch up front leading off your second paragraph. Here’s where it seems to me you are going (using your own words from paragraph 2): “Developing a formal MBC system is an important overlooked step for system-wide EBP implementation in community settings. A critical component of many EBPs is the use of routine client outcome data to inform treatment decision-making, also known as measurement-based care (MBC).4-6 This distinct skill offers transparency in treatment outcome to consumers and provides valuable data on service effectiveness at the consumer level. Individual-level (e.g., therapist, care coordinator) knowledge measurement developments have revealed a distinction in specific EBP and MBC knowledge within community mental health therapists suggesting that implementation strategies targeted specifically at MBC are warranted.7 Therefore, a critical first step in successful EBP implementation is multicontextual implementation strategies to promote therapist MBC adoption.”

3. In this merged paragraph 2, can you briefly operationally define “multicontextual implementation strategies” (see Powell et al., 2015) that you see as empirically linked to therapist MBC adoption? You mention interviewing to identify barrier and facilitators (aim 1). You also mention several other factors at the end of the first paragraph: funding, policies, organization, therapist, youth, etc. Do you have any evidence that suggests which are most critical? Which are related to the Hawai’i  MBC roll out? Do you have a theory or framework of EBP/MBC implementation that relates the potential multicontextual implementation barriers/facilitators you expect to identify in aim 1 in relation to provider MBC adoption, and to potential implementation strategies you plan to identify in aim 2?  

4. Organizing your aims according to a theory or framework will help understand how your study fits with the Hawai’i  rollout. When will this study occur relative to phases of the State’s implementation, as defined by implementation frameworks (e.g.,  Aarons et al., 2011: exploration phase, adoption decision/preparation phase, active implementation phase, sustainment phase).

5. I had a pretty big question between paragraphs 2 and 3, do you mean to propose that your study helps Hawai’i  develop “a formal MBC system”? Or, are you helping develop “mulitcontextual implementation strategies” for the State’s roll out? Or, are you evaluating the State’s roll-out after the fact?  All of these?

6. Paragraph 4 will have more impact if stated in active voice that succinctly states your aims. Leave the analytic detail to the numbered aims below. With or without the analytic pieces, I still question: Is your aim to support the MBC EHR adoption or observe it? For example, how will you support MBC EHR adoption by 1) determining groups of adopters (second sentence)? As written, this risks sounding tautological. As the last paragraph before your aims, it would be great if this second sentence better paralleled aim 1, which emphasizes both identification of barriers and facilitators identified via interview/focus groups, and LCA to identify implementation groups (are these the adopters?). Are adopters individuals, or organizations within the Division? Are groups being “targeted” or “identified?” These words sound conceptually and analytically distinct. At what phases of MBC implementation will latent classes be identified? 

7. In the second sentence of paragraph 4, it looks like you aim to support the MBC implementation by 2) tailoring implementation strategies.  This parallels aim 2, which is great.  In aim 2, how was conjoint analysis selected as the method for identifying acceptability and feasibility? How will discrete choice experiments be conducted and coded? Explain why you propose to use this approach and not others?

8. In the third sentence of paragraph 4, it looks like you aim to support the MBC implementation by 3) identifying individual and organizational determinants of MBC implementation. Is aim 3 an exploratory aim with post-hoc modeling? Is it an evaluation aim? There is a lot of jargon that could be explained and simplified, which would give you the space you need to explain what measures will comprise your determinant predictor variables and what measures will comprise your MBC implementation outcome variables. Does ‘systematic quantitative analysis,’ ‘hierarchical linear modeling,’ and ‘predictive models’ all refer to the same analyses?  If these are multi-level analyses, specify the model, the analysis software, and the way variance components at individual and organization levels will be handled in aim 3.

9. Will you be starting off your aim 1 and 2 activities, with standard tools or measures (e.g., CFIR, see Damschroder et al., 2009)? For aim 1, how many interviews and focus groups will you conduct and with which stakeholders? Who are these stakeholders from the divisions, what are their roles and how will they be recruited, engaged, etc.?

10. For aim 2, will you use the brief programmatic acceptability, appropriateness and feasibility measures (AIM, IAM and FIM, See Weiner et al., 2017)?  Will you just “identify” and “choose” strategies or will you also “implement” them?

11. If you are implementing the strategies you select in this study, implementation warrants its own specific aim. How will the barrier/facilitator, and strategy selection activities (aim 1, aim 2) relate to the Hawai’i  EHR rollout described in paragraph 3? I’m not sure how the large space dedicated to paragraph 3 sets up your aims and design.  You could probably make that paragraph work harder for you. Is paragraph 3 describing context of the study (if so, it could probably be reduced)? Or, is the multi-part implementation process described at the end of paragraph 3 meant to frame your research design? This is a great place to clarify how the timeline of the Hawai’i  effort and the timeline of activities proposed for this study correspond to one another. 

13. For aim 3, state the measures, justify and cite them. For example, the EBPAS-36 (Aarons et al., 2004, 2010; Rye et al., 2017), and the ORCA (Helfrich et al., 2009; see also Weiner et al., 2008). It is when I get to aim 3 that I begin to wonder whether your overall design is an observational study. Is it? Will the Hawai’i MBC implementation be rolling out and you’ll naturalistically observe implementation? Or, are you trying to improve MBC implementation through your aim 2 strategies? If it’s the later, how will aim 3  “implementation success” be operationalized, the measures above are measures of determinants of MBC implementation, not MBC implementation outcomes (e.g., reach, adoption, penetration, fidelity).

I still wondered about a lot after reading your aims. On the one hand, that’s good! It means I was curious about what you will do. I did sense that you’re proposing a project with a potential for impact.  On the other hand, possibly I’m confused, or you’re proposing several projects! By title, I see you are an evaluator with the Division, so perhaps you are really proposing 3 major studies throughout the multi-year roll-out? If so, it would help to state that clearly. Perhaps my questions will highlight opportunities for you to narrow to the overarching framework you will use to select the study design, methods and measures that are most important for you to achieve your aims.

Great work, Kelsie! I apologize that I didn’t have time to edit to make my list of questions shorter!";s:5:"xhtml";s:9693:"Hi Kelsie,<br /><br />Great work on your aims! I’m going to offer my feedback in the form of questions. Take my questions with a grain of salt. Other readers may not seek the exact same details.  However, since aims are supposed to be specific, I hope sharing the questions that I still had after reading your aims will help you with your next iteration of refinement.<br /><br />Reviewers typically form quick gestalt impressions of the study based on the aims. My takeaway was that your implementation problem statement could be summarized as,<br />“Measurement-based care is the first step to EBP implementation, but hindered by provider adoption.”  Is that what you intended?  Are you arguing that the challenges with provider adoption of MBC limits the reach of EBPs among youth served in community settings?  If so, I think you may be on to something important!  It may also be that you are trying to do quite a lot in this study. Which funding mechanism (budget and timeline) are you targeting? Knowing this helps readers understand whether you are proposing an appropriate scope for your questions.<br /><br />1. Beef up your justification for the link between MBC as a barrier in EBP adoption overall. And, justify your rationale for focusing specifically on the EBP implementation outcome of “adoption” when describing your EBP implementation problem in the first paragraph (see Proctor et al., 2011 or https://cpb-us-w2.wpmucdn.com/sites.wustl.edu/dist/6/786/files/2017/08/DIRC-implementation-outcomes-tool-dg_7-27-17_ab-27xbrka.pdf).  Try to instill great confidence in your readers that it is provider adoption of MBC that limits the EBP availability for youth.<br /><br />2. My favorite sentence in all of your aims, leads the last paragraph: “Developing a formal MBC system is an important overlooked step for system-wide EBP implementation in community settings.” I consider this your most compelling thesis, and it would pack a lot of punch up front leading off your second paragraph. Here’s where it seems to me you are going (using your own words from paragraph 2): “Developing a formal MBC system is an important overlooked step for system-wide EBP implementation in community settings. A critical component of many EBPs is the use of routine client outcome data to inform treatment decision-making, also known as measurement-based care (MBC).4-6 This distinct skill offers transparency in treatment outcome to consumers and provides valuable data on service effectiveness at the consumer level. Individual-level (e.g., therapist, care coordinator) knowledge measurement developments have revealed a distinction in specific EBP and MBC knowledge within community mental health therapists suggesting that implementation strategies targeted specifically at MBC are warranted.7 Therefore, a critical first step in successful EBP implementation is multicontextual implementation strategies to promote therapist MBC adoption.”<br /><br />3. In this merged paragraph 2, can you briefly operationally define “multicontextual implementation strategies” (see Powell et al., 2015) that you see as empirically linked to therapist MBC adoption? You mention interviewing to identify barrier and facilitators (aim 1). You also mention several other factors at the end of the first paragraph: funding, policies, organization, therapist, youth, etc. Do you have any evidence that suggests which are most critical? Which are related to the Hawai’i  MBC roll out? Do you have a theory or framework of EBP/MBC implementation that relates the potential multicontextual implementation barriers/facilitators you expect to identify in aim 1 in relation to provider MBC adoption, and to potential implementation strategies you plan to identify in aim 2?  <br /><br />4. Organizing your aims according to a theory or framework will help understand how your study fits with the Hawai’i  rollout. When will this study occur relative to phases of the State’s implementation, as defined by implementation frameworks (e.g.,  Aarons et al., 2011: exploration phase, adoption decision/preparation phase, active implementation phase, sustainment phase).<br /><br />5. I had a pretty big question between paragraphs 2 and 3, do you mean to propose that your study helps Hawai’i  develop “a formal MBC system”? Or, are you helping develop “mulitcontextual implementation strategies” for the State’s roll out? Or, are you evaluating the State’s roll-out after the fact?  All of these?<br /><br />6. Paragraph 4 will have more impact if stated in active voice that succinctly states your aims. Leave the analytic detail to the numbered aims below. With or without the analytic pieces, I still question: Is your aim to support the MBC EHR adoption or observe it? For example, how will you support MBC EHR adoption by 1) determining groups of adopters (second sentence)? As written, this risks sounding tautological. As the last paragraph before your aims, it would be great if this second sentence better paralleled aim 1, which emphasizes both identification of barriers and facilitators identified via interview/focus groups, and LCA to identify implementation groups (are these the adopters?). Are adopters individuals, or organizations within the Division? Are groups being “targeted” or “identified?” These words sound conceptually and analytically distinct. At what phases of MBC implementation will latent classes be identified? <br /><br />7. In the second sentence of paragraph 4, it looks like you aim to support the MBC implementation by 2) tailoring implementation strategies.  This parallels aim 2, which is great.  In aim 2, how was conjoint analysis selected as the method for identifying acceptability and feasibility? How will discrete choice experiments be conducted and coded? Explain why you propose to use this approach and not others?<br /><br />8. In the third sentence of paragraph 4, it looks like you aim to support the MBC implementation by 3) identifying individual and organizational determinants of MBC implementation. Is aim 3 an exploratory aim with post-hoc modeling? Is it an evaluation aim? There is a lot of jargon that could be explained and simplified, which would give you the space you need to explain what measures will comprise your determinant predictor variables and what measures will comprise your MBC implementation outcome variables. Does ‘systematic quantitative analysis,’ ‘hierarchical linear modeling,’ and ‘predictive models’ all refer to the same analyses?  If these are multi-level analyses, specify the model, the analysis software, and the way variance components at individual and organization levels will be handled in aim 3.<br /><br />9. Will you be starting off your aim 1 and 2 activities, with standard tools or measures (e.g., CFIR, see Damschroder et al., 2009)? For aim 1, how many interviews and focus groups will you conduct and with which stakeholders? Who are these stakeholders from the divisions, what are their roles and how will they be recruited, engaged, etc.?<br /><br />10. For aim 2, will you use the brief programmatic acceptability, appropriateness and feasibility measures (AIM, IAM and FIM, See Weiner et al., 2017)?  Will you just “identify” and “choose” strategies or will you also “implement” them?<br /><br />11. If you are implementing the strategies you select in this study, implementation warrants its own specific aim. How will the barrier/facilitator, and strategy selection activities (aim 1, aim 2) relate to the Hawai’i  EHR rollout described in paragraph 3? I’m not sure how the large space dedicated to paragraph 3 sets up your aims and design.  You could probably make that paragraph work harder for you. Is paragraph 3 describing context of the study (if so, it could probably be reduced)? Or, is the multi-part implementation process described at the end of paragraph 3 meant to frame your research design? This is a great place to clarify how the timeline of the Hawai’i  effort and the timeline of activities proposed for this study correspond to one another. <br /><br />13. For aim 3, state the measures, justify and cite them. For example, the EBPAS-36 (Aarons et al., 2004, 2010; Rye et al., 2017), and the ORCA (Helfrich et al., 2009; see also Weiner et al., 2008). It is when I get to aim 3 that I begin to wonder whether your overall design is an observational study. Is it? Will the Hawai’i MBC implementation be rolling out and you’ll naturalistically observe implementation? Or, are you trying to improve MBC implementation through your aim 2 strategies? If it’s the later, how will aim 3  “implementation success” be operationalized, the measures above are measures of determinants of MBC implementation, not MBC implementation outcomes (e.g., reach, adoption, penetration, fidelity).<br /><br />I still wondered about a lot after reading your aims. On the one hand, that’s good! It means I was curious about what you will do. I did sense that you’re proposing a project with a potential for impact.  On the other hand, possibly I’m confused, or you’re proposing several projects! By title, I see you are an evaluator with the Division, so perhaps you are really proposing 3 major studies throughout the multi-year roll-out? If so, it would help to state that clearly. Perhaps my questions will highlight opportunities for you to narrow to the overarching framework you will use to select the study design, methods and measures that are most important for you to achieve your aims.<br /><br />Great work, Kelsie! I apologize that I didn’t have time to edit to make my list of questions shorter!";s:6:"parent";s:32:"c07bc00a1899f6f501a3f02f667cd8d3";s:7:"replies";a:1:{i:0;s:32:"30caefe79f73cf1eb84f1881aaf2d17a";}s:4:"show";b:1;s:3:"cid";s:32:"9f7899eab6ac54f4b1461e806796ecdf";}s:32:"a10e6c6f679370977234d2ec80bf54ed";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535858646;}s:3:"raw";s:6708:"Hi Alex,

Great work on your aims! I’m going to offer my feedback in the form of questions. Take my questions with a grain of salt. Other readers may not seek the exact same details.  However, since aims are supposed to be specific, I hope that relaying the questions that I still had after reading your aims will help you with your next iteration of refinement.

Reviewers typically form a pretty quick gestalt impression of the project based on the aims. My takeaway was that your implementation problem statement could be summarized as, “Limited EBT availability is due to challenges with EBT financing, a hermeneutic literature review and fiscal mapping process will help.” Is that what you intended?  My read is that this has face validity, and you may contribute something through the R15 that will advance the field. But, it also left me wanting more justification/rationale for focusing on this factor over any others – in your text, not just your key references (13, 14, 15). 

1. I wondered throughout what financing strategies are.  The sentence in paragraph 2 that starts, “We define financial strategies…,” left me with more questions about what “resources” and “activities,” and what dimensions of “implementation” and “sustainment” you mean. Can you provide some examples that help me to concretely grasp what your fiscal mapping process will be? Where does funding typically come from for EBTs come from for youth mental health service agencies? How variable is funding over time and from service system to service system?  Will this be addressed?  How do you know that limited EBT adoption is about fiscal mapping/tailoring and not just limited resource constraints in community service systems overall? Tackle this head-on, and consider talking about how environments with fiscal constraints need tailoring/optimizing most of all to make use of limited resources – this makes sense to me.

2. When you say in paragraph 3 that the mapping process will “guide the tailored selection and coordination of financing strategies from a comprehensive, well-defined set of options,” I wonder about the state of the science. I do see that in aim 1, you will conduct a literature review. But, what do you already know about the literature that is prompting this iterative review. Are there fiscal mapping options available? Are they from the EBT implementation science literature or are you drawing from other literatures?  How do existing fiscal mapping strategies differ from intervention mapping (Powell et al., 2017) or other implementation strategies (Powell et al., 2015)? When you say at the end of paragraph 2, your team “…will produce innovative extensions of current best practices,” it makes me want more information about 1) what you believe are the current best practices, and 2) what you anticipate the extensions will need to be to improve EBT implementation.

3. Reading the aims, pragmatically, I never really doubted that addressing fiscal issues are important for implementing EBTs effectively. At the same time, I thought, almost certainly they are not the only important factor.  Are there some implementation outcomes (Proctor et al., 2011) that you think are particularly relevant?  When you mention activities in paragraph 1 such as “training and consultation, monitoring and feedback, “etc. I wondered, "Is this about early preparation to adopt (see EPIS, Aarons et al., 2011), is about fidelity and sustainment (Carroll et al., 2007; Hasson et al., 2010; Schoenwald et al., 2011)?"  Are you linking fiscal/financial dimensions across phases of implementation and across EBT implementation outcomes? Do you know what the dimensions are, but just not how important they will be based on literature review and stakeholder engagement?

4.  How does EBT adaptation to a particular youth mental health service agency relate to fiscal/financial issues? In aim 1, you mention “seven key dimensions,” what are they? How do they relate to aim 1c “importance, feasibility and contextual influences"? You mention your “expert panel of stakeholders,” in paragraph 3 and aim 1, specifying where they will be from and their sample size. But, how will specific individuals be selected to ensure adequate saturation of expertise related to the fiscal challenges of EBT implementation? For example, will they be budget directors? Managers? State and local funders?

5. As I mentioned above, before reading aim 1, I really wanted to know more about the state of existing financing strategies. How many are there? What do they address? Are they variable? Are they like to be mis-matched to certain service systems in a way that really highlights how important iterative hermeneutic and Delphi strategies really are for your implementation problem?

6. Reading aim 2, I wondered, in what ways might you encounter new challenges with fiscal mapping that aren’t address by implementation mapping? This is an opportunity to spell out/argue what intervention mapping leaves unaddressed.

7. Since the R15 should include plans to expose students to hands-on research, without specifying training plans (see https://grants.nih.gov/grants/peer/r_awards/R15_Guide_for_reviewers.pdf, https://grants.nih.gov/grants/peer/critiques/r15_D.htm), I wondered if you could add sentence in paragraph 1, and in paragraph 3 that identifies some University of Arkansas students as stakeholders in these EBT settings involved in activities, such as clinical psychologists in training, etc. What will be their role in the iterative processes of aims 1 and 2?

8. Aim 2b, should this be broken out into an exploratory aim or third aim?  Will you conduct one aim each year for the 3-year project period? Specify what methods for group model-building you will use. What type of model will you build? What is the added value of doing this aim beyond developing the fiscal mapping process?

Overall, addressing the fiscal or financial aspects of EBT implementation stuck me as a valuable contribution to the field. Your aims laid out an appropriate scope of research activities for the budget and timeline an R15. That means, your aims made a strong, positive impression! At the same time, my questions reflect some lingering doubts about specifically what gaps your aims will address. In other words, what dimensions are you covering with this financial/fiscal piece (besides just saying that) that existing approaches leave unaddressed? Perhaps my questions can inform edits that will really make your case undeniable to future readers/reviewers.

Great work on your aims, Alex! I apologize that I didn’t have time to edit to make my list of questions shorter! 
";s:5:"xhtml";s:6841:"Hi Alex,<br /><br />Great work on your aims! I’m going to offer my feedback in the form of questions. Take my questions with a grain of salt. Other readers may not seek the exact same details.  However, since aims are supposed to be specific, I hope that relaying the questions that I still had after reading your aims will help you with your next iteration of refinement.<br /><br />Reviewers typically form a pretty quick gestalt impression of the project based on the aims. My takeaway was that your implementation problem statement could be summarized as, “Limited EBT availability is due to challenges with EBT financing, a hermeneutic literature review and fiscal mapping process will help.” Is that what you intended?  My read is that this has face validity, and you may contribute something through the R15 that will advance the field. But, it also left me wanting more justification/rationale for focusing on this factor over any others – in your text, not just your key references (13, 14, 15). <br /><br />1. I wondered throughout what financing strategies are.  The sentence in paragraph 2 that starts, “We define financial strategies…,” left me with more questions about what “resources” and “activities,” and what dimensions of “implementation” and “sustainment” you mean. Can you provide some examples that help me to concretely grasp what your fiscal mapping process will be? Where does funding typically come from for EBTs come from for youth mental health service agencies? How variable is funding over time and from service system to service system?  Will this be addressed?  How do you know that limited EBT adoption is about fiscal mapping/tailoring and not just limited resource constraints in community service systems overall? Tackle this head-on, and consider talking about how environments with fiscal constraints need tailoring/optimizing most of all to make use of limited resources – this makes sense to me.<br /><br />2. When you say in paragraph 3 that the mapping process will “guide the tailored selection and coordination of financing strategies from a comprehensive, well-defined set of options,” I wonder about the state of the science. I do see that in aim 1, you will conduct a literature review. But, what do you already know about the literature that is prompting this iterative review. Are there fiscal mapping options available? Are they from the EBT implementation science literature or are you drawing from other literatures?  How do existing fiscal mapping strategies differ from intervention mapping (Powell et al., 2017) or other implementation strategies (Powell et al., 2015)? When you say at the end of paragraph 2, your team “…will produce innovative extensions of current best practices,” it makes me want more information about 1) what you believe are the current best practices, and 2) what you anticipate the extensions will need to be to improve EBT implementation.<br /><br />3. Reading the aims, pragmatically, I never really doubted that addressing fiscal issues are important for implementing EBTs effectively. At the same time, I thought, almost certainly they are not the only important factor.  Are there some implementation outcomes (Proctor et al., 2011) that you think are particularly relevant?  When you mention activities in paragraph 1 such as “training and consultation, monitoring and feedback, “etc. I wondered, &quot;Is this about early preparation to adopt (see EPIS, Aarons et al., 2011), is about fidelity and sustainment (Carroll et al., 2007; Hasson et al., 2010; Schoenwald et al., 2011)?&quot;  Are you linking fiscal/financial dimensions across phases of implementation and across EBT implementation outcomes? Do you know what the dimensions are, but just not how important they will be based on literature review and stakeholder engagement?<br /><br />4.  How does EBT adaptation to a particular youth mental health service agency relate to fiscal/financial issues? In aim 1, you mention “seven key dimensions,” what are they? How do they relate to aim 1c “importance, feasibility and contextual influences&quot;? You mention your “expert panel of stakeholders,” in paragraph 3 and aim 1, specifying where they will be from and their sample size. But, how will specific individuals be selected to ensure adequate saturation of expertise related to the fiscal challenges of EBT implementation? For example, will they be budget directors? Managers? State and local funders?<br /><br />5. As I mentioned above, before reading aim 1, I really wanted to know more about the state of existing financing strategies. How many are there? What do they address? Are they variable? Are they like to be mis-matched to certain service systems in a way that really highlights how important iterative hermeneutic and Delphi strategies really are for your implementation problem?<br /><br />6. Reading aim 2, I wondered, in what ways might you encounter new challenges with fiscal mapping that aren’t address by implementation mapping? This is an opportunity to spell out/argue what intervention mapping leaves unaddressed.<br /><br />7. Since the R15 should include plans to expose students to hands-on research, without specifying training plans (see https://grants.nih.gov/grants/peer/r_awards/R15_Guide_for_reviewers.pdf, https://grants.nih.gov/grants/peer/critiques/r15_D.htm), I wondered if you could add sentence in paragraph 1, and in paragraph 3 that identifies some University of Arkansas students as stakeholders in these EBT settings involved in activities, such as clinical psychologists in training, etc. What will be their role in the iterative processes of aims 1 and 2?<br /><br />8. Aim 2b, should this be broken out into an exploratory aim or third aim?  Will you conduct one aim each year for the 3-year project period? Specify what methods for group model-building you will use. What type of model will you build? What is the added value of doing this aim beyond developing the fiscal mapping process?<br /><br />Overall, addressing the fiscal or financial aspects of EBT implementation stuck me as a valuable contribution to the field. Your aims laid out an appropriate scope of research activities for the budget and timeline an R15. That means, your aims made a strong, positive impression! At the same time, my questions reflect some lingering doubts about specifically what gaps your aims will address. In other words, what dimensions are you covering with this financial/fiscal piece (besides just saying that) that existing approaches leave unaddressed? Perhaps my questions can inform edits that will really make your case undeniable to future readers/reviewers.<br /><br />Great work on your aims, Alex! I apologize that I didn’t have time to edit to make my list of questions shorter!";s:6:"parent";s:32:"7b12768ff77ef6d730490744d29f4357";s:7:"replies";a:1:{i:0;s:32:"008fbbd318e1fb74ffdcd60ba6a04389";}s:4:"show";b:1;s:3:"cid";s:32:"a10e6c6f679370977234d2ec80bf54ed";}s:32:"008fbbd318e1fb74ffdcd60ba6a04389";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"adopp";s:4:"name";s:9:"Alex Dopp";s:4:"mail";s:13:"dopp@uark.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536005957;}s:3:"raw";s:38:"Thank you for the insightful feedback!";s:5:"xhtml";s:38:"Thank you for the insightful feedback!";s:6:"parent";s:32:"a10e6c6f679370977234d2ec80bf54ed";s:7:"replies";a:1:{i:0;s:32:"1946867157e926b58a7d744ebb3b1916";}s:4:"show";b:1;s:3:"cid";s:32:"008fbbd318e1fb74ffdcd60ba6a04389";}s:32:"1ec2a86c5dcf9c94c585ac96380b2ed5";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bmaddox";s:4:"name";s:13:"Brenna Maddox";s:4:"mail";s:17:"maddoxb@upenn.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536079426;}s:3:"raw";s:115:"This is very helpful feedback! Thank you so much, and I look forward to learning more from you all over the course.";s:5:"xhtml";s:115:"This is very helpful feedback! Thank you so much, and I look forward to learning more from you all over the course.";s:6:"parent";s:32:"b2ff94bb72e4b38045efa4976ff15aa1";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"1ec2a86c5dcf9c94c585ac96380b2ed5";}s:32:"30caefe79f73cf1eb84f1881aaf2d17a";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"kokamura";s:4:"name";s:14:"Kelsie Okamura";s:4:"mail";s:26:"kelsie.h.okamura@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536116442;}s:3:"raw";s:166:"Hi Lindsey,

Thank you for these thoughtful questions and insightful feedback. I am looking forward to refining my aims and research project over the next few months!";s:5:"xhtml";s:176:"Hi Lindsey,<br /><br />Thank you for these thoughtful questions and insightful feedback. I am looking forward to refining my aims and research project over the next few months!";s:6:"parent";s:32:"9f7899eab6ac54f4b1461e806796ecdf";s:7:"replies";a:1:{i:0;s:32:"2c3b3091e813e3ca6f891fa840600167";}s:4:"show";b:1;s:3:"cid";s:32:"30caefe79f73cf1eb84f1881aaf2d17a";}s:32:"a426d99f6b0b8ece080e0c3c6fe8e1de";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"aprogovac";s:4:"name";s:12:"Ana Progovac";s:4:"mail";s:24:"aprogovac@challiance.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536267552;}s:3:"raw";s:1621:"PROGOVAC - Assignment #2


[1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?]

We currently do not have plans to measure fidelity for the evidence-based intervention. However, as the intervention becomes more routinized and has a more specific implementation blueprint attached, I think fidelity could be a component that is monitored. 


[2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?]

We are observing a naturalistic implementation of a Behavioral Health Home. What we have noticed is that the staff are making adaptations with respect to how they approach patient engagement for the wellness components. First of all, not all patients are truly eligible for all groups (i.e. only smokers are eligible for smoking cessation). But even within those people who are eligible, there is sometimes dissonance between what patients are interested in working on and what providers / the program would want to prioritize. Because shared decision-making is another key element of the program, and patient engagement and retention in the program *overall* is seen as more important than in any specific element, we find this adaptation interesting and perhaps important for other similar models to consider. ";s:5:"xhtml";s:1670:"PROGOVAC - Assignment #2<br /><br /><br />[1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?]<br /><br />We currently do not have plans to measure fidelity for the evidence-based intervention. However, as the intervention becomes more routinized and has a more specific implementation blueprint attached, I think fidelity could be a component that is monitored. <br /><br /><br />[2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?]<br /><br />We are observing a naturalistic implementation of a Behavioral Health Home. What we have noticed is that the staff are making adaptations with respect to how they approach patient engagement for the wellness components. First of all, not all patients are truly eligible for all groups (i.e. only smokers are eligible for smoking cessation). But even within those people who are eligible, there is sometimes dissonance between what patients are interested in working on and what providers / the program would want to prioritize. Because shared decision-making is another key element of the program, and patient engagement and retention in the program *overall* is seen as more important than in any specific element, we find this adaptation interesting and perhaps important for other similar models to consider.";s:6:"parent";N;s:7:"replies";a:2:{i:0;s:32:"46a808d22603d5d54d12c724aeef964b";i:1;s:32:"ff72a363a715ef22d0d19dab39a6c87b";}s:4:"show";b:1;s:3:"cid";s:32:"a426d99f6b0b8ece080e0c3c6fe8e1de";}s:32:"b0bde77ed4e592baa8fc1912e9ed272c";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"adopp";s:4:"name";s:9:"Alex Dopp";s:4:"mail";s:13:"dopp@uark.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1536284387;s:8:"modified";i:1536284450;}s:3:"raw";s:1798:"Dopp - Assignment #2

Assignment #2 - Dopp

I originally was unsure how much these questions applied to my proposed study, which involves a method for tailored selection of implementation strategies and will not measure anything related to the interventions that are actually implemented. However, reflecting on these topics brought up some new thoughts regarding the role of fidelity in my proposed Fiscal Mapping Process and I have made a note to add these into my application!

1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

No, because an EBI will not be implemented during my study. However, I am proposing to specify the five steps of the Fiscal Mapping Process, and this question made me recognize that having a well-specified strategy for tailoring could inform future efforts to monitor fidelity to that system (i.e., did agencies use the Fiscal Mapping Process as intended?).

2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

No, again because an EBI will not be implemented during my study. However, this question and the readings made me think about how monitoring fidelity/adaptations is an example of an ongoing implementation activity that can be difficult to sustain financially. I will be adding this to a figure that highlights the various activities, beyond direct service delivery, that are important to EBI impact yet are not a focus of sustained funding.";s:5:"xhtml";s:1858:"Dopp - Assignment #2<br /><br />Assignment #2 - Dopp<br /><br />I originally was unsure how much these questions applied to my proposed study, which involves a method for tailored selection of implementation strategies and will not measure anything related to the interventions that are actually implemented. However, reflecting on these topics brought up some new thoughts regarding the role of fidelity in my proposed Fiscal Mapping Process and I have made a note to add these into my application!<br /><br />1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />No, because an EBI will not be implemented during my study. However, I am proposing to specify the five steps of the Fiscal Mapping Process, and this question made me recognize that having a well-specified strategy for tailoring could inform future efforts to monitor fidelity to that system (i.e., did agencies use the Fiscal Mapping Process as intended?).<br /><br />2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />No, again because an EBI will not be implemented during my study. However, this question and the readings made me think about how monitoring fidelity/adaptations is an example of an ongoing implementation activity that can be difficult to sustain financially. I will be adding this to a figure that highlights the various activities, beyond direct service delivery, that are important to EBI impact yet are not a focus of sustained funding.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"6a7683523878fa07b0068609f1e3f087";}s:4:"show";b:1;s:3:"cid";s:32:"b0bde77ed4e592baa8fc1912e9ed272c";}s:32:"46f5840eb80062541a768a3cdae3e54d";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bmaddox";s:4:"name";s:13:"Brenna Maddox";s:4:"mail";s:17:"maddoxb@upenn.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536333147;}s:3:"raw";s:285:"Hello all, and happy Friday! I have a question for the facilitators: would you like us to email you a powerpoint of our "elevator pitch" for Monday's talk? I saw Dara's comment that it's up to the faculty in individual groups. Thanks, and I look forward to speaking with you on Monday!";s:5:"xhtml";s:310:"Hello all, and happy Friday! I have a question for the facilitators: would you like us to email you a powerpoint of our &quot;elevator pitch&quot; for Monday&#039;s talk? I saw Dara&#039;s comment that it&#039;s up to the faculty in individual groups. Thanks, and I look forward to speaking with you on Monday!";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"000663e38918965e098c24050e47f963";}s:4:"show";b:1;s:3:"cid";s:32:"46f5840eb80062541a768a3cdae3e54d";}s:32:"2c3b3091e813e3ca6f891fa840600167";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536345569;}s:3:"raw";s:81:"Agreed, I'm excited to learn more about your work, Kelsie! Have a great weekend.
";s:5:"xhtml";s:85:"Agreed, I&#039;m excited to learn more about your work, Kelsie! Have a great weekend.";s:6:"parent";s:32:"30caefe79f73cf1eb84f1881aaf2d17a";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"2c3b3091e813e3ca6f891fa840600167";}s:32:"1946867157e926b58a7d744ebb3b1916";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536345669;}s:3:"raw";s:93:"You're welcome, Alex. I look forward to learning more about your work!

Have a great weekend.";s:5:"xhtml";s:108:"You&#039;re welcome, Alex. I look forward to learning more about your work!<br /><br />Have a great weekend.";s:6:"parent";s:32:"008fbbd318e1fb74ffdcd60ba6a04389";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"1946867157e926b58a7d744ebb3b1916";}s:32:"f60ff022507300b7584f143e03917ee7";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bmaddox";s:4:"name";s:13:"Brenna Maddox";s:4:"mail";s:17:"maddoxb@upenn.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536348119;}s:3:"raw";s:1631:"MADDOX- Assignment #2

1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

Yes, I plan to measure and monitor the clinicians’ fidelity with which they deliver the CBT program. My current plan is to use behavioral rehearsal to assess fidelity, meaning that the clinicians will demonstrate the CBT strategies used in session via role-plays with a trained rater. To my knowledge, there is no research on the association between CBT fidelity and individual level outcomes for adults with autism and co-occurring anxiety/depression.

2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

I am currently in Year 2 of 3 of my F32 project, which is focused on adapting CBT for adults with autism and co-occurring anxiety/depression. This final product of my F32 will be tested in the effectiveness aim of my proposed study. Thus, the focus will not be on further adapting the modified CBT program. However, it will be important to consider how the intervention is adapted by clinicians in the community mental health setting. I would like to include some method (perhaps interviews or focus groups with a subset of participating clinicians) to gather feedback about how and why they chose to make adaptations during session. ";s:5:"xhtml";s:1670:"MADDOX- Assignment #2<br /><br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />Yes, I plan to measure and monitor the clinicians’ fidelity with which they deliver the CBT program. My current plan is to use behavioral rehearsal to assess fidelity, meaning that the clinicians will demonstrate the CBT strategies used in session via role-plays with a trained rater. To my knowledge, there is no research on the association between CBT fidelity and individual level outcomes for adults with autism and co-occurring anxiety/depression.<br /><br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />I am currently in Year 2 of 3 of my F32 project, which is focused on adapting CBT for adults with autism and co-occurring anxiety/depression. This final product of my F32 will be tested in the effectiveness aim of my proposed study. Thus, the focus will not be on further adapting the modified CBT program. However, it will be important to consider how the intervention is adapted by clinicians in the community mental health setting. I would like to include some method (perhaps interviews or focus groups with a subset of participating clinicians) to gather feedback about how and why they chose to make adaptations during session.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"a6052fea8200ef492d71dda1fe9590f6";}s:4:"show";b:1;s:3:"cid";s:32:"f60ff022507300b7584f143e03917ee7";}s:32:"62f972157b79531a2d2b3ef4ac6aa267";a:8:{s:4:"user";a:5:{s:2:"id";s:11:"rshepardson";s:4:"name";s:16:"Robyn Shepardson";s:4:"mail";s:23:"Robyn.Shepardson@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536348261;}s:3:"raw";s:9430:"SHEPARDSON - Assignment #2

1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

Yes, we plan to measure and monitor treatment fidelity for the anxiety intervention. As part of the preliminary work I am doing currently to develop the treatment manual, I will also be developing a treatment fidelity checklist. We intend to assess not only adherence to delivery of specified intervention content, but also competence in delivery (more focused on the skill and spirit of delivery, or the “how” instead of the “what”). I have not yet begun working on the treatment fidelity checklist, but I was planning to follow the guidance provided in the NIH Behavior Change Consortium best practices and recommendations for enhancing treatment fidelity in health behavior change studies (1). These recommendations cover five areas: study design, training providers, delivery of treatment, receipt of treatment, and enactment of treatment skills. This will allow me to consider a wide range of possible variables, including treatment dose, provider training procedures, common factors and other nonspecific treatment effects, adherence to the intervention protocol, contamination across conditions, and participant comprehension, skill building, and skill usage. I will have to determine whether all of these factors are relevant to my intervention, but the thorough guidance will help me think through all of the important considerations. 

Notably, I intend to consider aspects of the anxiety intervention as well as the unique treatment setting/model. Thus, I envision some fidelity items that would be related to specific anxiety intervention techniques and some that would be related to adhering to the structure and spirit of the Primary Care-Mental Health Integration (PC-MHI) treatment approach, which entails brief, time-limited treatment, ideally no more than 6, 30-minute sessions. 

My anxiety intervention is designed to be tailored to individual patients. For example, determination of the number, spacing, and selection of modules will be made by the therapist in collaboration with the patient (i.e., based on clinical judgment while also considering patient needs and preferences). Therefore, we have to build in flexibility to how we conceptualize and measure fidelity, as our measurement strategy has to account for the modular, flexible nature of the intervention. 

The anxiety intervention is primarily drawn from cognitive-behavioral therapy (CBT) techniques. Thus, I believe there is good evidence that treatment fidelity is associated with patient-level outcomes. Because the intervention is modular and allows for selection of specific modules based on patient presentation and treatment preferences, it includes a variety of intervention techniques. We will have to balance our expectations for depth versus breadth. My plan is to try to focus on essential elements, which will be clearly delineated in the treatment manual, and limit requirements related to helpful but non-essential elements to ensure that fidelity can realistically be reached. 

As far as the method of measuring fidelity, it would be ideal to audio record intervention sessions and have trained raters conduct fidelity ratings with a detailed checklist. Research shows that clinicians may overestimate the extent to which they are delivering intervention content compared to outside raters (2). (Live observation or video recordings definitely do not seem feasible, but I know of many VA treatment studies in which audio recording has been conducted.) We will have to determine if this type of procedure is feasible given the scope and budget of the grant. It is certainly my intended approach at this stage of research. Selecting a subsample of sessions to evaluate may make the approach more feasible. For long-term sustainability, a self-rating process (e.g., post-session checklist) would likely be more feasible for real-world practice, as most supervisors/administrators will not have time to listen to therapist session tapes. Self-report is less ideal, but it is important to me to ensure that implementation can occur in real-world practice settings, so I appreciate that many of the preferred procedures from the research world may not be realistic.

2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

Adaptations are always a strong possibility when the goal involves wider implementation. Toward the end of this year, I will be starting a hybrid I trial for an initial evaluation of my anxiety intervention. During that study I will be conducting a mixed methods process evaluation, which will help to begin examining the extent to which providers are able to deliver the intervention with fidelity. We will also be keeping field notes of observations throughout the trial, which will likely include ideas for adaptations as well as the rationale behind making any suggested changes. This preliminary work should give me good experience being mindful of possible adaptations that clinicians may seek, thus preparing me to proactively deal with proposed adaptations in the larger hybrid II trial that I am developing throughout TIDIRH. I certainly intend to keep detailed notes from stakeholder feedback regarding suggested modifications to the intervention. This will allow me to recognize likely site-specific adaptations as well as potential adaptations that may yield improvements and “positive deviance.”

I expect content modifications to occur at the individual recipient and provider level. Tailoring/tweaking/refining should always be expected because one size never fits all. In addition, I would expect providers to add elements, remove elements, shorten/condense elements, lengthen/extend elements, integrate other approaches into the intervention, loosen the structure, and of course, over time, drift. I expect such changes would be made sometimes ahead of time, and sometimes after realizing the original approach will not work for whatever reason, based on particular patient characteristics/reactions/expectations as well as individual provider preferences or perceived skill deficits. For example, I imagine certain Veterans may not be willing or able to engage in particular modules for a variety of reasons (e.g., impaired cognitive functioning, competing medical concerns, extreme cognitive rigidity). My hope is that the flexibility we have built into the intervention (modular approach in which therapist and patient work together to select modules) will accommodate this concern though. In my clinical experience, some providers are very “set in their ways” and may feel unable or unwilling to deliver particular interventions (e.g., “exposure is too scary for patients and too complicated to do in just a few brief sessions”), so they may feel the need to make changes to the intervention such as not offering certain modules. We hope to address this issue by providing thorough training, including obtaining clinician buy-in for the rationale for each module as well as step-by-step instructions and thorough training to increase self-efficacy for delivering all of the intervention content.

Over the longer term, I anticipate several possible contextual modifications. One would be related to format of treatment delivery. The intervention is designed as an individual treatment, but I could see an argument for attempting it in a group format to increase access to and efficiency of treatment. Another would be related to the population. The intervention is designed for primary care patients with subthreshold, mild, or moderate anxiety symptoms, consistent with the intended scope and population for PC-MHI. However, in the real world, many patients with severe, long-standing anxiety disorders are seen in the PC-MHI setting. Often these patients are unable or unwilling to seek specialty mental health care. It is possible that these patients may be treated with the anxiety intervention in PC-MHI, even though it is unlikely that the dosage of this intervention is sufficient to meet their treatment needs. Although it is unlikely to harm them, if the intervention did not yield appreciable improvement (due to mismatch between the treatment and their true needs), this may discourage engagement in future treatment. At the same time, it may be better to offer the intervention than to offer usual care or let the patient go without any treatment at all. 

I am quite new to the concept of intervention adaptations, so currently the only process I am aware of for monitoring and guiding adaptations is the one discussed in the Rabin article from this week’s material. I thought their process of having a straightforward tracking tool to document adaptations as they are made seemed fairly feasible. Having a standing weekly reminder to consider this would help ensure the study staff stays on top of it over the course of the trial. The stakeholder interviews to provide more in-depth information helps round out understanding of why certain adaptations were made. ";s:5:"xhtml";s:9539:"SHEPARDSON - Assignment #2<br /><br />1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />Yes, we plan to measure and monitor treatment fidelity for the anxiety intervention. As part of the preliminary work I am doing currently to develop the treatment manual, I will also be developing a treatment fidelity checklist. We intend to assess not only adherence to delivery of specified intervention content, but also competence in delivery (more focused on the skill and spirit of delivery, or the “how” instead of the “what”). I have not yet begun working on the treatment fidelity checklist, but I was planning to follow the guidance provided in the NIH Behavior Change Consortium best practices and recommendations for enhancing treatment fidelity in health behavior change studies (1). These recommendations cover five areas: study design, training providers, delivery of treatment, receipt of treatment, and enactment of treatment skills. This will allow me to consider a wide range of possible variables, including treatment dose, provider training procedures, common factors and other nonspecific treatment effects, adherence to the intervention protocol, contamination across conditions, and participant comprehension, skill building, and skill usage. I will have to determine whether all of these factors are relevant to my intervention, but the thorough guidance will help me think through all of the important considerations. <br /><br />Notably, I intend to consider aspects of the anxiety intervention as well as the unique treatment setting/model. Thus, I envision some fidelity items that would be related to specific anxiety intervention techniques and some that would be related to adhering to the structure and spirit of the Primary Care-Mental Health Integration (PC-MHI) treatment approach, which entails brief, time-limited treatment, ideally no more than 6, 30-minute sessions. <br /><br />My anxiety intervention is designed to be tailored to individual patients. For example, determination of the number, spacing, and selection of modules will be made by the therapist in collaboration with the patient (i.e., based on clinical judgment while also considering patient needs and preferences). Therefore, we have to build in flexibility to how we conceptualize and measure fidelity, as our measurement strategy has to account for the modular, flexible nature of the intervention. <br /><br />The anxiety intervention is primarily drawn from cognitive-behavioral therapy (CBT) techniques. Thus, I believe there is good evidence that treatment fidelity is associated with patient-level outcomes. Because the intervention is modular and allows for selection of specific modules based on patient presentation and treatment preferences, it includes a variety of intervention techniques. We will have to balance our expectations for depth versus breadth. My plan is to try to focus on essential elements, which will be clearly delineated in the treatment manual, and limit requirements related to helpful but non-essential elements to ensure that fidelity can realistically be reached. <br /><br />As far as the method of measuring fidelity, it would be ideal to audio record intervention sessions and have trained raters conduct fidelity ratings with a detailed checklist. Research shows that clinicians may overestimate the extent to which they are delivering intervention content compared to outside raters (2). (Live observation or video recordings definitely do not seem feasible, but I know of many VA treatment studies in which audio recording has been conducted.) We will have to determine if this type of procedure is feasible given the scope and budget of the grant. It is certainly my intended approach at this stage of research. Selecting a subsample of sessions to evaluate may make the approach more feasible. For long-term sustainability, a self-rating process (e.g., post-session checklist) would likely be more feasible for real-world practice, as most supervisors/administrators will not have time to listen to therapist session tapes. Self-report is less ideal, but it is important to me to ensure that implementation can occur in real-world practice settings, so I appreciate that many of the preferred procedures from the research world may not be realistic.<br /><br />2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />Adaptations are always a strong possibility when the goal involves wider implementation. Toward the end of this year, I will be starting a hybrid I trial for an initial evaluation of my anxiety intervention. During that study I will be conducting a mixed methods process evaluation, which will help to begin examining the extent to which providers are able to deliver the intervention with fidelity. We will also be keeping field notes of observations throughout the trial, which will likely include ideas for adaptations as well as the rationale behind making any suggested changes. This preliminary work should give me good experience being mindful of possible adaptations that clinicians may seek, thus preparing me to proactively deal with proposed adaptations in the larger hybrid II trial that I am developing throughout TIDIRH. I certainly intend to keep detailed notes from stakeholder feedback regarding suggested modifications to the intervention. This will allow me to recognize likely site-specific adaptations as well as potential adaptations that may yield improvements and “positive deviance.”<br /><br />I expect content modifications to occur at the individual recipient and provider level. Tailoring/tweaking/refining should always be expected because one size never fits all. In addition, I would expect providers to add elements, remove elements, shorten/condense elements, lengthen/extend elements, integrate other approaches into the intervention, loosen the structure, and of course, over time, drift. I expect such changes would be made sometimes ahead of time, and sometimes after realizing the original approach will not work for whatever reason, based on particular patient characteristics/reactions/expectations as well as individual provider preferences or perceived skill deficits. For example, I imagine certain Veterans may not be willing or able to engage in particular modules for a variety of reasons (e.g., impaired cognitive functioning, competing medical concerns, extreme cognitive rigidity). My hope is that the flexibility we have built into the intervention (modular approach in which therapist and patient work together to select modules) will accommodate this concern though. In my clinical experience, some providers are very “set in their ways” and may feel unable or unwilling to deliver particular interventions (e.g., “exposure is too scary for patients and too complicated to do in just a few brief sessions”), so they may feel the need to make changes to the intervention such as not offering certain modules. We hope to address this issue by providing thorough training, including obtaining clinician buy-in for the rationale for each module as well as step-by-step instructions and thorough training to increase self-efficacy for delivering all of the intervention content.<br /><br />Over the longer term, I anticipate several possible contextual modifications. One would be related to format of treatment delivery. The intervention is designed as an individual treatment, but I could see an argument for attempting it in a group format to increase access to and efficiency of treatment. Another would be related to the population. The intervention is designed for primary care patients with subthreshold, mild, or moderate anxiety symptoms, consistent with the intended scope and population for PC-MHI. However, in the real world, many patients with severe, long-standing anxiety disorders are seen in the PC-MHI setting. Often these patients are unable or unwilling to seek specialty mental health care. It is possible that these patients may be treated with the anxiety intervention in PC-MHI, even though it is unlikely that the dosage of this intervention is sufficient to meet their treatment needs. Although it is unlikely to harm them, if the intervention did not yield appreciable improvement (due to mismatch between the treatment and their true needs), this may discourage engagement in future treatment. At the same time, it may be better to offer the intervention than to offer usual care or let the patient go without any treatment at all. <br /><br />I am quite new to the concept of intervention adaptations, so currently the only process I am aware of for monitoring and guiding adaptations is the one discussed in the Rabin article from this week’s material. I thought their process of having a straightforward tracking tool to document adaptations as they are made seemed fairly feasible. Having a standing weekly reminder to consider this would help ensure the study staff stays on top of it over the course of the trial. The stakeholder interviews to provide more in-depth information helps round out understanding of why certain adaptations were made.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"cb0dfdeef063e4eeba3c8dc84eefa0ac";}s:4:"show";b:1;s:3:"cid";s:32:"62f972157b79531a2d2b3ef4ac6aa267";}s:32:"f1b8cb00ca6ce0569bc47138ff5db3b7";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"ipinsky";s:4:"name";s:12:"Ilana Pinsky";s:4:"mail";s:21:"pinskyilana@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536357442;}s:3:"raw";s:3349:"Pinsky - Assignment #2


1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

Yes, we will. In fact, the focus of our project is to set up a strategy to monitor the fidelity of our intervention (IPC) delivery. The details of the methodology that will be utilized is in process of development and this module of the TIDIRH is pivotal for this. The idea, for now, is to work using a 
mixed-methods  and a multi step approach. We are presently in the process of training 25 mental health workers in Mozambique that will be the trainers of all the rest of MH network in the country. These 25 future trainers were already exposed to two different series of IPC training and are now concluding the requirements to become certified IPC therapists (i.e: completing 3-5 cases and 75% or more supervision attendance). They are being supervised by Skype weekly by two world-recognized IPC experts. I am working with these experts to set up IPC competence criteria and the 25 Moz future trainers will be evaluated in accordance to this criteria. 
Our IPC fidelity project is inserted in our broad PRIDE sSA work, that aims to develop a MH system in Mozambique to cover most MH diagnosis in most of the country using a task-sharing model. In this way, the backbone of what we do is always taking into account how sustainable our actions are.
Our plan is that once we understand the variations within this first group, we will develop similar methods to enhance our goals of sustainability by incorporating all providers (specialists or not) involved in the study (primary care providers, community health workers, Psychiatric Technicians). We would also like to verify fidelity vs adaption issues by interviewing community leaders, clients, clients’ families. Eventually, we intend to explore if and how IPC was adapted across different settings and geographic contexts and if this impacted on implementation outcomes. 


2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

IPC is already an adaptation of IPT (Interpersonal Therapy), specially developed for primary care settings. It has been tested and adapted in a number of contexts and countries. Even so, it is likely that some adaptation will have to take place. We cannot foresee all kinds of adaptions that will be necessary because our study will cover a diverse range of locations, including several areas that receive absolutely no MH services at this point. In any way, because IPC has already been applied to a number of contexts worldwide, we think that major content modifications are unlikely to be necessary. Because the study will have 3 arms and a number of different providers will be involved, we are expecting to have mainly contextual modifications and possibly training and evaluation adaptations to fit the Mozambique Health system. As per the process to be used to guide this adaptation, both Stirman et al. and Rabins et al' paper give some guidance to the process. 


";s:5:"xhtml";s:3415:"Pinsky - Assignment #2<br /><br /><br />1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />Yes, we will. In fact, the focus of our project is to set up a strategy to monitor the fidelity of our intervention (IPC) delivery. The details of the methodology that will be utilized is in process of development and this module of the TIDIRH is pivotal for this. The idea, for now, is to work using a <br />mixed-methods  and a multi step approach. We are presently in the process of training 25 mental health workers in Mozambique that will be the trainers of all the rest of MH network in the country. These 25 future trainers were already exposed to two different series of IPC training and are now concluding the requirements to become certified IPC therapists (i.e: completing 3-5 cases and 75% or more supervision attendance). They are being supervised by Skype weekly by two world-recognized IPC experts. I am working with these experts to set up IPC competence criteria and the 25 Moz future trainers will be evaluated in accordance to this criteria. <br />Our IPC fidelity project is inserted in our broad PRIDE sSA work, that aims to develop a MH system in Mozambique to cover most MH diagnosis in most of the country using a task-sharing model. In this way, the backbone of what we do is always taking into account how sustainable our actions are.<br />Our plan is that once we understand the variations within this first group, we will develop similar methods to enhance our goals of sustainability by incorporating all providers (specialists or not) involved in the study (primary care providers, community health workers, Psychiatric Technicians). We would also like to verify fidelity vs adaption issues by interviewing community leaders, clients, clients’ families. Eventually, we intend to explore if and how IPC was adapted across different settings and geographic contexts and if this impacted on implementation outcomes. <br /><br /><br />2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />IPC is already an adaptation of IPT (Interpersonal Therapy), specially developed for primary care settings. It has been tested and adapted in a number of contexts and countries. Even so, it is likely that some adaptation will have to take place. We cannot foresee all kinds of adaptions that will be necessary because our study will cover a diverse range of locations, including several areas that receive absolutely no MH services at this point. In any way, because IPC has already been applied to a number of contexts worldwide, we think that major content modifications are unlikely to be necessary. Because the study will have 3 arms and a number of different providers will be involved, we are expecting to have mainly contextual modifications and possibly training and evaluation adaptations to fit the Mozambique Health system. As per the process to be used to guide this adaptation, both Stirman et al. and Rabins et al&#039; paper give some guidance to the process.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"9302324649a15248e5d6a11cfca5780c";}s:4:"show";b:1;s:3:"cid";s:32:"f1b8cb00ca6ce0569bc47138ff5db3b7";}s:32:"af40916a09f1b7c91c2d1934620daae8";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"kokamura";s:4:"name";s:14:"Kelsie Okamura";s:4:"mail";s:26:"kelsie.h.okamura@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536379195;}s:3:"raw";s:2944:"OKAMURA – Assignment #2

1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

We will be closely monitoring the MBC fidelity within our electronic case management system based on our operational definitions of MBC. We operationalize MBC behavior at the individual direct service provider/therapist level, given our reliance on the theory of reasoned action (planned behavior) for behavior change. We are currently brainstorming (see table below) various behaviors that we would want to monitor that would be related to MBC. Once we have a full list brainstormed, we will solicit feedback from our Clinical Services Office and Clinical Psychologists at the University of Hawaii at Manoa (our community-academic partnership). We will also ensure that these behaviors can be tracked in the new electronic case management system.

MBC Behavior/Use: Operational Definition
Dashboard: Use client level dashboard to monitor treatment progress; Make changes to clinical management plan based on dashboard changes; Frequency of dashboard use (in supervision, UR)
Standardized measure: Use standardized measures to monitor treatment progress; Make changes to clinical management plan based on standardized measures; Frequency of standardized measure use; Share feedback with consumers regarding standardized measures (measurement and feedback loop)
Evaluation to clinical formulation: Create clinical management plan based on initial mental health evaluation treatment focus areas and goals
Clinical formulation in mental health treatment plan: Match between mental health treatment plan (from contracted providers) and clinical management plan
Mental health treatment plan to direct progress note: Match between mental health treatment plan treatment focused areas, targets, and practice elements to direct progress note activities

The research on MBC (alone) directly improving client outcomes has not been empirically tested. Therefore, careful opertational definitions and observations are needed a priori for future research.



2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

It is likely that adaptations will be made to the MBC model within our system. The delivery and timing will likely be affected the most, given that many direct service providers and therapists often note that time is a significant barrier in utilizing MBC. When necessary, we will provide additional support to staff through training and ongoing consultation to streamline workflows and to monitor client outcomes to see if adjustments are necessary. 
";s:5:"xhtml";s:3037:"OKAMURA – Assignment #2<br /><br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />We will be closely monitoring the MBC fidelity within our electronic case management system based on our operational definitions of MBC. We operationalize MBC behavior at the individual direct service provider/therapist level, given our reliance on the theory of reasoned action (planned behavior) for behavior change. We are currently brainstorming (see table below) various behaviors that we would want to monitor that would be related to MBC. Once we have a full list brainstormed, we will solicit feedback from our Clinical Services Office and Clinical Psychologists at the University of Hawaii at Manoa (our community-academic partnership). We will also ensure that these behaviors can be tracked in the new electronic case management system.<br /><br />MBC Behavior/Use: Operational Definition<br />Dashboard: Use client level dashboard to monitor treatment progress; Make changes to clinical management plan based on dashboard changes; Frequency of dashboard use (in supervision, UR)<br />Standardized measure: Use standardized measures to monitor treatment progress; Make changes to clinical management plan based on standardized measures; Frequency of standardized measure use; Share feedback with consumers regarding standardized measures (measurement and feedback loop)<br />Evaluation to clinical formulation: Create clinical management plan based on initial mental health evaluation treatment focus areas and goals<br />Clinical formulation in mental health treatment plan: Match between mental health treatment plan (from contracted providers) and clinical management plan<br />Mental health treatment plan to direct progress note: Match between mental health treatment plan treatment focused areas, targets, and practice elements to direct progress note activities<br /><br />The research on MBC (alone) directly improving client outcomes has not been empirically tested. Therefore, careful opertational definitions and observations are needed a priori for future research.<br /><br /><br /><br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />It is likely that adaptations will be made to the MBC model within our system. The delivery and timing will likely be affected the most, given that many direct service providers and therapists often note that time is a significant barrier in utilizing MBC. When necessary, we will provide additional support to staff through training and ongoing consultation to streamline workflows and to monitor client outcomes to see if adjustments are necessary.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"b2bb0e7a6b0f51a057a949506d363cee";}s:4:"show";b:1;s:3:"cid";s:32:"af40916a09f1b7c91c2d1934620daae8";}s:32:"0ba18d1de91337139384621e648d6f20";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"aweaver";s:4:"name";s:12:"Addie Weaver";s:4:"mail";s:18:"weaverad@umich.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536591314;}s:3:"raw";s:3085:"WEAVER - Assignment #2

1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?
I don’t see this as a traditional mental health intervention project per se, as the goal is to initiate screening for postpartum depression (PPD) within rural WIC clinics in Michigan. An evidence-based screening tool, the Edinburgh Postnatal Depression Scale, will be the patient-level intervention. We will be assessing whether or not providers offer/introduce the screening tool to clients, whether they accurately score the tool, and whether they offer appropriate referrals/resources when clients screen positive for depression. I had been conceptualizing these as outcomes of the implementation intervention rather than indicators of fidelity, but now am wondering if these indicators cut across both. 
We plan to implement an educational intervention, guided by the Theoretical Domains Framework (TDF), with WIC staff in order to change behavior and introduce PPD screening as routine practice. The educational intervention will be tailored to WIC clinics based on a mixed methods assessment of barriers and facilitators, also guided by TDF, that will be completed in Aim 1 of the proposed project. We will assess the fidelity with which the educational intervention is delivered by tracking whether all planned components are adequately covered by facilitators/trainers. For the proposed pilot project, we will either have a research assistant attend the trainings or audio record trainings in order to have them rated for fidelity. 
2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?
There will be no adaptations made to the patient-level evidence-based screening for postpartum depression. As noted above, the Edinburgh Postnatal Depression Scale will be used to screen WIC clients at standard WIC appointments during the postpartum period (when infant is between 0-12 months old). 

The provider-level educational intervention will utilize evidence-based strategies from the existing literature, but will be adapted for the needs and context of rural WIC clinics. As stated previously, Aim 1 of this project will identify barriers and facilitators to implementing postpartum depression screening in rural Michigan WIC clinics by conducting semi-structured interviews, guided by the Theoretical Domains Framework (TDF), with WIC program staff (e.g., nurses, nutritionists), and administering a questionnaire, based upon the TDF, to WIC program staff. This information will be used to adapt the education intervention to align with the TDF domains identified in Aim 1 as most relevant to behavior change necessary to successfully initiate PPD screening among rural WIC staff members. 

";s:5:"xhtml";s:3122:"WEAVER - Assignment #2<br /><br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br />I don’t see this as a traditional mental health intervention project per se, as the goal is to initiate screening for postpartum depression (PPD) within rural WIC clinics in Michigan. An evidence-based screening tool, the Edinburgh Postnatal Depression Scale, will be the patient-level intervention. We will be assessing whether or not providers offer/introduce the screening tool to clients, whether they accurately score the tool, and whether they offer appropriate referrals/resources when clients screen positive for depression. I had been conceptualizing these as outcomes of the implementation intervention rather than indicators of fidelity, but now am wondering if these indicators cut across both. <br />We plan to implement an educational intervention, guided by the Theoretical Domains Framework (TDF), with WIC staff in order to change behavior and introduce PPD screening as routine practice. The educational intervention will be tailored to WIC clinics based on a mixed methods assessment of barriers and facilitators, also guided by TDF, that will be completed in Aim 1 of the proposed project. We will assess the fidelity with which the educational intervention is delivered by tracking whether all planned components are adequately covered by facilitators/trainers. For the proposed pilot project, we will either have a research assistant attend the trainings or audio record trainings in order to have them rated for fidelity. <br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br />There will be no adaptations made to the patient-level evidence-based screening for postpartum depression. As noted above, the Edinburgh Postnatal Depression Scale will be used to screen WIC clients at standard WIC appointments during the postpartum period (when infant is between 0-12 months old). <br /><br />The provider-level educational intervention will utilize evidence-based strategies from the existing literature, but will be adapted for the needs and context of rural WIC clinics. As stated previously, Aim 1 of this project will identify barriers and facilitators to implementing postpartum depression screening in rural Michigan WIC clinics by conducting semi-structured interviews, guided by the Theoretical Domains Framework (TDF), with WIC program staff (e.g., nurses, nutritionists), and administering a questionnaire, based upon the TDF, to WIC program staff. This information will be used to adapt the education intervention to align with the TDF domains identified in Aim 1 as most relevant to behavior change necessary to successfully initiate PPD screening among rural WIC staff members.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"0ba18d1de91337139384621e648d6f20";}s:32:"6a7683523878fa07b0068609f1e3f087";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dpintello";s:4:"name";s:14:"Denny Pintello";s:4:"mail";s:23:"Denise.Pintello@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536951329;}s:3:"raw";s:491:"Alex:

I am glad to see that you have augmented your proposed project by adding a new item/question/domain regarding fidelity (did agencies use the FMP as intended). I would hope that you consider doing the same regarding potential adaptation of the FMP, given the possibility that agencies using FMP might request to adapt it in some way. Perhaps within the mixed-methods design of your project, you can ask stakeholders to consider the impact of fidelity and adaptions on the FMP. 

Denny
";s:5:"xhtml";s:510:"Alex:<br /><br />I am glad to see that you have augmented your proposed project by adding a new item/question/domain regarding fidelity (did agencies use the FMP as intended). I would hope that you consider doing the same regarding potential adaptation of the FMP, given the possibility that agencies using FMP might request to adapt it in some way. Perhaps within the mixed-methods design of your project, you can ask stakeholders to consider the impact of fidelity and adaptions on the FMP. <br /><br />Denny";s:6:"parent";s:32:"b0bde77ed4e592baa8fc1912e9ed272c";s:7:"replies";a:1:{i:0;s:32:"e452c663cb5d5eed1fee454401584e76";}s:4:"show";b:1;s:3:"cid";s:32:"6a7683523878fa07b0068609f1e3f087";}s:32:"cb0dfdeef063e4eeba3c8dc84eefa0ac";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dpintello";s:4:"name";s:14:"Denny Pintello";s:4:"mail";s:23:"Denise.Pintello@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536951550;}s:3:"raw";s:1434:"Robyn:

Your comprehensive and thoughtful response to address fidelity nicely covers the all the high points, especially fidelity measurement. As you described: clinician self-report, supervisory assessment, and audio/video recordings that are rated by independent reviewers are traditional approaches – and there may opportunities to monitor fidelity using dashboards or involving creative combinations (i.e., comparing the accuracy of clinician self-rpt to independent reviewers) that may be intriguing for you to consider. 

Regarding adaptations – I highly appreciate that you value the fluid nature of adaptation and are committed to building in flexibility. This will be an important approach in conducting pragmatic research that is used and sustained. Given the nature of EB anxiety treatment and the reluctance of treatment providers to deliver exposure therapy, this unfortunate ‘adaptation’ is a reality across the country – and is one that you have clearly identified and have acknowledged the limitations involved. I am very glad to see that you are planning to monitor your hybrid 1 and that your mixed methods work will pay close attention to adaptations – which will be crucial for usability, scale-up and sustainability. 

While I wish that I could provide more concrete guidance, please know that you are definitely on the right track in addressing fidelity and adaptations in your proposed study.

Denny";s:5:"xhtml";s:1474:"Robyn:<br /><br />Your comprehensive and thoughtful response to address fidelity nicely covers the all the high points, especially fidelity measurement. As you described: clinician self-report, supervisory assessment, and audio/video recordings that are rated by independent reviewers are traditional approaches – and there may opportunities to monitor fidelity using dashboards or involving creative combinations (i.e., comparing the accuracy of clinician self-rpt to independent reviewers) that may be intriguing for you to consider. <br /><br />Regarding adaptations – I highly appreciate that you value the fluid nature of adaptation and are committed to building in flexibility. This will be an important approach in conducting pragmatic research that is used and sustained. Given the nature of EB anxiety treatment and the reluctance of treatment providers to deliver exposure therapy, this unfortunate ‘adaptation’ is a reality across the country – and is one that you have clearly identified and have acknowledged the limitations involved. I am very glad to see that you are planning to monitor your hybrid 1 and that your mixed methods work will pay close attention to adaptations – which will be crucial for usability, scale-up and sustainability. <br /><br />While I wish that I could provide more concrete guidance, please know that you are definitely on the right track in addressing fidelity and adaptations in your proposed study.<br /><br />Denny";s:6:"parent";s:32:"62f972157b79531a2d2b3ef4ac6aa267";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"cb0dfdeef063e4eeba3c8dc84eefa0ac";}s:32:"b2bb0e7a6b0f51a057a949506d363cee";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dpintello";s:4:"name";s:14:"Denny Pintello";s:4:"mail";s:23:"Denise.Pintello@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1536951777;s:8:"modified";i:1536951852;}s:3:"raw";s:2424:"Kelsie:

You have such a unique opportunity to directly impact the quality of MH service delivery at a state level – and to ensure that MBC is delivered with fidelity – which in turn, would enhance scale-up and sustainability. I did review your response to assignment #1 and wondered (like Lindsey highlighted in #13) if you were going to pull out one piece of this state-wide rollout to focus on for TIDHR, which if you did – I believe that this would offer you the best option to apply the principles of what we are covering on implementation science.  So - based on your responses to assignment #2, my comments are below.

Fidelity: you provided a list of items that seem related to measurement and fidelity. My questions focus on who is completing the measures and dashboard – is this largely based on provider self-report? If yes, are there other approaches that can validate the provider’s assessment? For instance, do clients, providers and supervisors provide input? Is there a way to compare the same indicators from the client/provider/supervisor level? As an example – if the provider claims that they delivered MBC during a session, is there a comparable assessment from a supervisor to provide a rating? Would there be an option for a client to report that they received any key components of MBC during their session – as well as any subsequent change in symptoms? It would be great if you could articulate your thinking about MBC fidelity in this state-wide case management system.

Adaptation: Your response is interesting – and as someone who also has worked in a state-wide system, I would be concerned that there is the potential for a number of adaptations on the admin, supervisory and provider level that might lead to both program drift and voltage drop, resulting in the possibility that a diluted version of MBC will be delivered. I would love to know more about how the dashboard monitoring will detect program drift and how training – and ‘re-training’ would address adaptation (that is not evidence-based).  For instance – Aaron Lyon is conducting a pre-implementation study to identify provider attitudes to determine if he can predict which providers are more likely to deliver EBPs. Perhaps this may be an approach to think about to determine which staff is more likely to deliver MBC as intended and perhaps track their fidelity and likelihood of adaptations…

Denny";s:5:"xhtml";s:2464:"Kelsie:<br /><br />You have such a unique opportunity to directly impact the quality of MH service delivery at a state level – and to ensure that MBC is delivered with fidelity – which in turn, would enhance scale-up and sustainability. I did review your response to assignment #1 and wondered (like Lindsey highlighted in #13) if you were going to pull out one piece of this state-wide rollout to focus on for TIDHR, which if you did – I believe that this would offer you the best option to apply the principles of what we are covering on implementation science.  So - based on your responses to assignment #2, my comments are below.<br /><br />Fidelity: you provided a list of items that seem related to measurement and fidelity. My questions focus on who is completing the measures and dashboard – is this largely based on provider self-report? If yes, are there other approaches that can validate the provider’s assessment? For instance, do clients, providers and supervisors provide input? Is there a way to compare the same indicators from the client/provider/supervisor level? As an example – if the provider claims that they delivered MBC during a session, is there a comparable assessment from a supervisor to provide a rating? Would there be an option for a client to report that they received any key components of MBC during their session – as well as any subsequent change in symptoms? It would be great if you could articulate your thinking about MBC fidelity in this state-wide case management system.<br /><br />Adaptation: Your response is interesting – and as someone who also has worked in a state-wide system, I would be concerned that there is the potential for a number of adaptations on the admin, supervisory and provider level that might lead to both program drift and voltage drop, resulting in the possibility that a diluted version of MBC will be delivered. I would love to know more about how the dashboard monitoring will detect program drift and how training – and ‘re-training’ would address adaptation (that is not evidence-based).  For instance – Aaron Lyon is conducting a pre-implementation study to identify provider attitudes to determine if he can predict which providers are more likely to deliver EBPs. Perhaps this may be an approach to think about to determine which staff is more likely to deliver MBC as intended and perhaps track their fidelity and likelihood of adaptations…<br /><br />Denny";s:6:"parent";s:32:"af40916a09f1b7c91c2d1934620daae8";s:7:"replies";a:1:{i:0;s:32:"4ede654dc669fb1f5787eb854ce86f57";}s:4:"show";b:1;s:3:"cid";s:32:"b2bb0e7a6b0f51a057a949506d363cee";}s:32:"000663e38918965e098c24050e47f963";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537031786;}s:3:"raw";s:141:"Hi Brenna,

I'm sorry I missed your message. I would love to see the slide decks, since I was unable to join the 9/10 call.

Thanks!

Lindsey";s:5:"xhtml";s:176:"Hi Brenna,<br /><br />I&#039;m sorry I missed your message. I would love to see the slide decks, since I was unable to join the 9/10 call.<br /><br />Thanks!<br /><br />Lindsey";s:6:"parent";s:32:"46f5840eb80062541a768a3cdae3e54d";s:7:"replies";a:1:{i:0;s:32:"f8ae21f531c01299b099d75288e7fdc9";}s:4:"show";b:1;s:3:"cid";s:32:"000663e38918965e098c24050e47f963";}s:32:"a6052fea8200ef492d71dda1fe9590f6";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1537032946;s:8:"modified";i:1537033324;}s:3:"raw";s:2694:"Hi Brenna,

I'm glad to hear you will assess CBT fidelity. Here are the questions I still had after reading your aims and assignment #2. I hope they will be useful to you as you continue to refine your project.

1. Can you say more about how the trained rater will rate the clinicians? How will the raters be trained? Do you have a CBT fidelity measure you will use/adapt? Are you developing a fidelity measure to your adapted CBT during the F32? Over how many role-play observations will you assess fidelity? Is this upfront assessment?  Can you justify why you will not do any ongoing fidelity assessment of sessions, and only use role-plays between clinicians and raters? What you will do if you find low fidelity or clinician drift? 

As you flesh out aim 2, even though you are unaware of research on CBT fidelity with your specific population, you will want to be able to cite research (at least briefly) on findings related to CBT fidelity and its relationship to CBT effectiveness more generally for depression and anxiety.

2. It sounds like you will have preliminary CBT adaptation data from your F32, since you plan to have made specific CBT adaptations for your population already. It would be great to report what you are adapting now and why you think that is likely to enhance effectiveness (your proposed aim 2; see my comment above about whether you are developing a fidelity measure for your adapted CBT during the F32, or will use an existing CBT fidelity measure). Then in your proposed study, can you incorporate into your description of your multi-component implementation strategy how you would expect clinicians adapting as necessary "on the fly" to vary across your arms?  For example, what specific influences do you expect opinion leaders to have on adaptation and fidelity related to your aim 1 and aim 2?

This relates to feedback Denny gave you on your aims: "AIM 1: Since you are manipulating the implementation strategy in Aim 1 and measuring the effectiveness of CBT treatment in AIM 2, I am wondering if this really is more of a hybrid type III. Are you adapting the CBT in some way to test the effectiveness? Usually, in a hybrid II, you are also manipulating the effectiveness in some way. As currently written, it is unclear if you are tinkering with it to boost effectiveness."

I was wondering about Hybrid II versus Hybrid III even more after reading your comments about fidelity/adaptation in your proposed study. Perhaps, my questions will help you further refine your planned/proposed design, clarifying what you will have already learned in the F32, as compared to what you will still need to learn in the proposed study.

Great work Brenna!

Lindsey";s:5:"xhtml";s:2799:"Hi Brenna,<br /><br />I&#039;m glad to hear you will assess CBT fidelity. Here are the questions I still had after reading your aims and assignment #2. I hope they will be useful to you as you continue to refine your project.<br /><br />1. Can you say more about how the trained rater will rate the clinicians? How will the raters be trained? Do you have a CBT fidelity measure you will use/adapt? Are you developing a fidelity measure to your adapted CBT during the F32? Over how many role-play observations will you assess fidelity? Is this upfront assessment?  Can you justify why you will not do any ongoing fidelity assessment of sessions, and only use role-plays between clinicians and raters? What you will do if you find low fidelity or clinician drift? <br /><br />As you flesh out aim 2, even though you are unaware of research on CBT fidelity with your specific population, you will want to be able to cite research (at least briefly) on findings related to CBT fidelity and its relationship to CBT effectiveness more generally for depression and anxiety.<br /><br />2. It sounds like you will have preliminary CBT adaptation data from your F32, since you plan to have made specific CBT adaptations for your population already. It would be great to report what you are adapting now and why you think that is likely to enhance effectiveness (your proposed aim 2; see my comment above about whether you are developing a fidelity measure for your adapted CBT during the F32, or will use an existing CBT fidelity measure). Then in your proposed study, can you incorporate into your description of your multi-component implementation strategy how you would expect clinicians adapting as necessary &quot;on the fly&quot; to vary across your arms?  For example, what specific influences do you expect opinion leaders to have on adaptation and fidelity related to your aim 1 and aim 2?<br /><br />This relates to feedback Denny gave you on your aims: &quot;AIM 1: Since you are manipulating the implementation strategy in Aim 1 and measuring the effectiveness of CBT treatment in AIM 2, I am wondering if this really is more of a hybrid type III. Are you adapting the CBT in some way to test the effectiveness? Usually, in a hybrid II, you are also manipulating the effectiveness in some way. As currently written, it is unclear if you are tinkering with it to boost effectiveness.&quot;<br /><br />I was wondering about Hybrid II versus Hybrid III even more after reading your comments about fidelity/adaptation in your proposed study. Perhaps, my questions will help you further refine your planned/proposed design, clarifying what you will have already learned in the F32, as compared to what you will still need to learn in the proposed study.<br /><br />Great work Brenna!<br /><br />Lindsey";s:6:"parent";s:32:"f60ff022507300b7584f143e03917ee7";s:7:"replies";a:1:{i:0;s:32:"a6893943fa4d437bfd2d9e21d1051203";}s:4:"show";b:1;s:3:"cid";s:32:"a6052fea8200ef492d71dda1fe9590f6";}s:32:"46a808d22603d5d54d12c724aeef964b";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537035423;}s:3:"raw";s:3146:"Hi Ana,

This is a very interesting project. Here are the questions I still had after reading your aims and assignment #2. I hope they will be useful to you as you continue to refine your project.

1. You mention that you do not plan to assess EBI fidelity, but I read this in your aims: "This promising “reverse integration” approach has encouraging early evidence of its effectiveness in the literature,2-4 including for reducing emergency room visits and increasing screenings.5" This makes me want to know me know the state of the science on this approach and what dimensions of BHH implementation you expect to be associated with effectiveness, as you will define/measure it in your proposed project (aim 1, 2nd phase). In your study, you will want to be able to compare the BHH outcomes you observe, against previous research, including the extent to which fidelity to key components of previous "reverse integration" models was or was not observed. This is likely going to be key to understanding interpreting your study findings in light of prior research.

2. In the context of your proposal, for aims 2 and 3, I would discuss adaptation in relationship to the five domains of Singer et al.'s (2018) Comprehensive Theory of Integration. Following Singer et al., you propose that it is important to distinguish and measure integration at five levels: structural and functional integration (organizational), interpersonal and normative integration (social features), and process integration. Therefore, to build from your proposed Aims rationale, relate these constructs of integration directly to EBI adaptation during implementation. The examples you provide in assignment #2 do not discuss the BBH implementation at CHA within the Comprehensive Theory of Integration framework that you set-up in your Aims. I think this is a missed opportunity to formalize in a theory-driven way your observations/understanding of BHH/reverse integration during your proposed observational study.

Then, to build a more programmatic theory-driven line of inquiry for your readers/reviewers, I would use the same framing of fidelity and adaptation you flesh out for aims 1 and 2, in aim 3. As written, you propose to "Conduct targeted semi-structured qualitative interviews with providers and stakeholders to assess (a) ongoing barriers/facilitators to implementation and sustainability at an existing BHH site, and (b) appropriateness, acceptability, feasibility, and barriers/facilitators at 2 potential BHH expansion sites." 

"Barriers and facilitators to implementation and sustainability" should relate reverse integration/BHH fidelity to its effectiveness (aim 1). Just as measures of appropriateness, acceptability and feasibility should relate the necessary adaptation of your EBI to integrate it within these implementation settings according to your integration theory (Singer et al., 2018; aim 2). 

Note: I am struggling to track your aims to your phases as written.  Perhaps you have ideas that could make this clearer on the page for your readers.  When I read aim 3, how does aim 3a and aim 3b fit with your proposed study phases?  
";s:5:"xhtml";s:3248:"Hi Ana,<br /><br />This is a very interesting project. Here are the questions I still had after reading your aims and assignment #2. I hope they will be useful to you as you continue to refine your project.<br /><br />1. You mention that you do not plan to assess EBI fidelity, but I read this in your aims: &quot;This promising “reverse integration” approach has encouraging early evidence of its effectiveness in the literature,2-4 including for reducing emergency room visits and increasing screenings.5&quot; This makes me want to know me know the state of the science on this approach and what dimensions of BHH implementation you expect to be associated with effectiveness, as you will define/measure it in your proposed project (aim 1, 2nd phase). In your study, you will want to be able to compare the BHH outcomes you observe, against previous research, including the extent to which fidelity to key components of previous &quot;reverse integration&quot; models was or was not observed. This is likely going to be key to understanding interpreting your study findings in light of prior research.<br /><br />2. In the context of your proposal, for aims 2 and 3, I would discuss adaptation in relationship to the five domains of Singer et al.&#039;s (2018) Comprehensive Theory of Integration. Following Singer et al., you propose that it is important to distinguish and measure integration at five levels: structural and functional integration (organizational), interpersonal and normative integration (social features), and process integration. Therefore, to build from your proposed Aims rationale, relate these constructs of integration directly to EBI adaptation during implementation. The examples you provide in assignment #2 do not discuss the BBH implementation at CHA within the Comprehensive Theory of Integration framework that you set-up in your Aims. I think this is a missed opportunity to formalize in a theory-driven way your observations/understanding of BHH/reverse integration during your proposed observational study.<br /><br />Then, to build a more programmatic theory-driven line of inquiry for your readers/reviewers, I would use the same framing of fidelity and adaptation you flesh out for aims 1 and 2, in aim 3. As written, you propose to &quot;Conduct targeted semi-structured qualitative interviews with providers and stakeholders to assess (a) ongoing barriers/facilitators to implementation and sustainability at an existing BHH site, and (b) appropriateness, acceptability, feasibility, and barriers/facilitators at 2 potential BHH expansion sites.&quot; <br /><br />&quot;Barriers and facilitators to implementation and sustainability&quot; should relate reverse integration/BHH fidelity to its effectiveness (aim 1). Just as measures of appropriateness, acceptability and feasibility should relate the necessary adaptation of your EBI to integrate it within these implementation settings according to your integration theory (Singer et al., 2018; aim 2). <br /><br />Note: I am struggling to track your aims to your phases as written.  Perhaps you have ideas that could make this clearer on the page for your readers.  When I read aim 3, how does aim 3a and aim 3b fit with your proposed study phases?";s:6:"parent";s:32:"a426d99f6b0b8ece080e0c3c6fe8e1de";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"46a808d22603d5d54d12c724aeef964b";}s:32:"9302324649a15248e5d6a11cfca5780c";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"nbowersox";s:4:"name";s:13:"Nick Bowersox";s:4:"mail";s:24:"Nicholas.Bowersox@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537064132;}s:3:"raw";s:3649:"Ilana - 

Thanks for providing more information about your planned project.  

Question 1: It sounds like the plan is to focus on establishing fidelity to IPT characteristics during the "training period" for your future trainers/therapists.  You mention the expectation that trainees will complete 3-5 cases and attend 75%+ of supervision sessions, which seems like a sensible minimum bar to consider trainees to be experts in IPT.  Based on this description, will you assume that trainees will continue to implement IPT with fidelity and without monitoring after the initial training period?  I think the last question under main question #1 for this week will be especially important for you, when deciding on expectations for fidelity: to what extent do you see positive response to IPT when it is delivered with limited fidelity to the guide materials?  Put another way, is the core component of your intervention IPT in particular or psychotherapy delivered in a manner similar to IPT?  This is an issue that comes up when considering care for specific MH subgroups (e.g., PTSD in particular) - for conditions that have a differential response to high-quality evidence-based treatment, it is important to ensure that the therapy that is being delivered closely matches the manuals/trainings provided by experts in that approach.  Do you have any preliminary outcome information that would allow you to assess some of these questions?  If IPT fidelty is essential to the effectiveness of this intervention, you could consider borrowing some approaches that have been used by large healthcare systems such as VA (i.e., the use of specific progress note templates which map onto specific characteristics of treatments; the recording and review of sessions by experts; hands-on, in-person trainings between experts and trainees), although these may not be a good fit to the setting and goals of your project.  An additional consideration is fidelity to the training of new therapists, given the "train the trainer" aspects of your proposed projects - how will you be sure that trainers are offering the same high-quality training to new trainees that they received from their mentors?  It could be helpful to have some limited "supervision of supervision" type interactions between more senior and junior trainees to help with this area.  Please let me know if it might be helpful to set up a call outside of our large-group TIDIRH meetings to discuss some of these ideas in more depth.

Question 2: It is wise to expect that your application of the IPT model to this new setting will require some modifications in approach relative to previous IPT interventions.  Given your experience with the setting, resources, staff, and care environment, can you estimate the ways that such modifications will strengthen the IPT intervention's ability to meet the needs of your target population?  To what extent will those modifications become expected, required aspects of the IPC intervention?  In other words, how will IPC fidelity incorporate modifications to the IPT intervention which allow the IPT intervention to be ideally targeted for Mozambique?  Do you have any approach to identify which modifications are needed (e.g., conversations with providers and patients to discuss their needs/resources/concerns with a program such as IPC)?  This are is worth fleshing out a bit, as it will inform the expectations that you communicate to your trainers, trainees, and patients.  It will also inform the creation of any fidelity monitoring tools you might decide to use to support delivery of IPC in a manner that meets your expectations.

Best,

- Nick";s:5:"xhtml";s:3734:"Ilana - <br /><br />Thanks for providing more information about your planned project.  <br /><br />Question 1: It sounds like the plan is to focus on establishing fidelity to IPT characteristics during the &quot;training period&quot; for your future trainers/therapists.  You mention the expectation that trainees will complete 3-5 cases and attend 75%+ of supervision sessions, which seems like a sensible minimum bar to consider trainees to be experts in IPT.  Based on this description, will you assume that trainees will continue to implement IPT with fidelity and without monitoring after the initial training period?  I think the last question under main question #1 for this week will be especially important for you, when deciding on expectations for fidelity: to what extent do you see positive response to IPT when it is delivered with limited fidelity to the guide materials?  Put another way, is the core component of your intervention IPT in particular or psychotherapy delivered in a manner similar to IPT?  This is an issue that comes up when considering care for specific MH subgroups (e.g., PTSD in particular) - for conditions that have a differential response to high-quality evidence-based treatment, it is important to ensure that the therapy that is being delivered closely matches the manuals/trainings provided by experts in that approach.  Do you have any preliminary outcome information that would allow you to assess some of these questions?  If IPT fidelty is essential to the effectiveness of this intervention, you could consider borrowing some approaches that have been used by large healthcare systems such as VA (i.e., the use of specific progress note templates which map onto specific characteristics of treatments; the recording and review of sessions by experts; hands-on, in-person trainings between experts and trainees), although these may not be a good fit to the setting and goals of your project.  An additional consideration is fidelity to the training of new therapists, given the &quot;train the trainer&quot; aspects of your proposed projects - how will you be sure that trainers are offering the same high-quality training to new trainees that they received from their mentors?  It could be helpful to have some limited &quot;supervision of supervision&quot; type interactions between more senior and junior trainees to help with this area.  Please let me know if it might be helpful to set up a call outside of our large-group TIDIRH meetings to discuss some of these ideas in more depth.<br /><br />Question 2: It is wise to expect that your application of the IPT model to this new setting will require some modifications in approach relative to previous IPT interventions.  Given your experience with the setting, resources, staff, and care environment, can you estimate the ways that such modifications will strengthen the IPT intervention&#039;s ability to meet the needs of your target population?  To what extent will those modifications become expected, required aspects of the IPC intervention?  In other words, how will IPC fidelity incorporate modifications to the IPT intervention which allow the IPT intervention to be ideally targeted for Mozambique?  Do you have any approach to identify which modifications are needed (e.g., conversations with providers and patients to discuss their needs/resources/concerns with a program such as IPC)?  This are is worth fleshing out a bit, as it will inform the expectations that you communicate to your trainers, trainees, and patients.  It will also inform the creation of any fidelity monitoring tools you might decide to use to support delivery of IPC in a manner that meets your expectations.<br /><br />Best,<br /><br />- Nick";s:6:"parent";s:32:"f1b8cb00ca6ce0569bc47138ff5db3b7";s:7:"replies";a:1:{i:0;s:32:"c1c5a37119c8b13d2c6ab79776c18abb";}s:4:"show";b:1;s:3:"cid";s:32:"9302324649a15248e5d6a11cfca5780c";}s:32:"750788d8375764e91e893228f60a4b96";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"nbowersox";s:4:"name";s:13:"Nick Bowersox";s:4:"mail";s:24:"Nicholas.Bowersox@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537130272;}s:3:"raw";s:3225:"Addie - 

Thank you for providing this additional feedback about your project.  This really sounds like your project has the potential to be high-impact for this group.  A few thoughts:

1. When I think of program fidelity, I try to think of the "essential components" that are needed for the intervention to be delivered as expected - these are the things that I want to focus upon for multiple reasons: to teach others how to deliver the intervention (implementation), to assess if the program is not meeting its goals (program evaluation), and to drive program support activities such as the development of training materials/education.  These can be distinguished from aspects that are more open to adaptation across sites to allow each site to implement the intervention in a manner that is most likely to succeed in the unique characteristics of the site/setting.  For your project, I see several aspects that could be considered "essential components" and the basis of your fidelity conceptualization: administration of the screening item to a specific subgroup of patients (should all women or just some be given the measure? Are you expecting that measure administration will be done for all women who meet a particular set of criteria or does provider judgement play a part in this decision?), linking scores on the measure to particular follow-up treatments.  There are several other aspects that could be formalized if you feel they are needed, such as the timeliness with which providers follow up on a positive screen (same-day? same week?), the process through which providers involve patients in the process of linking them to other services, the extent to which providers provide repeated screening/administration of the measure for patients they see on an ongoing basis, charting in the wake of a positive screen, etc.  Basically, what aspects of this intervention do you expect will occur in all cases when a patient who meets your clinical profile appears for treatment?  What aspects of this intervention do you consider essential, to the extent that you would like to focus training and evaluation of program on those aspects?  These considerations could help to guide decisions related to program fidelity and help to ensure that you have consistencies for some aspects of your program across providers, sites, and patients.

2. While I agree that it does not make sense to modify the screener across sites, I wonder if the other aspects of your program (linking to other services, providing follow-up screening, involving the patient in the process of referral to other services after a positive screen, responding to a positive screen and a rejection of an offer of linkages to a higher level of care) will benefit from some modification across sites based on differences in patient/provider relationships, treatment models, as well as available clinical resources and provider/provider linkages.

I would be happy to discuss any of the above (or other aspects of your program) outside of the next call if you would like.  As a bit of an aside, I am also in Ann Arbor, with an office in NCRC, so it would probably be possible to set up a meeting to discuss in person if you would like.

Best,

- Nick";s:5:"xhtml";s:3305:"Addie - <br /><br />Thank you for providing this additional feedback about your project.  This really sounds like your project has the potential to be high-impact for this group.  A few thoughts:<br /><br />1. When I think of program fidelity, I try to think of the &quot;essential components&quot; that are needed for the intervention to be delivered as expected - these are the things that I want to focus upon for multiple reasons: to teach others how to deliver the intervention (implementation), to assess if the program is not meeting its goals (program evaluation), and to drive program support activities such as the development of training materials/education.  These can be distinguished from aspects that are more open to adaptation across sites to allow each site to implement the intervention in a manner that is most likely to succeed in the unique characteristics of the site/setting.  For your project, I see several aspects that could be considered &quot;essential components&quot; and the basis of your fidelity conceptualization: administration of the screening item to a specific subgroup of patients (should all women or just some be given the measure? Are you expecting that measure administration will be done for all women who meet a particular set of criteria or does provider judgement play a part in this decision?), linking scores on the measure to particular follow-up treatments.  There are several other aspects that could be formalized if you feel they are needed, such as the timeliness with which providers follow up on a positive screen (same-day? same week?), the process through which providers involve patients in the process of linking them to other services, the extent to which providers provide repeated screening/administration of the measure for patients they see on an ongoing basis, charting in the wake of a positive screen, etc.  Basically, what aspects of this intervention do you expect will occur in all cases when a patient who meets your clinical profile appears for treatment?  What aspects of this intervention do you consider essential, to the extent that you would like to focus training and evaluation of program on those aspects?  These considerations could help to guide decisions related to program fidelity and help to ensure that you have consistencies for some aspects of your program across providers, sites, and patients.<br /><br />2. While I agree that it does not make sense to modify the screener across sites, I wonder if the other aspects of your program (linking to other services, providing follow-up screening, involving the patient in the process of referral to other services after a positive screen, responding to a positive screen and a rejection of an offer of linkages to a higher level of care) will benefit from some modification across sites based on differences in patient/provider relationships, treatment models, as well as available clinical resources and provider/provider linkages.<br /><br />I would be happy to discuss any of the above (or other aspects of your program) outside of the next call if you would like.  As a bit of an aside, I am also in Ann Arbor, with an office in NCRC, so it would probably be possible to set up a meeting to discuss in person if you would like.<br /><br />Best,<br /><br />- Nick";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"750788d8375764e91e893228f60a4b96";}s:32:"e452c663cb5d5eed1fee454401584e76";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"adopp";s:4:"name";s:9:"Alex Dopp";s:4:"mail";s:13:"dopp@uark.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537213987;}s:3:"raw";s:503:"That is a great point, Denny, thank you for pointing it out. My plan is to identify proposed mechanisms of action for the FMP in my pilot clinical trial (R34) after completion of this R15. A benefit of that could be distinguishing between core components of the FMP that require high fidelity vs. the adaptable periphery of the process - and identifying ways to measure both. I will continue thinking about these issues and hinting at these future plans as much as is reasonable within the R15 proposal.";s:5:"xhtml";s:503:"That is a great point, Denny, thank you for pointing it out. My plan is to identify proposed mechanisms of action for the FMP in my pilot clinical trial (R34) after completion of this R15. A benefit of that could be distinguishing between core components of the FMP that require high fidelity vs. the adaptable periphery of the process - and identifying ways to measure both. I will continue thinking about these issues and hinting at these future plans as much as is reasonable within the R15 proposal.";s:6:"parent";s:32:"6a7683523878fa07b0068609f1e3f087";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"e452c663cb5d5eed1fee454401584e76";}s:32:"bab72eedb9d6c12d7ad674803d8682a1";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"adopp";s:4:"name";s:9:"Alex Dopp";s:4:"mail";s:13:"dopp@uark.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537214082;}s:3:"raw";s:4655:"Dopp - Assignment #3

Assignment #3a - Models:
Which model or combination of models is most applicable to your proposed study and why?
How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

For my project, which will develop a fiscal mapping process and component financial strategies, I have selected the recently published Integrated Sustainability Framework (Shelton, Cooper, & Stirman, 2018) to guide my work. I will not be attempting to impact implementation outcomes in this study, but the ultimate goal of the fiscal mapping process is to improve sustainability of evidence-based treatments (EBTs) in youth mental health service agencies. The Integrated Sustainability Framework represents the dynamic interactions among outer and inner (i.e., intra- and inter-agency, respectively) contextual factors as well as program characteristics (interventions, interventionists, and processes). The prominence of funding and related factors (e.g., need for training, intervention cost) in the framework highlights how long-term financial viability is a key factor that underlies sustained EBT implementation, impact, and capacity building. 

The Integrated Sustainability Framework guides my focus and selection of measures for each Specific Aim of the project. Aim 1 seeks to generate a comprehensive compilation of financing strategies used in youth mental health service agencies, and thus is primarily concerned with understanding how these strategies fit into broad inner and outer contexts (i.e., the strategies will not be program-specific). For Aim 2, the development of the fiscal mapping process (which will assist agencies in selecting the optimal combination of strategies for EBT sustainment) requires a more granular understanding of how specific EBT program characteristics interact with financing strategies to influence sustainability. The potential EBTs of focus in my study are Parent-Child Interaction Therapy, Problematic Sexual Behavior – Cognitive-Behavioral Treatment, and Trauma-Focused Cognitive-Behavioral Therapy. 

Assignment #3b - Measures & Evaluations:
What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them? What processes are you measuring in your study, how are you measuring them, and why are you measuring them? Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

As I discussed in my response to the questions about fidelity, I will not measure most implementation outcomes in my study – the focus will be on developing the Fiscal Mapping Process which will be used to target implementation outcomes in future trials. However, I am planning to measure a number of determinants for my primary targeted outcome of EBT sustainability. Moreover, I will measure one implementation outcome – cost – that is a proximal predictor/determinant of long-term financial sustainability.

For Aim 1 (compilation of financing strategies), I will obtain ratings from the panel of expert stakeholder participants on each strategy’s: (a) availability in their funding environment; (b) suitability for sustaining different types of implementation activities; (c) [youth mental health service agencies only] contribution to their funding for EBT sustainment (% of total over the last 3 years); (d) importance (i.e., value) and (e) feasibility. For Aim 2 (fiscal mapping process), stakeholders from EBT intermediary organizations and youth mental health service agencies will complete the Program Sustainability Assessment Tool and a cost survey (broken down into costs for various implementation and sustainment activities) for each EBT with which they have experience. Furthermore, all members of the expert stakeholder panel will complete a participatory modeling exercise – which guides a group of stakeholders through the process of creating an initial conceptual model of systems structures – in which participants will discuss and identify relevant variables that could influence the fiscal mapping process. Following completion of the participatory modeling exercise, the project team will review the results, create simple systems dynamics diagrams to represent the model elements generated by the participants, and engage in qualitative content analysis of the modeling exercise transcripts to identify additional themes regarding the fiscal mapping process.";s:5:"xhtml";s:4738:"Dopp - Assignment #3<br /><br />Assignment #3a - Models:<br />Which model or combination of models is most applicable to your proposed study and why?<br />How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />For my project, which will develop a fiscal mapping process and component financial strategies, I have selected the recently published Integrated Sustainability Framework (Shelton, Cooper, &amp; Stirman, 2018) to guide my work. I will not be attempting to impact implementation outcomes in this study, but the ultimate goal of the fiscal mapping process is to improve sustainability of evidence-based treatments (EBTs) in youth mental health service agencies. The Integrated Sustainability Framework represents the dynamic interactions among outer and inner (i.e., intra- and inter-agency, respectively) contextual factors as well as program characteristics (interventions, interventionists, and processes). The prominence of funding and related factors (e.g., need for training, intervention cost) in the framework highlights how long-term financial viability is a key factor that underlies sustained EBT implementation, impact, and capacity building. <br /><br />The Integrated Sustainability Framework guides my focus and selection of measures for each Specific Aim of the project. Aim 1 seeks to generate a comprehensive compilation of financing strategies used in youth mental health service agencies, and thus is primarily concerned with understanding how these strategies fit into broad inner and outer contexts (i.e., the strategies will not be program-specific). For Aim 2, the development of the fiscal mapping process (which will assist agencies in selecting the optimal combination of strategies for EBT sustainment) requires a more granular understanding of how specific EBT program characteristics interact with financing strategies to influence sustainability. The potential EBTs of focus in my study are Parent-Child Interaction Therapy, Problematic Sexual Behavior – Cognitive-Behavioral Treatment, and Trauma-Focused Cognitive-Behavioral Therapy. <br /><br />Assignment #3b - Measures &amp; Evaluations:<br />What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them? What processes are you measuring in your study, how are you measuring them, and why are you measuring them? Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />As I discussed in my response to the questions about fidelity, I will not measure most implementation outcomes in my study – the focus will be on developing the Fiscal Mapping Process which will be used to target implementation outcomes in future trials. However, I am planning to measure a number of determinants for my primary targeted outcome of EBT sustainability. Moreover, I will measure one implementation outcome – cost – that is a proximal predictor/determinant of long-term financial sustainability.<br /><br />For Aim 1 (compilation of financing strategies), I will obtain ratings from the panel of expert stakeholder participants on each strategy’s: (a) availability in their funding environment; (b) suitability for sustaining different types of implementation activities; (c) [youth mental health service agencies only] contribution to their funding for EBT sustainment (% of total over the last 3 years); (d) importance (i.e., value) and (e) feasibility. For Aim 2 (fiscal mapping process), stakeholders from EBT intermediary organizations and youth mental health service agencies will complete the Program Sustainability Assessment Tool and a cost survey (broken down into costs for various implementation and sustainment activities) for each EBT with which they have experience. Furthermore, all members of the expert stakeholder panel will complete a participatory modeling exercise – which guides a group of stakeholders through the process of creating an initial conceptual model of systems structures – in which participants will discuss and identify relevant variables that could influence the fiscal mapping process. Following completion of the participatory modeling exercise, the project team will review the results, create simple systems dynamics diagrams to represent the model elements generated by the participants, and engage in qualitative content analysis of the modeling exercise transcripts to identify additional themes regarding the fiscal mapping process.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"963999ea634843dbf550c2f2d78316ff";}s:4:"show";b:1;s:3:"cid";s:32:"bab72eedb9d6c12d7ad674803d8682a1";}s:32:"4ede654dc669fb1f5787eb854ce86f57";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"kokamura";s:4:"name";s:14:"Kelsie Okamura";s:4:"mail";s:26:"kelsie.h.okamura@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537231991;}s:3:"raw";s:934:"Hi Denny,

Your comments and questions are so helpful. Thank you. As I mentioned during our small group discussion, we're building the plane as it's flying. Our TIDIRH assignments and feedback have been helpful to me in really focusing in on concrete behaviors and operationalizing a lot of what we are doing, both in terms of implementation and outcomes. 

One area that I think I can focus more on for TIDIRH is the use of the Ohio Scales for client outcome monitoring. Our system requires the Ohio Scales for all youth, but the adoption rate has been really low and our care coordinators struggle with the utility since our feedback loop has a serious time-lag. It's our hope that the case management system will vastly improve that, so we could target that specifically in our implementation plan as well as evaluation.

I'll do more thinking about your other questions but I hope this gets me in a more refined direction. Thanks!";s:5:"xhtml";s:984:"Hi Denny,<br /><br />Your comments and questions are so helpful. Thank you. As I mentioned during our small group discussion, we&#039;re building the plane as it&#039;s flying. Our TIDIRH assignments and feedback have been helpful to me in really focusing in on concrete behaviors and operationalizing a lot of what we are doing, both in terms of implementation and outcomes. <br /><br />One area that I think I can focus more on for TIDIRH is the use of the Ohio Scales for client outcome monitoring. Our system requires the Ohio Scales for all youth, but the adoption rate has been really low and our care coordinators struggle with the utility since our feedback loop has a serious time-lag. It&#039;s our hope that the case management system will vastly improve that, so we could target that specifically in our implementation plan as well as evaluation.<br /><br />I&#039;ll do more thinking about your other questions but I hope this gets me in a more refined direction. Thanks!";s:6:"parent";s:32:"b2bb0e7a6b0f51a057a949506d363cee";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"4ede654dc669fb1f5787eb854ce86f57";}s:32:"c1c5a37119c8b13d2c6ab79776c18abb";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"ipinsky";s:4:"name";s:12:"Ilana Pinsky";s:4:"mail";s:21:"pinskyilana@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537380984;}s:3:"raw";s:1751:"Nick,
Your comments and suggestions are all great. For instance, because we are working in a large setting, we do need to come up with ways to keep IPC fidelity close enough to ideal so that it is feasible to monitor in all levels: therapists, supervisors and trainers. After all, this is not only (or even mainly) about a research project, but it is about setting up a mental health system for a whole country (Moz). 
I see as some of our advantages at this point that:
1- there is no MH system in place, so we can build it well, instead of having to deconstruct and construct again (which could be more challenging);
2- we are working with the MH Ministry - they have been part of the process since day 1;
3- as far as IPC goes, we have on board at least 2 of the first and best American consultants on this subject (Myrna Weissman, that was one of the founders of this interventions as well as Kathy Clogerthy, that has been practicing and teaching IPC in a incredible number of different settings, including several low income countries). Both of them have been able to point to some of the central features of IPC that can assist us in the fidelity/adaptation process. 

On the other hand, we have several challenges, including:
1- because Moz has virtually none broad MH system, there is a lot we don't know exactly what to expect;
2- we will be doing task sharing with non-MH providers, so we can't count on them having previous MH experience (this can be "bad" or "good");
3- although IPC/IPT has been tested in many settings, it does not seem to exist still an agreement on a group of items that would represent its core.

Yes, I would appreciate if we could set up a call outside of the group times to discuss these issues. 
Thank you,
Ilana";s:5:"xhtml";s:1851:"Nick,<br />Your comments and suggestions are all great. For instance, because we are working in a large setting, we do need to come up with ways to keep IPC fidelity close enough to ideal so that it is feasible to monitor in all levels: therapists, supervisors and trainers. After all, this is not only (or even mainly) about a research project, but it is about setting up a mental health system for a whole country (Moz). <br />I see as some of our advantages at this point that:<br />1- there is no MH system in place, so we can build it well, instead of having to deconstruct and construct again (which could be more challenging);<br />2- we are working with the MH Ministry - they have been part of the process since day 1;<br />3- as far as IPC goes, we have on board at least 2 of the first and best American consultants on this subject (Myrna Weissman, that was one of the founders of this interventions as well as Kathy Clogerthy, that has been practicing and teaching IPC in a incredible number of different settings, including several low income countries). Both of them have been able to point to some of the central features of IPC that can assist us in the fidelity/adaptation process. <br /><br />On the other hand, we have several challenges, including:<br />1- because Moz has virtually none broad MH system, there is a lot we don&#039;t know exactly what to expect;<br />2- we will be doing task sharing with non-MH providers, so we can&#039;t count on them having previous MH experience (this can be &quot;bad&quot; or &quot;good&quot;);<br />3- although IPC/IPT has been tested in many settings, it does not seem to exist still an agreement on a group of items that would represent its core.<br /><br />Yes, I would appreciate if we could set up a call outside of the group times to discuss these issues. <br />Thank you,<br />Ilana";s:6:"parent";s:32:"9302324649a15248e5d6a11cfca5780c";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"c1c5a37119c8b13d2c6ab79776c18abb";}s:32:"f8ae21f531c01299b099d75288e7fdc9";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bmaddox";s:4:"name";s:13:"Brenna Maddox";s:4:"mail";s:17:"maddoxb@upenn.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537529936;}s:3:"raw";s:52:"Hi Lindsey, of course! I just emailed you my slides.";s:5:"xhtml";s:52:"Hi Lindsey, of course! I just emailed you my slides.";s:6:"parent";s:32:"000663e38918965e098c24050e47f963";s:7:"replies";a:1:{i:0;s:32:"580187dbf466d911d2b0f7d2a940fd1e";}s:4:"show";b:1;s:3:"cid";s:32:"f8ae21f531c01299b099d75288e7fdc9";}s:32:"a6893943fa4d437bfd2d9e21d1051203";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bmaddox";s:4:"name";s:13:"Brenna Maddox";s:4:"mail";s:17:"maddoxb@upenn.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537530046;}s:3:"raw";s:137:"Thank you so much for this helpful feedback! I will work on answering these questions and incorporating these suggestions moving forward.";s:5:"xhtml";s:137:"Thank you so much for this helpful feedback! I will work on answering these questions and incorporating these suggestions moving forward.";s:6:"parent";s:32:"a6052fea8200ef492d71dda1fe9590f6";s:7:"replies";a:1:{i:0;s:32:"f945b108b44cad0bdcd2c9f4a0f256f7";}s:4:"show";b:1;s:3:"cid";s:32:"a6893943fa4d437bfd2d9e21d1051203";}s:32:"9658a2d6f71708395f5f81183ca978f5";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bmaddox";s:4:"name";s:13:"Brenna Maddox";s:4:"mail";s:17:"maddoxb@upenn.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537544823;}s:3:"raw";s:4924:"MADDOX - Assignment #3a - Models:

1.	Which model or combination of models is most applicable to your proposed study and why?

My proposed study relies on Proctor’s Conceptual Model of Implementation Research. This model is applicable to my study because it provides an evaluation framework, and I am focused on evaluating the effectiveness of different implementation strategies. I like how this framework depicts the process of implementation from the starting point of an evidence-based practice (in my study, CBT modified for adults with autism), all the way through how implementation strategies are hypothesized to affect outcomes. For my study, it is helpful that the model distinguishes between the different types of outcomes (implementation, service, client).

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 

The Conceptual Model of Implementation Research guides and informs several key aspects of my study. It clearly highlights my hypothesis that implementation strategies affect implementation and client outcomes. It also provides a list of outcomes to consider. For possible implementation outcomes, it was helpful to review the included options of acceptability, adoption, appropriateness, costs, feasibility, fidelity, penetration, and sustainability. In keeping with this model, I will distinguish between implementation outcomes and client outcomes in my proposed study.

MADDOX - Assignment #3b - Measures & Evaluations:

1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

The primary implementation outcomes are adoption and fidelity. I will measure clinicians’ adoption of CBT for their adult clients with autism and anxiety/depression through self-report surveys and administrative data. I will measure fidelity through a combination of behavioral rehearsal with clinicians and session audio recordings. I will use a fidelity measure for the adapted CBT program that I am currently developing with my F32 study. I chose these implementation outcomes because they are relevant to the early to mid implementation stages and are closely linked to client-level outcomes. The client-level outcomes I will measure are anxiety and depression symptoms, as measured by the self-reported Beck Inventories and the clinician-rated Mini-International Neuropsychiatric Interview. I chose to measure clients’ anxiety and depression symptoms because I am interested in the effectiveness of the CBT intervention at the client level in a large-scale study (data which my current smaller-scale F32 study cannot provide).

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

I am interested in measuring clinicians’ knowledge about the treatment, clinicians’ skills to use the treatment, clinicians’ confidence to use the treatment effectively, clinicians’ attitudes and beliefs about their ability to deliver CBT to people with autism, and clinicians’ attitudes and beliefs about the appropriateness and effectiveness of using CBT with people with autism. Based on pilot data, we have learned that clinicians do not feel competent or confident in treating adults with autism. They report limited training, knowledge, and comfort in caring for adults with autism. Based on some preliminary interview data, a major barrier is clinicians’ beliefs and attitudes that (1) CBT is not appropriate or effective for people with autism, and (2) as mental health providers (separate from the developmental disability system), they are unequipped to treat people with autism. I am not sure the best way to measure these targets; I am currently envisioning a mix between self-report survey data and more in-depth semi-structured interviews or focus groups. 

3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

Based on conversations with Dr. Lauren Brookman-Frazee (F32 consultant) about her experiences rolling out AIM-HI, I am interested in assessing the co-benefit of clinicians’ increased use and improved fidelity of CBT for adults WITHOUT autism who present with anxiety and depression. Although this would not be a major focus of my study, I could gather some preliminary data through a self-report survey and an additional question in qualitative interviews with clinicians. One possible unintended consequence is that clinicians with adult clients with autism could focus exclusively on treating anxiety and depression, and overlook other important areas of concern. I could gather the clients’ perspective on this by asking about satisfaction with services. ";s:5:"xhtml";s:5037:"MADDOX - Assignment #3a - Models:<br /><br />1.	Which model or combination of models is most applicable to your proposed study and why?<br /><br />My proposed study relies on Proctor’s Conceptual Model of Implementation Research. This model is applicable to my study because it provides an evaluation framework, and I am focused on evaluating the effectiveness of different implementation strategies. I like how this framework depicts the process of implementation from the starting point of an evidence-based practice (in my study, CBT modified for adults with autism), all the way through how implementation strategies are hypothesized to affect outcomes. For my study, it is helpful that the model distinguishes between the different types of outcomes (implementation, service, client).<br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br /><br />The Conceptual Model of Implementation Research guides and informs several key aspects of my study. It clearly highlights my hypothesis that implementation strategies affect implementation and client outcomes. It also provides a list of outcomes to consider. For possible implementation outcomes, it was helpful to review the included options of acceptability, adoption, appropriateness, costs, feasibility, fidelity, penetration, and sustainability. In keeping with this model, I will distinguish between implementation outcomes and client outcomes in my proposed study.<br /><br />MADDOX - Assignment #3b - Measures &amp; Evaluations:<br /><br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />The primary implementation outcomes are adoption and fidelity. I will measure clinicians’ adoption of CBT for their adult clients with autism and anxiety/depression through self-report surveys and administrative data. I will measure fidelity through a combination of behavioral rehearsal with clinicians and session audio recordings. I will use a fidelity measure for the adapted CBT program that I am currently developing with my F32 study. I chose these implementation outcomes because they are relevant to the early to mid implementation stages and are closely linked to client-level outcomes. The client-level outcomes I will measure are anxiety and depression symptoms, as measured by the self-reported Beck Inventories and the clinician-rated Mini-International Neuropsychiatric Interview. I chose to measure clients’ anxiety and depression symptoms because I am interested in the effectiveness of the CBT intervention at the client level in a large-scale study (data which my current smaller-scale F32 study cannot provide).<br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />I am interested in measuring clinicians’ knowledge about the treatment, clinicians’ skills to use the treatment, clinicians’ confidence to use the treatment effectively, clinicians’ attitudes and beliefs about their ability to deliver CBT to people with autism, and clinicians’ attitudes and beliefs about the appropriateness and effectiveness of using CBT with people with autism. Based on pilot data, we have learned that clinicians do not feel competent or confident in treating adults with autism. They report limited training, knowledge, and comfort in caring for adults with autism. Based on some preliminary interview data, a major barrier is clinicians’ beliefs and attitudes that (1) CBT is not appropriate or effective for people with autism, and (2) as mental health providers (separate from the developmental disability system), they are unequipped to treat people with autism. I am not sure the best way to measure these targets; I am currently envisioning a mix between self-report survey data and more in-depth semi-structured interviews or focus groups. <br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />Based on conversations with Dr. Lauren Brookman-Frazee (F32 consultant) about her experiences rolling out AIM-HI, I am interested in assessing the co-benefit of clinicians’ increased use and improved fidelity of CBT for adults WITHOUT autism who present with anxiety and depression. Although this would not be a major focus of my study, I could gather some preliminary data through a self-report survey and an additional question in qualitative interviews with clinicians. One possible unintended consequence is that clinicians with adult clients with autism could focus exclusively on treating anxiety and depression, and overlook other important areas of concern. I could gather the clients’ perspective on this by asking about satisfaction with services.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"ea0a11f7afbf4cf3c3ff71baa6803ef6";}s:4:"show";b:1;s:3:"cid";s:32:"9658a2d6f71708395f5f81183ca978f5";}s:32:"a95c46571af04b7a04ce171c5f6e38da";a:8:{s:4:"user";a:5:{s:2:"id";s:11:"rshepardson";s:4:"name";s:16:"Robyn Shepardson";s:4:"mail";s:23:"Robyn.Shepardson@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537553861;}s:3:"raw";s:11381:"SHEPARDSON - Assignment #3a - Models:

1. Which model or combination of models is most applicable to your proposed study and why?
I am struggling with this question. Although I have the study loosely planned out, I now realize that my plan was essentially based on common sense and empirical precedent, rather than any established theory, model, or framework (TMF) from implementation science. I could pick one model and try to justify it, but instead I will be transparent that I am overwhelmed by the number of existing TMFs. In particular, the high degree of overlap and redundancy among them leaves me not knowing where to start to try to identify one that would be most relevant or helpful. I do find the results of the Tabak et al. (1) review helpful since they categorized the TMFs along the various dimensions. But even if I narrow it down to considering TMFs within the D=I or I>D categories, there are still ~30 TMFs to choose from. I feel unequipped to evaluate the merits of the various TMFs, and I will need more time to dive into the options further. I need more time to review the literature, so I am afraid I cannot resolve this question within the two weeks we have devoted to this module. I found Nilsen’s review (2) helpful in classifying the different types/foci of TMFs, but I am having a hard time even determining what overarching aim I should focus on for my study. It seems like identifying a theory (versus a model or framework) would be best for having a basis of why I would expect certain variables to influence implementation outcomes. I find many of the models and frameworks to be too broad and vague to be useful. For example, it’s wonderful that CFIR includes such a wide array of constructs that may be relevant to implementation, but given practical limitations on how much can be examined within one study, how does one pick and choose which of those many options to focus on? In any regard, all of this is helping me to understand how difficult it is to conduct research, and especially get funded to conduct research, in this field. I will continue to think this through and discuss with my mentors to make progress in identifying a specific TMF that would make sense for my study. 

Prior to beginning TIDIRH, I was planning to use the RE-AIM framework to guide the evaluation of implementation of my newly developed intervention. I appreciate the variety of dimensions included in this evaluation framework. Now I think I might supplement RE-AIM constructs with other implementation outcomes identified by Proctor et al. to cover all the important bases.

2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 
Although I have not been able to select a model yet, it would be helpful for ensuring the study has a cohesive thread. It should inform the: predictions made in the hypotheses, selection of measures and key outcomes, assessment of relevant processes, and selection of implementation strategies. Clarifying the model would allow the researcher to ensure that all of the key constructs are being assessed, so that the hypothesized relationships can be tested empirically. It helps to confirm that the selected implementation strategies are targeting the correct constructs to yield the expected change in behavior.

SHEPARDSON Assignment #3b - Measures & Evaluations:

1. What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?
My study is a hybrid II effectiveness-implementation design. For the effectiveness aspect of the trial, the primary patient-level clinical outcome is anxiety symptom severity because we are evaluating an anxiety intervention. We will measure this outcome using the Generalized Anxiety Disorder-7 (GAD-7) (3) self-report questionnaire, a validated, brief measure that is widely used in VHA PC-MHI and is applicable across the anxiety disorders beyond GAD. The other primary patient-level clinical outcome is depressive symptom severity, which is included given the high comorbidity between anxiety and depression symptoms (4). This will be measured by the Patient Health Questionnaire-9 (PHQ-9) self-report questionnaire (5), a validated measure that is widely used in VHA primary care and PC-MHI as part of mandated annual depression screening. The GAD-7 and PHQ-9 are the recommended measures for anxiety and depression, respectively, in VHA’s measurement-based care initiative, so they are routinely used in clinical practice. The GAD-7 and PHQ-9 both have high internal consistency (αs >= .86 in primary care samples) and have demonstrated construct and criterion validity (5,6). The total score for both measures is sensitive to change across treatment (7,8). Given that improving patient functioning is a central goal of PC-MHI services (9), I am also measuring two secondary patient-level clinical outcomes that will complement the symptom severity measures: functional impairment from anxiety and depressive symptoms. We will use the Overall Anxiety Severity and Impairment Scale (OASIS) (10) and Overall Depression Severity and Impairment Scale (ODSIS) (11). These 5-item scales demonstrate internal consistency (αs >= .84) and validity (12,13). Other patient-level outcomes include treatment satisfaction, acceptability, and credibility. I will assess satisfaction with the intervention using the Client Satisfaction Questionnaire, an 8-item self-report questionnaire with established reliability and validity (14). I will assess treatment credibility using a 4-item adapted version (15) of the Expectancy Rating Scale (16). I plan to assess acceptability using a semi-structured interview that has been used in our team’s prior studies of brief PC-MHI interventions and aims to assess satisfaction and perceived helpfulness of specific components of the intervention. I am not entirely sure if acceptability and satisfaction are different constructs, but I like having a qualitative component, which will allow for feedback on specific aspects of the intervention, to supplement the quantitative satisfaction measure. 

For the implementation aspect of the trial, the primary outcomes will likely be feasibility, adoption, acceptability, appropriateness, and fidelity. Since this will be the first effort to implement the intervention, I am more focused on initial implementation (versus sustainment, although that is obviously extremely important). I will have to keep thinking about this, but right now I envision adoption as being measured through the proportion of clinicians eligible to participate in the study (i.e., receiving training/supervision in the intervention so they can deliver it in their own clinical practice) who enroll as study clinicians and the proportion of those enrolled who actually complete the initial training/supervision to learn the intervention. 

Patient-level measures of feasibility that are not self-report (not sure if I am conceptualizing this correctly; these measures will not be administered to patients, but rather these are captured through patient behavior) would include rate of recruitment (number of eligible patients identified per month), rate of enrollment (proportion of eligible patients who enroll in the study), initial engagement (proportion of consented participants who do not attend any intervention sessions), number of intervention sessions attended, treatment completion (proportion of participants who complete at least 3 sessions), and overall attrition rates (proportion who miss follow-up sessions and/or assessments). 

For the other outcomes, previously I would have been likely to resort to using mostly “home-grown” measures, but with my new knowledge from TIDIRH, I will attempt to use established measures with documented psychometric properties. Clinician-level self-report measures of feasibility, acceptability, and appropriateness will comprise the 4-item Feasibility of Intervention Measure, Acceptability of Intervention Measure, and Intervention Appropriate Measure, respectively, all of which demonstrated good internal consistency, test-rest reliability, and content, structural, and known-groups validity (17). 

To measure intervention fidelity, the most likely approach will be to audio record intervention sessions and have trained raters conduct fidelity ratings with a detailed checklist. The checklist will encompass adherence to specific essential components as well as competent delivery. To ensure this approach is feasible in a large multi-site study, we may need to select a subsample of sessions to evaluate (e.g., initial and 1 or 2 follow-up sessions) for each patient. Other non-self report clinician-level measures of fidelity include the mean number and duration of sessions, which will help to evaluate whether treatment is being delivered consistent with the PC-MHI model of service delivery. (I am a bit confused between some aspects of feasibility and fidelity. I originally thought of number/duration of sessions as being related to feasibility, but now I think it is more related to fidelity in terms of dose.)

2. What processes are you measuring in your study, how are you measuring them, and why are you measuring them?
I’m not sure if I am interpreting this question correctly, but I am interested in several processes at the patient-, provider, and clinic- levels. For patients, I want to understand the patient experience of considering whether to try the intervention, actually receiving the intervention (which is more structured and skills-oriented than most routine care), and more generally receiving treatment in the PC-MHI setting. For providers, I am interested in how they decide whether to train in a new intervention, the experience of the training process in terms of ease and utility, whether the intervention seems to address a clinical need/gap and thus is worth the investment of time, perceived benefits and costs of expanding their treatment repertoire, how patient complexity and productivity pressures impact intervention delivery, and perceived administrative barriers to using the intervention. Within a clinic, I am interested in leadership’s decision to support or even encourage clinicians pursuing training in a new evidence-based intervention and willingness to reevaluate clinic processes to accommodate clinicians who want to expand their repertoires to include more evidence-based interventions. Currently my inclination would be to assess these things qualitatively through semi-structured interviews, as it does not seem like they could be measured with quantitative measures. Again, given my lack of clarity on a TMF, I may be misinterpreting “processes” here.

3. Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? 
At this juncture, I’m not sure what specifically I should be on the lookout for. However, since I am doing a hybrid II trial, I will likely be doing at least an informal process evaluation as we go. This should help us stay open and observant to be able to recognize possible unintended benefits or consequences of implementing the intervention. ";s:5:"xhtml";s:11528:"SHEPARDSON - Assignment #3a - Models:<br /><br />1. Which model or combination of models is most applicable to your proposed study and why?<br />I am struggling with this question. Although I have the study loosely planned out, I now realize that my plan was essentially based on common sense and empirical precedent, rather than any established theory, model, or framework (TMF) from implementation science. I could pick one model and try to justify it, but instead I will be transparent that I am overwhelmed by the number of existing TMFs. In particular, the high degree of overlap and redundancy among them leaves me not knowing where to start to try to identify one that would be most relevant or helpful. I do find the results of the Tabak et al. (1) review helpful since they categorized the TMFs along the various dimensions. But even if I narrow it down to considering TMFs within the D=I or I&gt;D categories, there are still ~30 TMFs to choose from. I feel unequipped to evaluate the merits of the various TMFs, and I will need more time to dive into the options further. I need more time to review the literature, so I am afraid I cannot resolve this question within the two weeks we have devoted to this module. I found Nilsen’s review (2) helpful in classifying the different types/foci of TMFs, but I am having a hard time even determining what overarching aim I should focus on for my study. It seems like identifying a theory (versus a model or framework) would be best for having a basis of why I would expect certain variables to influence implementation outcomes. I find many of the models and frameworks to be too broad and vague to be useful. For example, it’s wonderful that CFIR includes such a wide array of constructs that may be relevant to implementation, but given practical limitations on how much can be examined within one study, how does one pick and choose which of those many options to focus on? In any regard, all of this is helping me to understand how difficult it is to conduct research, and especially get funded to conduct research, in this field. I will continue to think this through and discuss with my mentors to make progress in identifying a specific TMF that would make sense for my study. <br /><br />Prior to beginning TIDIRH, I was planning to use the RE-AIM framework to guide the evaluation of implementation of my newly developed intervention. I appreciate the variety of dimensions included in this evaluation framework. Now I think I might supplement RE-AIM constructs with other implementation outcomes identified by Proctor et al. to cover all the important bases.<br /><br />2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br />Although I have not been able to select a model yet, it would be helpful for ensuring the study has a cohesive thread. It should inform the: predictions made in the hypotheses, selection of measures and key outcomes, assessment of relevant processes, and selection of implementation strategies. Clarifying the model would allow the researcher to ensure that all of the key constructs are being assessed, so that the hypothesized relationships can be tested empirically. It helps to confirm that the selected implementation strategies are targeting the correct constructs to yield the expected change in behavior.<br /><br />SHEPARDSON Assignment #3b - Measures &amp; Evaluations:<br /><br />1. What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br />My study is a hybrid II effectiveness-implementation design. For the effectiveness aspect of the trial, the primary patient-level clinical outcome is anxiety symptom severity because we are evaluating an anxiety intervention. We will measure this outcome using the Generalized Anxiety Disorder-7 (GAD-7) (3) self-report questionnaire, a validated, brief measure that is widely used in VHA PC-MHI and is applicable across the anxiety disorders beyond GAD. The other primary patient-level clinical outcome is depressive symptom severity, which is included given the high comorbidity between anxiety and depression symptoms (4). This will be measured by the Patient Health Questionnaire-9 (PHQ-9) self-report questionnaire (5), a validated measure that is widely used in VHA primary care and PC-MHI as part of mandated annual depression screening. The GAD-7 and PHQ-9 are the recommended measures for anxiety and depression, respectively, in VHA’s measurement-based care initiative, so they are routinely used in clinical practice. The GAD-7 and PHQ-9 both have high internal consistency (αs &gt;= .86 in primary care samples) and have demonstrated construct and criterion validity (5,6). The total score for both measures is sensitive to change across treatment (7,8). Given that improving patient functioning is a central goal of PC-MHI services (9), I am also measuring two secondary patient-level clinical outcomes that will complement the symptom severity measures: functional impairment from anxiety and depressive symptoms. We will use the Overall Anxiety Severity and Impairment Scale (OASIS) (10) and Overall Depression Severity and Impairment Scale (ODSIS) (11). These 5-item scales demonstrate internal consistency (αs &gt;= .84) and validity (12,13). Other patient-level outcomes include treatment satisfaction, acceptability, and credibility. I will assess satisfaction with the intervention using the Client Satisfaction Questionnaire, an 8-item self-report questionnaire with established reliability and validity (14). I will assess treatment credibility using a 4-item adapted version (15) of the Expectancy Rating Scale (16). I plan to assess acceptability using a semi-structured interview that has been used in our team’s prior studies of brief PC-MHI interventions and aims to assess satisfaction and perceived helpfulness of specific components of the intervention. I am not entirely sure if acceptability and satisfaction are different constructs, but I like having a qualitative component, which will allow for feedback on specific aspects of the intervention, to supplement the quantitative satisfaction measure. <br /><br />For the implementation aspect of the trial, the primary outcomes will likely be feasibility, adoption, acceptability, appropriateness, and fidelity. Since this will be the first effort to implement the intervention, I am more focused on initial implementation (versus sustainment, although that is obviously extremely important). I will have to keep thinking about this, but right now I envision adoption as being measured through the proportion of clinicians eligible to participate in the study (i.e., receiving training/supervision in the intervention so they can deliver it in their own clinical practice) who enroll as study clinicians and the proportion of those enrolled who actually complete the initial training/supervision to learn the intervention. <br /><br />Patient-level measures of feasibility that are not self-report (not sure if I am conceptualizing this correctly; these measures will not be administered to patients, but rather these are captured through patient behavior) would include rate of recruitment (number of eligible patients identified per month), rate of enrollment (proportion of eligible patients who enroll in the study), initial engagement (proportion of consented participants who do not attend any intervention sessions), number of intervention sessions attended, treatment completion (proportion of participants who complete at least 3 sessions), and overall attrition rates (proportion who miss follow-up sessions and/or assessments). <br /><br />For the other outcomes, previously I would have been likely to resort to using mostly “home-grown” measures, but with my new knowledge from TIDIRH, I will attempt to use established measures with documented psychometric properties. Clinician-level self-report measures of feasibility, acceptability, and appropriateness will comprise the 4-item Feasibility of Intervention Measure, Acceptability of Intervention Measure, and Intervention Appropriate Measure, respectively, all of which demonstrated good internal consistency, test-rest reliability, and content, structural, and known-groups validity (17). <br /><br />To measure intervention fidelity, the most likely approach will be to audio record intervention sessions and have trained raters conduct fidelity ratings with a detailed checklist. The checklist will encompass adherence to specific essential components as well as competent delivery. To ensure this approach is feasible in a large multi-site study, we may need to select a subsample of sessions to evaluate (e.g., initial and 1 or 2 follow-up sessions) for each patient. Other non-self report clinician-level measures of fidelity include the mean number and duration of sessions, which will help to evaluate whether treatment is being delivered consistent with the PC-MHI model of service delivery. (I am a bit confused between some aspects of feasibility and fidelity. I originally thought of number/duration of sessions as being related to feasibility, but now I think it is more related to fidelity in terms of dose.)<br /><br />2. What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br />I’m not sure if I am interpreting this question correctly, but I am interested in several processes at the patient-, provider, and clinic- levels. For patients, I want to understand the patient experience of considering whether to try the intervention, actually receiving the intervention (which is more structured and skills-oriented than most routine care), and more generally receiving treatment in the PC-MHI setting. For providers, I am interested in how they decide whether to train in a new intervention, the experience of the training process in terms of ease and utility, whether the intervention seems to address a clinical need/gap and thus is worth the investment of time, perceived benefits and costs of expanding their treatment repertoire, how patient complexity and productivity pressures impact intervention delivery, and perceived administrative barriers to using the intervention. Within a clinic, I am interested in leadership’s decision to support or even encourage clinicians pursuing training in a new evidence-based intervention and willingness to reevaluate clinic processes to accommodate clinicians who want to expand their repertoires to include more evidence-based interventions. Currently my inclination would be to assess these things qualitatively through semi-structured interviews, as it does not seem like they could be measured with quantitative measures. Again, given my lack of clarity on a TMF, I may be misinterpreting “processes” here.<br /><br />3. Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? <br />At this juncture, I’m not sure what specifically I should be on the lookout for. However, since I am doing a hybrid II trial, I will likely be doing at least an informal process evaluation as we go. This should help us stay open and observant to be able to recognize possible unintended benefits or consequences of implementing the intervention.";s:6:"parent";N;s:7:"replies";a:2:{i:0;s:32:"01fc826960f7427ce6c0fda0fe9b2d87";i:1;s:32:"bd8d0b2b870a45ce822e2d0b7ee86d01";}s:4:"show";b:1;s:3:"cid";s:32:"a95c46571af04b7a04ce171c5f6e38da";}s:32:"7a6d1020519397f05f8654c05feac1e4";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"aprogovac";s:4:"name";s:12:"Ana Progovac";s:4:"mail";s:24:"aprogovac@challiance.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537557188;}s:3:"raw";s:5653:"PROGOVAC - Assignment #3a - Models:

***1. Which model or combination of models is most applicable to your proposed study and why?

I have chosen the CFIR in combination with the Comprehensive Theory of Integration Singer et al. (2018) to apply to my proposed study.

Many existing evaluations of integrated care programs do not delve deep enough into the extent to which program elements are fully integrated into care. This may in part be due to the fact that comprehensive theoretical models of integration (and when it has truly been achieved) have been lacking. Singer et al. (2018) posit in their Comprehensive Theory of Integration7 that we should distinguish and measure integration at five levels organized in 3 domains: structural and functional integration (organizational), interpersonal and normative integration (social features), and process integration.7 Implementation Science research methods have also been specifically called upon to help understand the nuance underlying mixed results from integrating mental health into primary care.8 Combining a comprehensive theory of integration with implementation science research methods, and specifically use of the CFIR, is need to help researchers and clinical stakeholders better study real-world BHH effectiveness, including for sub-groups4,9  as well as barriers/facilitators to sustaining and expanding safety net BHH models, in order to support better program implementation and dissemination. 



***2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

Both of these models are key components of study design and selection of measures, etc. 

Typically, we think of a model as being implemented when the components are in place. However, Singer et al. point out that having these higher level component structures in place (essentially the Structural and Functional levels of integration) is not enough to achieve the true spirit of integration. I think this is the missing component in many studies of integration and in my experience this is also what I've seen can be extremely challenging about measuring whether a practice has truly "integrated" its program: it is very difficult to measure whether normative and interpersonal, and even process-level integration have truly occurred. 

Therefore, our hypothesis is that indeed, integration at *all 5 levels* of Singer's proposed typology is necessary to achieve full integration, and that therefore one would ideally study the implementation processes that contribute to each of these levels of integration. We would then link the degree to which integration occurred to patient and staff level outcomes. 

PROGOVAC - Assignment #3b - Measures & Evaluations:

***1. What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

To assess the intervention’s effectiveness, we employ quasi-experimental quantitative modeling (propensity-score weighted generalized estimating equations) of quality of primary wellness care and mental health care (process measures indicating high quality, such as adequate routine primary care; measures of utilization signaling low quality, such as hospitalizations for ambulatory care sensitive conditions), and outcome measures of physical health risk factors (cardiometabolic monitoring and reductions to BMI among appropriate patient groups). These analyses include longer-term evaluations of program effectiveness, as well as sub-group analysis based on level of patient engagement and sociodemographic variables. We are using the electronic health record (EHR) to capture these measures. 

  To assess the degree of integration (implementation outcome; this would include a number of sub-components), we combine observation and evaluation of program processes with in-depth interviews with clinician/staff and health system administrators, as well as patients as appropriate. We will map existing program elements with the five levels of integration described by Singer et al. We will then focus observational and qualitative data collection on understanding how specific selected constructs from the Consolidated Framework for Implementation Research11 (CFIR) promoted a greater degree of integration of the existing BHH program.  
 

***2. What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

One critical component of Singer et al.'s model is assessing the degree of process integration.

In the case of the BHH implementation, this would include a detailed understanding of how patients are jointly handled by the multidisciplinary care team at the BHH site, and how this site works to integrate care across other sites and settings (for example, what happens as part of the innovation when a patient is discharged from an inpatient stay). 



***3. Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

We intend to examine co-benefits or unintended consequences of the implementation, primarily at the patient level at this point by conducting sub-group analysis by race/ethnicity and gender to try to understand whether the program was equally engaging or impactful for all participants. 

Given that this study includes a qualitative component, we also anticipate that there will be unexpected findings about what may have been a co-benefit or unintended consequence. ";s:5:"xhtml";s:5876:"PROGOVAC - Assignment #3a - Models:<br /><br />***1. Which model or combination of models is most applicable to your proposed study and why?<br /><br />I have chosen the CFIR in combination with the Comprehensive Theory of Integration Singer et al. (2018) to apply to my proposed study.<br /><br />Many existing evaluations of integrated care programs do not delve deep enough into the extent to which program elements are fully integrated into care. This may in part be due to the fact that comprehensive theoretical models of integration (and when it has truly been achieved) have been lacking. Singer et al. (2018) posit in their Comprehensive Theory of Integration7 that we should distinguish and measure integration at five levels organized in 3 domains: structural and functional integration (organizational), interpersonal and normative integration (social features), and process integration.7 Implementation Science research methods have also been specifically called upon to help understand the nuance underlying mixed results from integrating mental health into primary care.8 Combining a comprehensive theory of integration with implementation science research methods, and specifically use of the CFIR, is need to help researchers and clinical stakeholders better study real-world BHH effectiveness, including for sub-groups4,9  as well as barriers/facilitators to sustaining and expanding safety net BHH models, in order to support better program implementation and dissemination. <br /><br /><br /><br />***2. How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />Both of these models are key components of study design and selection of measures, etc. <br /><br />Typically, we think of a model as being implemented when the components are in place. However, Singer et al. point out that having these higher level component structures in place (essentially the Structural and Functional levels of integration) is not enough to achieve the true spirit of integration. I think this is the missing component in many studies of integration and in my experience this is also what I&#039;ve seen can be extremely challenging about measuring whether a practice has truly &quot;integrated&quot; its program: it is very difficult to measure whether normative and interpersonal, and even process-level integration have truly occurred. <br /><br />Therefore, our hypothesis is that indeed, integration at *all 5 levels* of Singer&#039;s proposed typology is necessary to achieve full integration, and that therefore one would ideally study the implementation processes that contribute to each of these levels of integration. We would then link the degree to which integration occurred to patient and staff level outcomes. <br /><br />PROGOVAC - Assignment #3b - Measures &amp; Evaluations:<br /><br />***1. What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />To assess the intervention’s effectiveness, we employ quasi-experimental quantitative modeling (propensity-score weighted generalized estimating equations) of quality of primary wellness care and mental health care (process measures indicating high quality, such as adequate routine primary care; measures of utilization signaling low quality, such as hospitalizations for ambulatory care sensitive conditions), and outcome measures of physical health risk factors (cardiometabolic monitoring and reductions to BMI among appropriate patient groups). These analyses include longer-term evaluations of program effectiveness, as well as sub-group analysis based on level of patient engagement and sociodemographic variables. We are using the electronic health record (EHR) to capture these measures. <br /><br />  To assess the degree of integration (implementation outcome; this would include a number of sub-components), we combine observation and evaluation of program processes with in-depth interviews with clinician/staff and health system administrators, as well as patients as appropriate. We will map existing program elements with the five levels of integration described by Singer et al. We will then focus observational and qualitative data collection on understanding how specific selected constructs from the Consolidated Framework for Implementation Research11 (CFIR) promoted a greater degree of integration of the existing BHH program.  <br /> <br /><br />***2. What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />One critical component of Singer et al.&#039;s model is assessing the degree of process integration.<br /><br />In the case of the BHH implementation, this would include a detailed understanding of how patients are jointly handled by the multidisciplinary care team at the BHH site, and how this site works to integrate care across other sites and settings (for example, what happens as part of the innovation when a patient is discharged from an inpatient stay). <br /><br /><br /><br />***3. Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />We intend to examine co-benefits or unintended consequences of the implementation, primarily at the patient level at this point by conducting sub-group analysis by race/ethnicity and gender to try to understand whether the program was equally engaging or impactful for all participants. <br /><br />Given that this study includes a qualitative component, we also anticipate that there will be unexpected findings about what may have been a co-benefit or unintended consequence.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"5306f5a38c2e0f5244ae831721a1711c";}s:4:"show";b:1;s:3:"cid";s:32:"7a6d1020519397f05f8654c05feac1e4";}s:32:"ff72a363a715ef22d0d19dab39a6c87b";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"aprogovac";s:4:"name";s:12:"Ana Progovac";s:4:"mail";s:24:"aprogovac@challiance.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537557881;}s:3:"raw";s:1147:"Thanks Lindsay for all your thoughtful feedback! 

Yes, I am finding the format of the responses difficult because I have this laid out much more nicely in a word doc (with images), and it's hard to adapt this to text-only basis. The project I am working on now would *only* be in the Phase 2. Phase 1 is essentially completed, and Phase III is the "next phase." Essentially what I laid out in the Concept paper was a research arc. However I see how this is now confusing, and I appreciate your help making sure I link up these components more effectively.

I think your comment about fidelity is an important issue; BHH models as they are adopted now tend to share many components, but at the same time, many of the implementations are naturalistic and health settings and providers are not picking "off the shelf" models - and there is still a lot of work to be done in this area to determine what the key components really truly are. That is why what I'm proposing is in the Hybrid study stage. I think that's great feedback to think of this as a type of adaptation and link this with the Comprehensive Theory. 

Thank you for your feedback. 

";s:5:"xhtml";s:1209:"Thanks Lindsay for all your thoughtful feedback! <br /><br />Yes, I am finding the format of the responses difficult because I have this laid out much more nicely in a word doc (with images), and it&#039;s hard to adapt this to text-only basis. The project I am working on now would *only* be in the Phase 2. Phase 1 is essentially completed, and Phase III is the &quot;next phase.&quot; Essentially what I laid out in the Concept paper was a research arc. However I see how this is now confusing, and I appreciate your help making sure I link up these components more effectively.<br /><br />I think your comment about fidelity is an important issue; BHH models as they are adopted now tend to share many components, but at the same time, many of the implementations are naturalistic and health settings and providers are not picking &quot;off the shelf&quot; models - and there is still a lot of work to be done in this area to determine what the key components really truly are. That is why what I&#039;m proposing is in the Hybrid study stage. I think that&#039;s great feedback to think of this as a type of adaptation and link this with the Comprehensive Theory. <br /><br />Thank you for your feedback.";s:6:"parent";s:32:"a426d99f6b0b8ece080e0c3c6fe8e1de";s:7:"replies";a:1:{i:0;s:32:"2f4dbc8f2124f2bf2af6e94e5f5e5b82";}s:4:"show";b:1;s:3:"cid";s:32:"ff72a363a715ef22d0d19dab39a6c87b";}s:32:"0e331e9cae6f8a2e9e503d99e6763342";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"ipinsky";s:4:"name";s:12:"Ilana Pinsky";s:4:"mail";s:21:"pinskyilana@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537562938;}s:3:"raw";s:4419:"Pinsky- Assignment #3a - Models:
1.	Which model or combination of models is most applicable to your proposed study and why?
Our large U19 implementation study (PRIDE), where my TIDIRH study (IPC fidelity) is embedded uses CFIR and R-AIM as frameworks. For my IPC fidelity study, I am inclined to rely on the Dynamic Sustainability Framework, from Chambers et al. (2013) because it tackles into a vital point for our study, that is sustainability. This model proposes what I see as very  real-world based way to deal with continuous adaptation, considering that adjustment and refining programs as more effective than over emphasizing  fidelity to an initial protocol. 

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 
It makes a lot of sense to me the idea/hypothesis that DSF poses concerning change as a central influence on sustainability. In terms of processes, DSF lays out the idea that interventions can be continually improved, so that a process to monitor adaptations in a periodic way should be part of our study. I don’t feel DSF really proposes measures – I think it is a model that is still in the process of being tested. However, a few papers we read in the last TIDIRH model (fidelity/adaptation) can give ideas on how to organize methods to assess modifications (Rabin et al., 2018 and Stirman et al., 2013). 

Pinsky - Assignment #3b - Measures & Evaluations:
1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?
My primary implementation outcome is fidelity to IPC, but I also want to measure intervention adaptation as it relates to sustainability, as I commented above. In what it concerns fidelity, we are in the process of defining the core components of our intervention, as there seems to be variation of this topic in the field, even if IPC has been studied for a while. We are doing this by working with our main IPC consultant, that is a leader in the field and has been our main trainer in Mozambique at this point. We are trying to set up a fidelity process that will be used not only at this first stage, when external consultants are training the Moz team, but that could also be applied moving forward, when this team being trained will become the country’s trainers. Ideally, fidelity would not only be measured by self-report (but direct observation and other methods may not be feasible). Parallel to the fidelity/adaptation measures, a new development in our study that should assist in the process is that we are developing IPC scripts  to be used in tablets. The idea is guide IPC delivery with the use of the tablets, which can also be used to facilitate supervision. 

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?
Processes that could be measured could include the monitoring of variance/changes in IPC delivery by levels (clinics, districts, provinces). The Moz team that is being trained now will be tasked with training several districts in their provinces and it is very possible that adaptations are different by province or even by geographical area (more rural vs more urban settings). We are also measuring several provider/clinic level questions concerning training and delivery of IPC interventions. These measures include barriers/facilitators, knowledge and beliefs about the intervention, self-efficacy, attitudes, motivations. These variables will probably be measured qualitatively by semi-structured interviews and possibly focus groups. We will also be measuring engagement/buy in in several different levels as well (client, provider, clinic, district, province).

3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?
As this is basically the implementation of a new system of care in a country, we anticipate that will be several co-benefits and unintended consequences. In terms of Implementation measures, we will be collecting information from several sources and levels continuously, using semi-structure interviews, the SIC as well as other outcome implementation measures related to reach, adoption, etc. 
";s:5:"xhtml";s:4496:"Pinsky- Assignment #3a - Models:<br />1.	Which model or combination of models is most applicable to your proposed study and why?<br />Our large U19 implementation study (PRIDE), where my TIDIRH study (IPC fidelity) is embedded uses CFIR and R-AIM as frameworks. For my IPC fidelity study, I am inclined to rely on the Dynamic Sustainability Framework, from Chambers et al. (2013) because it tackles into a vital point for our study, that is sustainability. This model proposes what I see as very  real-world based way to deal with continuous adaptation, considering that adjustment and refining programs as more effective than over emphasizing  fidelity to an initial protocol. <br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br />It makes a lot of sense to me the idea/hypothesis that DSF poses concerning change as a central influence on sustainability. In terms of processes, DSF lays out the idea that interventions can be continually improved, so that a process to monitor adaptations in a periodic way should be part of our study. I don’t feel DSF really proposes measures – I think it is a model that is still in the process of being tested. However, a few papers we read in the last TIDIRH model (fidelity/adaptation) can give ideas on how to organize methods to assess modifications (Rabin et al., 2018 and Stirman et al., 2013). <br /><br />Pinsky - Assignment #3b - Measures &amp; Evaluations:<br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br />My primary implementation outcome is fidelity to IPC, but I also want to measure intervention adaptation as it relates to sustainability, as I commented above. In what it concerns fidelity, we are in the process of defining the core components of our intervention, as there seems to be variation of this topic in the field, even if IPC has been studied for a while. We are doing this by working with our main IPC consultant, that is a leader in the field and has been our main trainer in Mozambique at this point. We are trying to set up a fidelity process that will be used not only at this first stage, when external consultants are training the Moz team, but that could also be applied moving forward, when this team being trained will become the country’s trainers. Ideally, fidelity would not only be measured by self-report (but direct observation and other methods may not be feasible). Parallel to the fidelity/adaptation measures, a new development in our study that should assist in the process is that we are developing IPC scripts  to be used in tablets. The idea is guide IPC delivery with the use of the tablets, which can also be used to facilitate supervision. <br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br />Processes that could be measured could include the monitoring of variance/changes in IPC delivery by levels (clinics, districts, provinces). The Moz team that is being trained now will be tasked with training several districts in their provinces and it is very possible that adaptations are different by province or even by geographical area (more rural vs more urban settings). We are also measuring several provider/clinic level questions concerning training and delivery of IPC interventions. These measures include barriers/facilitators, knowledge and beliefs about the intervention, self-efficacy, attitudes, motivations. These variables will probably be measured qualitatively by semi-structured interviews and possibly focus groups. We will also be measuring engagement/buy in in several different levels as well (client, provider, clinic, district, province).<br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br />As this is basically the implementation of a new system of care in a country, we anticipate that will be several co-benefits and unintended consequences. In terms of Implementation measures, we will be collecting information from several sources and levels continuously, using semi-structure interviews, the SIC as well as other outcome implementation measures related to reach, adoption, etc.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"b3b3b27cd65c546dd44ac300fca38621";}s:4:"show";b:1;s:3:"cid";s:32:"0e331e9cae6f8a2e9e503d99e6763342";}s:32:"f3ed431fdaaa8baff95f89d35dd8d741";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"kokamura";s:4:"name";s:14:"Kelsie Okamura";s:4:"mail";s:26:"kelsie.h.okamura@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537597841;}s:3:"raw";s:4871:"OKAMURA - Assignment #3a - Models:

1.	Which model or combination of models is most applicable to your proposed study and why?
After learning more about the types of implementation models and theories, I realized that our study is using a determinant framework, classic theory, and evaluation framework for assessing the implementation process. It also helped me to understand that our study is really an observational study on the process and evaluation of the implementation of MBC in our existing service system. 

The determinant framework that guides our conceptualization and categorization of implementation strategies is the Consolidated Framework for Implementation Research. Of the five domains, we are particularly interested in the inner setting (i.e., organizational culture and readiness for change) and the individuals involved (i.e., knowledge, beliefs, attitudes, and self-efficacy). This determinant framework is guided by an adaptation to classic theory, Rogers’ Diffusion of Innovations Theory, that integrates organizational context into individual-level determinants (Mandell, Okamura, & Fishman, 2017). For the evaluation, I plan to use the Stages of Implementation Change model to assess stages of implementation through the pre-implementation, implementation, and sustainability phases.

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?
The CFIR in combination with the Diffusion of Innovations theory will guide measurement selection of organizational and individual level determinants of implementation adoption. The Stages of Implementation Change will also aid in organizing and observing implementation strategies. Since conceptualizing this as an observational study, examining the effectiveness of a specific implementation strategy may not be a primary aim of the study. Instead, it might be more meaningful to describe the implementation process and examine predictive validity of organizational and individual constructs. 

Assignment #3b - Measures & Evaluations:
3.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?
The implementation outcome of adoption will be measured through frequency in utilizing dashboards during monthly case reviews for youth. These data will be observed through timeliness queries and clicks within the EHR. An additional layer of measurement will be done in a subset of cases through observation in monthly utilization management meetings. Our service and client outcomes will be examined in aggregate by family guidance center to examine changes in symptoms, functioning, crises and sentinel events, and use of practices derived from the evidence-base. 

4.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?
Implementation strategies will be observed and coded to align with the Stages of Implementation Change. Additionally, organizational and individual direct service provider determinants will be measured. Specifically, we are proposing to use the Organizational Readiness for Change measure to examine organizational motivation for changes, resources, and staff attributes that might affect adoption of an innovation. Additionally, building off previous work examining individual level determinants of MBC within our system of care, we will use the Attitudes toward Standardized Assessment Scale to measure attitudes toward MBC. Furthermore, the Maslach Burnout Inventory will be administered to assess provider burnout related to exhaustion, cynicism, and professional efficacy. These organizational and individual measures will be administered at pre and post-implementation to examine predictive validity to our outcome of adoption.

5.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?
One area that I believe might be an unintended consequence of implementation is team-cohesiveness and managerial support. This is one area that we are beginning to take a closer look to the literature about given that much of our workflows around MBC require teamwork across multiple roles within a family guidance center. We are also interested in utilizing the System Usability Scale (self-report measure of ease in using an electronic health record system) at multiple time points during pre-implementation (as the system is rolled out to our divisions) to assess readiness for just the IT/EHR system itself (which may be used as a moderator in future analyses).

**sorry for the delay on my end on this assignment. It's early Saturday morning on the east coast right now. ";s:5:"xhtml";s:4983:"OKAMURA - Assignment #3a - Models:<br /><br />1.	Which model or combination of models is most applicable to your proposed study and why?<br />After learning more about the types of implementation models and theories, I realized that our study is using a determinant framework, classic theory, and evaluation framework for assessing the implementation process. It also helped me to understand that our study is really an observational study on the process and evaluation of the implementation of MBC in our existing service system. <br /><br />The determinant framework that guides our conceptualization and categorization of implementation strategies is the Consolidated Framework for Implementation Research. Of the five domains, we are particularly interested in the inner setting (i.e., organizational culture and readiness for change) and the individuals involved (i.e., knowledge, beliefs, attitudes, and self-efficacy). This determinant framework is guided by an adaptation to classic theory, Rogers’ Diffusion of Innovations Theory, that integrates organizational context into individual-level determinants (Mandell, Okamura, &amp; Fishman, 2017). For the evaluation, I plan to use the Stages of Implementation Change model to assess stages of implementation through the pre-implementation, implementation, and sustainability phases.<br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br />The CFIR in combination with the Diffusion of Innovations theory will guide measurement selection of organizational and individual level determinants of implementation adoption. The Stages of Implementation Change will also aid in organizing and observing implementation strategies. Since conceptualizing this as an observational study, examining the effectiveness of a specific implementation strategy may not be a primary aim of the study. Instead, it might be more meaningful to describe the implementation process and examine predictive validity of organizational and individual constructs. <br /><br />Assignment #3b - Measures &amp; Evaluations:<br />3.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br />The implementation outcome of adoption will be measured through frequency in utilizing dashboards during monthly case reviews for youth. These data will be observed through timeliness queries and clicks within the EHR. An additional layer of measurement will be done in a subset of cases through observation in monthly utilization management meetings. Our service and client outcomes will be examined in aggregate by family guidance center to examine changes in symptoms, functioning, crises and sentinel events, and use of practices derived from the evidence-base. <br /><br />4.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br />Implementation strategies will be observed and coded to align with the Stages of Implementation Change. Additionally, organizational and individual direct service provider determinants will be measured. Specifically, we are proposing to use the Organizational Readiness for Change measure to examine organizational motivation for changes, resources, and staff attributes that might affect adoption of an innovation. Additionally, building off previous work examining individual level determinants of MBC within our system of care, we will use the Attitudes toward Standardized Assessment Scale to measure attitudes toward MBC. Furthermore, the Maslach Burnout Inventory will be administered to assess provider burnout related to exhaustion, cynicism, and professional efficacy. These organizational and individual measures will be administered at pre and post-implementation to examine predictive validity to our outcome of adoption.<br /><br />5.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br />One area that I believe might be an unintended consequence of implementation is team-cohesiveness and managerial support. This is one area that we are beginning to take a closer look to the literature about given that much of our workflows around MBC require teamwork across multiple roles within a family guidance center. We are also interested in utilizing the System Usability Scale (self-report measure of ease in using an electronic health record system) at multiple time points during pre-implementation (as the system is rolled out to our divisions) to assess readiness for just the IT/EHR system itself (which may be used as a moderator in future analyses).<br /><br />**sorry for the delay on my end on this assignment. It&#039;s early Saturday morning on the east coast right now.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"9da0d757ff5955cfc4d410e529276de8";}s:4:"show";b:1;s:3:"cid";s:32:"f3ed431fdaaa8baff95f89d35dd8d741";}s:32:"f945b108b44cad0bdcd2c9f4a0f256f7";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537834196;}s:3:"raw";s:104:"I'm glad my thoughts may be helpful to you, Brenna! 
I am really enjoying learning more about your work.";s:5:"xhtml";s:114:"I&#039;m glad my thoughts may be helpful to you, Brenna! <br />I am really enjoying learning more about your work.";s:6:"parent";s:32:"a6893943fa4d437bfd2d9e21d1051203";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"f945b108b44cad0bdcd2c9f4a0f256f7";}s:32:"580187dbf466d911d2b0f7d2a940fd1e";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537834267;}s:3:"raw";s:25:"Got 'em! Thanks Brenna :)";s:5:"xhtml";s:30:"Got &#039;em! Thanks Brenna :)";s:6:"parent";s:32:"f8ae21f531c01299b099d75288e7fdc9";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"580187dbf466d911d2b0f7d2a940fd1e";}s:32:"2f4dbc8f2124f2bf2af6e94e5f5e5b82";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537834970;}s:3:"raw";s:632:"Hi Ana,

Yes, the text-only basis would typically be hard for me too, as I really like tables and figures. But, sometimes I may rely on these visuals too much. The text format may actually be helpful if you have a couple spots that could be made clearer, like I often do :)

I am really excited about your use of the Comprehensive Theory for this EBI. If you can strengthen your link to the areas of fidelity/adaptation, which implementation research reviewers will expect to see, then it will better highlight the potential contribution of your work for advancing implementation science. And that, is a very good thing. Great work!";s:5:"xhtml";s:652:"Hi Ana,<br /><br />Yes, the text-only basis would typically be hard for me too, as I really like tables and figures. But, sometimes I may rely on these visuals too much. The text format may actually be helpful if you have a couple spots that could be made clearer, like I often do :)<br /><br />I am really excited about your use of the Comprehensive Theory for this EBI. If you can strengthen your link to the areas of fidelity/adaptation, which implementation research reviewers will expect to see, then it will better highlight the potential contribution of your work for advancing implementation science. And that, is a very good thing. Great work!";s:6:"parent";s:32:"ff72a363a715ef22d0d19dab39a6c87b";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"2f4dbc8f2124f2bf2af6e94e5f5e5b82";}s:32:"f8d59cf403112e302b19178218fea91c";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"aweaver";s:4:"name";s:12:"Addie Weaver";s:4:"mail";s:18:"weaverad@umich.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537982984;}s:3:"raw";s:5036:"WEAVER - Assignment #3

Assignment #3a - Models:

1.	Which model or combination of models is most applicable to your proposed study and why?

The Theoretical Domains Framework (TDF) was selected as it aligns with the primary focus of this project, which relates to individual behavior change of WIC staff members who would be initiating PPD screening. Providing a theoretical lens for viewing cognitive, affective, social, and environmental influences on behavior, the TDF provides insight to both individual-level and organizational-level factors. Although there has been some criticism of the TDF as adequately addressing organizational-level factors, they are captured in the theoretical domains focused on Environmental Context and Resources, Social Influences, Social/Professional Role and Identity, and Behavioral Regulation. Additionally, the TDF offers a versatile framework that has been effectively utilized to address a range of implementation issues. Therefore, the TDF is relevant for guiding aspects of all three aims of this project: assessing implementation barriers and facilitators while providing a structure for analysis/coding; informing the identification and development of appropriate behavioral change techniques; and exploring potential mechanisms of change. 

After reviewing the modules, I am also considering whether using both the TDF and Consolidating Framework for Implementation Research (CFIR) would strengthen the proposal. CFIR’s focus on context (e.g., intervention characteristics; inner setting; outer setting) and process (in addition to individual characteristics) may complement TDF and address some criticisms of the framework. Though using the CFIR may also increase the scope of the project, as it may necessitate focusing on multiple stakeholder groups (e.g., clients; administrators), in addition to the frontline staff (e.g., nutritionists and nurses) who would be administering the PPD screening.

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

As noted above, the TDF is relevant for informing all three aims of this project. Specifically, using the TDF to assess implementation barriers and facilitators related to screening for postpartum depression in rural WIC clinics will allow us to identify targets for behavior change. These targets will inform the implementation strategies we select to facilitate behavior change and initiate screening for postpartum depression as standard practice in these settings. Additionally, we will select measures to assess our behavior change targets in order to see whether our implementation strategies have led to change in these domains over time (e.g., change in knowledge of postpartum depression). Analyses of this data will inform the need for further intervention (e.g., booster sessions; targeting different behavior change domains; improved integration of screening in work flow) to maximize uptake of screening among staff as well as the acceptability and sustainability of screening as standard practice. 

Assignment #3b - Measures & Evaluations:

1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

As the goal of this intervention is to initiate screening for PPD within rural WIC clinics in Michigan, the primary outcome is to assess the change in PPD screening rates. This will allow us to understand if the intervention impacted provider behavior. Given the importance connecting rural women who screen positive for PPD with appropriate treatment, the change in the number of mental health referrals would be a secondary outcome. Additionally, client refusal rates for PPD screening is another important secondary outcome that informs our understanding of the acceptability of providing PPD screening in this non-healthcare setting. We will also assess the change in WIC staff members’ knowledge, attitude, and behaviors as related to TDF domains guiding the implementation strategies.

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

Process outcomes of interest would be staff-identified barriers and facilitators to implementing the PPD screening tool. Understanding these process barriers and facilitators will allow for identifying additional interventions that may be necessary to integrate PPD screening as part of standard services/care within rural WIC clinics and will also inform potential refinements or changes to the intervention strategy that will improve uptake and better prepare for scaling up.

3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

We do not plan to assess any potential co-benefits or unintended consequences of the implementation at this time. 
";s:5:"xhtml";s:5168:"WEAVER - Assignment #3<br /><br />Assignment #3a - Models:<br /><br />1.	Which model or combination of models is most applicable to your proposed study and why?<br /><br />The Theoretical Domains Framework (TDF) was selected as it aligns with the primary focus of this project, which relates to individual behavior change of WIC staff members who would be initiating PPD screening. Providing a theoretical lens for viewing cognitive, affective, social, and environmental influences on behavior, the TDF provides insight to both individual-level and organizational-level factors. Although there has been some criticism of the TDF as adequately addressing organizational-level factors, they are captured in the theoretical domains focused on Environmental Context and Resources, Social Influences, Social/Professional Role and Identity, and Behavioral Regulation. Additionally, the TDF offers a versatile framework that has been effectively utilized to address a range of implementation issues. Therefore, the TDF is relevant for guiding aspects of all three aims of this project: assessing implementation barriers and facilitators while providing a structure for analysis/coding; informing the identification and development of appropriate behavioral change techniques; and exploring potential mechanisms of change. <br /><br />After reviewing the modules, I am also considering whether using both the TDF and Consolidating Framework for Implementation Research (CFIR) would strengthen the proposal. CFIR’s focus on context (e.g., intervention characteristics; inner setting; outer setting) and process (in addition to individual characteristics) may complement TDF and address some criticisms of the framework. Though using the CFIR may also increase the scope of the project, as it may necessitate focusing on multiple stakeholder groups (e.g., clients; administrators), in addition to the frontline staff (e.g., nutritionists and nurses) who would be administering the PPD screening.<br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />As noted above, the TDF is relevant for informing all three aims of this project. Specifically, using the TDF to assess implementation barriers and facilitators related to screening for postpartum depression in rural WIC clinics will allow us to identify targets for behavior change. These targets will inform the implementation strategies we select to facilitate behavior change and initiate screening for postpartum depression as standard practice in these settings. Additionally, we will select measures to assess our behavior change targets in order to see whether our implementation strategies have led to change in these domains over time (e.g., change in knowledge of postpartum depression). Analyses of this data will inform the need for further intervention (e.g., booster sessions; targeting different behavior change domains; improved integration of screening in work flow) to maximize uptake of screening among staff as well as the acceptability and sustainability of screening as standard practice. <br /><br />Assignment #3b - Measures &amp; Evaluations:<br /><br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />As the goal of this intervention is to initiate screening for PPD within rural WIC clinics in Michigan, the primary outcome is to assess the change in PPD screening rates. This will allow us to understand if the intervention impacted provider behavior. Given the importance connecting rural women who screen positive for PPD with appropriate treatment, the change in the number of mental health referrals would be a secondary outcome. Additionally, client refusal rates for PPD screening is another important secondary outcome that informs our understanding of the acceptability of providing PPD screening in this non-healthcare setting. We will also assess the change in WIC staff members’ knowledge, attitude, and behaviors as related to TDF domains guiding the implementation strategies.<br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />Process outcomes of interest would be staff-identified barriers and facilitators to implementing the PPD screening tool. Understanding these process barriers and facilitators will allow for identifying additional interventions that may be necessary to integrate PPD screening as part of standard services/care within rural WIC clinics and will also inform potential refinements or changes to the intervention strategy that will improve uptake and better prepare for scaling up.<br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />We do not plan to assess any potential co-benefits or unintended consequences of the implementation at this time.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"f8d59cf403112e302b19178218fea91c";}s:32:"9da0d757ff5955cfc4d410e529276de8";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dpintello";s:4:"name";s:14:"Denny Pintello";s:4:"mail";s:23:"Denise.Pintello@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538167005;}s:3:"raw";s:2468:"(3a) MODELS
Kelsie: It is great that you have clearly thought about the utility of applying various theoretical approaches to frame your study. In reviewing your prior responses, I still remain unclear as to what is your specific research question. I believe that it might focus on “can a state-wide system implement MBC with fidelity that will result in a reduction of symptoms among youth?” If this is in the ballpark, then you may want to consider a model that also includes the system level, such as the “Conceptual Model of Implementation Research.” CFIR is a strong model, yet does not take into account the system level. If you keep CFIR, then you are limiting your primary focus on an organizational level, and would miss out on capturing the complexity of systemic level factors. Just food for thought. 

(3b) MEASURES AND EVALUATIONS
Kelsie: This is a nice description about the use of the dashboard and the specific instruments (ORC and Maslach Burnout Inv that you plan to use. Again – if I knew your research question, it would help me track how you are using these approaches that would link the theoretical model to the variables that you are measuring, how the dashboard, ORC and the MBI are measuring your mediators, and how these are associated with your mental health outcomes. I was thinking about what might be a good diagram for what I am trying to convey and remembered David Mandell’s PPT from the 2017 D&I Conference. The great thing is that you are cited on this 😊 and are very familiar with this framework! This is perfect because the figures in slide #17 might help.
 
So – what would be helpful for me (and I suspect for Nick and Lindsey) is for you to map your study onto a similar visual framework – and then link how the various implementation strategies (spell these out) will manipulate knowledge, attitudes (measured by ORC and MBI) that in turn, will influence the ‘intention’ of staff to implement MBC with fidelity. I hope this makes sense – and it would really help us in providing meaningful feedback for you.  

Regarding benefits and consequences – I like the idea of tracking team cohesiveness and managerial support. Consequences to consider would include various levels of staff or client level resistance to the program (attitudes and behaviors) that may contribute to program drift. Using the System Usability Scale is good, yet a qualitative approach to dive deeper is something to consider. 
";s:5:"xhtml";s:2510:"(3a) MODELS<br />Kelsie: It is great that you have clearly thought about the utility of applying various theoretical approaches to frame your study. In reviewing your prior responses, I still remain unclear as to what is your specific research question. I believe that it might focus on “can a state-wide system implement MBC with fidelity that will result in a reduction of symptoms among youth?” If this is in the ballpark, then you may want to consider a model that also includes the system level, such as the “Conceptual Model of Implementation Research.” CFIR is a strong model, yet does not take into account the system level. If you keep CFIR, then you are limiting your primary focus on an organizational level, and would miss out on capturing the complexity of systemic level factors. Just food for thought. <br /><br />(3b) MEASURES AND EVALUATIONS<br />Kelsie: This is a nice description about the use of the dashboard and the specific instruments (ORC and Maslach Burnout Inv that you plan to use. Again – if I knew your research question, it would help me track how you are using these approaches that would link the theoretical model to the variables that you are measuring, how the dashboard, ORC and the MBI are measuring your mediators, and how these are associated with your mental health outcomes. I was thinking about what might be a good diagram for what I am trying to convey and remembered David Mandell’s PPT from the 2017 D&amp;I Conference. The great thing is that you are cited on this 😊 and are very familiar with this framework! This is perfect because the figures in slide #17 might help.<br /> <br />So – what would be helpful for me (and I suspect for Nick and Lindsey) is for you to map your study onto a similar visual framework – and then link how the various implementation strategies (spell these out) will manipulate knowledge, attitudes (measured by ORC and MBI) that in turn, will influence the ‘intention’ of staff to implement MBC with fidelity. I hope this makes sense – and it would really help us in providing meaningful feedback for you.  <br /><br />Regarding benefits and consequences – I like the idea of tracking team cohesiveness and managerial support. Consequences to consider would include various levels of staff or client level resistance to the program (attitudes and behaviors) that may contribute to program drift. Using the System Usability Scale is good, yet a qualitative approach to dive deeper is something to consider.";s:6:"parent";s:32:"f3ed431fdaaa8baff95f89d35dd8d741";s:7:"replies";a:1:{i:0;s:32:"1dde5abb12cd0153f6e774f83ddbdb9f";}s:4:"show";b:1;s:3:"cid";s:32:"9da0d757ff5955cfc4d410e529276de8";}s:32:"5306f5a38c2e0f5244ae831721a1711c";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dpintello";s:4:"name";s:14:"Denny Pintello";s:4:"mail";s:23:"Denise.Pintello@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538167331;}s:3:"raw";s:2259:"(3a) MODELS
Ana: In reading about your proposed study, I was intrigued by the innovation of the ‘reverse integration approach’, yet also echo both Nick and Lindsey’s prior feedback that this has the potential to be quite thorny and found myself concerned that the complexity may bog down the ability to accurate test what you are proposing. Singer’s comprehensive theory of integration covers quite a bit of ground and there is much to like about CFIR, yet I believe that you are also interested in testing system-level factors. If yes – then you may want to consider a model that also includes the system level, such as the “Conceptual Model of Implementation Research.” If you are not, never mind 😊. As an FYI – many of us at NIMH love parsimony, so having one theoretical framework and/or one model to guide your study goes a long way. 

Regarding how your model will inform other aspects - I would love to know more about what specific components (driven by your theory) are you measuring, how, what are the mediators you are planning to study (or manipulate) at each of the five levels, and how these are associated with the proximal and distal mental health outcomes.

(3b) MEASURES AND EVALUATIONS
Ana – I really appreciate the way you frame your assessment of both the effectiveness of the intervention as well as the degree of integration. As a hybrid II – this is right on target. I would like to see more specificity regarding additional quantitative measures that you are thinking about for this study. Your outcomes are clear, yet I do not see anything here on mental health or SMI symptoms – which if you were thinking about submitting this to NIMH – we would require. Also at NIMH, we require that all D&I studies include data on clinical outcomes so that we know that your beautifully implemented study actually reduces symptoms or improves overall MH functioning. I am also glad to see that there may be an exploratory nature to your study by including qualitative methods to examine unintended consequences and benefits. Lastly, I hope you get a chance to look at Gail Daumit’s work on SMI and cardiovascular risk. She is also a leader in D&I research field and your interests seem to dovetail nicely with hers. ";s:5:"xhtml";s:2296:"(3a) MODELS<br />Ana: In reading about your proposed study, I was intrigued by the innovation of the ‘reverse integration approach’, yet also echo both Nick and Lindsey’s prior feedback that this has the potential to be quite thorny and found myself concerned that the complexity may bog down the ability to accurate test what you are proposing. Singer’s comprehensive theory of integration covers quite a bit of ground and there is much to like about CFIR, yet I believe that you are also interested in testing system-level factors. If yes – then you may want to consider a model that also includes the system level, such as the “Conceptual Model of Implementation Research.” If you are not, never mind 😊. As an FYI – many of us at NIMH love parsimony, so having one theoretical framework and/or one model to guide your study goes a long way. <br /><br />Regarding how your model will inform other aspects - I would love to know more about what specific components (driven by your theory) are you measuring, how, what are the mediators you are planning to study (or manipulate) at each of the five levels, and how these are associated with the proximal and distal mental health outcomes.<br /><br />(3b) MEASURES AND EVALUATIONS<br />Ana – I really appreciate the way you frame your assessment of both the effectiveness of the intervention as well as the degree of integration. As a hybrid II – this is right on target. I would like to see more specificity regarding additional quantitative measures that you are thinking about for this study. Your outcomes are clear, yet I do not see anything here on mental health or SMI symptoms – which if you were thinking about submitting this to NIMH – we would require. Also at NIMH, we require that all D&amp;I studies include data on clinical outcomes so that we know that your beautifully implemented study actually reduces symptoms or improves overall MH functioning. I am also glad to see that there may be an exploratory nature to your study by including qualitative methods to examine unintended consequences and benefits. Lastly, I hope you get a chance to look at Gail Daumit’s work on SMI and cardiovascular risk. She is also a leader in D&amp;I research field and your interests seem to dovetail nicely with hers.";s:6:"parent";s:32:"7a6d1020519397f05f8654c05feac1e4";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"5306f5a38c2e0f5244ae831721a1711c";}s:32:"ea0a11f7afbf4cf3c3ff71baa6803ef6";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"nbowersox";s:4:"name";s:13:"Nick Bowersox";s:4:"mail";s:24:"Nicholas.Bowersox@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538184614;}s:3:"raw";s:6256:"Brenna - 

Thank you for this explanation of some aspects of your planned project.  Here are my thoughts:

1. Use of Proctor's Model: this seems reasonable, given your focus on EBT-based intervention and especially your interest in the evaluation of implementation impact.  What I like about Proctor's model is the attention it gives to a multitude of impact areas, as well as separating out the various impacts on the system, process, and patients.  This really allows for a clear demonstration of program (and implementation) impact as well as generating information that can be useful to engage stakeholders and assess program fidelity.  However, it seems a bit light in terms of the focus on implementation strategies and different aspects/techniques that may be useful to support program adoption (e.g., toolkit development, training and monitoring, assessment of potential implementation barriers, development of program champions, etc), so you will need to be sure that these areas are well-developed beyond what is represented in the model.  Full disclosure - I am not an expert in this model, so it may be that these areas are well-described but not formally included in the core figures and conceptual models involved with this approach.
2. How the model will inform your project:  I agree with your perspective - Proctor's model appears to have a heavy program evaluation focus, which can be a very good thing.  It will direct you to consider program impacts in multiple areas and to more fully understand the ways that the program changes the care environment and care process than other models which are more focused on program adoption over program effect.  However, as stated in comment #1, it may be useful to also consider models which focus on program adoption methods (e.g., practice facilitation) to help to develop the spread/adoption aspects of your program in addition to Proctor's model which focuses evaluation.
3. A point of clarification related to your project: your initial description of the project (from Assignment #1) suggested that there are two major challenges related to the effective delivery of care to adults with ASD: clinicians not accepting them as patients due to a lack of confidence/perceived lack of adequate training, and clinicians accepting them but not delivering high-quality treatments that are a good fit to the needs of ASD patients.  Does your project attempt to focus only on this second point, so that the goal is for providers to do a better job with the ASD patients they would be normally accepting into their practice, or to both improve the quality of therapy provided to ASD patients as well as increase the likelihood that ASD patients will be offered therapy in the first place?  IF you have an interest in this second aspect, you might want to expand your outcomes assessment to include intake/acceptance rates or consider number of sessions/frequency of suboptimal number of sessions that would not meet the standard for your EBT.  Similarly, if you are interested in this comfort piece from the perspective of the provider, it might be interesting to assess provider confidence in working with adults with ASD, perhaps before and after their participation in the training. (I see that this is mentioned later in your response)
4. Another minor point of clarification: your fidelity and adoption metrics are based on the delivery of ASD EBT, not just CBT more broadly defined, correct?  This will be an important distinction to make when describing and evaluating your project.
5. I would be interested to hear more specifics about how you are operationalizing "fidelity" to your treatment - is this based on counts of number of sessions, or the inclusion of specific therapeutic techniques within the progress notes of sessions, or the discussion of particular content areas in session, etc?  Similarly, will each session by judged to meet expectations or not on a global level, or will there be a gradient to determine fidelity?  In a broader scope, what sorts of measures of fidelity will you use across multiple therapy sessions (e.g., number of sessions attended, gaps between sessions, level of fidelity of individual sessions) and how will providers be "graded" in terms of their fidelity to expectations?
6. Will you have a corrective mechanism in place to respond to provider who are not delivering the intervention with a high level of fidelity?  If so, what will this look like?  Will it be triggered by concerns raised by the provider or via external monitoring of performance?
7. You mention an interest in addressing provider attitudes related to working with persons with ASD.  What sorts of interventions are you planning to address these attitudes?  I'm thinking that something along the lines of the intervention to break through stigma and reduce social distance (such as some of the work by Corrigan et al) could be one approach you could consider, by providing education and/or information on the perspectives of persons with ASD related to treatment, could help to break through these barriers a bit and increase the likelihood that providers will be open to take on patients with ASD.
8.  I would be interested to hear a bit more about your plans to assess clinician skills in delivering the intervention and the extent to which external expert review versus clinical self-report will inform this process.
9. Your discussion of unintended benefits seems sensible to me.  I was also thinking that this issue of a lack of comfort in working with ASD patients may extend beyond therapy settings to other care areas (e.g., Primary Care) and could contribute to under-treatment in these environments as well.  It might be interesting to see if the therapists can affect the care that is delivered in these other settings via advocacy for the patient as well as (potentially) providing some education to other providers.  This idea makes sense in an integrated care setting such as the VA, but may be less likely in other settings where there is less direct interface/communication between providers.

All in all, this sounds like an exciting and well-thought-out project.  Please let me know if you would like to discuss any of the above comments further.

Best,

- Nick";s:5:"xhtml";s:6391:"Brenna - <br /><br />Thank you for this explanation of some aspects of your planned project.  Here are my thoughts:<br /><br />1. Use of Proctor&#039;s Model: this seems reasonable, given your focus on EBT-based intervention and especially your interest in the evaluation of implementation impact.  What I like about Proctor&#039;s model is the attention it gives to a multitude of impact areas, as well as separating out the various impacts on the system, process, and patients.  This really allows for a clear demonstration of program (and implementation) impact as well as generating information that can be useful to engage stakeholders and assess program fidelity.  However, it seems a bit light in terms of the focus on implementation strategies and different aspects/techniques that may be useful to support program adoption (e.g., toolkit development, training and monitoring, assessment of potential implementation barriers, development of program champions, etc), so you will need to be sure that these areas are well-developed beyond what is represented in the model.  Full disclosure - I am not an expert in this model, so it may be that these areas are well-described but not formally included in the core figures and conceptual models involved with this approach.<br />2. How the model will inform your project:  I agree with your perspective - Proctor&#039;s model appears to have a heavy program evaluation focus, which can be a very good thing.  It will direct you to consider program impacts in multiple areas and to more fully understand the ways that the program changes the care environment and care process than other models which are more focused on program adoption over program effect.  However, as stated in comment #1, it may be useful to also consider models which focus on program adoption methods (e.g., practice facilitation) to help to develop the spread/adoption aspects of your program in addition to Proctor&#039;s model which focuses evaluation.<br />3. A point of clarification related to your project: your initial description of the project (from Assignment #1) suggested that there are two major challenges related to the effective delivery of care to adults with ASD: clinicians not accepting them as patients due to a lack of confidence/perceived lack of adequate training, and clinicians accepting them but not delivering high-quality treatments that are a good fit to the needs of ASD patients.  Does your project attempt to focus only on this second point, so that the goal is for providers to do a better job with the ASD patients they would be normally accepting into their practice, or to both improve the quality of therapy provided to ASD patients as well as increase the likelihood that ASD patients will be offered therapy in the first place?  IF you have an interest in this second aspect, you might want to expand your outcomes assessment to include intake/acceptance rates or consider number of sessions/frequency of suboptimal number of sessions that would not meet the standard for your EBT.  Similarly, if you are interested in this comfort piece from the perspective of the provider, it might be interesting to assess provider confidence in working with adults with ASD, perhaps before and after their participation in the training. (I see that this is mentioned later in your response)<br />4. Another minor point of clarification: your fidelity and adoption metrics are based on the delivery of ASD EBT, not just CBT more broadly defined, correct?  This will be an important distinction to make when describing and evaluating your project.<br />5. I would be interested to hear more specifics about how you are operationalizing &quot;fidelity&quot; to your treatment - is this based on counts of number of sessions, or the inclusion of specific therapeutic techniques within the progress notes of sessions, or the discussion of particular content areas in session, etc?  Similarly, will each session by judged to meet expectations or not on a global level, or will there be a gradient to determine fidelity?  In a broader scope, what sorts of measures of fidelity will you use across multiple therapy sessions (e.g., number of sessions attended, gaps between sessions, level of fidelity of individual sessions) and how will providers be &quot;graded&quot; in terms of their fidelity to expectations?<br />6. Will you have a corrective mechanism in place to respond to provider who are not delivering the intervention with a high level of fidelity?  If so, what will this look like?  Will it be triggered by concerns raised by the provider or via external monitoring of performance?<br />7. You mention an interest in addressing provider attitudes related to working with persons with ASD.  What sorts of interventions are you planning to address these attitudes?  I&#039;m thinking that something along the lines of the intervention to break through stigma and reduce social distance (such as some of the work by Corrigan et al) could be one approach you could consider, by providing education and/or information on the perspectives of persons with ASD related to treatment, could help to break through these barriers a bit and increase the likelihood that providers will be open to take on patients with ASD.<br />8.  I would be interested to hear a bit more about your plans to assess clinician skills in delivering the intervention and the extent to which external expert review versus clinical self-report will inform this process.<br />9. Your discussion of unintended benefits seems sensible to me.  I was also thinking that this issue of a lack of comfort in working with ASD patients may extend beyond therapy settings to other care areas (e.g., Primary Care) and could contribute to under-treatment in these environments as well.  It might be interesting to see if the therapists can affect the care that is delivered in these other settings via advocacy for the patient as well as (potentially) providing some education to other providers.  This idea makes sense in an integrated care setting such as the VA, but may be less likely in other settings where there is less direct interface/communication between providers.<br /><br />All in all, this sounds like an exciting and well-thought-out project.  Please let me know if you would like to discuss any of the above comments further.<br /><br />Best,<br /><br />- Nick";s:6:"parent";s:32:"9658a2d6f71708395f5f81183ca978f5";s:7:"replies";a:1:{i:0;s:32:"bbc2bbe91558f907a29f94314b1d44b9";}s:4:"show";b:1;s:3:"cid";s:32:"ea0a11f7afbf4cf3c3ff71baa6803ef6";}s:32:"963999ea634843dbf550c2f2d78316ff";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"nbowersox";s:4:"name";s:13:"Nick Bowersox";s:4:"mail";s:24:"Nicholas.Bowersox@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538186444;}s:3:"raw";s:3726:"Alex - 

Thank you for providing this additional explanation for your project plans.  I will admit that your project is a bit nontraditional relative to most of the other projects I have worked with, so please take my feedback with a grain of salt.  My thoughts upon reviewing your comments above:
1. You state that your goal is to focus on the viability of different financing approaches in supporting sustainable youth programming, rather than looking into the implementation process for any particular EBT or financing strategy.  With this in mind, your study is basically a comprehensive program evaluation rather than an attempt to promote the adoption of any particular treatment or financing strategy, correct?  Do you have plans for an eventual implementation arm (in a later project) which will be based on the information that you collect from this project?  It seems sensible to focus on helping sites adopt the most effective funding strategy once this is identified would help to support funding sustainability.
2. Another question on conceptualization - it sounds like you are not planning on evaluating treatment fidelity or treatment quality for your EBTs of focus in this study.  It seems to me that the quality of services could contribute to funding support and the sustainability of funding.  Are your current plans to assume that all sites that are reporting EBT delivery are offering services that are high-quality and delivered with high fidelity to treatment guidelines?  Or will you be assessing treatment fidelity in your consideration of treatment delivery characteristics?
3. I would be interested in hearing a bit more about your operationalization of "specific EBT program characteristics" that you are planning to include in your study.  What aspects of the care environment and service delivery are you thinking of including in your models - staffing? availability of services?  enrollment/amount of services delivered? patient mix and characteristics?  Given the focus of your project, it seems that you will want to be very careful in defining this area and selecting the right mix of measures to capture the complexity of treatment delivery in different settings, by different providers, to different patients, in environments with different amounts of resources and relationships with funding sources.
4. This may be beyond the scope of what you have planned, but it might be interesting to get the perspective of funding agencies/financial stakeholders in your assessment of factors that influence funding availability.  One core piece of implementation approaches is considering all the various stakeholders who will play a role in supporting the intervention and finding ways to engage each of them in the implementation spread process.  It seems that the funding stakeholders could play important roles in making funding decisions, so attempts to get their perspectives on programs that will gain their support versus not gain their support could be valuable to include.  Based on your final paragraph above, it sounds like you are making efforts to gain this perspective.

All in all, this is a very interesting project.  As I mentioned,it may be valuable to think about what will happen after the initial evaluation and synthesis aspects of this project are completed and you have some findings/recommendations that result from this work.  At that point, it may make sense to develop an implementation training approach to train sites in how to transition their youth programming to a version that will be compatible with sustained funding and/or assist them in obtaining this funding - this could represent a real "research to practice" transition of your findings.

Thanks,

- Nick
";s:5:"xhtml";s:3805:"Alex - <br /><br />Thank you for providing this additional explanation for your project plans.  I will admit that your project is a bit nontraditional relative to most of the other projects I have worked with, so please take my feedback with a grain of salt.  My thoughts upon reviewing your comments above:<br />1. You state that your goal is to focus on the viability of different financing approaches in supporting sustainable youth programming, rather than looking into the implementation process for any particular EBT or financing strategy.  With this in mind, your study is basically a comprehensive program evaluation rather than an attempt to promote the adoption of any particular treatment or financing strategy, correct?  Do you have plans for an eventual implementation arm (in a later project) which will be based on the information that you collect from this project?  It seems sensible to focus on helping sites adopt the most effective funding strategy once this is identified would help to support funding sustainability.<br />2. Another question on conceptualization - it sounds like you are not planning on evaluating treatment fidelity or treatment quality for your EBTs of focus in this study.  It seems to me that the quality of services could contribute to funding support and the sustainability of funding.  Are your current plans to assume that all sites that are reporting EBT delivery are offering services that are high-quality and delivered with high fidelity to treatment guidelines?  Or will you be assessing treatment fidelity in your consideration of treatment delivery characteristics?<br />3. I would be interested in hearing a bit more about your operationalization of &quot;specific EBT program characteristics&quot; that you are planning to include in your study.  What aspects of the care environment and service delivery are you thinking of including in your models - staffing? availability of services?  enrollment/amount of services delivered? patient mix and characteristics?  Given the focus of your project, it seems that you will want to be very careful in defining this area and selecting the right mix of measures to capture the complexity of treatment delivery in different settings, by different providers, to different patients, in environments with different amounts of resources and relationships with funding sources.<br />4. This may be beyond the scope of what you have planned, but it might be interesting to get the perspective of funding agencies/financial stakeholders in your assessment of factors that influence funding availability.  One core piece of implementation approaches is considering all the various stakeholders who will play a role in supporting the intervention and finding ways to engage each of them in the implementation spread process.  It seems that the funding stakeholders could play important roles in making funding decisions, so attempts to get their perspectives on programs that will gain their support versus not gain their support could be valuable to include.  Based on your final paragraph above, it sounds like you are making efforts to gain this perspective.<br /><br />All in all, this is a very interesting project.  As I mentioned,it may be valuable to think about what will happen after the initial evaluation and synthesis aspects of this project are completed and you have some findings/recommendations that result from this work.  At that point, it may make sense to develop an implementation training approach to train sites in how to transition their youth programming to a version that will be compatible with sustained funding and/or assist them in obtaining this funding - this could represent a real &quot;research to practice&quot; transition of your findings.<br /><br />Thanks,<br /><br />- Nick";s:6:"parent";s:32:"bab72eedb9d6c12d7ad674803d8682a1";s:7:"replies";a:1:{i:0;s:32:"530d8e29b679a6503aac72a21e3f7d15";}s:4:"show";b:1;s:3:"cid";s:32:"963999ea634843dbf550c2f2d78316ff";}s:32:"530d8e29b679a6503aac72a21e3f7d15";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"adopp";s:4:"name";s:9:"Alex Dopp";s:4:"mail";s:13:"dopp@uark.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538397216;}s:3:"raw";s:1626:"Hi Nick,

Thank you for these helpful comments! I agree that my proposal is a bit different, but it makes the most sense given my current institutional resources to start with something more broad/exploratory. I appreciate your taking my ideas in stride :) To answer your questions:

1. Yes, the plan is to test the impact of the Fiscal Mapping Process on implementation outcomes (fidelity, sustainment) and clinical outcomes in future studies. The R15 mechanism emphasizes building research capacity so the ability to articulate follow-up studies is important. Almost a full page of my research strategy is dedicated to describing future work.

2. I will not assess fidelity in this study but, as noted above, I will assess it in future studies that evaluate the FMP.

3. I am still refining how I will measure "specific EBT program characteristics" in this project. Some of this will be left open to participant feedback, but I still intend to specify a core set of measures/characteristics up front.

4. Yes, financing partners will definitely be included as key members of our expert participant panel. I am currently refining my proposal to better emphasize their role. For example, I have been focusing on youth mental health service agencies as the "users" of the FMP, but now my thinking is starting to shift as a tool that would be used for planning across stakeholder groups to finance EBTs. One important outcome from the proposed project will be participant input on the respective roles of youth mental health service agencies, financing partners, and EBT intermediary/purveyor organizations within the FMP.

Alex";s:5:"xhtml";s:1706:"Hi Nick,<br /><br />Thank you for these helpful comments! I agree that my proposal is a bit different, but it makes the most sense given my current institutional resources to start with something more broad/exploratory. I appreciate your taking my ideas in stride :) To answer your questions:<br /><br />1. Yes, the plan is to test the impact of the Fiscal Mapping Process on implementation outcomes (fidelity, sustainment) and clinical outcomes in future studies. The R15 mechanism emphasizes building research capacity so the ability to articulate follow-up studies is important. Almost a full page of my research strategy is dedicated to describing future work.<br /><br />2. I will not assess fidelity in this study but, as noted above, I will assess it in future studies that evaluate the FMP.<br /><br />3. I am still refining how I will measure &quot;specific EBT program characteristics&quot; in this project. Some of this will be left open to participant feedback, but I still intend to specify a core set of measures/characteristics up front.<br /><br />4. Yes, financing partners will definitely be included as key members of our expert participant panel. I am currently refining my proposal to better emphasize their role. For example, I have been focusing on youth mental health service agencies as the &quot;users&quot; of the FMP, but now my thinking is starting to shift as a tool that would be used for planning across stakeholder groups to finance EBTs. One important outcome from the proposed project will be participant input on the respective roles of youth mental health service agencies, financing partners, and EBT intermediary/purveyor organizations within the FMP.<br /><br />Alex";s:6:"parent";s:32:"963999ea634843dbf550c2f2d78316ff";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"530d8e29b679a6503aac72a21e3f7d15";}s:32:"b3b3b27cd65c546dd44ac300fca38621";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538406878;}s:3:"raw";s:4566:"Hi Ilana!

You propose to frame your study of IPC fidelity using the dynamic sustainability framework (DSF). Your primary method of measuring your fidelity outcome is self-report. You mention many process and context domains (clinics, districts, provinces; rural v. urban) and many provider/clinic domains (barriers/facilitators, knowledge and beliefs, self-efficacy, attitudes, motivations) to be assessed via semi-structured interviews or focus groups. You also note "buy-in" at several levels.  

In proposing DSF you're emphasizing a very intriguing framework that I have been interested in for some time! However, it is quite challenging to do what the DSF really proposes. I'd like to encourage you to really read that article closely to bring the concepts of "fit" and "dynamic sustainability" to outline your discussion of your constructs/measures. Central to DSF is moving from a static view to a dynamic view. As a result, your proposed measurement plan, and your thinking about key constructs needs to incorporate the dynamics of change over time. Possible tip: You likely can significantly strengthen your self-report measures in ways that are consistent with DSF, if you use your tablet to more passively/automatically track fidelity/adaptation over time (Note: the over-time piece is key).

My primary questions are:
Why does the DSF lead you to measure these constructs? How do the multi-layered constructs you propose to measure relate to one another within the DSF? Can you describe your measurement plan more narratively according to the key terms and conceptualization of the DSF? You posit that sustainability is closely related to fidelity and adaptation, but you should state how very clearly. What do you anticipate to be their interplay in your investigation? Consider reviewing the articles that have cited the DSF over the last 5 years (i.e., the citation network) to find possible exemplars that you could follow and cite.

For example, you say that you are interested in fidelity and adaptation, can you describe their dynamics in terms of the DSF (implementation, sustainability, sustainment, voltage drop, program drift)? It sounds like you are still defining core IPC intervention components, but concretely naming and describing anticipated challenges to delivering these components with fidelity, or expected areas in need of adaptation according to DSF would make a much more cogent outline of your implementation research question in the proposed study.  

In the draft answers above, there is not yet enough rationale for the list of constructs you propose to measure. Readers need to understand why they are important and have been selected (over other constructs) for measurement. In general, listing several possible implementation-related constructs without providing explanation of their interdependence lacks focus, whereas you want to make a compelling case to reviewers that you will advance generalizable implementation science knowledge with your study. In your specific case, you propose that fidelity/adaptation/sustainability dynamics are important. Therefore, I recommend that you triage and organize your measurement of key constructs over time using the DSF language of intervention, fit to the practice setting, and fit to the ecological system (see Figure 2, DSF, Chambers et al., 2013).

It may also be helpful to very briefly discuss the links between your more narrow focus on fidelity and adaptation, within the parent study CFIR and Re-AIM frameworks. For example, you can note links or distinctions about what your study will add by studying IPC implementation using DSF rather than the CFIR, and evaluating your fidelity outcome using DSF rather than Re-AIM. This will help make it plain why your study is an important complement to the parent study that will answer key questions that will advance science on its own.

Since fidelity measures are your primary outcome, and you propose to "set up a fidelity process," talk more concretely about how this will be operationalized. Can you describe it using the DSF? If not, what else is guiding you? For parsimony, I am not proposing another framework, as you have your hands full with the DSF already :) (and with the parent study, you already have 3 frameworks). But, you may need to cite more literature, for example the Stirman et al., 2012 review of sustainability of new programs and innovations (and the articles that have cited Stirman et al., since publication).

I hope this helps you as you make your next revisions. Very interesting work, Ilana!";s:5:"xhtml";s:4701:"Hi Ilana!<br /><br />You propose to frame your study of IPC fidelity using the dynamic sustainability framework (DSF). Your primary method of measuring your fidelity outcome is self-report. You mention many process and context domains (clinics, districts, provinces; rural v. urban) and many provider/clinic domains (barriers/facilitators, knowledge and beliefs, self-efficacy, attitudes, motivations) to be assessed via semi-structured interviews or focus groups. You also note &quot;buy-in&quot; at several levels.  <br /><br />In proposing DSF you&#039;re emphasizing a very intriguing framework that I have been interested in for some time! However, it is quite challenging to do what the DSF really proposes. I&#039;d like to encourage you to really read that article closely to bring the concepts of &quot;fit&quot; and &quot;dynamic sustainability&quot; to outline your discussion of your constructs/measures. Central to DSF is moving from a static view to a dynamic view. As a result, your proposed measurement plan, and your thinking about key constructs needs to incorporate the dynamics of change over time. Possible tip: You likely can significantly strengthen your self-report measures in ways that are consistent with DSF, if you use your tablet to more passively/automatically track fidelity/adaptation over time (Note: the over-time piece is key).<br /><br />My primary questions are:<br />Why does the DSF lead you to measure these constructs? How do the multi-layered constructs you propose to measure relate to one another within the DSF? Can you describe your measurement plan more narratively according to the key terms and conceptualization of the DSF? You posit that sustainability is closely related to fidelity and adaptation, but you should state how very clearly. What do you anticipate to be their interplay in your investigation? Consider reviewing the articles that have cited the DSF over the last 5 years (i.e., the citation network) to find possible exemplars that you could follow and cite.<br /><br />For example, you say that you are interested in fidelity and adaptation, can you describe their dynamics in terms of the DSF (implementation, sustainability, sustainment, voltage drop, program drift)? It sounds like you are still defining core IPC intervention components, but concretely naming and describing anticipated challenges to delivering these components with fidelity, or expected areas in need of adaptation according to DSF would make a much more cogent outline of your implementation research question in the proposed study.  <br /><br />In the draft answers above, there is not yet enough rationale for the list of constructs you propose to measure. Readers need to understand why they are important and have been selected (over other constructs) for measurement. In general, listing several possible implementation-related constructs without providing explanation of their interdependence lacks focus, whereas you want to make a compelling case to reviewers that you will advance generalizable implementation science knowledge with your study. In your specific case, you propose that fidelity/adaptation/sustainability dynamics are important. Therefore, I recommend that you triage and organize your measurement of key constructs over time using the DSF language of intervention, fit to the practice setting, and fit to the ecological system (see Figure 2, DSF, Chambers et al., 2013).<br /><br />It may also be helpful to very briefly discuss the links between your more narrow focus on fidelity and adaptation, within the parent study CFIR and Re-AIM frameworks. For example, you can note links or distinctions about what your study will add by studying IPC implementation using DSF rather than the CFIR, and evaluating your fidelity outcome using DSF rather than Re-AIM. This will help make it plain why your study is an important complement to the parent study that will answer key questions that will advance science on its own.<br /><br />Since fidelity measures are your primary outcome, and you propose to &quot;set up a fidelity process,&quot; talk more concretely about how this will be operationalized. Can you describe it using the DSF? If not, what else is guiding you? For parsimony, I am not proposing another framework, as you have your hands full with the DSF already :) (and with the parent study, you already have 3 frameworks). But, you may need to cite more literature, for example the Stirman et al., 2012 review of sustainability of new programs and innovations (and the articles that have cited Stirman et al., since publication).<br /><br />I hope this helps you as you make your next revisions. Very interesting work, Ilana!";s:6:"parent";s:32:"0e331e9cae6f8a2e9e503d99e6763342";s:7:"replies";a:1:{i:0;s:32:"cdb55744b7352091f21c888c88fc8a01";}s:4:"show";b:1;s:3:"cid";s:32:"b3b3b27cd65c546dd44ac300fca38621";}s:32:"01fc826960f7427ce6c0fda0fe9b2d87";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538409833;}s:3:"raw";s:4720:"Hi Robyn!

I can certainly understand why learning about the proliferation of implementation theory, models and frameworks (TMF) feels overwhelming! The growing number of TMF says at least a few things about what the state of implementation science is as a developing discipline. First, it sounds like more work has been done to guide implementation than you were aware of prior to TIDIRH. Second, the content and conceptualization of the many TMF underscores the complex scientific phenomena that implementation science addresses. 

Third, my own read of the current literature is that we are still greatly in need of theory-guided work to identify mechanisms of implementation change. I appreciate Nilsen too for bringing clarity to what theory does that models and frameworks may not, for exactly the reasons you state: "...so that the hypothesized relationships can be tested empirically. It helps to confirm that the selected implementation strategies are targeting the correct constructs to yield the expected change in behavior."

It sounds like you have considered narrowing to key shared constructs, but note that there are still many domains in play. What if we work our way backward from your key effectiveness and implementation outcomes? For effectiveness you propose GAD-7 and PHQ-9 as your primary measures. You clearly understand the psychometrics of these standardized, normed, symptom measures very well. I recommend that you strive to develop your written description of implementation outcome measures to be as strong as the effectiveness outcomes. Your fidelity section is strong, you may be well on your way.

You say: "For the implementation aspect of the trial, the primary outcomes will likely be feasibility, adoption, acceptability, appropriateness, and fidelity." Therefore, reading your plan to measure fidelity, it sounds like out of all the outcomes in Proctor and Re-AIM, you expect that fidelity is the important for your study? Is that right? Can you even further justify that outcome?

Note that is really hard to argue (at least to justify and argue well) that all implementation outcomes are all equally important in one single study. I recommend you keep your focus principally on one, or else two very closely related constructs likely to be impacted by the same implementation strategy (i.e., just as you need to justify use of GAD-7 and PHQ-9, two brief, pragmatic, highly correlated symptom measures for two frequently comorbid presenting concerns). For example, provider EBP adoption is different from EBP fidelity, but depending on the study, may be more closely related than a study focused on increasing the reach of an EBP among a patient population. 

If you pick the primary implementation outcome(s) of adoption and fidelity, what do you expect will be a key mechanism of change in that outcome (or both outcomes)? Perhaps you conceptualize provider ratings of feasibility, appropriateness and acceptability as constructs that will significantly influence or explain adoption and/or fidelity as either moderators or mediators? You mention the brief, pragmatic and psychometrically valid FIM, IAM and AIM measures, which is great! Because, just like clinical scales, if you want to test a good moderation or mediation model, you need measures with good psychometric properties.

What implementation strategy is most likely to affect these constructs and increase your anxiety intervention in PC-MHI? In my view, identifying a strategy to affect a key mechanism is key to advance the field of implementation. Since you propose a hybrid II design, for more ideas and guidance, consider focusing your literature review of the importance of fidelity to EBP effectiveness and work your way back to implementation factors related to EBP fidelity. You mention some constructs, such as leadership support, how do you expect that to relate to adoption or fidelity? What does the literature say about the link between leadership support and adoption versus leadership support and fidelity?

I hope these ideas encourage you not to try to do too much. Use the implementation science literature, but still narrow down to a feasible study that has a strong theoretical and empirical basis with falsifiable hypothesis tests. Just like your expertise with clinical research, keep in mind the goal of implementation science is to test research hypotheses that will provide generalizable knowledge about implementation. Every study needs a manageable scope, and a justified empirical rationale to advance science and inform implementation practice. Reading your answers, I think you are closer than you may think.

I really look forward to seeing future revisions of this section, Robyn!
";s:5:"xhtml";s:4829:"Hi Robyn!<br /><br />I can certainly understand why learning about the proliferation of implementation theory, models and frameworks (TMF) feels overwhelming! The growing number of TMF says at least a few things about what the state of implementation science is as a developing discipline. First, it sounds like more work has been done to guide implementation than you were aware of prior to TIDIRH. Second, the content and conceptualization of the many TMF underscores the complex scientific phenomena that implementation science addresses. <br /><br />Third, my own read of the current literature is that we are still greatly in need of theory-guided work to identify mechanisms of implementation change. I appreciate Nilsen too for bringing clarity to what theory does that models and frameworks may not, for exactly the reasons you state: &quot;...so that the hypothesized relationships can be tested empirically. It helps to confirm that the selected implementation strategies are targeting the correct constructs to yield the expected change in behavior.&quot;<br /><br />It sounds like you have considered narrowing to key shared constructs, but note that there are still many domains in play. What if we work our way backward from your key effectiveness and implementation outcomes? For effectiveness you propose GAD-7 and PHQ-9 as your primary measures. You clearly understand the psychometrics of these standardized, normed, symptom measures very well. I recommend that you strive to develop your written description of implementation outcome measures to be as strong as the effectiveness outcomes. Your fidelity section is strong, you may be well on your way.<br /><br />You say: &quot;For the implementation aspect of the trial, the primary outcomes will likely be feasibility, adoption, acceptability, appropriateness, and fidelity.&quot; Therefore, reading your plan to measure fidelity, it sounds like out of all the outcomes in Proctor and Re-AIM, you expect that fidelity is the important for your study? Is that right? Can you even further justify that outcome?<br /><br />Note that is really hard to argue (at least to justify and argue well) that all implementation outcomes are all equally important in one single study. I recommend you keep your focus principally on one, or else two very closely related constructs likely to be impacted by the same implementation strategy (i.e., just as you need to justify use of GAD-7 and PHQ-9, two brief, pragmatic, highly correlated symptom measures for two frequently comorbid presenting concerns). For example, provider EBP adoption is different from EBP fidelity, but depending on the study, may be more closely related than a study focused on increasing the reach of an EBP among a patient population. <br /><br />If you pick the primary implementation outcome(s) of adoption and fidelity, what do you expect will be a key mechanism of change in that outcome (or both outcomes)? Perhaps you conceptualize provider ratings of feasibility, appropriateness and acceptability as constructs that will significantly influence or explain adoption and/or fidelity as either moderators or mediators? You mention the brief, pragmatic and psychometrically valid FIM, IAM and AIM measures, which is great! Because, just like clinical scales, if you want to test a good moderation or mediation model, you need measures with good psychometric properties.<br /><br />What implementation strategy is most likely to affect these constructs and increase your anxiety intervention in PC-MHI? In my view, identifying a strategy to affect a key mechanism is key to advance the field of implementation. Since you propose a hybrid II design, for more ideas and guidance, consider focusing your literature review of the importance of fidelity to EBP effectiveness and work your way back to implementation factors related to EBP fidelity. You mention some constructs, such as leadership support, how do you expect that to relate to adoption or fidelity? What does the literature say about the link between leadership support and adoption versus leadership support and fidelity?<br /><br />I hope these ideas encourage you not to try to do too much. Use the implementation science literature, but still narrow down to a feasible study that has a strong theoretical and empirical basis with falsifiable hypothesis tests. Just like your expertise with clinical research, keep in mind the goal of implementation science is to test research hypotheses that will provide generalizable knowledge about implementation. Every study needs a manageable scope, and a justified empirical rationale to advance science and inform implementation practice. Reading your answers, I think you are closer than you may think.<br /><br />I really look forward to seeing future revisions of this section, Robyn!";s:6:"parent";s:32:"a95c46571af04b7a04ce171c5f6e38da";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"01fc826960f7427ce6c0fda0fe9b2d87";}s:32:"bd8d0b2b870a45ce822e2d0b7ee86d01";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538409834;}s:3:"raw";s:4720:"Hi Robyn!

I can certainly understand why learning about the proliferation of implementation theory, models and frameworks (TMF) feels overwhelming! The growing number of TMF says at least a few things about what the state of implementation science is as a developing discipline. First, it sounds like more work has been done to guide implementation than you were aware of prior to TIDIRH. Second, the content and conceptualization of the many TMF underscores the complex scientific phenomena that implementation science addresses. 

Third, my own read of the current literature is that we are still greatly in need of theory-guided work to identify mechanisms of implementation change. I appreciate Nilsen too for bringing clarity to what theory does that models and frameworks may not, for exactly the reasons you state: "...so that the hypothesized relationships can be tested empirically. It helps to confirm that the selected implementation strategies are targeting the correct constructs to yield the expected change in behavior."

It sounds like you have considered narrowing to key shared constructs, but note that there are still many domains in play. What if we work our way backward from your key effectiveness and implementation outcomes? For effectiveness you propose GAD-7 and PHQ-9 as your primary measures. You clearly understand the psychometrics of these standardized, normed, symptom measures very well. I recommend that you strive to develop your written description of implementation outcome measures to be as strong as the effectiveness outcomes. Your fidelity section is strong, you may be well on your way.

You say: "For the implementation aspect of the trial, the primary outcomes will likely be feasibility, adoption, acceptability, appropriateness, and fidelity." Therefore, reading your plan to measure fidelity, it sounds like out of all the outcomes in Proctor and Re-AIM, you expect that fidelity is the important for your study? Is that right? Can you even further justify that outcome?

Note that is really hard to argue (at least to justify and argue well) that all implementation outcomes are all equally important in one single study. I recommend you keep your focus principally on one, or else two very closely related constructs likely to be impacted by the same implementation strategy (i.e., just as you need to justify use of GAD-7 and PHQ-9, two brief, pragmatic, highly correlated symptom measures for two frequently comorbid presenting concerns). For example, provider EBP adoption is different from EBP fidelity, but depending on the study, may be more closely related than a study focused on increasing the reach of an EBP among a patient population. 

If you pick the primary implementation outcome(s) of adoption and fidelity, what do you expect will be a key mechanism of change in that outcome (or both outcomes)? Perhaps you conceptualize provider ratings of feasibility, appropriateness and acceptability as constructs that will significantly influence or explain adoption and/or fidelity as either moderators or mediators? You mention the brief, pragmatic and psychometrically valid FIM, IAM and AIM measures, which is great! Because, just like clinical scales, if you want to test a good moderation or mediation model, you need measures with good psychometric properties.

What implementation strategy is most likely to affect these constructs and increase your anxiety intervention in PC-MHI? In my view, identifying a strategy to affect a key mechanism is key to advance the field of implementation. Since you propose a hybrid II design, for more ideas and guidance, consider focusing your literature review of the importance of fidelity to EBP effectiveness and work your way back to implementation factors related to EBP fidelity. You mention some constructs, such as leadership support, how do you expect that to relate to adoption or fidelity? What does the literature say about the link between leadership support and adoption versus leadership support and fidelity?

I hope these ideas encourage you not to try to do too much. Use the implementation science literature, but still narrow down to a feasible study that has a strong theoretical and empirical basis with falsifiable hypothesis tests. Just like your expertise with clinical research, keep in mind the goal of implementation science is to test research hypotheses that will provide generalizable knowledge about implementation. Every study needs a manageable scope, and a justified empirical rationale to advance science and inform implementation practice. Reading your answers, I think you are closer than you may think.

I really look forward to seeing future revisions of this section, Robyn!
";s:5:"xhtml";s:4829:"Hi Robyn!<br /><br />I can certainly understand why learning about the proliferation of implementation theory, models and frameworks (TMF) feels overwhelming! The growing number of TMF says at least a few things about what the state of implementation science is as a developing discipline. First, it sounds like more work has been done to guide implementation than you were aware of prior to TIDIRH. Second, the content and conceptualization of the many TMF underscores the complex scientific phenomena that implementation science addresses. <br /><br />Third, my own read of the current literature is that we are still greatly in need of theory-guided work to identify mechanisms of implementation change. I appreciate Nilsen too for bringing clarity to what theory does that models and frameworks may not, for exactly the reasons you state: &quot;...so that the hypothesized relationships can be tested empirically. It helps to confirm that the selected implementation strategies are targeting the correct constructs to yield the expected change in behavior.&quot;<br /><br />It sounds like you have considered narrowing to key shared constructs, but note that there are still many domains in play. What if we work our way backward from your key effectiveness and implementation outcomes? For effectiveness you propose GAD-7 and PHQ-9 as your primary measures. You clearly understand the psychometrics of these standardized, normed, symptom measures very well. I recommend that you strive to develop your written description of implementation outcome measures to be as strong as the effectiveness outcomes. Your fidelity section is strong, you may be well on your way.<br /><br />You say: &quot;For the implementation aspect of the trial, the primary outcomes will likely be feasibility, adoption, acceptability, appropriateness, and fidelity.&quot; Therefore, reading your plan to measure fidelity, it sounds like out of all the outcomes in Proctor and Re-AIM, you expect that fidelity is the important for your study? Is that right? Can you even further justify that outcome?<br /><br />Note that is really hard to argue (at least to justify and argue well) that all implementation outcomes are all equally important in one single study. I recommend you keep your focus principally on one, or else two very closely related constructs likely to be impacted by the same implementation strategy (i.e., just as you need to justify use of GAD-7 and PHQ-9, two brief, pragmatic, highly correlated symptom measures for two frequently comorbid presenting concerns). For example, provider EBP adoption is different from EBP fidelity, but depending on the study, may be more closely related than a study focused on increasing the reach of an EBP among a patient population. <br /><br />If you pick the primary implementation outcome(s) of adoption and fidelity, what do you expect will be a key mechanism of change in that outcome (or both outcomes)? Perhaps you conceptualize provider ratings of feasibility, appropriateness and acceptability as constructs that will significantly influence or explain adoption and/or fidelity as either moderators or mediators? You mention the brief, pragmatic and psychometrically valid FIM, IAM and AIM measures, which is great! Because, just like clinical scales, if you want to test a good moderation or mediation model, you need measures with good psychometric properties.<br /><br />What implementation strategy is most likely to affect these constructs and increase your anxiety intervention in PC-MHI? In my view, identifying a strategy to affect a key mechanism is key to advance the field of implementation. Since you propose a hybrid II design, for more ideas and guidance, consider focusing your literature review of the importance of fidelity to EBP effectiveness and work your way back to implementation factors related to EBP fidelity. You mention some constructs, such as leadership support, how do you expect that to relate to adoption or fidelity? What does the literature say about the link between leadership support and adoption versus leadership support and fidelity?<br /><br />I hope these ideas encourage you not to try to do too much. Use the implementation science literature, but still narrow down to a feasible study that has a strong theoretical and empirical basis with falsifiable hypothesis tests. Just like your expertise with clinical research, keep in mind the goal of implementation science is to test research hypotheses that will provide generalizable knowledge about implementation. Every study needs a manageable scope, and a justified empirical rationale to advance science and inform implementation practice. Reading your answers, I think you are closer than you may think.<br /><br />I really look forward to seeing future revisions of this section, Robyn!";s:6:"parent";s:32:"a95c46571af04b7a04ce171c5f6e38da";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"bd8d0b2b870a45ce822e2d0b7ee86d01";}s:32:"699df0c9b4cae41c61680da5929876f4";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538414203;}s:3:"raw";s:4564:"Hi Addie!

I am enjoying learning more about your work! You have a home base with the TDF, but there are things that you may have thought through, but that are still hidden from me as a reader. As written into assignments 1, 2 and 3 above, you clearly see the TDF as relevant for your proposed three aims. What needs to be fleshed out is the "how" and the "why." You state that TDF "...cognitive, affective, social, and environmental influences on behavior..." informed your study aims, but I am not reading in the drafts how these TDF theoretical domains relate to these aims:

1.	Identify barriers and facilitators to implementing postpartum depression screening in rural Michigan WIC clinics by...
2.	Identify and tailor appropriate behavior change techniques to implement PPD screening based upon what was learned about barriers and facilitators in Aim 1.
3.	Implement and assess the impact of behavior change techniques, developed in Aim 2, on PPD screening rates in one rural Michigan WIC clinic by...

The "by..." parts of both aim 1 and aim 3 mention TDF explicitly, but I still don't understand why TDF was selected, and what you expect to find interviewing using TDF that move the needle in relationship to your stated goal/primary outcome: "...the goal of this intervention is to initiate screening for PPD within rural WIC clinics in Michigan, the primary outcome is to assess the change in PPD screening rates." Consider editing after the "by..." with more specific domains and measures of those domains from the TDF.

Doing these edits will help you unpack the TDF to make more precise statements about how specific domains relate to your implementation gap. As Denny mentioned, perhaps more background about prior work in this area will help to justify use of TDF to set up your study.  For example, as you describe the need for PPD screening in rural WIC clinics, you can set-up the argument for what you expect to be the implementation challenges related to this implementation setting, and how TDF informed the conceptualization of your proposed study to advance science and address this gap.

Perhaps this will also be helpful to you: How do you conceptualize screening rates as an implementation outcome? I see increasing PPD screening rates as possibly "penetration" in the Proctor et al. conceptualization? Do you? Clarifying the nature of your implementation outcome will help you to narrow down the TDF domains to explain the implementation problem, and hypothesized mechanisms of change/implementation strategies in relationship to your primary outcome.
 
Once you flesh out the more detailed justification, you should be able tell whether the TDF adequately informs your proposed aims. As currently drafted, you are telling readers that TDF informs your study without providing an explanation that shows us how and why. For this reason, I would strongly encourage you not to propose both the CFIR and TDF, rather to stick with justifying the links between TDF and your aims in more detail for now. Then, after you do this work, if you find TDF is not adequate for your research questions/your problem, then you may consider a need to change to another framework. But, it will be even more challenging to adequately justify, measure and test questions related both frameworks.

This may also be helpful to relate assignment #3 back to your aims. Your highly dependent aims really need better specification on the page so that each is doing some important work in your study on its own: 
- How will TDF help you to identify barriers/facilitators (aim 1), what barriers/facilitators do you expect and how does TDF help you understand them? How will achieving aim 1 advance implementation science?
- How will the TDF interview help you to adapt change techniques (aim 2)? How is the TDF used to make implementation strategy adaptations in prior research? How does that prior research inform and relate your current study? How will achieving aim 2 advance implementation science?
- And, how do you expect adaptations informed by TDF to increase screening rates (aim 3)? How will achieving aim 3 advance implementation science?

Try re-working each sentence of your specific aims as a statement about the relationship between the TDF and your problem that will advance implementation science. For example, using TDF, could you frame your aims more like hypothesis tests? Once you've tried this exercise, you can figure out the best measures for these hypothesis tests.

You're on your way, Addie! I look forward to seeing future revisions.




";s:5:"xhtml";s:4759:"Hi Addie!<br /><br />I am enjoying learning more about your work! You have a home base with the TDF, but there are things that you may have thought through, but that are still hidden from me as a reader. As written into assignments 1, 2 and 3 above, you clearly see the TDF as relevant for your proposed three aims. What needs to be fleshed out is the &quot;how&quot; and the &quot;why.&quot; You state that TDF &quot;...cognitive, affective, social, and environmental influences on behavior...&quot; informed your study aims, but I am not reading in the drafts how these TDF theoretical domains relate to these aims:<br /><br />1.	Identify barriers and facilitators to implementing postpartum depression screening in rural Michigan WIC clinics by...<br />2.	Identify and tailor appropriate behavior change techniques to implement PPD screening based upon what was learned about barriers and facilitators in Aim 1.<br />3.	Implement and assess the impact of behavior change techniques, developed in Aim 2, on PPD screening rates in one rural Michigan WIC clinic by...<br /><br />The &quot;by...&quot; parts of both aim 1 and aim 3 mention TDF explicitly, but I still don&#039;t understand why TDF was selected, and what you expect to find interviewing using TDF that move the needle in relationship to your stated goal/primary outcome: &quot;...the goal of this intervention is to initiate screening for PPD within rural WIC clinics in Michigan, the primary outcome is to assess the change in PPD screening rates.&quot; Consider editing after the &quot;by...&quot; with more specific domains and measures of those domains from the TDF.<br /><br />Doing these edits will help you unpack the TDF to make more precise statements about how specific domains relate to your implementation gap. As Denny mentioned, perhaps more background about prior work in this area will help to justify use of TDF to set up your study.  For example, as you describe the need for PPD screening in rural WIC clinics, you can set-up the argument for what you expect to be the implementation challenges related to this implementation setting, and how TDF informed the conceptualization of your proposed study to advance science and address this gap.<br /><br />Perhaps this will also be helpful to you: How do you conceptualize screening rates as an implementation outcome? I see increasing PPD screening rates as possibly &quot;penetration&quot; in the Proctor et al. conceptualization? Do you? Clarifying the nature of your implementation outcome will help you to narrow down the TDF domains to explain the implementation problem, and hypothesized mechanisms of change/implementation strategies in relationship to your primary outcome.<br /> <br />Once you flesh out the more detailed justification, you should be able tell whether the TDF adequately informs your proposed aims. As currently drafted, you are telling readers that TDF informs your study without providing an explanation that shows us how and why. For this reason, I would strongly encourage you not to propose both the CFIR and TDF, rather to stick with justifying the links between TDF and your aims in more detail for now. Then, after you do this work, if you find TDF is not adequate for your research questions/your problem, then you may consider a need to change to another framework. But, it will be even more challenging to adequately justify, measure and test questions related both frameworks.<br /><br />This may also be helpful to relate assignment #3 back to your aims. Your highly dependent aims really need better specification on the page so that each is doing some important work in your study on its own: <br />- How will TDF help you to identify barriers/facilitators (aim 1), what barriers/facilitators do you expect and how does TDF help you understand them? How will achieving aim 1 advance implementation science?<br />- How will the TDF interview help you to adapt change techniques (aim 2)? How is the TDF used to make implementation strategy adaptations in prior research? How does that prior research inform and relate your current study? How will achieving aim 2 advance implementation science?<br />- And, how do you expect adaptations informed by TDF to increase screening rates (aim 3)? How will achieving aim 3 advance implementation science?<br /><br />Try re-working each sentence of your specific aims as a statement about the relationship between the TDF and your problem that will advance implementation science. For example, using TDF, could you frame your aims more like hypothesis tests? Once you&#039;ve tried this exercise, you can figure out the best measures for these hypothesis tests.<br /><br />You&#039;re on your way, Addie! I look forward to seeing future revisions.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"699df0c9b4cae41c61680da5929876f4";}s:32:"bbc2bbe91558f907a29f94314b1d44b9";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bmaddox";s:4:"name";s:13:"Brenna Maddox";s:4:"mail";s:17:"maddoxb@upenn.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538581727;}s:3:"raw";s:159:"Hi Nick, thank you so much for these insightful questions and helpful points! Incorporating your feedback will definitely strengthen my proposal. Thanks again!";s:5:"xhtml";s:159:"Hi Nick, thank you so much for these insightful questions and helpful points! Incorporating your feedback will definitely strengthen my proposal. Thanks again!";s:6:"parent";s:32:"ea0a11f7afbf4cf3c3ff71baa6803ef6";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"bbc2bbe91558f907a29f94314b1d44b9";}s:32:"94fd72389f4c70f3140ebb4e84d1a157";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"adopp";s:4:"name";s:9:"Alex Dopp";s:4:"mail";s:13:"dopp@uark.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538663222;}s:3:"raw";s:4906:"Dopp - Assignment #4

1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

Given the limited research available on strategies for the financial sustainment of EBTs, my proposed research project is exploratory and observational in nature. My revised Specific Aims are as follows (including an explanation of the need for more exploratory research in each domain): 

(1) Generate a comprehensive compilation of EBT financing strategies used in youth mental health services. Strategic planning efforts for EBT sustainment require sufficient understanding of the options available to achieve a goal or solve a problem. Findings from previous research suggest that dedicated efforts will be needed to adequately describe all available financing strategies. A comprehensive, well-specified compilation of financing strategies will provide a foundational basis for strategic planning of EBT sustainment efforts in youth mental health services.

(2) Develop a fiscal mapping process that will guide the identification and selection of financing strategies for EBT sustainment within youth mental health services. A compilation of financing strategies is necessary, but not sufficient, for identifying and selecting the optimal combination of financing strategies for an EBT sustainment effort. In recent years, implementation experts have begun to emphasize the importance of “tailored selection” of implementation strategies. Tailoring – essentially a form of strategic planning – refers to selection of strategies that match the goals, strengths, and needs of a given EBT implementation effort. Methods of tailoring implementation strategies are in their infancy, and to date none have focused on strategic selection of financing strategies in particular. However, intervention mapping shows promise for the strategic planning of financing strategies. Intervention mapping is a well-specified, multi-step method for developing interventions (or implementation strategies) based on theory, research evidence, and stakeholder perspectives. We propose to develop a related fiscal mapping process that will guide youth mental health service agencies, funding agencies, and purveyors/intermediaries in their strategic planning efforts to finance EBT sustainment.  

Although the design of this study is relatively straightforward, I am already planning follow-up clinical trials. One of the purposes of the R15 mechanism is to develop greater research capacity for the institution and investigator. Therefore, in my proposal, I articulate how we plan to leverage the findings and increased research capacity resulting from this project into an increasingly rigorous program of research. Once we have developed the fiscal mapping process, we will first apply for funding for a pilot study (R34) to consolidate the knowledge gained into a protocol that will test the impact of that process on implementation (e.g., sustainability, EBT fidelity) outcomes. We will develop procedures that support the use of the fiscal mapping process by community organizations, pilot-test pragmatic measures for tracking fidelity to (and adaptation of) the process, and collect data on candidate mechanisms by which the process might influence sustainment. If successful, that study will be followed by an application for a large-scale, randomized hybrid implementation-effectiveness trial (R01) to rigorously test the impact – and mechanisms – of the finalized fiscal mapping process on sustainment and EBT fidelity outcomes while monitoring long-term impact on clinical outcomes (i.e., youth mental health symptoms). Moreover, both planned studies will employ simulation modeling approaches – informed in part by the data collected in Aim 2b of this project – to more fully estimate the impact of the fiscal mapping process on sustainment outcomes (beyond what is observed in a given sample). To promote generalizability of the fiscal mapping process, these planned studies will also examine a wide variety of EBTs beyond the three considered in the proposed R15 project.

2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?

Yes, my study will incorporate feedback from an expert stakeholder panel using a variety of qualitative (open-ended survey questions, structured Webinar discussion, completion of sample fiscal maps, participatory model-building exercise) and quantitative (questionnaires and rating scales) methods. These data will be collected concurrently and merged together in order to triangulate (a) the most comprehensive, useful compilation of financing strategies and (b) the most useful and generalizable approach to the fiscal mapping process.";s:5:"xhtml";s:4976:"Dopp - Assignment #4<br /><br />1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />Given the limited research available on strategies for the financial sustainment of EBTs, my proposed research project is exploratory and observational in nature. My revised Specific Aims are as follows (including an explanation of the need for more exploratory research in each domain): <br /><br />(1) Generate a comprehensive compilation of EBT financing strategies used in youth mental health services. Strategic planning efforts for EBT sustainment require sufficient understanding of the options available to achieve a goal or solve a problem. Findings from previous research suggest that dedicated efforts will be needed to adequately describe all available financing strategies. A comprehensive, well-specified compilation of financing strategies will provide a foundational basis for strategic planning of EBT sustainment efforts in youth mental health services.<br /><br />(2) Develop a fiscal mapping process that will guide the identification and selection of financing strategies for EBT sustainment within youth mental health services. A compilation of financing strategies is necessary, but not sufficient, for identifying and selecting the optimal combination of financing strategies for an EBT sustainment effort. In recent years, implementation experts have begun to emphasize the importance of “tailored selection” of implementation strategies. Tailoring – essentially a form of strategic planning – refers to selection of strategies that match the goals, strengths, and needs of a given EBT implementation effort. Methods of tailoring implementation strategies are in their infancy, and to date none have focused on strategic selection of financing strategies in particular. However, intervention mapping shows promise for the strategic planning of financing strategies. Intervention mapping is a well-specified, multi-step method for developing interventions (or implementation strategies) based on theory, research evidence, and stakeholder perspectives. We propose to develop a related fiscal mapping process that will guide youth mental health service agencies, funding agencies, and purveyors/intermediaries in their strategic planning efforts to finance EBT sustainment.  <br /><br />Although the design of this study is relatively straightforward, I am already planning follow-up clinical trials. One of the purposes of the R15 mechanism is to develop greater research capacity for the institution and investigator. Therefore, in my proposal, I articulate how we plan to leverage the findings and increased research capacity resulting from this project into an increasingly rigorous program of research. Once we have developed the fiscal mapping process, we will first apply for funding for a pilot study (R34) to consolidate the knowledge gained into a protocol that will test the impact of that process on implementation (e.g., sustainability, EBT fidelity) outcomes. We will develop procedures that support the use of the fiscal mapping process by community organizations, pilot-test pragmatic measures for tracking fidelity to (and adaptation of) the process, and collect data on candidate mechanisms by which the process might influence sustainment. If successful, that study will be followed by an application for a large-scale, randomized hybrid implementation-effectiveness trial (R01) to rigorously test the impact – and mechanisms – of the finalized fiscal mapping process on sustainment and EBT fidelity outcomes while monitoring long-term impact on clinical outcomes (i.e., youth mental health symptoms). Moreover, both planned studies will employ simulation modeling approaches – informed in part by the data collected in Aim 2b of this project – to more fully estimate the impact of the fiscal mapping process on sustainment outcomes (beyond what is observed in a given sample). To promote generalizability of the fiscal mapping process, these planned studies will also examine a wide variety of EBTs beyond the three considered in the proposed R15 project.<br /><br />2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br /><br />Yes, my study will incorporate feedback from an expert stakeholder panel using a variety of qualitative (open-ended survey questions, structured Webinar discussion, completion of sample fiscal maps, participatory model-building exercise) and quantitative (questionnaires and rating scales) methods. These data will be collected concurrently and merged together in order to triangulate (a) the most comprehensive, useful compilation of financing strategies and (b) the most useful and generalizable approach to the fiscal mapping process.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"c8ca6956424cf9183fec144b98317c4b";}s:4:"show";b:1;s:3:"cid";s:32:"94fd72389f4c70f3140ebb4e84d1a157";}s:32:"f66d0922d41eaf8d6776e856f7c6dcec";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bmaddox";s:4:"name";s:13:"Brenna Maddox";s:4:"mail";s:17:"maddoxb@upenn.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538665745;}s:3:"raw";s:2669:"MADDOX - Assignment #4:

1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

My proposed study design is an RCT – I plan to randomly assign outpatient clinicians who serve adults with autism and co-occurring anxiety/depression to receive either implementation as usual or the multifaceted implementation strategy. I chose an RCT because I want to control the assignment to implementation strategy conditions and control for confounding variables. I am also considering whether a cluster RCT would be appropriate because I will be partnering with clinicians from multiple community mental health centers. 
As described in previous assignments, my proposed study is an effectiveness-implementation hybrid trial because I will be dual testing a clinical intervention (modified CBT for adults with autism and co-occurring anxiety/depression) and implementation strategies. The feedback from the facilitators about whether my study is a Type II or Type III has been very helpful. Given the current lack of research on CBT for adults with autism (i.e., there have been no effectiveness studies, and my current F32 study is focused on provider outcomes, not client outcomes), I think a Type II is most appropriate. I currently view my effectiveness and implementation aims as co-primary. I realize now that I made the implementation aim first in Assignment 1 due to the focus of this course, but I now wonder if I should switch the order of my specific aims to better fit with the usual Type II grant proposal format. I am excited about using a hybrid design because my goal is to streamline research in the field of autism in adulthood, in order to more quickly make improvements to clinical service in the “real world.” Slow progress in translating research to practice has particularly been a problem for adults with autism, given most autism research to date has focused on children.

2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?

Yes, I will incorporate both quantitative and qualitative measures in my study. The approach I plan to use, in terms of structure, function, and process, is:
-	Simultaneous collection and analysis of quantitative and qualitative data
-	Complementarity, using the quantitative data to evaluate outcomes and qualitative data to evaluate process
-	Embedded, conducting a qualitative study focused on process embedded within an RCT of effectiveness and implementation outcomes";s:5:"xhtml";s:2729:"MADDOX - Assignment #4:<br /><br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />My proposed study design is an RCT – I plan to randomly assign outpatient clinicians who serve adults with autism and co-occurring anxiety/depression to receive either implementation as usual or the multifaceted implementation strategy. I chose an RCT because I want to control the assignment to implementation strategy conditions and control for confounding variables. I am also considering whether a cluster RCT would be appropriate because I will be partnering with clinicians from multiple community mental health centers. <br />As described in previous assignments, my proposed study is an effectiveness-implementation hybrid trial because I will be dual testing a clinical intervention (modified CBT for adults with autism and co-occurring anxiety/depression) and implementation strategies. The feedback from the facilitators about whether my study is a Type II or Type III has been very helpful. Given the current lack of research on CBT for adults with autism (i.e., there have been no effectiveness studies, and my current F32 study is focused on provider outcomes, not client outcomes), I think a Type II is most appropriate. I currently view my effectiveness and implementation aims as co-primary. I realize now that I made the implementation aim first in Assignment 1 due to the focus of this course, but I now wonder if I should switch the order of my specific aims to better fit with the usual Type II grant proposal format. I am excited about using a hybrid design because my goal is to streamline research in the field of autism in adulthood, in order to more quickly make improvements to clinical service in the “real world.” Slow progress in translating research to practice has particularly been a problem for adults with autism, given most autism research to date has focused on children.<br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br /><br />Yes, I will incorporate both quantitative and qualitative measures in my study. The approach I plan to use, in terms of structure, function, and process, is:<br />-	Simultaneous collection and analysis of quantitative and qualitative data<br />-	Complementarity, using the quantitative data to evaluate outcomes and qualitative data to evaluate process<br />-	Embedded, conducting a qualitative study focused on process embedded within an RCT of effectiveness and implementation outcomes";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"00c5191873d7fe84164f0f3b8a496f7e";}s:4:"show";b:1;s:3:"cid";s:32:"f66d0922d41eaf8d6776e856f7c6dcec";}s:32:"4d1e378a566b4d52d5a4a3e01f693bf1";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"ipinsky";s:4:"name";s:12:"Ilana Pinsky";s:4:"mail";s:21:"pinskyilana@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538674243;}s:3:"raw";s:1756:"PINSKY. Assignment # 4
1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?
Our design is a (district) randomized, three-arm hybrid effectiveness-implementation type 2. We will be working in three geographically separate Mozambican provinces (Nampula, Sofala, Gaza) and in rural, peri-urban (i.e., rural urban transition zone; N=6), and urban ) clinics. The first part of the implementation trial will last 2 years, after which the delivery pathway showing the highest overall effectiveness  will then be implemented in districts from the other two arms for two additional “cross-over” years. 
The fidelity piece, that I described for TIDIHR is part of the constructs to be followed in this trial.

2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?
Yes, we will. Throughout the trial and the “cross-over” years describe above, qualitative and other process data will complement structured assessments to examine implementation, sustainability, and scale-up. 
The taxonomy of the design should be as follows: we are still debating if the structure will be sequential or simultaneous; the function is  complementary ( to elaborate upon the quantitative findings to understand the process of implementation as experienced by stakeholders); and the process is connecting (having the qualitative data set build upon the quantitative data set). 
In order to integrate the quantitative and qualitative methods, we will follow the recently released NIH guidelines for best practices in mixed methods.";s:5:"xhtml";s:1796:"PINSKY. Assignment # 4<br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br />Our design is a (district) randomized, three-arm hybrid effectiveness-implementation type 2. We will be working in three geographically separate Mozambican provinces (Nampula, Sofala, Gaza) and in rural, peri-urban (i.e., rural urban transition zone; N=6), and urban ) clinics. The first part of the implementation trial will last 2 years, after which the delivery pathway showing the highest overall effectiveness  will then be implemented in districts from the other two arms for two additional “cross-over” years. <br />The fidelity piece, that I described for TIDIHR is part of the constructs to be followed in this trial.<br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br />Yes, we will. Throughout the trial and the “cross-over” years describe above, qualitative and other process data will complement structured assessments to examine implementation, sustainability, and scale-up. <br />The taxonomy of the design should be as follows: we are still debating if the structure will be sequential or simultaneous; the function is  complementary ( to elaborate upon the quantitative findings to understand the process of implementation as experienced by stakeholders); and the process is connecting (having the qualitative data set build upon the quantitative data set). <br />In order to integrate the quantitative and qualitative methods, we will follow the recently released NIH guidelines for best practices in mixed methods.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"c5efd88ff8513c74d3837f4d38c63243";}s:4:"show";b:1;s:3:"cid";s:32:"4d1e378a566b4d52d5a4a3e01f693bf1";}s:32:"91d14fcda0b357b8a7e61ad74f269c37";a:8:{s:4:"user";a:5:{s:2:"id";s:11:"rshepardson";s:4:"name";s:16:"Robyn Shepardson";s:4:"mail";s:23:"Robyn.Shepardson@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538678802;}s:3:"raw";s:5145:"SHEPARDSON - Assignment #4

1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

My research program entails the development, evaluation, and implementation of a brief anxiety intervention for VA Primary Care-Mental Health Integration (PC-MHI) settings. Following treatment development, we will first be conducting a hybrid type I effectiveness-implementation trial (1). The primary goal of this RCT is to evaluate the effectiveness of the brief anxiety intervention, compared to PC-MHI usual care, on the primary outcome of anxiety symptom severity. The secondary goal is to collect preliminary data on implementation barriers and facilitators that may affect future real-world uptake. We are therefore conducting a mixed methods process evaluation (2), informed by the RE-AIM framework (3), of intervention implementation to identify possible provider-, and clinic-level barriers and facilitators that may affect future intervention adoption, fidelity, and sustainability.

We are conducting an effectiveness trial, rather than an efficacy trial, because we are not creating a brand new treatment, but rather adapting existing evidence-based techniques shown to be effective in longer formats into a new intervention package in a format suitable for PC-MHI. We are using a hybrid I design, rather than hybrid II or III (1), because the intervention package is being tested in this particular form for the first time, so we still need to establish effectiveness. At the same time, to reduce the lag between treatment development and translation into clinical practice (4), it is important to begin collecting preliminary data on implementation barriers and facilitators to inform next steps. Overall, the hybrid type I design is appropriate because the brief anxiety intervention will have strong face validity, a base of indirect evidence (from specialty mental health and PC-MHI), and minimal risk.

The next step after my current study is the proposed study I am now developing in TIDIHR, which will be a hybrid type II trial (1) to test the brief anxiety intervention, as well as an evidence-based implementation strategy, in a larger multi-site RCT. Our co-equal research questions will be: (1) Does the brief anxiety intervention improve upon patient clinical outcomes obtained in PC-MHI usual care? and (2) Does an evidence-based implementation strategy improve upon VA PC-MHI standard implementation support? Specifically, we plan to evaluate external facilitation (5), which has been used successfully in prior national PC-MHI program rollouts (6). The hybrid II design will be appropriate (1) assuming the current hybrid I trial yields the expected results (if not, I will obviously have to reevaluate everything), as there will be good face validity, indirect support, and minimal risk for both the intervention and the implementation strategy being applied within PC-MHI. There is informal evidence of “implementation momentum” based on public communications between PC-MHI leadership and clinicians in the field. Given that the brief anxiety intervention will have only been formally tested in the one hybrid I trial, there will still be good reason to collect additional effectiveness data. Finally, the implementation strategy we are testing should not be overly taxing to stakeholders as it is consistent with current VA implementation efforts (6), which supports its feasibility outside of the research study.

2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? 

I do plan to use mixed methods in the proposed hybrid II trial. For the implementation evaluation especially, I envision employing concurrent and/or sequential quantitative and qualitative data collection that will together inform interpretation (7). There will be quantitative indicators of implementation outcomes such as feasibility, adoption, acceptability, and fidelity. This could range from administrative data and self-report measures to scores on fidelity checklists. These quantitative measures would likely receive more weight or emphasis compared to qualitative data. However, I definitely want to supplement this with qualitative data collection to allow for more in-depth understanding of how the implementation process in particular went. I will conduct semi-structured interviews with key stakeholders such as clinicians, leadership, administrators, and possibly patients. Overall, qualitative data will be used to provide further insight into quantitative data (e.g., clinicians’ qualitative feedback on implementation challenges will be used to help understand the story behind quantitative adoption and fidelity data). The interview data would provide the context or full story of what happened to yield the specific quantitative outcomes. I think of this as calling for an embedded design in which the points of interface would include data collection and data interpretation (8). ";s:5:"xhtml";s:5204:"SHEPARDSON - Assignment #4<br /><br />1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />My research program entails the development, evaluation, and implementation of a brief anxiety intervention for VA Primary Care-Mental Health Integration (PC-MHI) settings. Following treatment development, we will first be conducting a hybrid type I effectiveness-implementation trial (1). The primary goal of this RCT is to evaluate the effectiveness of the brief anxiety intervention, compared to PC-MHI usual care, on the primary outcome of anxiety symptom severity. The secondary goal is to collect preliminary data on implementation barriers and facilitators that may affect future real-world uptake. We are therefore conducting a mixed methods process evaluation (2), informed by the RE-AIM framework (3), of intervention implementation to identify possible provider-, and clinic-level barriers and facilitators that may affect future intervention adoption, fidelity, and sustainability.<br /><br />We are conducting an effectiveness trial, rather than an efficacy trial, because we are not creating a brand new treatment, but rather adapting existing evidence-based techniques shown to be effective in longer formats into a new intervention package in a format suitable for PC-MHI. We are using a hybrid I design, rather than hybrid II or III (1), because the intervention package is being tested in this particular form for the first time, so we still need to establish effectiveness. At the same time, to reduce the lag between treatment development and translation into clinical practice (4), it is important to begin collecting preliminary data on implementation barriers and facilitators to inform next steps. Overall, the hybrid type I design is appropriate because the brief anxiety intervention will have strong face validity, a base of indirect evidence (from specialty mental health and PC-MHI), and minimal risk.<br /><br />The next step after my current study is the proposed study I am now developing in TIDIHR, which will be a hybrid type II trial (1) to test the brief anxiety intervention, as well as an evidence-based implementation strategy, in a larger multi-site RCT. Our co-equal research questions will be: (1) Does the brief anxiety intervention improve upon patient clinical outcomes obtained in PC-MHI usual care? and (2) Does an evidence-based implementation strategy improve upon VA PC-MHI standard implementation support? Specifically, we plan to evaluate external facilitation (5), which has been used successfully in prior national PC-MHI program rollouts (6). The hybrid II design will be appropriate (1) assuming the current hybrid I trial yields the expected results (if not, I will obviously have to reevaluate everything), as there will be good face validity, indirect support, and minimal risk for both the intervention and the implementation strategy being applied within PC-MHI. There is informal evidence of “implementation momentum” based on public communications between PC-MHI leadership and clinicians in the field. Given that the brief anxiety intervention will have only been formally tested in the one hybrid I trial, there will still be good reason to collect additional effectiveness data. Finally, the implementation strategy we are testing should not be overly taxing to stakeholders as it is consistent with current VA implementation efforts (6), which supports its feasibility outside of the research study.<br /><br />2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? <br /><br />I do plan to use mixed methods in the proposed hybrid II trial. For the implementation evaluation especially, I envision employing concurrent and/or sequential quantitative and qualitative data collection that will together inform interpretation (7). There will be quantitative indicators of implementation outcomes such as feasibility, adoption, acceptability, and fidelity. This could range from administrative data and self-report measures to scores on fidelity checklists. These quantitative measures would likely receive more weight or emphasis compared to qualitative data. However, I definitely want to supplement this with qualitative data collection to allow for more in-depth understanding of how the implementation process in particular went. I will conduct semi-structured interviews with key stakeholders such as clinicians, leadership, administrators, and possibly patients. Overall, qualitative data will be used to provide further insight into quantitative data (e.g., clinicians’ qualitative feedback on implementation challenges will be used to help understand the story behind quantitative adoption and fidelity data). The interview data would provide the context or full story of what happened to yield the specific quantitative outcomes. I think of this as calling for an embedded design in which the points of interface would include data collection and data interpretation (8).";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"ee80b650bbf5034efa419ca86d7e7477";}s:4:"show";b:1;s:3:"cid";s:32:"91d14fcda0b357b8a7e61ad74f269c37";}s:32:"940ddd25e15b3ed46422c451cd69006e";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"aprogovac";s:4:"name";s:12:"Ana Progovac";s:4:"mail";s:24:"aprogovac@challiance.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538766711;}s:3:"raw";s:2506:"PROGOVAC - ASSIGNMENT #4
Assignment #4:
**************What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?**********
I am currently working on the Hybrid Type II study, where we are measuring long-term effectiveness over implementation years 2-4 and also include sub-group analyses, as well as pinpointing implementation strategies likely to be effective during program expansion to 2 other CHA sites via qualitative interviews with staff and health system administrators. 

From there, the results of the Hybrid Type II will be helpful as we propose a Hybrid Type III design which will test the implementation strategies developed during the 2nd Phase (as these models are rolled out to the 2 other sites). 

Therefore, the Hybrid Type II is currently an observational trial with quasi-experimental analysis for the quantitative data (using propensity-score matching). It is also mixed methods in that it incorporates interviews with providers, patients, and administrators. 

The Hybrid Type III would move more into an experimental trial design because we would then be testing specific implementation strategies. However, it is still a small n (only 2 new sites), so without too much potential to randomize. We could do a stepped wedge type design to phase-in different implementation strategies at different times, but it would still be difficult to isolate providers from each other. This Hybrid Type III would also include qualitative work via semi-structured interviews and other types of unstructured data collection. 

**********Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?***********

Yes. I think D&I work without mixed methods leaves a lot of richness on the table. We are using qualitative interviews in the Hybrid Type II (strictly observational phase) to better inform understanding and selection of the implementation strategies which we will routinize and then test more deliberately in the Hybrid Type III phase. 

We will consider an explanatory sequential design (using quantitative data from Years 2 and 3 of program implementation to inform qualitative interviews and then conduct very targeted interviews about what may improve specific implementation strategies, and how to make these feasible in the next phase of roll-out). ";s:5:"xhtml";s:2584:"PROGOVAC - ASSIGNMENT #4<br />Assignment #4:<br />**************What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?**********<br />I am currently working on the Hybrid Type II study, where we are measuring long-term effectiveness over implementation years 2-4 and also include sub-group analyses, as well as pinpointing implementation strategies likely to be effective during program expansion to 2 other CHA sites via qualitative interviews with staff and health system administrators. <br /><br />From there, the results of the Hybrid Type II will be helpful as we propose a Hybrid Type III design which will test the implementation strategies developed during the 2nd Phase (as these models are rolled out to the 2 other sites). <br /><br />Therefore, the Hybrid Type II is currently an observational trial with quasi-experimental analysis for the quantitative data (using propensity-score matching). It is also mixed methods in that it incorporates interviews with providers, patients, and administrators. <br /><br />The Hybrid Type III would move more into an experimental trial design because we would then be testing specific implementation strategies. However, it is still a small n (only 2 new sites), so without too much potential to randomize. We could do a stepped wedge type design to phase-in different implementation strategies at different times, but it would still be difficult to isolate providers from each other. This Hybrid Type III would also include qualitative work via semi-structured interviews and other types of unstructured data collection. <br /><br />**********Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?***********<br /><br />Yes. I think D&amp;I work without mixed methods leaves a lot of richness on the table. We are using qualitative interviews in the Hybrid Type II (strictly observational phase) to better inform understanding and selection of the implementation strategies which we will routinize and then test more deliberately in the Hybrid Type III phase. <br /><br />We will consider an explanatory sequential design (using quantitative data from Years 2 and 3 of program implementation to inform qualitative interviews and then conduct very targeted interviews about what may improve specific implementation strategies, and how to make these feasible in the next phase of roll-out).";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"0747f638776a25f78e759d356936d412";}s:4:"show";b:1;s:3:"cid";s:32:"940ddd25e15b3ed46422c451cd69006e";}s:32:"1dde5abb12cd0153f6e774f83ddbdb9f";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"kokamura";s:4:"name";s:14:"Kelsie Okamura";s:4:"mail";s:26:"kelsie.h.okamura@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538810639;}s:3:"raw";s:390:"Denny,

Thank you for your question around my research question and aims. I hope my answer this week helps to focus your feedback a bit more. I am very appreciative for your recommendation of the conceptual model of implementation research. I am curious as to how to measure some of the system level factors (e.g., administrator/leadership) that are more process-type factors. Thanks again!";s:5:"xhtml";s:400:"Denny,<br /><br />Thank you for your question around my research question and aims. I hope my answer this week helps to focus your feedback a bit more. I am very appreciative for your recommendation of the conceptual model of implementation research. I am curious as to how to measure some of the system level factors (e.g., administrator/leadership) that are more process-type factors. Thanks again!";s:6:"parent";s:32:"9da0d757ff5955cfc4d410e529276de8";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"1dde5abb12cd0153f6e774f83ddbdb9f";}s:32:"61ec5b447c005aa9cbe22b4c7a03ce70";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"kokamura";s:4:"name";s:14:"Kelsie Okamura";s:4:"mail";s:26:"kelsie.h.okamura@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538810674;}s:3:"raw";s:2724:"OKAMURA – ASSIGNMENT #4

1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?
We plan to use an observational and explanatory research design to answer the question “What are important organizational- and individual-level MBC determinants in two large behavioral health divisions?” Specifically, we are looking to answer the following: (a) Does organizational climate impact the use of MBC, (b) Does burnout decrease the use of MBC, (c) Do attitudes toward standardized assessment increase MBC, and (d) Can an EHR accurately measure MBC within community mental health? A tertiary aim to our study is to also understand if teamwork and group cohesiveness (i.e., co-management of care) improve the use of evidence-based practices and outcomes. The observational and explanatory research design is the best design to answer these questions given the scarcity of literature examining MBC specifically. Most of the research related to MBC argues for the importance of MBC and provides recommendations for embedding MBC in community settings through quality assurance and improvement plans. This study will elucidate important MBC determinants through mixed methods across two distinct behavioral health divisions.

2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?
Yes, we plan to include a mixed methods design to the study using sequential QUALquan design to determine a taxonomy and framework for future MBC studies. This will be done by incorporating qualitative interviews and focus groups to inform determinant measures of MBC within the divisions. Theme reduction will refine hypotheses to select pragmatic implementation measures. For example, we have conducted 19 interviews and focus groups with key members of both divisions since August 2018. From these, themes were identified related to leadership and organizational culture (e.g., “how do we get our Clinical Psychologists to review and attend monthly case review meetings?”), attitudes toward and current process of MBC (e.g., “why do we deliver the Ohio Scales every month when we don’t do anything with it?”), and staff turnover and burnout (e.g., one division is evidencing a high degree of middle management turnover). We will continue feedback sessions with key members of both divisions when implementation of the EHR system occurs in early 2019. Additionally, qualitative data will be used in observational measurement development to assess MBC fidelity relative to EHR monitoring.";s:5:"xhtml";s:2754:"OKAMURA – ASSIGNMENT #4<br /><br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br />We plan to use an observational and explanatory research design to answer the question “What are important organizational- and individual-level MBC determinants in two large behavioral health divisions?” Specifically, we are looking to answer the following: (a) Does organizational climate impact the use of MBC, (b) Does burnout decrease the use of MBC, (c) Do attitudes toward standardized assessment increase MBC, and (d) Can an EHR accurately measure MBC within community mental health? A tertiary aim to our study is to also understand if teamwork and group cohesiveness (i.e., co-management of care) improve the use of evidence-based practices and outcomes. The observational and explanatory research design is the best design to answer these questions given the scarcity of literature examining MBC specifically. Most of the research related to MBC argues for the importance of MBC and provides recommendations for embedding MBC in community settings through quality assurance and improvement plans. This study will elucidate important MBC determinants through mixed methods across two distinct behavioral health divisions.<br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br />Yes, we plan to include a mixed methods design to the study using sequential QUALquan design to determine a taxonomy and framework for future MBC studies. This will be done by incorporating qualitative interviews and focus groups to inform determinant measures of MBC within the divisions. Theme reduction will refine hypotheses to select pragmatic implementation measures. For example, we have conducted 19 interviews and focus groups with key members of both divisions since August 2018. From these, themes were identified related to leadership and organizational culture (e.g., “how do we get our Clinical Psychologists to review and attend monthly case review meetings?”), attitudes toward and current process of MBC (e.g., “why do we deliver the Ohio Scales every month when we don’t do anything with it?”), and staff turnover and burnout (e.g., one division is evidencing a high degree of middle management turnover). We will continue feedback sessions with key members of both divisions when implementation of the EHR system occurs in early 2019. Additionally, qualitative data will be used in observational measurement development to assess MBC fidelity relative to EHR monitoring.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"187ce64f873b1293b2264671840ec46c";}s:4:"show";b:1;s:3:"cid";s:32:"61ec5b447c005aa9cbe22b4c7a03ce70";}s:32:"dfe8e2a65ce99233132c652f83df2f79";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"aweaver";s:4:"name";s:12:"Addie Weaver";s:4:"mail";s:18:"weaverad@umich.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538859605;}s:3:"raw";s:12629:"WEAVER - Assignment #4

1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

This project seeks to implement screening for postpartum depression (PPD) in rural WIC clinics in the state of Michigan. Barriers and facilitators to PPD screening in rural WIC clinics (Aim 1) will be assessed via mixed methods, and findings will inform the development of behavior change techniques to implement screening in rural WIC clinics (Aim 2). Mixed methods, including a one-group pre-/post- test and qualitative interviews with staff, will also be used to assess the impact of the behavior change techniques on PPD screening rates and staff behavior over time (Aim 3). Study aims, associated research activities, and a timeline are summarized in Table 1, below. Note to TIDIHR Instructors: Table would not format properly so I cannot include it in this format. I am happy to email if it would be helpful. 
		
This implementation research will be guided by the Theoretical Domains Framework (TDF). The TDF is a consolidated theoretical framework that incorporates 128 constructs from 33 behavior change theories into 14 theoretical construct domains.43-45 TDF domains and associated constructs include: Knowledge (procedural knowledge, knowledge of condition/scientific rationale; Skills (competence, ability, interpersonal/practice skills); Social/Professional Role and Identity (professional confidence, group identity organizational commitment); Beliefs about Capabilities (self-efficacy, perceived competence); Optimism (optimism, pessimism); Beliefs about Consequences (outcome expectancies); Reinforcement (rewards, punishment); Intentions (stability of intentions); Goals (goal/target setting, action planning, implementation intention); Memory, Attention, and Decision Processes (memory, attention control, cognitive overload); Environmental Context and Resources (organizational culture/climate, material resources); Social Influences (social pressure/support, group conformity/norms); Emotion (anxiety, depression, stress); and Behavioral Regulation (self-monitoring, action planning).

The TDF is well researched and has been widely used in a variety of contexts to inform and address implementation problems, including problem analysis, theorizing pathways of change, designing intervention, identifying appropriate process measures, testing pathways of change, and as a coding framework for analysis.43,46 Additionally, the TDF offers a versatile framework that has been effectively utilized to address a range of implementation issues. Therefore, the TDF is relevant for guiding aspects of all three aims of this project: assessing implementation barriers and facilitators while providing a structure for analysis/coding; informing the identification and development of appropriate behavioral change techniques; and exploring potential mechanisms of change. 

Aim 1: Identify barriers and facilitators to implementing PPD screening in rural Michigan WIC clinics

Mixed methods will be used to identify barriers and facilitators to implementing PPD screening in rural Michigan WIC clinics. Semi-structured qualitative interviews, informed by the Theoretical Domains Framework (TDF), will be conducted with rural WIC clinic staff (e.g., nurses, nutritionists). A cross-sectional survey, also informed by the TDF, will be administered to rural WIC staff to assess barriers and facilitators to PPD screening, as well as clinic characteristics, staff characteristics, perceptions of depression among clients, and receptivity to addressing clients’ depressive symptoms on-site. 

This phase will target WIC staff and clinics in rural Michigan. Rural-Urban Continuum Code (RUCC) classifications of urbanicity will be used to identify WIC clinics and staff located in rural counties. Rural-Urban Continuum Codes (RUCCs), a county-level classification scheme developed by the U.S. Department of Agriculture measure rurality by population size and adjacency to metropolitan areas. The RUCCs contain 9 categories. For this study, RUCCs will be collapsed into the following 3 categories: urban (codes 1-3), suburban (codes as 4-6), and rural (codes 7-9). 

Qualitative interviews with WIC staff. A stratified cluster random sampling strategy will be used to identify WIC clinics where one staff member will be invited to participate in a semi-structured qualitative interview. Regions of the state (i.e., SW, SE, NW, SE, UP) will be strata and three WIC clinics will be randomly selected from each strata. Randomly selected WIC clinics will be contacted about this study and a staff member who provides direct service to clients will be invited to participate in the interview. If a clinic declines to participate, we will randomly select another from that stratum. Refusal rates will be tracked as a sign of feasibility and acceptability of PPD screening. It is expected that 15 WIC staff members will be interviewed. In addition to the TDF-informed interview guide, staff will be asked about their role in the clinic (e.g., nurse, nutritionist), educational background, previous mental health training/education, current clinic workflow, and receptivity to addres;’\sing depression in the WIC clinic. 
Interviews will be conducted by phone by two research team members, one who will conduct the interview and one who will act as a scribe. All interviews will be audio recorded with participants’ permission. Rapid-cycle data analysis will be used to identify salient barriers and facilitators to PPD screening in rural Michigan WIC clinics. 

Cross-sectional surveys with WIC staff. Using a purposive sampling strategy, a cross-sectional survey will be administered to staff across all rural Michigan WIC clinics. Based upon the RUCCs, described above, all WIC clinics located in a rural county will receive a mail survey. The survey will draw from an existing survey of TDF construct domains and will be informed by the results of qualitative interviews with WIC staff. Questions concerning clinic size, resources, and capacity, client characteristics, and participating staff characteristics will be included as well. Survey instructions will indicate that a staff member who provides direct services to clients should complete the survey. An initial mailing and two reminders (2 weeks and one month apart) will be sent to maximize the response rate. Survey data will be analyzed in SPSS using descriptive and bivariate statistics, including independent samples t-tests and chi-square tests. 

Aim 2: Identify and tailor behavior change techniques to implement PPD screening 

The results from mixed methods research assessing barriers and facilitators of implementing PPD screening in rural Michigan WIC clinics will inform the identification and tailoring of behavior change techniques. The research team will integrate findings from Aim 1, informed by the TDF, with research-informed implementation strategies. 

Though the design and tailoring of the implementation intervention based upon behavior change techniques will be informed by staff-identified barriers and facilitators (Aim 1), TDF domains that appear most relevant to targeted individual behavior change have been selected to inform a preliminary implementation intervention design comprised of education interventions and audit and feedback (see Table 2, below; Note to TIDIHR Instructors: Same problem with table - happy to email as needed). Both intervention components are grounded in the implementation science literature and are well established, highly used strategies. The preliminary implementation intervention is also informed by interventions utilized in one identified study that has successfully initiated PPD screening within WIC clinics in an urban community.47

In addition, during research activities associated with Aim 1, a rural Michigan WIC clinic that expresses interest in moving forward with PPD screening and shows signs of being an early adopter, will be identified as a pilot site for the implementation intervention. Key stakeholders (WIC administrators and staff) from the identified pilot site will provide input and feedback regarding the implementation intervention to improve feasibility and acceptability.

Aim 3: Implement and assess the impact of behavior change techniques 

A mixed methods design, including a one group pre-/post- test assessing change over time in PPD screening rates and WIC staff behavior in one rural Michigan WIC clinic and qualitative interviews with WIC clinic staff, will be used to assess the impact of the behavior change techniques developed in Aim 2.

One group pre-/post test. A one group pre-/post test will be use to assess the impact of behavior change techniques on PPD screening rates and WIC staff behavior over time. During the three months prior to delivering the behavior change techniques (i.e., educational interventions; audit and feedback), PPD screening rates will be assessed as a pre-test measure. PPD screening rate will be calculated by dividing the number of PPD screening tools administered to postpartum clients over a three-month period by the total number of postpartum client visits over the same time periods. Postpartum clients, whose infant is between birth and 12 months old, would be screened at each WIC visit; therefore the screening rate is based on the number of visits rather than the number of clients. This data will be obtained via client chart audit. Before beginning the behavior change techniques comprising the implementation intervention, all WIC staff at the targeted rural clinic who provide direct service to clients will be asked to complete a questionnaire assessing staff members’ baseline knowledge, behavior, and attitudes as related to TDF domains. This questionnaire will also include some key questions about staff characteristics (e.g., role (e.g., nurse, nutritionist), education level, length of time practicing). All clinic staff providing direct service to clients will participate in behavior change techniques, developed in Aim 2, to initiate PPD screening. After the conclusion of the behavior change techniques, PPD screening rates will be assessed over a three-month period via client chart audit. All staff participating in the behavior change techniques will complete a post-test assessment of their knowledge, behavior, and attitudes related to TDF domains. These data will allow us to examine the change over time in PPD screening rates as well as the change over time in staff knowledge, attitudes, and behavior. Examining change in staff knowledge, attitudes, and behaviors over time will help identify potential mechanisms of change that may assist the research team in understanding how and why the behavior change techniques did or did not work. Please note that the one group pre-/post- design will also be used to assess secondary outcomes, including mental health referrals and client refusal.

Appropriate bivariate statistical analyses, including chi-square tests and paired-samples t-tests, will be used to assess change over time in outcome variables of interest. Additionally, regression analyses will be used examine potential predictors of screening among WIC staff, including TDF domains and staff characteristics (e.g., role, education level, previous mental health training). 

Qualitative interviews with WIC staff. Beginning six weeks into the 3-month period in which PPD screening rates will be measured, WIC staff will be invited to participate in a semi-structured qualitative interview, guided by the TDF domains. This interview will provide insight to barriers and facilitators to implementing PPD screening and help make sense of quantitative findings. Interview data will also suggest necessary refinements or changes to behavior change techniques in order to increase acceptability and sustainability of screening as standard practice. 

Two members of the research team will conduct interviews with WIC staff by phone at staff members’ convenience. One research team member will conduct the interview and the second research team member will scribe. Interviews will be recorded with participants’ permission. Rapid-cycle data analysis will be used to identify themes emerging from the qualitative data. 

2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?

Yes, mixed methods are incorporated into the study design, and described, above.
";s:5:"xhtml";s:12843:"WEAVER - Assignment #4<br /><br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />This project seeks to implement screening for postpartum depression (PPD) in rural WIC clinics in the state of Michigan. Barriers and facilitators to PPD screening in rural WIC clinics (Aim 1) will be assessed via mixed methods, and findings will inform the development of behavior change techniques to implement screening in rural WIC clinics (Aim 2). Mixed methods, including a one-group pre-/post- test and qualitative interviews with staff, will also be used to assess the impact of the behavior change techniques on PPD screening rates and staff behavior over time (Aim 3). Study aims, associated research activities, and a timeline are summarized in Table 1, below. Note to TIDIHR Instructors: Table would not format properly so I cannot include it in this format. I am happy to email if it would be helpful. <br />		<br />This implementation research will be guided by the Theoretical Domains Framework (TDF). The TDF is a consolidated theoretical framework that incorporates 128 constructs from 33 behavior change theories into 14 theoretical construct domains.43-45 TDF domains and associated constructs include: Knowledge (procedural knowledge, knowledge of condition/scientific rationale; Skills (competence, ability, interpersonal/practice skills); Social/Professional Role and Identity (professional confidence, group identity organizational commitment); Beliefs about Capabilities (self-efficacy, perceived competence); Optimism (optimism, pessimism); Beliefs about Consequences (outcome expectancies); Reinforcement (rewards, punishment); Intentions (stability of intentions); Goals (goal/target setting, action planning, implementation intention); Memory, Attention, and Decision Processes (memory, attention control, cognitive overload); Environmental Context and Resources (organizational culture/climate, material resources); Social Influences (social pressure/support, group conformity/norms); Emotion (anxiety, depression, stress); and Behavioral Regulation (self-monitoring, action planning).<br /><br />The TDF is well researched and has been widely used in a variety of contexts to inform and address implementation problems, including problem analysis, theorizing pathways of change, designing intervention, identifying appropriate process measures, testing pathways of change, and as a coding framework for analysis.43,46 Additionally, the TDF offers a versatile framework that has been effectively utilized to address a range of implementation issues. Therefore, the TDF is relevant for guiding aspects of all three aims of this project: assessing implementation barriers and facilitators while providing a structure for analysis/coding; informing the identification and development of appropriate behavioral change techniques; and exploring potential mechanisms of change. <br /><br />Aim 1: Identify barriers and facilitators to implementing PPD screening in rural Michigan WIC clinics<br /><br />Mixed methods will be used to identify barriers and facilitators to implementing PPD screening in rural Michigan WIC clinics. Semi-structured qualitative interviews, informed by the Theoretical Domains Framework (TDF), will be conducted with rural WIC clinic staff (e.g., nurses, nutritionists). A cross-sectional survey, also informed by the TDF, will be administered to rural WIC staff to assess barriers and facilitators to PPD screening, as well as clinic characteristics, staff characteristics, perceptions of depression among clients, and receptivity to addressing clients’ depressive symptoms on-site. <br /><br />This phase will target WIC staff and clinics in rural Michigan. Rural-Urban Continuum Code (RUCC) classifications of urbanicity will be used to identify WIC clinics and staff located in rural counties. Rural-Urban Continuum Codes (RUCCs), a county-level classification scheme developed by the U.S. Department of Agriculture measure rurality by population size and adjacency to metropolitan areas. The RUCCs contain 9 categories. For this study, RUCCs will be collapsed into the following 3 categories: urban (codes 1-3), suburban (codes as 4-6), and rural (codes 7-9). <br /><br />Qualitative interviews with WIC staff. A stratified cluster random sampling strategy will be used to identify WIC clinics where one staff member will be invited to participate in a semi-structured qualitative interview. Regions of the state (i.e., SW, SE, NW, SE, UP) will be strata and three WIC clinics will be randomly selected from each strata. Randomly selected WIC clinics will be contacted about this study and a staff member who provides direct service to clients will be invited to participate in the interview. If a clinic declines to participate, we will randomly select another from that stratum. Refusal rates will be tracked as a sign of feasibility and acceptability of PPD screening. It is expected that 15 WIC staff members will be interviewed. In addition to the TDF-informed interview guide, staff will be asked about their role in the clinic (e.g., nurse, nutritionist), educational background, previous mental health training/education, current clinic workflow, and receptivity to addres;’\sing depression in the WIC clinic. <br />Interviews will be conducted by phone by two research team members, one who will conduct the interview and one who will act as a scribe. All interviews will be audio recorded with participants’ permission. Rapid-cycle data analysis will be used to identify salient barriers and facilitators to PPD screening in rural Michigan WIC clinics. <br /><br />Cross-sectional surveys with WIC staff. Using a purposive sampling strategy, a cross-sectional survey will be administered to staff across all rural Michigan WIC clinics. Based upon the RUCCs, described above, all WIC clinics located in a rural county will receive a mail survey. The survey will draw from an existing survey of TDF construct domains and will be informed by the results of qualitative interviews with WIC staff. Questions concerning clinic size, resources, and capacity, client characteristics, and participating staff characteristics will be included as well. Survey instructions will indicate that a staff member who provides direct services to clients should complete the survey. An initial mailing and two reminders (2 weeks and one month apart) will be sent to maximize the response rate. Survey data will be analyzed in SPSS using descriptive and bivariate statistics, including independent samples t-tests and chi-square tests. <br /><br />Aim 2: Identify and tailor behavior change techniques to implement PPD screening <br /><br />The results from mixed methods research assessing barriers and facilitators of implementing PPD screening in rural Michigan WIC clinics will inform the identification and tailoring of behavior change techniques. The research team will integrate findings from Aim 1, informed by the TDF, with research-informed implementation strategies. <br /><br />Though the design and tailoring of the implementation intervention based upon behavior change techniques will be informed by staff-identified barriers and facilitators (Aim 1), TDF domains that appear most relevant to targeted individual behavior change have been selected to inform a preliminary implementation intervention design comprised of education interventions and audit and feedback (see Table 2, below; Note to TIDIHR Instructors: Same problem with table - happy to email as needed). Both intervention components are grounded in the implementation science literature and are well established, highly used strategies. The preliminary implementation intervention is also informed by interventions utilized in one identified study that has successfully initiated PPD screening within WIC clinics in an urban community.47<br /><br />In addition, during research activities associated with Aim 1, a rural Michigan WIC clinic that expresses interest in moving forward with PPD screening and shows signs of being an early adopter, will be identified as a pilot site for the implementation intervention. Key stakeholders (WIC administrators and staff) from the identified pilot site will provide input and feedback regarding the implementation intervention to improve feasibility and acceptability.<br /><br />Aim 3: Implement and assess the impact of behavior change techniques <br /><br />A mixed methods design, including a one group pre-/post- test assessing change over time in PPD screening rates and WIC staff behavior in one rural Michigan WIC clinic and qualitative interviews with WIC clinic staff, will be used to assess the impact of the behavior change techniques developed in Aim 2.<br /><br />One group pre-/post test. A one group pre-/post test will be use to assess the impact of behavior change techniques on PPD screening rates and WIC staff behavior over time. During the three months prior to delivering the behavior change techniques (i.e., educational interventions; audit and feedback), PPD screening rates will be assessed as a pre-test measure. PPD screening rate will be calculated by dividing the number of PPD screening tools administered to postpartum clients over a three-month period by the total number of postpartum client visits over the same time periods. Postpartum clients, whose infant is between birth and 12 months old, would be screened at each WIC visit; therefore the screening rate is based on the number of visits rather than the number of clients. This data will be obtained via client chart audit. Before beginning the behavior change techniques comprising the implementation intervention, all WIC staff at the targeted rural clinic who provide direct service to clients will be asked to complete a questionnaire assessing staff members’ baseline knowledge, behavior, and attitudes as related to TDF domains. This questionnaire will also include some key questions about staff characteristics (e.g., role (e.g., nurse, nutritionist), education level, length of time practicing). All clinic staff providing direct service to clients will participate in behavior change techniques, developed in Aim 2, to initiate PPD screening. After the conclusion of the behavior change techniques, PPD screening rates will be assessed over a three-month period via client chart audit. All staff participating in the behavior change techniques will complete a post-test assessment of their knowledge, behavior, and attitudes related to TDF domains. These data will allow us to examine the change over time in PPD screening rates as well as the change over time in staff knowledge, attitudes, and behavior. Examining change in staff knowledge, attitudes, and behaviors over time will help identify potential mechanisms of change that may assist the research team in understanding how and why the behavior change techniques did or did not work. Please note that the one group pre-/post- design will also be used to assess secondary outcomes, including mental health referrals and client refusal.<br /><br />Appropriate bivariate statistical analyses, including chi-square tests and paired-samples t-tests, will be used to assess change over time in outcome variables of interest. Additionally, regression analyses will be used examine potential predictors of screening among WIC staff, including TDF domains and staff characteristics (e.g., role, education level, previous mental health training). <br /><br />Qualitative interviews with WIC staff. Beginning six weeks into the 3-month period in which PPD screening rates will be measured, WIC staff will be invited to participate in a semi-structured qualitative interview, guided by the TDF domains. This interview will provide insight to barriers and facilitators to implementing PPD screening and help make sense of quantitative findings. Interview data will also suggest necessary refinements or changes to behavior change techniques in order to increase acceptability and sustainability of screening as standard practice. <br /><br />Two members of the research team will conduct interviews with WIC staff by phone at staff members’ convenience. One research team member will conduct the interview and the second research team member will scribe. Interviews will be recorded with participants’ permission. Rapid-cycle data analysis will be used to identify themes emerging from the qualitative data. <br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br /><br />Yes, mixed methods are incorporated into the study design, and described, above.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"b7aac1fc0ca966ad211a3bf3870072db";}s:4:"show";b:1;s:3:"cid";s:32:"dfe8e2a65ce99233132c652f83df2f79";}s:32:"00c5191873d7fe84164f0f3b8a496f7e";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dpintello";s:4:"name";s:14:"Denny Pintello";s:4:"mail";s:23:"Denise.Pintello@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539366417;}s:3:"raw";s:2002:"PROPOSED DESIGN
Brenna: Thank you for providing a clear description of your design – as well as including your thinking process behind your decision-making. I really like the approach that you are proposing: an RCT (maybe cluster RCT) using a hybrid II to test effectiveness of CBT among adults with ASD and test implementation strategies to enhance clinician attitudes, knowledge and skills/proficiency in the delivery of CBT (I hope I have the correct targets). Based on your description, I can picture it nicely – I suspect that 40 clinicians will be assigned to each group which should be a strong design to answer your research question. So - in summary – this looks good!

The only caveat is: what is the current research base on the effectiveness of CBT with autistic adults? If there has already been a few studies to date, this would be great so that you can build off of this prior work with a hybrid II. If there has only been 1-2 small studies, you may need to do a pilot effectiveness study (hybrid type I) before moving onto a hybrid II.  If there are no studies, you may have to establish the efficacy, which takes you out of the ballpark of implementation science for a few years. Please talk with your mentors to gain their thoughts on this. 

MIXED METHODS
I am very glad to see that you included Greg Aaron’s mixed method design typologies here. I would have loved to see more detail regarding how the specific qualitative approaches (interviews, focus groups, and with who) would be conducted and what process components would be included. Also – it would be helpful to learn more if, over the past few weeks, you have been able to identify which quantitative methods - surveys/measures might you use with the clinicians. These instruments would need to tap into knowledge, attitudes and skills – and through your design, explain how the qualitative work would deepen your understanding of these targets and their mechanisms of action. Overall – very well done! 
Denny";s:5:"xhtml";s:2037:"PROPOSED DESIGN<br />Brenna: Thank you for providing a clear description of your design – as well as including your thinking process behind your decision-making. I really like the approach that you are proposing: an RCT (maybe cluster RCT) using a hybrid II to test effectiveness of CBT among adults with ASD and test implementation strategies to enhance clinician attitudes, knowledge and skills/proficiency in the delivery of CBT (I hope I have the correct targets). Based on your description, I can picture it nicely – I suspect that 40 clinicians will be assigned to each group which should be a strong design to answer your research question. So - in summary – this looks good!<br /><br />The only caveat is: what is the current research base on the effectiveness of CBT with autistic adults? If there has already been a few studies to date, this would be great so that you can build off of this prior work with a hybrid II. If there has only been 1-2 small studies, you may need to do a pilot effectiveness study (hybrid type I) before moving onto a hybrid II.  If there are no studies, you may have to establish the efficacy, which takes you out of the ballpark of implementation science for a few years. Please talk with your mentors to gain their thoughts on this. <br /><br />MIXED METHODS<br />I am very glad to see that you included Greg Aaron’s mixed method design typologies here. I would have loved to see more detail regarding how the specific qualitative approaches (interviews, focus groups, and with who) would be conducted and what process components would be included. Also – it would be helpful to learn more if, over the past few weeks, you have been able to identify which quantitative methods - surveys/measures might you use with the clinicians. These instruments would need to tap into knowledge, attitudes and skills – and through your design, explain how the qualitative work would deepen your understanding of these targets and their mechanisms of action. Overall – very well done! <br />Denny";s:6:"parent";s:32:"f66d0922d41eaf8d6776e856f7c6dcec";s:7:"replies";a:1:{i:0;s:32:"6d70456cef3e0bf7a33431edde1fb3da";}s:4:"show";b:1;s:3:"cid";s:32:"00c5191873d7fe84164f0f3b8a496f7e";}s:32:"b7aac1fc0ca966ad211a3bf3870072db";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dpintello";s:4:"name";s:14:"Denny Pintello";s:4:"mail";s:23:"Denise.Pintello@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1539366499;s:8:"modified";i:1539724870;}s:3:"raw";s:2791:"Addie: Below is your additional response to assignment #4; and my comment will follow:

ADDIE: I actually found the Palinkas et al. article most helpful in thinking through how the mixed methods would function in my project. I understand that you are not requiring me to redo the assignment but wanted to share the thinking I have been doing around the design that was not reflected in my response. For example, for my third aim, which is assessing the implementation intervention, I view the mixed methods design structure as QUAN-->qual, as the primary purpose will be quantitatively assessing the impact of the implementation intervention. The qualitative interviews with staff proposed as part of Aim 3 will allow us to better understand our quantitative findings, specifically learning more about why we did or not find the change we hypothesized. Therefore, I also see this as having a complementarity function for the process of connecting the data sets. I found the mixed methods design proposed in Aim 1 to be a bit more challenging to specify; though it was important for me to think through. It seems to me that the design in Aim 1 is a QUAL-->Quan where we will begin by conducting qualitative interviews focused on barriers and facilitators to screening for postpartum depression with a small number of staff working in rural WIC clinics in Michigan and then utilize the information we learn from those interviews to inform development of a survey that will be sent to all rural WIC clinics in the state. I became a bit confused in the function domain as I see the mixed methods as functioning from convergence and complementarity standpoints as we will be interested in whether the same conclusions about barriers/facilitator are reached across the two types of data (convergence) and in the broader reach of the quantitative survey will provide a breadth of understanding around this issue (complementarity). However, this is also development since the qualitative interviews will help us to develop the quantitative survey. This aim may be one of merging or connecting. We will be able to triangulate some information but we will also use the qualitative findings to expand the questions we may ask in our quantitative survey. 

DENNY: Based on this additional information, I agree that your aims appear to use qualitative --> quantitative --> qualitative with an initial focus on convergence and Aim 3 focusing on diving more deeply to understand the impact (complementary). One possibility is to map out your hypotheses/questions a priori and develop a table similar to Greg’s to map out convergence – and then you might be able to add another column for complementary findings (Aim 3). I would think that a funder might find this type of table to be of interest.   ";s:5:"xhtml";s:2820:"Addie: Below is your additional response to assignment #4; and my comment will follow:<br /><br />ADDIE: I actually found the Palinkas et al. article most helpful in thinking through how the mixed methods would function in my project. I understand that you are not requiring me to redo the assignment but wanted to share the thinking I have been doing around the design that was not reflected in my response. For example, for my third aim, which is assessing the implementation intervention, I view the mixed methods design structure as QUAN--&gt;qual, as the primary purpose will be quantitatively assessing the impact of the implementation intervention. The qualitative interviews with staff proposed as part of Aim 3 will allow us to better understand our quantitative findings, specifically learning more about why we did or not find the change we hypothesized. Therefore, I also see this as having a complementarity function for the process of connecting the data sets. I found the mixed methods design proposed in Aim 1 to be a bit more challenging to specify; though it was important for me to think through. It seems to me that the design in Aim 1 is a QUAL--&gt;Quan where we will begin by conducting qualitative interviews focused on barriers and facilitators to screening for postpartum depression with a small number of staff working in rural WIC clinics in Michigan and then utilize the information we learn from those interviews to inform development of a survey that will be sent to all rural WIC clinics in the state. I became a bit confused in the function domain as I see the mixed methods as functioning from convergence and complementarity standpoints as we will be interested in whether the same conclusions about barriers/facilitator are reached across the two types of data (convergence) and in the broader reach of the quantitative survey will provide a breadth of understanding around this issue (complementarity). However, this is also development since the qualitative interviews will help us to develop the quantitative survey. This aim may be one of merging or connecting. We will be able to triangulate some information but we will also use the qualitative findings to expand the questions we may ask in our quantitative survey. <br /><br />DENNY: Based on this additional information, I agree that your aims appear to use qualitative --&gt; quantitative --&gt; qualitative with an initial focus on convergence and Aim 3 focusing on diving more deeply to understand the impact (complementary). One possibility is to map out your hypotheses/questions a priori and develop a table similar to Greg’s to map out convergence – and then you might be able to add another column for complementary findings (Aim 3). I would think that a funder might find this type of table to be of interest.";s:6:"parent";s:32:"dfe8e2a65ce99233132c652f83df2f79";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"b7aac1fc0ca966ad211a3bf3870072db";}s:32:"ee80b650bbf5034efa419ca86d7e7477";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"nbowersox";s:4:"name";s:13:"Nick Bowersox";s:4:"mail";s:24:"Nicholas.Bowersox@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539371919;}s:3:"raw";s:3596:"Robyn - 

Thank you for this information.  I always feel that your responses are well-thought-out (which is probably helped by the included numbered references).  

Your proposed plan to focus on the effectiveness of your intervention first is a sensible one, given that you are implementing a relatively new intervention, or at least an intervention into a new setting.  Occasionally, I will see proposals which include variations in implementation approach with interventions that have not yet been adequately evaluated, and such proposals always make me concerned about the ability to separate out intervention effects from implementation challenges. It is also good to see that you will be taking advantage of the access to providers to collect information on experienced barriers to implementation (which will provide fodder for later efforts to improve/vary your implementation approach).  The RE-AIM framework seems a good fit to the proposed project for evaluation purposes.

While I expect that your intervention will be found to be effective in promoting positive outcomes during your Type I trial, have you considered how you might change your approach in your proposed Type II trial if you get feedback that modifications would improve the intervention?  In other words, how comfortable would you be in modifying your intervention if you receive feedback that it is not as effective as you expect? I see you mention this in your response, but the question of "How much can we change this before it becomes something significantly different from what we tested?" is one that may be worth considering as you move from Type I to Type II approaches.

With regard to your planned implementation approach, I seem to remember that you are developing a toolkit to support the implementation process for your intervention, correct?  If this is the case, you might want to consider using facilitation as a secondary implementation approach, perhaps for sites that are struggling to effectively implement the program using the toolkit alone.  This feedback is based on the idea that facilitation is a very high-resource implementation strategy which may be challenging to apply to all new sites interested in implementing your project.  Alternatively, you could consider a less intensive approach to facilitation, such as developing a community of practice which is overseen by a facilitator or an approach where the facilitator provides some front-end assessments of fidelity but does not actively engage with sites unless they are found to not be delivering the intervention as expected.

I agree that a mixed method approach is the most appropriate way to collect data relevant to this study.  The qualitative/stakeholder feedback approach is sensible to me, although I wonder about your plans to collect quantitative information related to implementation and fidelity.  You mention fidelity checklists - is this something that you are planning on developing or do you have particular tools in mind?  Also, have you developed clear parameters for your expectations for therapy delivery "as expected"?  I have seen approaches where this is based on number of sessions, patient feedback, or frequency of sessions.  I would be interested to hear your current thinking about the best way to capture this concept or your plans to do so.

Again, very interesting project.  I think some consideration about operationalization (which you may have already done) would help to flesh out some of these areas and help you to avoid having to figure out important study aspects "on the fly."

Best,

- Nick";s:5:"xhtml";s:3706:"Robyn - <br /><br />Thank you for this information.  I always feel that your responses are well-thought-out (which is probably helped by the included numbered references).  <br /><br />Your proposed plan to focus on the effectiveness of your intervention first is a sensible one, given that you are implementing a relatively new intervention, or at least an intervention into a new setting.  Occasionally, I will see proposals which include variations in implementation approach with interventions that have not yet been adequately evaluated, and such proposals always make me concerned about the ability to separate out intervention effects from implementation challenges. It is also good to see that you will be taking advantage of the access to providers to collect information on experienced barriers to implementation (which will provide fodder for later efforts to improve/vary your implementation approach).  The RE-AIM framework seems a good fit to the proposed project for evaluation purposes.<br /><br />While I expect that your intervention will be found to be effective in promoting positive outcomes during your Type I trial, have you considered how you might change your approach in your proposed Type II trial if you get feedback that modifications would improve the intervention?  In other words, how comfortable would you be in modifying your intervention if you receive feedback that it is not as effective as you expect? I see you mention this in your response, but the question of &quot;How much can we change this before it becomes something significantly different from what we tested?&quot; is one that may be worth considering as you move from Type I to Type II approaches.<br /><br />With regard to your planned implementation approach, I seem to remember that you are developing a toolkit to support the implementation process for your intervention, correct?  If this is the case, you might want to consider using facilitation as a secondary implementation approach, perhaps for sites that are struggling to effectively implement the program using the toolkit alone.  This feedback is based on the idea that facilitation is a very high-resource implementation strategy which may be challenging to apply to all new sites interested in implementing your project.  Alternatively, you could consider a less intensive approach to facilitation, such as developing a community of practice which is overseen by a facilitator or an approach where the facilitator provides some front-end assessments of fidelity but does not actively engage with sites unless they are found to not be delivering the intervention as expected.<br /><br />I agree that a mixed method approach is the most appropriate way to collect data relevant to this study.  The qualitative/stakeholder feedback approach is sensible to me, although I wonder about your plans to collect quantitative information related to implementation and fidelity.  You mention fidelity checklists - is this something that you are planning on developing or do you have particular tools in mind?  Also, have you developed clear parameters for your expectations for therapy delivery &quot;as expected&quot;?  I have seen approaches where this is based on number of sessions, patient feedback, or frequency of sessions.  I would be interested to hear your current thinking about the best way to capture this concept or your plans to do so.<br /><br />Again, very interesting project.  I think some consideration about operationalization (which you may have already done) would help to flesh out some of these areas and help you to avoid having to figure out important study aspects &quot;on the fly.&quot;<br /><br />Best,<br /><br />- Nick";s:6:"parent";s:32:"91d14fcda0b357b8a7e61ad74f269c37";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"ee80b650bbf5034efa419ca86d7e7477";}s:32:"0747f638776a25f78e759d356936d412";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"nbowersox";s:4:"name";s:13:"Nick Bowersox";s:4:"mail";s:24:"Nicholas.Bowersox@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539374548;}s:3:"raw";s:3780:"Hi, Ana - it's good to hear a bit more about your plans in this area.  You have a unique situation in which you have just enough sites to assess implementation barriers which may be location specific, but few enough that you can still engage in a really "deep dive" to assess these areas without this approach being overwhelming.  Even so, I wonder if four (?) sites will be enough to allow you to fully appreciate the various barriers/facilitators to implementation for such a "big idea" project.

You mention that your planned implementation approaches to be used for the new sites will be based on conversations with stakeholders at sites which are already participating in the program.  Do you have a particular implementation strategy which you are planning to modify based on feedback, or are you considering several potential strategies and planning on choosing one based on this feedback?  What strategy did you use to get the program implemented at your initial site?

I don't follow your use of propensity scoring in this context.  Are you saying that you will be assessing if patients who would benefit from the program are actually receiving the program?  OR perhaps ensuring that your comparison cohort (who did not receive the program) does not differ in important ways from the group that did receive the intervention which could also influence their likelihood to receive the intervention, other than random assignment to a comparison group?

It's a smart move to collect stakeholder feedback from multiple levels when evaluating a program - the perspectives of providers, administrators, and patients will all help to ensure that you develop an implementation approach that effectively "sells" the program to all of these stakeholders and offers each of them something that leads to their support of the program.

I agree that a Hybrid III approach would be challenging given the relatively small number of participating sites.  Your plan to isolate providers could help to get around this issue a bit, although as you point out this could be challenging to maintain.  A sequential stepped-wedge approach is also a smart way to try to make the most of what you have to work with for this project.  Also, are all participating pilot sites in the same geographic area/health care network?  This may lead to the identification of implementation challenges that are less representative to other settings.  Even so, I always think it is better to conduct a study and be aware of your limitations than to give up on a project, so perhaps you can proceed and think of some creative ways around these limitations (maybe stakeholder interviews at non-pilot sites who are briefed about the program and asked to offer their perspectives on the implementation process?)

I agree with you about the value of mixed methods, especially when dealing with intensive and complicated interventions such as you are proposing.  Your plan to move from quantitative to qualitative to focused qualitative interview/data collection approaches is classic foundational program evaluation methods and makes a lot of sense - this approach can really help to zero in on what is occurring within a program and develop a nuanced understanding of the current situation and how to improve it.  Taking this stepped data collection approach can be quite time-consuming, however - I would be interested to hear about your planned timeline for this process and the ways that you will be able to benefit from data collected during intermediate steps rather than having to wait until the end of the process to act on your findings.

All in all, I think you made it pretty easy for me this week - you have clearly thought a lot about these topics and have a fairly strong plan.  Nice work!

Best,

- Nick";s:5:"xhtml";s:3905:"Hi, Ana - it&#039;s good to hear a bit more about your plans in this area.  You have a unique situation in which you have just enough sites to assess implementation barriers which may be location specific, but few enough that you can still engage in a really &quot;deep dive&quot; to assess these areas without this approach being overwhelming.  Even so, I wonder if four (?) sites will be enough to allow you to fully appreciate the various barriers/facilitators to implementation for such a &quot;big idea&quot; project.<br /><br />You mention that your planned implementation approaches to be used for the new sites will be based on conversations with stakeholders at sites which are already participating in the program.  Do you have a particular implementation strategy which you are planning to modify based on feedback, or are you considering several potential strategies and planning on choosing one based on this feedback?  What strategy did you use to get the program implemented at your initial site?<br /><br />I don&#039;t follow your use of propensity scoring in this context.  Are you saying that you will be assessing if patients who would benefit from the program are actually receiving the program?  OR perhaps ensuring that your comparison cohort (who did not receive the program) does not differ in important ways from the group that did receive the intervention which could also influence their likelihood to receive the intervention, other than random assignment to a comparison group?<br /><br />It&#039;s a smart move to collect stakeholder feedback from multiple levels when evaluating a program - the perspectives of providers, administrators, and patients will all help to ensure that you develop an implementation approach that effectively &quot;sells&quot; the program to all of these stakeholders and offers each of them something that leads to their support of the program.<br /><br />I agree that a Hybrid III approach would be challenging given the relatively small number of participating sites.  Your plan to isolate providers could help to get around this issue a bit, although as you point out this could be challenging to maintain.  A sequential stepped-wedge approach is also a smart way to try to make the most of what you have to work with for this project.  Also, are all participating pilot sites in the same geographic area/health care network?  This may lead to the identification of implementation challenges that are less representative to other settings.  Even so, I always think it is better to conduct a study and be aware of your limitations than to give up on a project, so perhaps you can proceed and think of some creative ways around these limitations (maybe stakeholder interviews at non-pilot sites who are briefed about the program and asked to offer their perspectives on the implementation process?)<br /><br />I agree with you about the value of mixed methods, especially when dealing with intensive and complicated interventions such as you are proposing.  Your plan to move from quantitative to qualitative to focused qualitative interview/data collection approaches is classic foundational program evaluation methods and makes a lot of sense - this approach can really help to zero in on what is occurring within a program and develop a nuanced understanding of the current situation and how to improve it.  Taking this stepped data collection approach can be quite time-consuming, however - I would be interested to hear about your planned timeline for this process and the ways that you will be able to benefit from data collected during intermediate steps rather than having to wait until the end of the process to act on your findings.<br /><br />All in all, I think you made it pretty easy for me this week - you have clearly thought a lot about these topics and have a fairly strong plan.  Nice work!<br /><br />Best,<br /><br />- Nick";s:6:"parent";s:32:"940ddd25e15b3ed46422c451cd69006e";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"0747f638776a25f78e759d356936d412";}s:32:"c5efd88ff8513c74d3837f4d38c63243";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"nbowersox";s:4:"name";s:13:"Nick Bowersox";s:4:"mail";s:24:"Nicholas.Bowersox@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539375743;}s:3:"raw";s:2106:"Hi, Ilana - thanks for this additional information.

You are describing a fairly ambitious study design, which will provide some really great information if you can pull it off.  I seem to remember that you had a relatively limited number of providers to participate in this study, based on a small number of providers who are able to offer mental health care in this region - is that correct?  If so, will you have adequate expert support/staffing to cover three areas and multiple clinics within each area?  You will want to be sure that your exert facilitators have adequate time and resources to effectively provide training and oversight/fidelity monitoring across all of these care locations.

Apologies - I know that you may have discussed this previously, but what sorts of quantitative data are you planning on collecting in this study?  The stakeholder interviews make sense (and will serve this project well), but I was unclear on the sources of quantitative data and how this would interface with the information collected from stakeholder interviews.

Also, given that you are proposing to implement an intervention into a novel setting, be aware that a type II design, where you are varying the implementation approach while also assessing implementation effectiveness, can be a bit risky.  If you ultimately do not find an intervention effect, it can be difficult to distinguish between results due to implementation challenges and results due to intervention ineffectiveness.  Even a relatively small pilot demonstration which focuses on intervention effectiveness can help to address this concern.

You mention that "the delivery pathway showing the highest overall effectiveness will be implemented in districts from the other two arms."  Can you talk a little bit about the planned implementation approaches that you will be testing in the three arms?  Given that there may be unique implementation barriers/facilitators in different locations, how will you distinguish regional barriers/facilitators from differential effectiveness of different implementation approaches?

Best,

- Nick";s:5:"xhtml";s:2176:"Hi, Ilana - thanks for this additional information.<br /><br />You are describing a fairly ambitious study design, which will provide some really great information if you can pull it off.  I seem to remember that you had a relatively limited number of providers to participate in this study, based on a small number of providers who are able to offer mental health care in this region - is that correct?  If so, will you have adequate expert support/staffing to cover three areas and multiple clinics within each area?  You will want to be sure that your exert facilitators have adequate time and resources to effectively provide training and oversight/fidelity monitoring across all of these care locations.<br /><br />Apologies - I know that you may have discussed this previously, but what sorts of quantitative data are you planning on collecting in this study?  The stakeholder interviews make sense (and will serve this project well), but I was unclear on the sources of quantitative data and how this would interface with the information collected from stakeholder interviews.<br /><br />Also, given that you are proposing to implement an intervention into a novel setting, be aware that a type II design, where you are varying the implementation approach while also assessing implementation effectiveness, can be a bit risky.  If you ultimately do not find an intervention effect, it can be difficult to distinguish between results due to implementation challenges and results due to intervention ineffectiveness.  Even a relatively small pilot demonstration which focuses on intervention effectiveness can help to address this concern.<br /><br />You mention that &quot;the delivery pathway showing the highest overall effectiveness will be implemented in districts from the other two arms.&quot;  Can you talk a little bit about the planned implementation approaches that you will be testing in the three arms?  Given that there may be unique implementation barriers/facilitators in different locations, how will you distinguish regional barriers/facilitators from differential effectiveness of different implementation approaches?<br /><br />Best,<br /><br />- Nick";s:6:"parent";s:32:"4d1e378a566b4d52d5a4a3e01f693bf1";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"c5efd88ff8513c74d3837f4d38c63243";}s:32:"c8ca6956424cf9183fec144b98317c4b";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1539561324;s:8:"modified";i:1539561407;}s:3:"raw";s:3214:"Hi Alex,

It was great to read the progress you've made to make your Specific Aims clearer :)
At the same, I am not yet able to evaluate how your methods support your aims. I provided some ideas below that I hope will be fruitful for developing your design.

Describe your proposed mixed methods design. Are you considering anything from the TIDIRH lecture? 

You propose use of the following "qualitative" methods (open-ended survey questions, structured Webinar discussion, completion of sample fiscal maps, participatory model-building exercise).
- Each one should be justified as the most appropriate method, operationally defined, and linked to its purpose for supporting a specific aim.
- Note that each underspecified method you mention can ding your proposal on the "approach" review criteria. 
- It would be better to propose less and clarify and support more, so reviewers clearly understand your rationale and can evaluate whether your methods will achieve your aims.
- In particular, I would really detail the background research that supports your approach to a) a review of fiscal mapping strategies, and b) participatory model-building, which are central to your project.

You also propose use of "quantitative" methods (questionnaires and rating scales).
- You really need to describe which measures you will use, their psychometric properties in prior research, how you will evaluate their psychometric properties in your study, methods of statistical analysis and why these (selection measures, approach to analyses) are valid and reliable for your intended research questions. 

Your "triangulation" statement needs to be defined and better supported. You mention two components but don't use or cite a standard definition of triangulation methodology. Better would be to say: "Following Denzin, 2012 (see Triangulation 2.0) we will..." 
- Explain why triangulation is critical to advance this science. 
- Specify your methods for triangulation and the purpose of triangulation.
- Are you triangulating: theories, data sources, methods? 
- As written, it reads like you are bringing together your fiscal mapping review work and participatory modeling, but not your qualitative and quantitive research methods into a cogent study design.

Try this to help:
Using a mixed methods design (Aarons et. al), we will:
aim 1. Generate a comprehensive compilation of EBT financing strategies [...defined as...] using criteria...[describe well-validated search and review methods procedures from Cochrane Collaborative or...?] A systematic review, a meta-analysis, a scoping review, a realist synthesis?? Describe this and reference the relevant standard reporting guidelines, e.g., http://www.prisma-statement.org
aim 2. Develop a fiscal mapping process using participatory modeling procedures(Hovmand, 2014; Anderson and Richardson?) developed by for...

One last thing, as written now, it not yet clear how your aims and methods for the R15 will inform an R01.

I hope this helps! I recommend drafting these details and getting them down "on the page" sooner rather than later, that way you can benefit from even richer feedback from Denny, Nick and myself.

Alex, I'm very interested in your project! ";s:5:"xhtml";s:3443:"Hi Alex,<br /><br />It was great to read the progress you&#039;ve made to make your Specific Aims clearer :)<br />At the same, I am not yet able to evaluate how your methods support your aims. I provided some ideas below that I hope will be fruitful for developing your design.<br /><br />Describe your proposed mixed methods design. Are you considering anything from the TIDIRH lecture? <br /><br />You propose use of the following &quot;qualitative&quot; methods (open-ended survey questions, structured Webinar discussion, completion of sample fiscal maps, participatory model-building exercise).<br />- Each one should be justified as the most appropriate method, operationally defined, and linked to its purpose for supporting a specific aim.<br />- Note that each underspecified method you mention can ding your proposal on the &quot;approach&quot; review criteria. <br />- It would be better to propose less and clarify and support more, so reviewers clearly understand your rationale and can evaluate whether your methods will achieve your aims.<br />- In particular, I would really detail the background research that supports your approach to a) a review of fiscal mapping strategies, and b) participatory model-building, which are central to your project.<br /><br />You also propose use of &quot;quantitative&quot; methods (questionnaires and rating scales).<br />- You really need to describe which measures you will use, their psychometric properties in prior research, how you will evaluate their psychometric properties in your study, methods of statistical analysis and why these (selection measures, approach to analyses) are valid and reliable for your intended research questions. <br /><br />Your &quot;triangulation&quot; statement needs to be defined and better supported. You mention two components but don&#039;t use or cite a standard definition of triangulation methodology. Better would be to say: &quot;Following Denzin, 2012 (see Triangulation 2.0) we will...&quot; <br />- Explain why triangulation is critical to advance this science. <br />- Specify your methods for triangulation and the purpose of triangulation.<br />- Are you triangulating: theories, data sources, methods? <br />- As written, it reads like you are bringing together your fiscal mapping review work and participatory modeling, but not your qualitative and quantitive research methods into a cogent study design.<br /><br />Try this to help:<br />Using a mixed methods design (Aarons et. al), we will:<br />aim 1. Generate a comprehensive compilation of EBT financing strategies [...defined as...] using criteria...[describe well-validated search and review methods procedures from Cochrane Collaborative or...?] A systematic review, a meta-analysis, a scoping review, a realist synthesis?? Describe this and reference the relevant standard reporting guidelines, e.g., http://www.prisma-statement.org<br />aim 2. Develop a fiscal mapping process using participatory modeling procedures(Hovmand, 2014; Anderson and Richardson?) developed by for...<br /><br />One last thing, as written now, it not yet clear how your aims and methods for the R15 will inform an R01.<br /><br />I hope this helps! I recommend drafting these details and getting them down &quot;on the page&quot; sooner rather than later, that way you can benefit from even richer feedback from Denny, Nick and myself.<br /><br />Alex, I&#039;m very interested in your project!";s:6:"parent";s:32:"94fd72389f4c70f3140ebb4e84d1a157";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"c8ca6956424cf9183fec144b98317c4b";}s:32:"187ce64f873b1293b2264671840ec46c";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1539562980;s:8:"modified";i:1539563069;}s:3:"raw";s:3298:"Hi Kelsie!

You say you would like to use a mixed methods design. Have you decided what type? Anything from the TIDIRH lecture?

You also propose an "observational and explanatory research design to answer the question “What are important organizational- and individual-level MBC determinants in two large behavioral health divisions?” 
This question is great! I would really narrow down and support your approach to answering this question.
- At the same time, it is not clear why you selected this research design or how it fits with your mixed methods design. Your draft needs a clearer structure describing the way you will evaluate determinants and what measures you will use.

Your response includes quite a list of aims/research questions! It would be helpful to prioritize them, and link each within your overarching design.
I have some questions that may help you see what I mean.

Here's the list of your aims/research questions that I got from your post - I follow up with a question it raised for me:
(a) Does organizational climate impact the use of MBC - LZ: which organizational climate measure will you use, how was it selected, has it been used in similar prior research with community clinic staff, what are the psychometric properties, how will you evaluate them in the present study, what analyses will be used?
(b) Does burnout decrease the use of MBC - LZ: same as (a) which measure, etc.?
(c) Do attitudes toward standardized assessment increase MBC, - LZ: same as (a) which measure, etc.?
(d) Can an EHR accurately measure MBC within community mental health? - LZ: How will this be assessed?
- understand if teamwork and group cohesiveness (i.e., co-management of care) improve the use of evidence-based practices and outcomes. - LZ: same as (a) which measure, etc.?
- determine a taxonomy and framework for future MBC studies - LZ: How will this be determined?
- inform determinant measures of MBC within the Divisions - LZ: Does your design and analysis plan evaluate determinant measures? If so, how?

I think these are all important questions that could advance scientific understanding of MBC implementation! And I can see how they flow logically from the 19 interviews and focus groups you've conducted to date. At the same time, I cannot evaluate yet whether you are proposing a study that will advance implementation science without clearer operational definitions and justification of your proposed procedures (over others you may have considered).

You also list methods and their purpose, but as a reader, I need more detail about your rationale/justification for their selection for this intended purpose and specification of exactly what you will do. Here are the examples that stood out to me:
- use sequential qual/quan design to determine a taxonomy and framework for future MBC studies (what is the sequence, what taxonomy approach will you use?)
- use theme reduction to refine hypotheses to select pragmatic implementation measures (I'm not sure I'm getting this one yet).

I think these are very important research questions related to use of MBC in your study settings. And, you are in a good position to answer them, Kelsie!

I hope my questions will highlight places that could be "pinned down" and made a little clearer with your next revisions!
";s:5:"xhtml";s:3472:"Hi Kelsie!<br /><br />You say you would like to use a mixed methods design. Have you decided what type? Anything from the TIDIRH lecture?<br /><br />You also propose an &quot;observational and explanatory research design to answer the question “What are important organizational- and individual-level MBC determinants in two large behavioral health divisions?” <br />This question is great! I would really narrow down and support your approach to answering this question.<br />- At the same time, it is not clear why you selected this research design or how it fits with your mixed methods design. Your draft needs a clearer structure describing the way you will evaluate determinants and what measures you will use.<br /><br />Your response includes quite a list of aims/research questions! It would be helpful to prioritize them, and link each within your overarching design.<br />I have some questions that may help you see what I mean.<br /><br />Here&#039;s the list of your aims/research questions that I got from your post - I follow up with a question it raised for me:<br />(a) Does organizational climate impact the use of MBC - LZ: which organizational climate measure will you use, how was it selected, has it been used in similar prior research with community clinic staff, what are the psychometric properties, how will you evaluate them in the present study, what analyses will be used?<br />(b) Does burnout decrease the use of MBC - LZ: same as (a) which measure, etc.?<br />(c) Do attitudes toward standardized assessment increase MBC, - LZ: same as (a) which measure, etc.?<br />(d) Can an EHR accurately measure MBC within community mental health? - LZ: How will this be assessed?<br />- understand if teamwork and group cohesiveness (i.e., co-management of care) improve the use of evidence-based practices and outcomes. - LZ: same as (a) which measure, etc.?<br />- determine a taxonomy and framework for future MBC studies - LZ: How will this be determined?<br />- inform determinant measures of MBC within the Divisions - LZ: Does your design and analysis plan evaluate determinant measures? If so, how?<br /><br />I think these are all important questions that could advance scientific understanding of MBC implementation! And I can see how they flow logically from the 19 interviews and focus groups you&#039;ve conducted to date. At the same time, I cannot evaluate yet whether you are proposing a study that will advance implementation science without clearer operational definitions and justification of your proposed procedures (over others you may have considered).<br /><br />You also list methods and their purpose, but as a reader, I need more detail about your rationale/justification for their selection for this intended purpose and specification of exactly what you will do. Here are the examples that stood out to me:<br />- use sequential qual/quan design to determine a taxonomy and framework for future MBC studies (what is the sequence, what taxonomy approach will you use?)<br />- use theme reduction to refine hypotheses to select pragmatic implementation measures (I&#039;m not sure I&#039;m getting this one yet).<br /><br />I think these are very important research questions related to use of MBC in your study settings. And, you are in a good position to answer them, Kelsie!<br /><br />I hope my questions will highlight places that could be &quot;pinned down&quot; and made a little clearer with your next revisions!";s:6:"parent";s:32:"61ec5b447c005aa9cbe22b4c7a03ce70";s:7:"replies";a:1:{i:0;s:32:"7ac1c68ad6f1d9da1ca8c98d64fcb1ba";}s:4:"show";b:1;s:3:"cid";s:32:"187ce64f873b1293b2264671840ec46c";}s:32:"6d70456cef3e0bf7a33431edde1fb3da";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bmaddox";s:4:"name";s:13:"Brenna Maddox";s:4:"mail";s:17:"maddoxb@upenn.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539781106;}s:3:"raw";s:365:"Hi Denny,
Your feedback is incredibly helpful - thank you! There have been at least 5 studies on CBT for anxiety/depression in adults with autism. I will talk with David next week when we meet about his thoughts on whether a hybrid type I or II makes the most sense moving forward. I am working on finalizing my plans for quantitative methods as well. Thanks again!";s:5:"xhtml";s:370:"Hi Denny,<br />Your feedback is incredibly helpful - thank you! There have been at least 5 studies on CBT for anxiety/depression in adults with autism. I will talk with David next week when we meet about his thoughts on whether a hybrid type I or II makes the most sense moving forward. I am working on finalizing my plans for quantitative methods as well. Thanks again!";s:6:"parent";s:32:"00c5191873d7fe84164f0f3b8a496f7e";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"6d70456cef3e0bf7a33431edde1fb3da";}s:32:"8e7ed03cc182fe2a6799184e44a62848";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bmaddox";s:4:"name";s:13:"Brenna Maddox";s:4:"mail";s:17:"maddoxb@upenn.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539786040;}s:3:"raw";s:3210:"MADDOX - Assignment #5:

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

I will be focusing on several implementation strategies in my proposed study: therapist training, consultation, and opinion leader involvement. The therapist training will involve a one-day, interactive workshop about adults with ASD and CBT for this population. The consultation will be similar to clinical supervision – a clinician who is expert in CBT for adults with autism will meet weekly with newly trained clinicians to review cases and answer questions. Consultation will last for 6 months after the training workshop. I selected training and consultation as strategies to target knowledge about CBT for adults with autism, skills to use CBT for adults with autism, and confidence about using CBT for adults with autism. Clinician training is a common implementation strategy in mental health services, but recent research has shown that training alone is unlikely to change clinician behavior. Post-training consultation or coaching is important for translating increased knowledge to improved practice. I selected opinion leaders as an implementation strategy to change clinicians’ attitudes and beliefs that (1) CBT is not appropriate or effective for people with ASD, and (2) as mental health providers (separate from the developmental disability system), they are unequipped to treat people with ASD. I am in the process of gathering information from community mental health agency leaders and clinicians about the most effective ways for opinion leaders to influence clinicians. For example, opinion leaders could send out emails to clinicians encouraging the use of CBT with adults with autism or host a meeting or presentation for clinicians about the positive effects of mental health providers delivering CBT for adults with autism. 

2.	How might you link specific implementation strategies to the context in which your work is set?

My proposed implementation strategies are based on preliminary data from the content in which my work is set – specifically, urban community mental health centers. From our TIDIRH lecture for this module, I found the slide about matching implementation strategies to determinants very helpful. My current F32 study has helped me to identify the determinants in the community mental health setting. For example, for the identified determinant of lack of knowledge, my corresponding implementation strategies are an interactive workshop and consultation. For the identified determinant of clinician beliefs and attitudes, my corresponding implementation strategy is peer influence by way of opinion leaders. Thus, I am able to tailor the proposed implementation strategies to address barriers identified through previous data collection.

3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?

N/A";s:5:"xhtml";s:3270:"MADDOX - Assignment #5:<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />I will be focusing on several implementation strategies in my proposed study: therapist training, consultation, and opinion leader involvement. The therapist training will involve a one-day, interactive workshop about adults with ASD and CBT for this population. The consultation will be similar to clinical supervision – a clinician who is expert in CBT for adults with autism will meet weekly with newly trained clinicians to review cases and answer questions. Consultation will last for 6 months after the training workshop. I selected training and consultation as strategies to target knowledge about CBT for adults with autism, skills to use CBT for adults with autism, and confidence about using CBT for adults with autism. Clinician training is a common implementation strategy in mental health services, but recent research has shown that training alone is unlikely to change clinician behavior. Post-training consultation or coaching is important for translating increased knowledge to improved practice. I selected opinion leaders as an implementation strategy to change clinicians’ attitudes and beliefs that (1) CBT is not appropriate or effective for people with ASD, and (2) as mental health providers (separate from the developmental disability system), they are unequipped to treat people with ASD. I am in the process of gathering information from community mental health agency leaders and clinicians about the most effective ways for opinion leaders to influence clinicians. For example, opinion leaders could send out emails to clinicians encouraging the use of CBT with adults with autism or host a meeting or presentation for clinicians about the positive effects of mental health providers delivering CBT for adults with autism. <br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />My proposed implementation strategies are based on preliminary data from the content in which my work is set – specifically, urban community mental health centers. From our TIDIRH lecture for this module, I found the slide about matching implementation strategies to determinants very helpful. My current F32 study has helped me to identify the determinants in the community mental health setting. For example, for the identified determinant of lack of knowledge, my corresponding implementation strategies are an interactive workshop and consultation. For the identified determinant of clinician beliefs and attitudes, my corresponding implementation strategy is peer influence by way of opinion leaders. Thus, I am able to tailor the proposed implementation strategies to address barriers identified through previous data collection.<br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /><br />N/A";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"cd6f5b95031297b87a2c2427029378f9";}s:4:"show";b:1;s:3:"cid";s:32:"8e7ed03cc182fe2a6799184e44a62848";}s:32:"65cad28f2e9526773d29ea6c4594ba27";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"adopp";s:4:"name";s:9:"Alex Dopp";s:4:"mail";s:13:"dopp@uark.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539955701;}s:3:"raw";s:13677:"Dopp - Assignment #5

1. If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

My project seeks to develop a fiscal mapping process (and component financing strategies) that will guide strategic planning efforts to sustainably fund evidence-based treatments in youth mental health services. I define financing strategies as implementation strategies that secure and direct financial resources to support activities involved in implementation and/or sustainment of EBTs. Thus, I seek to develop tools that will help better understand and coordinate a wide variety of implementation strategies in the financing domain, rather than selecting and focusing on specific implementation strategies.

The proposed project is grounded within Schell et al.’s public health sustainability framework, which identifies eight core domains (including funding stability) of capacity for sustainment in public health programs. The central domain in that framework is strategic planning – defined as processes that guide a program’s directions, goals, and strategies – which coordinates all sustainability domains into an outcome-oriented plan (e.g., one may need to build partnerships that improve organizational capacity before funding stability is possible). My Specific Aims, described next in more detail, are strongly informed by the central importance of strategic planning in the selection of implementation strategies (including financing strategies) to support sustainment efforts.

Aim 1: Compilation of financing strategies. Strategic planning requires sufficient understanding of the options available to achieve a goal or solve a problem. Therefore, I will first seek to generate a comprehensive compilation of financing strategies used in youth mental health services. Over the past decade, a national group of implementation experts has begun a broader effort to compile and describe implementation strategies. This effort has provided a much-needed set of best-practice strategies for implementation research and practice. However, closer examination reveals that their proposed category of “financial strategies” included strategies that were poorly specified (e.g., “Fund and contract for the clinical innovation,” “Access new funding”) and did not make important distinctions between financing strategies and financial incentives. The latter are grounded in the study of behavioral economics of various incentives (e.g., financial, social) for EBT implementation, and thus fall outside the scope of the proposed research. As a step toward better description of financing strategies, I described pay-for-success contracts (one promising financing strategy) using seven key dimensions for implementation strategies in a preliminary study. That study illustrated the complexity involved in specifying a financing strategy; for instance, pay-for-success contracts involve principal actors across the service delivery and financing systems (e.g., private investors, government payer, nonprofit service delivery agency, evaluation team), all with different roles throughout the implementation process. These findings suggest that dedicated efforts will be needed to adequately describe all available financing strategies. A comprehensive, well-specified compilation of financing strategies will provide a foundational basis for strategic planning of EBT sustainment efforts in youth mental health services.

Aim 2: Fiscal mapping process. A compilation of financing strategies is necessary, but not sufficient, for identifying and selecting the optimal combination of financing strategies for an EBT sustainment effort. In recent years, implementation experts have begun to emphasize the importance of “tailored selection” of implementation strategies. Tailoring – essentially a form of strategic planning – refers to selection of strategies that match the goals, strengths, and needs of a given EBT implementation effort (e.g., use of training to increase provider knowledge vs. dissemination of information to increase public support for a program; selection of financing strategies that best fit the local cultural and legal environment). Unfortunately, financing strategies are often considered complex or infeasible by implementation experts, mental health service agencies, and purveyor/intermediary organizations, indicating that the various stakeholders within youth mental health services will require support to successfully make use of these strategies. Such findings are consistent with a preliminary study recently completed by myself and other members of the project team, as part of a larger project in which we implemented Problematic Sexual Behavior – Cognitive-Behavioral Therapy (PSB-CBT) programs at six sites nationwide. Across qualitative interviews with 12 agency administrators and 36 stakeholder partners, 83% of interviewees (including 100% of administrators) mentioned that planning for sustained funding was challenging. They also identified 9 different categories of possible future funding sources, including insurance, state and local agencies, and private foundations. No single funding source was expected to fully sustain a PSB-CBT program (e.g., many did not fund caregiver groups) and the most promising funding sources varied across states. Overall, interviewees indicated that strategic use of “creative funding strategies” and “blending” multiple funding sources (including a mix of public and private dollars) was necessary to sustain PSB-CBT.

The above findings suggest that a tailoring (i.e., strategic planning) process for financing strategies would facilitate EBT sustainment efforts. Methods of tailoring implementation strategies are in their infancy, and to date none have focused on strategic selection of financing strategies in particular. However, a recent paper by a number of prominent implementation experts identified several promising approaches. Of the methods identified, intervention mapping shows the greatest promise for the strategic planning of financing strategies. Intervention mapping is a well-specified, multi-step method for developing interventions (or implementation strategies) based on theory, research evidence, and stakeholder perspectives. Intervention mapping has been widely applied in medicine and behavioral health. It is especially well suited to addressing pragmatic issues in implementation, which is critical for funding decisions, whereas other proposed methods are better suited to achieving clarity around conceptual issues (e.g., achieving consensus on desired implementation outcomes). I propose to adapt this process into a fiscal mapping process that will guide youth mental health service agencies, funding agencies, and purveyors/intermediaries in their strategic planning efforts to finance EBT sustainment.


2. How might you link specific implementation strategies to the context in which your work is set?

Throughout the proposed project, I will collect data that will assist stakeholders from youth mental health services in linking specific financing strategies to the contexts in which their work is set. Specifically, each Specific Aim contains a sub-aim focused on describing characteristics that will help guide tailored selection (strategic planning) of financing strategies during the fiscal mapping process.

Aim 1b: Describe characteristics that inform strategy selection. Once specification of the financial strategies is complete, the expert panel of stakeholders (40 representatives from EBT intermediaries, youth mental health service agencies, funding agencies) will complete a follow-up online survey in which they will describe key characteristics that would inform the potential selection of each strategy during the strategic planning process. Specifically, for each strategy, participants will provide ratings of its (a) availability in their setting (yes/no); (b) level of suitability for sustaining different types of implementation activities (listed in Figure 1); (c) importance (i.e., expected impact if used successfully); and (d) feasibility (i.e., expected likelihood of success). All ratings will be on a scale of 1 (“not at all”) to 5 (“extremely”). The instructions will make it clear that each of these items are separate ratings (e.g., a strategy with low feasibility may still be important) and that these ratings represent an average across the youth mental health service settings in which that participant works. Participants from youth mental health service agencies only will also indicate each strategy’s (e) contribution to their funding for EBT sustainment (as a percentage of total funding over the last 3 years). Finally, to better understand contextual influences on the other ratings, participants will complete the Program Sustainability Assessment Tool. This reliable and valid 40-item instrument, based on the public health sustainability framework that frames our project, measures the 8 domains of sustainability capacity: political support, funding stability, partnerships, organizational capacity, program evaluation activities, program adaptation, communications about the program, and strategic planning. Participants will complete the instrument with reference to the specific EBTs with which they work (this project focuses on three EBTs for youth mental health problems). Stakeholders from youth mental health service agencies will report on their own EBT programs, whereas stakeholders from intermediary organizations and funding agencies will report on an average or representative program. The Program Sustainability Assessment Tool produces a profile of sustainment capacity strengths and needs, including an average score for each domain and an overall average sustainability index score.

During the latter half of Year 1 of the project, I will revise the plan for the fiscal mapping process based on the results of Aim 1. The five proposed steps in the fiscal mapping process (Financial Needs Assessment for EBT sustainment; Specification of Funding Objectives; identification of relevant Financing Strategies; selection of financing strategies that make up a Coordinated Funding Plan; Ongoing Monitoring) balance specificity with flexibility so that I can effectively respond to any possible outcomes of Aim 1. For example, I will refine the description of Step 3 (Financing Strategies) to reflect better the financing strategies including in our final compilation (Aim 1a), and I will incorporate information on which strategies have the greatest value index (Suitability x Importance x Feasibility) and are best suited to given contexts (Aim 1b) into Step 4 (Coordinated Funding Plan). Even in the unlikely event that Aim 1 does not produce a satisfactory compilation of financing strategies, I can leverage the depth of understanding developed through Aim 1 to develop an alternative version of the fiscal mapping process.

Aim 2b: Describe key actors, processes, and contextual influences in the fiscal mapping process. The youth mental health services ecosystem within which the fiscal mapping process unfolds is best represented as a “complex adaptive system” that involves numerous components interacting with each other and changing over time. Thus, no observational study or clinical trial can capture the full range of possible pathways and outcomes that may occur during the fiscal mapping process. Instead, in future projects, I intend to use simulation modeling techniques for complex systems to adequately model the impact of the process on sustainability of EBT financing. As a first step toward building simulation models, I will conduct a participatory modeling exercise with our expert panel during the Webinar at the end of Aim 2a. Participatory modeling is a technique from systems science that guides a group of stakeholders through the process of creating a conceptual model of systems structures. I will serve in the facilitator role and will introduce the exercise by reading a problem statement (e.g., “You and your colleagues want to use the fiscal mapping process in your strategic planning efforts for EBT sustainment”) and then guide the participants through the conceptualization phase. For each step of the fiscal mapping process, the participants will be asked identify the actors (e.g., individuals involved from youth service agencies, funding agencies, and/or EBT purveyors/intermediaries), the actions that each actor takes (e.g., request information, apply for funding, allocate funds to EBT sustainment activities), and contextual factors that influence outcomes at each step (e.g., legal constraints on financing strategies). Discussion will be highly structured, with participants using the virtual “hand raise” feature of WebEx and making comments for 1 minute each. My Project Coordinator will serve as the recorder who keeps track of system elements throughout the exercise. Following completion of the modeling exercise, the project team will review the results and create a systems dynamics diagram to represent the conceptual model generated by the participants. This diagram will expand on the steps of the fiscal mapping process (Aim 2a) by describing how the process unfolds.


3. If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?

N/A";s:5:"xhtml";s:13817:"Dopp - Assignment #5<br /><br />1. If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />My project seeks to develop a fiscal mapping process (and component financing strategies) that will guide strategic planning efforts to sustainably fund evidence-based treatments in youth mental health services. I define financing strategies as implementation strategies that secure and direct financial resources to support activities involved in implementation and/or sustainment of EBTs. Thus, I seek to develop tools that will help better understand and coordinate a wide variety of implementation strategies in the financing domain, rather than selecting and focusing on specific implementation strategies.<br /><br />The proposed project is grounded within Schell et al.’s public health sustainability framework, which identifies eight core domains (including funding stability) of capacity for sustainment in public health programs. The central domain in that framework is strategic planning – defined as processes that guide a program’s directions, goals, and strategies – which coordinates all sustainability domains into an outcome-oriented plan (e.g., one may need to build partnerships that improve organizational capacity before funding stability is possible). My Specific Aims, described next in more detail, are strongly informed by the central importance of strategic planning in the selection of implementation strategies (including financing strategies) to support sustainment efforts.<br /><br />Aim 1: Compilation of financing strategies. Strategic planning requires sufficient understanding of the options available to achieve a goal or solve a problem. Therefore, I will first seek to generate a comprehensive compilation of financing strategies used in youth mental health services. Over the past decade, a national group of implementation experts has begun a broader effort to compile and describe implementation strategies. This effort has provided a much-needed set of best-practice strategies for implementation research and practice. However, closer examination reveals that their proposed category of “financial strategies” included strategies that were poorly specified (e.g., “Fund and contract for the clinical innovation,” “Access new funding”) and did not make important distinctions between financing strategies and financial incentives. The latter are grounded in the study of behavioral economics of various incentives (e.g., financial, social) for EBT implementation, and thus fall outside the scope of the proposed research. As a step toward better description of financing strategies, I described pay-for-success contracts (one promising financing strategy) using seven key dimensions for implementation strategies in a preliminary study. That study illustrated the complexity involved in specifying a financing strategy; for instance, pay-for-success contracts involve principal actors across the service delivery and financing systems (e.g., private investors, government payer, nonprofit service delivery agency, evaluation team), all with different roles throughout the implementation process. These findings suggest that dedicated efforts will be needed to adequately describe all available financing strategies. A comprehensive, well-specified compilation of financing strategies will provide a foundational basis for strategic planning of EBT sustainment efforts in youth mental health services.<br /><br />Aim 2: Fiscal mapping process. A compilation of financing strategies is necessary, but not sufficient, for identifying and selecting the optimal combination of financing strategies for an EBT sustainment effort. In recent years, implementation experts have begun to emphasize the importance of “tailored selection” of implementation strategies. Tailoring – essentially a form of strategic planning – refers to selection of strategies that match the goals, strengths, and needs of a given EBT implementation effort (e.g., use of training to increase provider knowledge vs. dissemination of information to increase public support for a program; selection of financing strategies that best fit the local cultural and legal environment). Unfortunately, financing strategies are often considered complex or infeasible by implementation experts, mental health service agencies, and purveyor/intermediary organizations, indicating that the various stakeholders within youth mental health services will require support to successfully make use of these strategies. Such findings are consistent with a preliminary study recently completed by myself and other members of the project team, as part of a larger project in which we implemented Problematic Sexual Behavior – Cognitive-Behavioral Therapy (PSB-CBT) programs at six sites nationwide. Across qualitative interviews with 12 agency administrators and 36 stakeholder partners, 83% of interviewees (including 100% of administrators) mentioned that planning for sustained funding was challenging. They also identified 9 different categories of possible future funding sources, including insurance, state and local agencies, and private foundations. No single funding source was expected to fully sustain a PSB-CBT program (e.g., many did not fund caregiver groups) and the most promising funding sources varied across states. Overall, interviewees indicated that strategic use of “creative funding strategies” and “blending” multiple funding sources (including a mix of public and private dollars) was necessary to sustain PSB-CBT.<br /><br />The above findings suggest that a tailoring (i.e., strategic planning) process for financing strategies would facilitate EBT sustainment efforts. Methods of tailoring implementation strategies are in their infancy, and to date none have focused on strategic selection of financing strategies in particular. However, a recent paper by a number of prominent implementation experts identified several promising approaches. Of the methods identified, intervention mapping shows the greatest promise for the strategic planning of financing strategies. Intervention mapping is a well-specified, multi-step method for developing interventions (or implementation strategies) based on theory, research evidence, and stakeholder perspectives. Intervention mapping has been widely applied in medicine and behavioral health. It is especially well suited to addressing pragmatic issues in implementation, which is critical for funding decisions, whereas other proposed methods are better suited to achieving clarity around conceptual issues (e.g., achieving consensus on desired implementation outcomes). I propose to adapt this process into a fiscal mapping process that will guide youth mental health service agencies, funding agencies, and purveyors/intermediaries in their strategic planning efforts to finance EBT sustainment.<br /><br /><br />2. How might you link specific implementation strategies to the context in which your work is set?<br /><br />Throughout the proposed project, I will collect data that will assist stakeholders from youth mental health services in linking specific financing strategies to the contexts in which their work is set. Specifically, each Specific Aim contains a sub-aim focused on describing characteristics that will help guide tailored selection (strategic planning) of financing strategies during the fiscal mapping process.<br /><br />Aim 1b: Describe characteristics that inform strategy selection. Once specification of the financial strategies is complete, the expert panel of stakeholders (40 representatives from EBT intermediaries, youth mental health service agencies, funding agencies) will complete a follow-up online survey in which they will describe key characteristics that would inform the potential selection of each strategy during the strategic planning process. Specifically, for each strategy, participants will provide ratings of its (a) availability in their setting (yes/no); (b) level of suitability for sustaining different types of implementation activities (listed in Figure 1); (c) importance (i.e., expected impact if used successfully); and (d) feasibility (i.e., expected likelihood of success). All ratings will be on a scale of 1 (“not at all”) to 5 (“extremely”). The instructions will make it clear that each of these items are separate ratings (e.g., a strategy with low feasibility may still be important) and that these ratings represent an average across the youth mental health service settings in which that participant works. Participants from youth mental health service agencies only will also indicate each strategy’s (e) contribution to their funding for EBT sustainment (as a percentage of total funding over the last 3 years). Finally, to better understand contextual influences on the other ratings, participants will complete the Program Sustainability Assessment Tool. This reliable and valid 40-item instrument, based on the public health sustainability framework that frames our project, measures the 8 domains of sustainability capacity: political support, funding stability, partnerships, organizational capacity, program evaluation activities, program adaptation, communications about the program, and strategic planning. Participants will complete the instrument with reference to the specific EBTs with which they work (this project focuses on three EBTs for youth mental health problems). Stakeholders from youth mental health service agencies will report on their own EBT programs, whereas stakeholders from intermediary organizations and funding agencies will report on an average or representative program. The Program Sustainability Assessment Tool produces a profile of sustainment capacity strengths and needs, including an average score for each domain and an overall average sustainability index score.<br /><br />During the latter half of Year 1 of the project, I will revise the plan for the fiscal mapping process based on the results of Aim 1. The five proposed steps in the fiscal mapping process (Financial Needs Assessment for EBT sustainment; Specification of Funding Objectives; identification of relevant Financing Strategies; selection of financing strategies that make up a Coordinated Funding Plan; Ongoing Monitoring) balance specificity with flexibility so that I can effectively respond to any possible outcomes of Aim 1. For example, I will refine the description of Step 3 (Financing Strategies) to reflect better the financing strategies including in our final compilation (Aim 1a), and I will incorporate information on which strategies have the greatest value index (Suitability x Importance x Feasibility) and are best suited to given contexts (Aim 1b) into Step 4 (Coordinated Funding Plan). Even in the unlikely event that Aim 1 does not produce a satisfactory compilation of financing strategies, I can leverage the depth of understanding developed through Aim 1 to develop an alternative version of the fiscal mapping process.<br /><br />Aim 2b: Describe key actors, processes, and contextual influences in the fiscal mapping process. The youth mental health services ecosystem within which the fiscal mapping process unfolds is best represented as a “complex adaptive system” that involves numerous components interacting with each other and changing over time. Thus, no observational study or clinical trial can capture the full range of possible pathways and outcomes that may occur during the fiscal mapping process. Instead, in future projects, I intend to use simulation modeling techniques for complex systems to adequately model the impact of the process on sustainability of EBT financing. As a first step toward building simulation models, I will conduct a participatory modeling exercise with our expert panel during the Webinar at the end of Aim 2a. Participatory modeling is a technique from systems science that guides a group of stakeholders through the process of creating a conceptual model of systems structures. I will serve in the facilitator role and will introduce the exercise by reading a problem statement (e.g., “You and your colleagues want to use the fiscal mapping process in your strategic planning efforts for EBT sustainment”) and then guide the participants through the conceptualization phase. For each step of the fiscal mapping process, the participants will be asked identify the actors (e.g., individuals involved from youth service agencies, funding agencies, and/or EBT purveyors/intermediaries), the actions that each actor takes (e.g., request information, apply for funding, allocate funds to EBT sustainment activities), and contextual factors that influence outcomes at each step (e.g., legal constraints on financing strategies). Discussion will be highly structured, with participants using the virtual “hand raise” feature of WebEx and making comments for 1 minute each. My Project Coordinator will serve as the recorder who keeps track of system elements throughout the exercise. Following completion of the modeling exercise, the project team will review the results and create a systems dynamics diagram to represent the conceptual model generated by the participants. This diagram will expand on the steps of the fiscal mapping process (Aim 2a) by describing how the process unfolds.<br /><br /><br />3. If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /><br />N/A";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"9c4bbd5e75af30559bac80154b046af3";}s:4:"show";b:1;s:3:"cid";s:32:"65cad28f2e9526773d29ea6c4594ba27";}s:32:"22305c431d38d3f762a47b051c03db0a";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"aweaver";s:4:"name";s:12:"Addie Weaver";s:4:"mail";s:18:"weaverad@umich.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1539977092;s:8:"modified";i:1539977436;}s:3:"raw";s:6251:"WEAVER - Assignment #5:
1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

Using Powell’s (2012) Compilation as a guide, I am focusing on the following implementation strategies across the three aims of my proposed study: 

Aim 1: 1) Assess for readiness and identify barriers (Information Gathering) at provider/administrator level. During this time, members of the study team will also be: 2) Identifying and preparing champions (Build Buy-In) within one rural WIC clinic that is interested in initiating postpartum depression screening and participating in the implementation intervention. 

Aim 2: 1) Develop effective educational materials (Develop Materials); 2) Tailor strategies to overcome barriers and honor preferences (Select Strategies). The development of education materials will be tailored based on what we learned about barriers to implementing screening as well as preferences for delivering an educational intervention. 

Aim 3: 1) Conduct educational meetings targeting WIC staff who would be administering postpartum depression screening tools. To further support our educational intervention, we plan to use the following strategies: 2) Make training dynamic in order to be interactive and account for the context of the WIC clinic and 3) Conduct weekly educational outreach visits for one month after the educational meetings. We also plan to utilize 4) Audit and feedback as a Quality Management Strategy. We will provide monthly reports of postpartum depression screening rates, prevalence of clients who screening positive for depression, and number of referrals made for clients who screen positive) for three months after the initial educational meeting. 

The strategies were selected because they have an evidence-base, have been widely used in implementation research, and align with the scientific goals of individual study Aims. The strategy for Aim 3 was also informed by existing literature examining initiating postpartum depression screening in a WIC clinic in the metro DC area. One identified study (Perry et al., 2103) utilized educational interventions, including in-service training, weekly briefings, and onsite consultation, to initiate PPD screening within WIC clinics. Perry and colleagues’ (2013) educational intervention specifically included training and technical assistance that utilized both bottom-up and top-down approaches. This included developing scripts and guidelines for addressing unfamiliar emotional questions, vignettes and role play exercises to practice techniques in vivo, impromptu weekly briefings with WIC staff, frequent onsite consultation, and 2 days of in-service training. Although Perry’s work is helpful to inform our thinking about implementation strategies to support Aim 3, we recognize that the rural context and associated barriers/facilitators to initiating postpartum depression screening is likely be very different than the urban context in DC; therefore tailoring strategies for the context of rural WIC clinics (as emphasized in Aim 2) is critical.

Side Note/Question: I am wondering if it is common to have so many implementation strategies guiding a relatively small scale proposal. At first, I was just thinking about implementation strategies related to Aim 3, but as I reviewed material for this week, it seemed that I was using implementation strategies across all Aims (even if I previously had not been naming/defining them). 

2.	How might you link specific implementation strategies to the context in which your work is set?

As noted before, I am thinking a lot about the importance of tailoring specific implementation strategies to the context of rural WIC clinics. We are intentionally gathering information on barriers and readiness to initiate postpartum depression screening in Aim 1. This will inform the tailoring of the educational materials we are developing in Aim 2. We will be able to specifically address identified barriers (connected to TDF domains; will allow us to better understand potential determinants of behavior) as well as identify the best way to deliver the educational content. We see identifying and preparing a local champion (someone internal to the rural WIC clinic) as key to assisting in this process and providing local/context-specific knowledge. In some of my other intervention projects, we have a community advisory board comprised of stakeholders who provide feedback on all elements of our research project. After reviewing the Powell article on tailoring, I’m wondering if this may be another strategy to employ for this project, especially to provide input to the content and delivery style(s) of the educational intervention in Aim 3. Having a community advisory board (or similar group of stakeholders) would allow us to pursue methods such as a conjoint analysis, which seems like a potential approach for linking specific implementation strategies to the context for this work.   

One issue I am still grappling with is whether/how to engage client voices in this process. Although the primary stakeholder group in my proposed study is the WIC staff/providers who would be administering postpartum depression screening tools, I also see clients as a critical stakeholder group. For example, it seems potentially important to know if they will find postpartum depression screening at the WIC visits acceptable. However, perhaps we can begin to get at this by assessing screening refusal rates...? It seems within the scope of this small pilot, it may make sense to focus on providers but perhaps build in some focus groups with clients after screening has begun or at the end of the study period. I would be interested in any thoughts instructors may have around this issue. 

3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?

N/A. I am selecting implementation strategies, as specified in response to item 1. 
";s:5:"xhtml";s:6364:"WEAVER - Assignment #5:<br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />Using Powell’s (2012) Compilation as a guide, I am focusing on the following implementation strategies across the three aims of my proposed study: <br /><br />Aim 1: 1) Assess for readiness and identify barriers (Information Gathering) at provider/administrator level. During this time, members of the study team will also be: 2) Identifying and preparing champions (Build Buy-In) within one rural WIC clinic that is interested in initiating postpartum depression screening and participating in the implementation intervention. <br /><br />Aim 2: 1) Develop effective educational materials (Develop Materials); 2) Tailor strategies to overcome barriers and honor preferences (Select Strategies). The development of education materials will be tailored based on what we learned about barriers to implementing screening as well as preferences for delivering an educational intervention. <br /><br />Aim 3: 1) Conduct educational meetings targeting WIC staff who would be administering postpartum depression screening tools. To further support our educational intervention, we plan to use the following strategies: 2) Make training dynamic in order to be interactive and account for the context of the WIC clinic and 3) Conduct weekly educational outreach visits for one month after the educational meetings. We also plan to utilize 4) Audit and feedback as a Quality Management Strategy. We will provide monthly reports of postpartum depression screening rates, prevalence of clients who screening positive for depression, and number of referrals made for clients who screen positive) for three months after the initial educational meeting. <br /><br />The strategies were selected because they have an evidence-base, have been widely used in implementation research, and align with the scientific goals of individual study Aims. The strategy for Aim 3 was also informed by existing literature examining initiating postpartum depression screening in a WIC clinic in the metro DC area. One identified study (Perry et al., 2103) utilized educational interventions, including in-service training, weekly briefings, and onsite consultation, to initiate PPD screening within WIC clinics. Perry and colleagues’ (2013) educational intervention specifically included training and technical assistance that utilized both bottom-up and top-down approaches. This included developing scripts and guidelines for addressing unfamiliar emotional questions, vignettes and role play exercises to practice techniques in vivo, impromptu weekly briefings with WIC staff, frequent onsite consultation, and 2 days of in-service training. Although Perry’s work is helpful to inform our thinking about implementation strategies to support Aim 3, we recognize that the rural context and associated barriers/facilitators to initiating postpartum depression screening is likely be very different than the urban context in DC; therefore tailoring strategies for the context of rural WIC clinics (as emphasized in Aim 2) is critical.<br /><br />Side Note/Question: I am wondering if it is common to have so many implementation strategies guiding a relatively small scale proposal. At first, I was just thinking about implementation strategies related to Aim 3, but as I reviewed material for this week, it seemed that I was using implementation strategies across all Aims (even if I previously had not been naming/defining them). <br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />As noted before, I am thinking a lot about the importance of tailoring specific implementation strategies to the context of rural WIC clinics. We are intentionally gathering information on barriers and readiness to initiate postpartum depression screening in Aim 1. This will inform the tailoring of the educational materials we are developing in Aim 2. We will be able to specifically address identified barriers (connected to TDF domains; will allow us to better understand potential determinants of behavior) as well as identify the best way to deliver the educational content. We see identifying and preparing a local champion (someone internal to the rural WIC clinic) as key to assisting in this process and providing local/context-specific knowledge. In some of my other intervention projects, we have a community advisory board comprised of stakeholders who provide feedback on all elements of our research project. After reviewing the Powell article on tailoring, I’m wondering if this may be another strategy to employ for this project, especially to provide input to the content and delivery style(s) of the educational intervention in Aim 3. Having a community advisory board (or similar group of stakeholders) would allow us to pursue methods such as a conjoint analysis, which seems like a potential approach for linking specific implementation strategies to the context for this work.   <br /><br />One issue I am still grappling with is whether/how to engage client voices in this process. Although the primary stakeholder group in my proposed study is the WIC staff/providers who would be administering postpartum depression screening tools, I also see clients as a critical stakeholder group. For example, it seems potentially important to know if they will find postpartum depression screening at the WIC visits acceptable. However, perhaps we can begin to get at this by assessing screening refusal rates...? It seems within the scope of this small pilot, it may make sense to focus on providers but perhaps build in some focus groups with clients after screening has begun or at the end of the study period. I would be interested in any thoughts instructors may have around this issue. <br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /><br />N/A. I am selecting implementation strategies, as specified in response to item 1.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"b4b8301bcf3394ebd4f42989e60bbfd4";}s:4:"show";b:1;s:3:"cid";s:32:"22305c431d38d3f762a47b051c03db0a";}s:32:"fb8c557aad9650006f0e75905593d245";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"ipinsky";s:4:"name";s:12:"Ilana Pinsky";s:4:"mail";s:21:"pinskyilana@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1539977538;s:8:"modified";i:1539978529;}s:3:"raw";s:4068:"PINSKY - Assignment #5:

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

The implementation strategies I will be focusing include conducting therapist training, clinical supervision, tailoring/adapting the intervention when necessary, use of tablets to deliver the intervention and use of mass media. In addition and importantly, involvement of key decision makers and including and preparing champions.
 - Training: we already offered two 2-week training sessions with two expert IPC therapists, one American (- senior expert) and the other Brazilian - speaking Portuguese, the language spoken in Mozambique. We are also planning a further training to happen sometime early next year that will go over teaching techniques (as the Moz group that is being trained now will become future IPC trainers for the whole country) and rely a lot in role playing (checking on how each individual in the Moz group would do the teaching of sections of IPC). 
- Clinical supervision: we are given a very strong focus to clinical supervision. The same 2 IPC experts described above have been leading 3 weekly supervision groups, each one lasting about 90 minutes with the members of each Moz province. To be considered an IPC therapist, each Moz trainee had to complete 4 sessions with 3 clients, attend at least 75% of the supervision sessions and show good knowledge and use of the techniques and spirit of IPC. To become supervisors, they will need to supervise 5 cases (and attend at least 75% of supervision sessions and good grasp of how to be an IPC supervisor (e.g.: creating a supporting environment for supervision, encouraging trainees to give each other feedback and suggestions, etc.)
- use of tablets to deliver the intervention: as all of the therapists that will ultimately apply IPC in Moz are non-specialists, we decided to develop and implement scripts for the EBPs we will be using, including IPC. In this way, we aim to facilitate the non-specialist work and increase IPC fidelity;
- use of mass media - the Moz group that we are working with currently to prepare to train all the future IPC therapists are MH specialists and many of them are connected to the Moz Health Ministry. Building in their experience with teaching epilepsy intervention to non-specialists (in Moz epilepsy is considered a MH issue), we came to the conclusion that Mental Health campaigns, especially using radio, is pivotal for the spread of the word about the work that will be done with IPC and getting the population and primary care providers involved and interested in the work that will be developed.
- involvement of key decision makers and including and preparing champions: connected to the item above, we involved since the pre-implementation of the project leaders as the head of Mental Health in Mozambique, as well as other key people in the Health Ministry. In addition, some of these people are being trained and supervised in IPC, advising and working together on adapting the intervention when necessary.

2.	How might you link specific implementation strategies to the context in which your work is set?

Our group has been working with the Moz Mental Health group for a few years and our whole project was built upon what they identified as the countries needs (i.e. extending mental health, developing tasking shifting, implementing evidence-based strategies). The project has been built all the way based on continuous feedback, including when obstacle appear (every day!). In this way, as with the rest of the project, all the implementation measures were thought of and chosen linking to the specific Mozambique context.

3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?

N/A";s:5:"xhtml";s:4153:"PINSKY - Assignment #5:<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />The implementation strategies I will be focusing include conducting therapist training, clinical supervision, tailoring/adapting the intervention when necessary, use of tablets to deliver the intervention and use of mass media. In addition and importantly, involvement of key decision makers and including and preparing champions.<br /> - Training: we already offered two 2-week training sessions with two expert IPC therapists, one American (- senior expert) and the other Brazilian - speaking Portuguese, the language spoken in Mozambique. We are also planning a further training to happen sometime early next year that will go over teaching techniques (as the Moz group that is being trained now will become future IPC trainers for the whole country) and rely a lot in role playing (checking on how each individual in the Moz group would do the teaching of sections of IPC). <br />- Clinical supervision: we are given a very strong focus to clinical supervision. The same 2 IPC experts described above have been leading 3 weekly supervision groups, each one lasting about 90 minutes with the members of each Moz province. To be considered an IPC therapist, each Moz trainee had to complete 4 sessions with 3 clients, attend at least 75% of the supervision sessions and show good knowledge and use of the techniques and spirit of IPC. To become supervisors, they will need to supervise 5 cases (and attend at least 75% of supervision sessions and good grasp of how to be an IPC supervisor (e.g.: creating a supporting environment for supervision, encouraging trainees to give each other feedback and suggestions, etc.)<br />- use of tablets to deliver the intervention: as all of the therapists that will ultimately apply IPC in Moz are non-specialists, we decided to develop and implement scripts for the EBPs we will be using, including IPC. In this way, we aim to facilitate the non-specialist work and increase IPC fidelity;<br />- use of mass media - the Moz group that we are working with currently to prepare to train all the future IPC therapists are MH specialists and many of them are connected to the Moz Health Ministry. Building in their experience with teaching epilepsy intervention to non-specialists (in Moz epilepsy is considered a MH issue), we came to the conclusion that Mental Health campaigns, especially using radio, is pivotal for the spread of the word about the work that will be done with IPC and getting the population and primary care providers involved and interested in the work that will be developed.<br />- involvement of key decision makers and including and preparing champions: connected to the item above, we involved since the pre-implementation of the project leaders as the head of Mental Health in Mozambique, as well as other key people in the Health Ministry. In addition, some of these people are being trained and supervised in IPC, advising and working together on adapting the intervention when necessary.<br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />Our group has been working with the Moz Mental Health group for a few years and our whole project was built upon what they identified as the countries needs (i.e. extending mental health, developing tasking shifting, implementing evidence-based strategies). The project has been built all the way based on continuous feedback, including when obstacle appear (every day!). In this way, as with the rest of the project, all the implementation measures were thought of and chosen linking to the specific Mozambique context.<br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /><br />N/A";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"44db5694c5f141b0ec4cdd472db49800";}s:4:"show";b:1;s:3:"cid";s:32:"fb8c557aad9650006f0e75905593d245";}s:32:"cd4ffb9cb29177800213c1ca2b41f841";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"aprogovac";s:4:"name";s:12:"Ana Progovac";s:4:"mail";s:24:"aprogovac@challiance.org";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539993335;}s:3:"raw";s:2662:"PROGOVAC - 

1) If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

In the observational year 1 of the implementation, Key implementation strategies included: mandating changes, accessing new funding, revising professional roles, conducting educational meetings and ongoing training with clinicians and staff before and during implementation, and creating a learning collaborative (during weekly all-team meetings) to discuss emerging implementation issues. Individual clinicians also had direct support from supervisors and the program manager.  

Based on provider interviews in phase 1 of this study, implementation strategies of revising professional roles toward a team-based approach as well as the ongoing learning collaborative at weekly team meetings were important to boosting mutual support for providers caring for patients with complex behavioral health and social needs. 

To address ongoing implementation challenges at the current BHH or to facilitate implementation at other sites, several additional implementation strategies may be beneficial, including Providing more feedback to providers (“audit and feedback”), engaging patient consumers by more systematically. In the current study phase we plan to further refine the list of successful implementation strategies and ask providers and administrators which of the suggested implementation strategies we have devised may be feasible in the context of expanding this work. 


2) How might you link specific implementation strategies to the context in which your work is set?

We have thoroughly studied the context at the pilot site, and plan to conduct interviews and site observations with providers and administrators at the 2 potential expansion sites in order to gauge the context for implementation at these sites and how they compare to the pilot site (and generalize more broadly). 


3) If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?

In the existing site, we have been working with providers to document when specific implementation activities occurred in order to gain a sharper understanding of how specific strategies may link to key patient outcomes, and we are planning to update results for year 2 and year 3 of program implementation at the pilot site to examine these potential associations. ";s:5:"xhtml";s:2751:"PROGOVAC - <br /><br />1) If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />In the observational year 1 of the implementation, Key implementation strategies included: mandating changes, accessing new funding, revising professional roles, conducting educational meetings and ongoing training with clinicians and staff before and during implementation, and creating a learning collaborative (during weekly all-team meetings) to discuss emerging implementation issues. Individual clinicians also had direct support from supervisors and the program manager.  <br /><br />Based on provider interviews in phase 1 of this study, implementation strategies of revising professional roles toward a team-based approach as well as the ongoing learning collaborative at weekly team meetings were important to boosting mutual support for providers caring for patients with complex behavioral health and social needs. <br /><br />To address ongoing implementation challenges at the current BHH or to facilitate implementation at other sites, several additional implementation strategies may be beneficial, including Providing more feedback to providers (“audit and feedback”), engaging patient consumers by more systematically. In the current study phase we plan to further refine the list of successful implementation strategies and ask providers and administrators which of the suggested implementation strategies we have devised may be feasible in the context of expanding this work. <br /><br /><br />2) How might you link specific implementation strategies to the context in which your work is set?<br /><br />We have thoroughly studied the context at the pilot site, and plan to conduct interviews and site observations with providers and administrators at the 2 potential expansion sites in order to gauge the context for implementation at these sites and how they compare to the pilot site (and generalize more broadly). <br /><br /><br />3) If your proposed study does not involve selecting implementation strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br /><br />In the existing site, we have been working with providers to document when specific implementation activities occurred in order to gain a sharper understanding of how specific strategies may link to key patient outcomes, and we are planning to update results for year 2 and year 3 of program implementation at the pilot site to examine these potential associations.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"5a594950d9a970110b99ca80d5b9cb59";}s:4:"show";b:1;s:3:"cid";s:32:"cd4ffb9cb29177800213c1ca2b41f841";}s:32:"7ac1c68ad6f1d9da1ca8c98d64fcb1ba";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"kokamura";s:4:"name";s:14:"Kelsie Okamura";s:4:"mail";s:26:"kelsie.h.okamura@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540067371;}s:3:"raw";s:624:"Hi Lindsay,

Thank you so much for these thoughtful questions. I feel like you and Denny have been so helpful in helping me to narrow down my research question and scale the study to what is actually feasible. As you can imagine, working in the public-sector, we make decisions based on "ISLAGIATT" and my hope is that we can produce something generalizable for future systems that want to adopt MBC. Your feedback has been critical for me to think about what careful design we can construct and evaluate to produce generalizable knowlege. I'm hoping that my revised concept note will reflect that (in a much smaller scale)!";s:5:"xhtml";s:649:"Hi Lindsay,<br /><br />Thank you so much for these thoughtful questions. I feel like you and Denny have been so helpful in helping me to narrow down my research question and scale the study to what is actually feasible. As you can imagine, working in the public-sector, we make decisions based on &quot;ISLAGIATT&quot; and my hope is that we can produce something generalizable for future systems that want to adopt MBC. Your feedback has been critical for me to think about what careful design we can construct and evaluate to produce generalizable knowlege. I&#039;m hoping that my revised concept note will reflect that (in a much smaller scale)!";s:6:"parent";s:32:"187ce64f873b1293b2264671840ec46c";s:7:"replies";a:1:{i:0;s:32:"f12d7fdc36bb7fb412422d6b59b9547d";}s:4:"show";b:1;s:3:"cid";s:32:"7ac1c68ad6f1d9da1ca8c98d64fcb1ba";}s:32:"c0577d5f2b5fd25c12d38ec5444bc4f4";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"kokamura";s:4:"name";s:14:"Kelsie Okamura";s:4:"mail";s:26:"kelsie.h.okamura@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540068353;}s:3:"raw";s:2422:"OKAMURA – ASSIGNMENT #5

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?
Our project will be a naturalistic observation so we are not selecting specific implementation strategies. However, we will be categorizing them using the Stages of Implementation Change to document these strategies. The table in the lecture regarding specifying why implementation strategies were selected would be helpful for us to document rationale and the targeted actor relative to the CFIR and implementation framework. 

2.	How might you link specific implementation strategies to the context in which your work is set?
Given that our implementation is occurring a public-sector service system, much of our implementation strategies are multicontextual. The feedback garnered through our interviews and focus groups have helped to identify potential determinants of MBC adoption that require further evaluation and measurement. For example, much of the feedback we have gathered thus far indicate that leadership and supervision may be important determinants to using MBC within ongoing treatment review. It is hoped that with systematic measurement of some of these determinants, we are able to develop a model (per Denny’s great comment about our model from the 2017 D&I conference) of MBC adoption to tailor implementation strategies in the future.

3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?
We will be able to measure key differences in implementation variation (e.g., time, population) by documenting the implementation strategies employed through the SIC framework and elaborating on the intended function through the use of the table for specifying and reporting strategies. My hypothesis is that much of the strategies adopted address organizational/process change and less on individual knowledge, belief, or attitude change. This is one of the important reasons why measuring individual level constructs at pre-implementation will be helpful to determining what (if any) implementation strategies are needed at the individual level. 
";s:5:"xhtml";s:2469:"OKAMURA – ASSIGNMENT #5<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br />Our project will be a naturalistic observation so we are not selecting specific implementation strategies. However, we will be categorizing them using the Stages of Implementation Change to document these strategies. The table in the lecture regarding specifying why implementation strategies were selected would be helpful for us to document rationale and the targeted actor relative to the CFIR and implementation framework. <br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br />Given that our implementation is occurring a public-sector service system, much of our implementation strategies are multicontextual. The feedback garnered through our interviews and focus groups have helped to identify potential determinants of MBC adoption that require further evaluation and measurement. For example, much of the feedback we have gathered thus far indicate that leadership and supervision may be important determinants to using MBC within ongoing treatment review. It is hoped that with systematic measurement of some of these determinants, we are able to develop a model (per Denny’s great comment about our model from the 2017 D&amp;I conference) of MBC adoption to tailor implementation strategies in the future.<br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?<br />We will be able to measure key differences in implementation variation (e.g., time, population) by documenting the implementation strategies employed through the SIC framework and elaborating on the intended function through the use of the table for specifying and reporting strategies. My hypothesis is that much of the strategies adopted address organizational/process change and less on individual knowledge, belief, or attitude change. This is one of the important reasons why measuring individual level constructs at pre-implementation will be helpful to determining what (if any) implementation strategies are needed at the individual level.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"440c441246236f3ce12a2fc0f46b4bdc";}s:4:"show";b:1;s:3:"cid";s:32:"c0577d5f2b5fd25c12d38ec5444bc4f4";}s:32:"8cbe9b561f3f11ef94321d753240628a";a:8:{s:4:"user";a:5:{s:2:"id";s:11:"rshepardson";s:4:"name";s:16:"Robyn Shepardson";s:4:"mail";s:23:"Robyn.Shepardson@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540322851;}s:3:"raw";s:6575:"SHEPARDSON – ASSIGNMENT #5

1. If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?
Within the Veterans Health Administration, there is strong support for utilization of evidence-based treatments. VHA has a variety of strategies at the policy, provider, local system, and patient level designed to support implementation of evidence-based psychotherapy (1). Thus, the broad policy context sets a supportive stage for my intervention, which was designed specifically to be Veteran-centered (e.g., modules designed based on Veterans’ treatment preferences [2], intervention procedures and materials incorporate Veteran feedback, offers patients choice of modules). My focus with implementation strategies at this time is thus at the provider and local system level. I am focusing on these areas because local leadership support is needed to introduce any new training or practice initiatives, and staff training and support is needed for clinicians to learn new interventions. A combination of interactive assistance, training/educating stakeholders, and supporting clinicians (3) will address the major implementation challenges in a relatively efficient way. We wish to evaluate whether a basic implementation package that is relatively low-cost will be sufficient to obtain the implementation outcomes we hope for.

I am planning to compare what I call Standard Implementation Support (SIS) for the intervention to SIS plus external facilitation. SIS consists of implementation strategies commonly used within VHA psychotherapy rollouts (4,5):
(1) Conducting educational meetings for key stakeholder groups (e.g., initial meetings with site/clinic leadership such as primary care and behavioral health service chiefs and PC-MHI team leads to obtain buy-in to support release time for provider training and participation, initial meetings with PC-MHI providers to obtain buy-in to invest time in learning this new intervention)
(2) Conducting provider training (e.g., virtual webinars for didactic training about the intervention and demonstration of key content, role plays for skills practice)
(3) Developing and distributing educational materials (e.g., treatment manual, example scripting, therapist tips, rationale for using the intervention, patient handouts)
(4) Providing clinical supervision for providers (it is not feasible to require the consultation cases and months of weekly supervision as in VHA EBP rollouts, but we will offer clinical supervision and consultation to anyone who wants extra support learning to routinely deliver the intervention)
(5) Providing centralized technical assistance (for PC-MHI providers focused on common implementation challenges available by email/phone)

Currently we plan to evaluate the added impact of external facilitation in combination with SIS. In external facilitation, a facilitator who is not stationed at the local site where the intervention is being implemented, works closely with local stakeholders (e.g., PC-MHI team lead or providers) to identify the specific barriers and facilitators to implementation and to accordingly select, develop, and adapt to the local context interventions and tools to support implementation of the intervention in question (6,7). The specific implementation strategies that would be relevant will depend on the local context, although we should have some indication of what would be useful based on the pilot hybrid I trial I am currently conducting. Strategies could include things such as conducting local needs assessments or educational outreach efforts, providing audit and feedback, and reminding clinicians if implementation challenges occur more so at the level of PC-MHI providers, or conducting program marketing, identifying/preparing champions, and conducting local consensus discussions if the implementation challenges occur more so at the local system level. We selected external facilitation because this approach has been used successfully within VHA in the past to implement the broader PC-MHI platform of care (8). Several of my colleagues at my Center are not only experts in the PC-MHI model and brief behavioral interventions (similar format/approach as the brief anxiety intervention I am evaluating), but also in external facilitation, as they regularly employ this implementation strategy in their implementation/education roles. They would therefore be able to offer external facilitation specifically focused on implementation of the anxiety intervention relatively easily (i.e., without a big investment of their time to learn the method, context, or intervention). If these individuals were not available to assist with my study, I am not sure if I could consider external facilitation. 

2. How might you link specific implementation strategies to the context in which your work is set?
I am currently conducting a hybrid type I RCT comparing the brief anxiety intervention to PC-MHI usual care. We are conducting a mixed methods process evaluation (9), informed by the RE-AIM framework (10), of implementation to identify possible provider-, and clinic-level barriers and facilitators that may affect future intervention adoption, fidelity, and sustainability. I will use findings from this process evaluation to refine the implementation strategies selected for my subsequent hybrid II trial. Many of the same sites/clinics will be used in the current hybrid I trial and the future hybrid II trial, so we should obtain helpful insights about what level of implementation we need to focus our efforts on. Historically we have been able to obtain strong leadership support for evaluating evidence-based PC-MHI interventions, including access to staff release time for participation in research studies of various types. We anticipate that leadership support will continue, especially if we can appeal to how practice change would help address local concerns such as improving access to care in PC-MHI. Thus, we expect that implementation barriers will be more at the provider level, which have tended to be factors such as resistance to “manualized” treatments and limited time given busy patient schedules. 

3. If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? 
N/A";s:5:"xhtml";s:6665:"SHEPARDSON – ASSIGNMENT #5<br /><br />1. If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br />Within the Veterans Health Administration, there is strong support for utilization of evidence-based treatments. VHA has a variety of strategies at the policy, provider, local system, and patient level designed to support implementation of evidence-based psychotherapy (1). Thus, the broad policy context sets a supportive stage for my intervention, which was designed specifically to be Veteran-centered (e.g., modules designed based on Veterans’ treatment preferences [2], intervention procedures and materials incorporate Veteran feedback, offers patients choice of modules). My focus with implementation strategies at this time is thus at the provider and local system level. I am focusing on these areas because local leadership support is needed to introduce any new training or practice initiatives, and staff training and support is needed for clinicians to learn new interventions. A combination of interactive assistance, training/educating stakeholders, and supporting clinicians (3) will address the major implementation challenges in a relatively efficient way. We wish to evaluate whether a basic implementation package that is relatively low-cost will be sufficient to obtain the implementation outcomes we hope for.<br /><br />I am planning to compare what I call Standard Implementation Support (SIS) for the intervention to SIS plus external facilitation. SIS consists of implementation strategies commonly used within VHA psychotherapy rollouts (4,5):<br />(1) Conducting educational meetings for key stakeholder groups (e.g., initial meetings with site/clinic leadership such as primary care and behavioral health service chiefs and PC-MHI team leads to obtain buy-in to support release time for provider training and participation, initial meetings with PC-MHI providers to obtain buy-in to invest time in learning this new intervention)<br />(2) Conducting provider training (e.g., virtual webinars for didactic training about the intervention and demonstration of key content, role plays for skills practice)<br />(3) Developing and distributing educational materials (e.g., treatment manual, example scripting, therapist tips, rationale for using the intervention, patient handouts)<br />(4) Providing clinical supervision for providers (it is not feasible to require the consultation cases and months of weekly supervision as in VHA EBP rollouts, but we will offer clinical supervision and consultation to anyone who wants extra support learning to routinely deliver the intervention)<br />(5) Providing centralized technical assistance (for PC-MHI providers focused on common implementation challenges available by email/phone)<br /><br />Currently we plan to evaluate the added impact of external facilitation in combination with SIS. In external facilitation, a facilitator who is not stationed at the local site where the intervention is being implemented, works closely with local stakeholders (e.g., PC-MHI team lead or providers) to identify the specific barriers and facilitators to implementation and to accordingly select, develop, and adapt to the local context interventions and tools to support implementation of the intervention in question (6,7). The specific implementation strategies that would be relevant will depend on the local context, although we should have some indication of what would be useful based on the pilot hybrid I trial I am currently conducting. Strategies could include things such as conducting local needs assessments or educational outreach efforts, providing audit and feedback, and reminding clinicians if implementation challenges occur more so at the level of PC-MHI providers, or conducting program marketing, identifying/preparing champions, and conducting local consensus discussions if the implementation challenges occur more so at the local system level. We selected external facilitation because this approach has been used successfully within VHA in the past to implement the broader PC-MHI platform of care (8). Several of my colleagues at my Center are not only experts in the PC-MHI model and brief behavioral interventions (similar format/approach as the brief anxiety intervention I am evaluating), but also in external facilitation, as they regularly employ this implementation strategy in their implementation/education roles. They would therefore be able to offer external facilitation specifically focused on implementation of the anxiety intervention relatively easily (i.e., without a big investment of their time to learn the method, context, or intervention). If these individuals were not available to assist with my study, I am not sure if I could consider external facilitation. <br /><br />2. How might you link specific implementation strategies to the context in which your work is set?<br />I am currently conducting a hybrid type I RCT comparing the brief anxiety intervention to PC-MHI usual care. We are conducting a mixed methods process evaluation (9), informed by the RE-AIM framework (10), of implementation to identify possible provider-, and clinic-level barriers and facilitators that may affect future intervention adoption, fidelity, and sustainability. I will use findings from this process evaluation to refine the implementation strategies selected for my subsequent hybrid II trial. Many of the same sites/clinics will be used in the current hybrid I trial and the future hybrid II trial, so we should obtain helpful insights about what level of implementation we need to focus our efforts on. Historically we have been able to obtain strong leadership support for evaluating evidence-based PC-MHI interventions, including access to staff release time for participation in research studies of various types. We anticipate that leadership support will continue, especially if we can appeal to how practice change would help address local concerns such as improving access to care in PC-MHI. Thus, we expect that implementation barriers will be more at the provider level, which have tended to be factors such as resistance to “manualized” treatments and limited time given busy patient schedules. <br /><br />3. If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? <br />N/A";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"d8739e1931265ba6dec0f6c3bba76364";}s:4:"show";b:1;s:3:"cid";s:32:"8cbe9b561f3f11ef94321d753240628a";}s:32:"f12d7fdc36bb7fb412422d6b59b9547d";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540418939;}s:3:"raw";s:84:"Great Kelsie! I'm glad our questions are helpful for you as you narrow your scope :)";s:5:"xhtml";s:89:"Great Kelsie! I&#039;m glad our questions are helpful for you as you narrow your scope :)";s:6:"parent";s:32:"7ac1c68ad6f1d9da1ca8c98d64fcb1ba";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"f12d7fdc36bb7fb412422d6b59b9547d";}s:32:"9c4bbd5e75af30559bac80154b046af3";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dpintello";s:4:"name";s:14:"Denny Pintello";s:4:"mail";s:23:"Denise.Pintello@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540504211;}s:3:"raw";s:2333:"Alex: 

Your study proposes to add new knowledge to the D&I field by enhancing our understanding of financial implementation strategies through the development of a fiscal mapping process. Unfortunately (or perhaps, fortunately!) for you, I have no expertise in economics, which limits my ability to help guide you. Despite this disclaimer, I do have a few thoughts to share and hopefully, some of it might be helpful.

First – I would like to echo what Lindsey brought up for that last assignment – and invite you to “propose less and clarify and support more.” You did a great job responding to the questions – yet by including quite a bit of text from your specific aims that seem less related – your responses were buried in the lede and were difficult to follow. Please know that this feedback is important as you write any future application – and you want the reviewers to easily follow your key points and be excited about your study.  

I appreciate the interesting point you made regarding Waltz’s categorization of implementation strategies – particularly #8 – Use of Financial Strategies – and noticed that you are primarily focusing on ‘accessing new funding.’ While I do understand that behavioral economics provides the foundation for incentives and disincentives, I would like to learn more about why you are not looking at these domains – perhaps not in the R15, yet for future research. Given the proposed fiscal mapping process – and the ‘tailored selection’ of implementation strategies – this process would seem to involve decision-making, which of course, includes someone weighing the pros and cons of incentives/disincentives, which in turn, pulls in behavioral economics. I encourage you to consider the impact of these domains for future studies. 

I really like your description of the youth MH services system as a ‘complex adaptive system’ – and appreciate your plan to use simulation modeling. Again – more specifics would be helpful here – and a clear example with a hypothesis of one fiscal implementation strategy (pay-for-success, blending, braided) and how this might look like if it was entered into a simulation model – and how it could impact all of the actors would be welcomed.

Overall, your work in TIDIRH seems to be progressing nicely …  
";s:5:"xhtml";s:2384:"Alex: <br /><br />Your study proposes to add new knowledge to the D&amp;I field by enhancing our understanding of financial implementation strategies through the development of a fiscal mapping process. Unfortunately (or perhaps, fortunately!) for you, I have no expertise in economics, which limits my ability to help guide you. Despite this disclaimer, I do have a few thoughts to share and hopefully, some of it might be helpful.<br /><br />First – I would like to echo what Lindsey brought up for that last assignment – and invite you to “propose less and clarify and support more.” You did a great job responding to the questions – yet by including quite a bit of text from your specific aims that seem less related – your responses were buried in the lede and were difficult to follow. Please know that this feedback is important as you write any future application – and you want the reviewers to easily follow your key points and be excited about your study.  <br /><br />I appreciate the interesting point you made regarding Waltz’s categorization of implementation strategies – particularly #8 – Use of Financial Strategies – and noticed that you are primarily focusing on ‘accessing new funding.’ While I do understand that behavioral economics provides the foundation for incentives and disincentives, I would like to learn more about why you are not looking at these domains – perhaps not in the R15, yet for future research. Given the proposed fiscal mapping process – and the ‘tailored selection’ of implementation strategies – this process would seem to involve decision-making, which of course, includes someone weighing the pros and cons of incentives/disincentives, which in turn, pulls in behavioral economics. I encourage you to consider the impact of these domains for future studies. <br /><br />I really like your description of the youth MH services system as a ‘complex adaptive system’ – and appreciate your plan to use simulation modeling. Again – more specifics would be helpful here – and a clear example with a hypothesis of one fiscal implementation strategy (pay-for-success, blending, braided) and how this might look like if it was entered into a simulation model – and how it could impact all of the actors would be welcomed.<br /><br />Overall, your work in TIDIRH seems to be progressing nicely …";s:6:"parent";s:32:"65cad28f2e9526773d29ea6c4594ba27";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"9c4bbd5e75af30559bac80154b046af3";}s:32:"440c441246236f3ce12a2fc0f46b4bdc";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dpintello";s:4:"name";s:14:"Denny Pintello";s:4:"mail";s:23:"Denise.Pintello@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540504415;}s:3:"raw";s:2241:"Kelsie:

Hi there … I would like to offer what I hope is meaningful feedback regarding your responses to this assignment. While your study proposes a naturalistic observation, it seems as though your State will have to identify various implementation strategies to roll out MBC, which is dependent on which stage you are in SIC. So – the initial phase of SIC is to explore pre-implementation engagement (Stage 1 – which includes potential initiation of an implementation strategy), and then on to the feasibility (Stage 2 – which includes initiation of organizational implementation strategies). I am still unclear which implementation strategies that the State is considering testing for this project. So … I suspect that the implementation strategies may include at least 3 of the categories covered in the TIDIRH lecture by Dr. Adsul (#5: training/education; #6: supporting clinicians or #9: change infrastructure). Even though I am just guessing based on your responses, I cannot tell if the initiative is truly well-thought out, or if the State is throwing the kitchen sink at this to see what implementation strategy will stick. We need you to go through and – based on the lecture, hypothesize which implementation strategies might emerge – and how they might manipulate the target/determinant of MBC adoption. 

Regarding the impact of the initiative within the State agency context: I would love to know more about how you hypothesize the impact of potential implementation strategies (perhaps training/education, supporting clinicians, changing the infrastructure) on knowledge, attitudes and intentions at the leadership, mid-management and provider level. You stated that you envision that “much of the strategies adopted address org/process change and less on individual … change” – it would be great to hear more about why you think this – and what would be the potential implications (i.e.; generalizable to other States?)

Overall – I hope you hear that I really like your work on this project, yet I remain unclear about the key specifics – and invite you to clearly articulate it – and consider asking another colleague to review it and share feedback when you write up your next assignment. 
";s:5:"xhtml";s:2269:"Kelsie:<br /><br />Hi there … I would like to offer what I hope is meaningful feedback regarding your responses to this assignment. While your study proposes a naturalistic observation, it seems as though your State will have to identify various implementation strategies to roll out MBC, which is dependent on which stage you are in SIC. So – the initial phase of SIC is to explore pre-implementation engagement (Stage 1 – which includes potential initiation of an implementation strategy), and then on to the feasibility (Stage 2 – which includes initiation of organizational implementation strategies). I am still unclear which implementation strategies that the State is considering testing for this project. So … I suspect that the implementation strategies may include at least 3 of the categories covered in the TIDIRH lecture by Dr. Adsul (#5: training/education; #6: supporting clinicians or #9: change infrastructure). Even though I am just guessing based on your responses, I cannot tell if the initiative is truly well-thought out, or if the State is throwing the kitchen sink at this to see what implementation strategy will stick. We need you to go through and – based on the lecture, hypothesize which implementation strategies might emerge – and how they might manipulate the target/determinant of MBC adoption. <br /><br />Regarding the impact of the initiative within the State agency context: I would love to know more about how you hypothesize the impact of potential implementation strategies (perhaps training/education, supporting clinicians, changing the infrastructure) on knowledge, attitudes and intentions at the leadership, mid-management and provider level. You stated that you envision that “much of the strategies adopted address org/process change and less on individual … change” – it would be great to hear more about why you think this – and what would be the potential implications (i.e.; generalizable to other States?)<br /><br />Overall – I hope you hear that I really like your work on this project, yet I remain unclear about the key specifics – and invite you to clearly articulate it – and consider asking another colleague to review it and share feedback when you write up your next assignment.";s:6:"parent";s:32:"c0577d5f2b5fd25c12d38ec5444bc4f4";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"440c441246236f3ce12a2fc0f46b4bdc";}s:32:"d8739e1931265ba6dec0f6c3bba76364";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"dpintello";s:4:"name";s:14:"Denny Pintello";s:4:"mail";s:23:"Denise.Pintello@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540504526;}s:3:"raw";s:1515:"Robyn:

Thank you for your thoughtful response – I especially appreciate your application of the TIDIRH lecture using the Waltz categories of implementations strategies (#2 interactive assistance, #5 train/educate, and #6 supporting clinicians) to your study.  Your thinking about the inclusion of external facilitation is interesting – and I believe that it would provide added value to the study. 

I see that your study has shifted over the course of TIDIRH, so forgive me if I am not keeping up 😊. From what I am gathering, this appears to now be a hybrid II because you are planning to manipulate the SIS training by using external facilitators. The strategies you listed are realistic and address the key components – and most importantly reflect the TIDIRH lecture!! 

One thought – in an effort to promote sustainability is to add a component where the external facilitators train clinic staff to serve as internal facilitators – so that the project has the potential to continue on long after your study is over.  Your idea of recruiting the same sites from the hybrid I to participate in the hybrid II is cost effective and administratively efficient, yet reviewers may be concerned about potential bias among sites in participating, compared to naïve sites. Just something to think about.

Overall - your written responses throughout the assignments to date have been consistent, clear and easy to follow – and I truly appreciate the thoughtfulness of your work and your progress so far.
";s:5:"xhtml";s:1554:"Robyn:<br /><br />Thank you for your thoughtful response – I especially appreciate your application of the TIDIRH lecture using the Waltz categories of implementations strategies (#2 interactive assistance, #5 train/educate, and #6 supporting clinicians) to your study.  Your thinking about the inclusion of external facilitation is interesting – and I believe that it would provide added value to the study. <br /><br />I see that your study has shifted over the course of TIDIRH, so forgive me if I am not keeping up 😊. From what I am gathering, this appears to now be a hybrid II because you are planning to manipulate the SIS training by using external facilitators. The strategies you listed are realistic and address the key components – and most importantly reflect the TIDIRH lecture!! <br /><br />One thought – in an effort to promote sustainability is to add a component where the external facilitators train clinic staff to serve as internal facilitators – so that the project has the potential to continue on long after your study is over.  Your idea of recruiting the same sites from the hybrid I to participate in the hybrid II is cost effective and administratively efficient, yet reviewers may be concerned about potential bias among sites in participating, compared to naïve sites. Just something to think about.<br /><br />Overall - your written responses throughout the assignments to date have been consistent, clear and easy to follow – and I truly appreciate the thoughtfulness of your work and your progress so far.";s:6:"parent";s:32:"8cbe9b561f3f11ef94321d753240628a";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"d8739e1931265ba6dec0f6c3bba76364";}s:32:"b4b8301bcf3394ebd4f42989e60bbfd4";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"nbowersox";s:4:"name";s:13:"Nick Bowersox";s:4:"mail";s:24:"Nicholas.Bowersox@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1540612342;s:8:"modified";i:1540612358;}s:3:"raw";s:4237:"Addie - 

I can tell that you have given this topic a lot of thought.  A few clarifying questions/comments:

1. About the sequencing of your Aims.  You mention in Aim 2 that you will be focusing on the development of educational materials - will these serve as the basis of the training of your site champions (mentioned in Aim 1) and their training of site staff to perform PPD screens (mentioned in Aim 3)?  In other words, will you be unable to begin training your champions until these materials are created, or do you have plans/materials that will allow you to provide this training while educational materials are being developed?

2. Have you considered implementation models to inform your approach in providing training and corrective feedback/fidelity assessment to site champions?  I am thinking about approaches such as implementation facilitation as approaches that could be a good fit to your project, both in terms of training your champions, then also allowing your champions to provide training to other staff at their sites.  If you are not planning on using facilitation, what models are you considering to inform your process of providing training, oversight, support, and monitoring for your champions?  The Perry study that you cite sounds like it may have used a modified facilitation approach to help with adoption of their PPD screening program - I am concerned that an education and monitoring approach alone may not be adequate to identify and discuss barriers to program implementation at your sites.

3. To your question, studies can use multiple implementation strategies, if they address multiple unique program areas.  However, there has also been a movement to expand some strategies that focus on the main areas of concern for a project (e.g., training champions to properly implement the program and get others on board) that have been expanded to include additional supplemental techniques in the manner of a "toolbox" to help to ensure that a study approach is consistent and organized around the main area of concern.  Here is an example of a training manual for implementation facilitation that includes discussion of such a toolbox (I know it's a bit long: https://www.queri.research.va.gov/tools/implementation/Facilitation-Manual.pdf) - the authors of this manual have worked to develop a list of associated tools that could be used to cover many of the areas that you mention in your aims and allow for some consolidation of approach, including the use of virtual facilitation to support remote sites.

4. I agree with the value of a stakeholder board to provide consultation, but also agree with your view that this project will ultimately depend on the engagement of quality program champions at your sites who will be the ones to support project implementation and ensure program fidelity.  It seems that this is the core element that will need to be targeted, with other project aspects (assessment of barriers and facilitators, modification of educational materials) being directly informed by these persons.

5. To your question about the consumer's voice, I agree that it can be helpful to get their perspective about a project, although this is generally only essential when a program is not doing well.  As you mention, if the program is effectively implemented and functioning as expected, you can infer that the patients are viewing the screens as acceptable (they would otherwise be voting with their feet).  You might consider developing a patient questionnaire if you are getting feedback from your champions that patient buy-in as a barrier to effective project implementation, although I would focus on other stakeholders for your first implementation efforts.  Similarly, if you decide to assess patient preferences and experiences related to PPD screening, you might want to develop a broader evaluation of care perspective and preferences that might extend beyond this project, given the amount of effort needed to collect this information from patients - you may not want to devote your limited time and resources to developing such a questionnaire at this point in your project process.

As always, please let me know if you would like to discuss further.

Thanks,

- Nick";s:5:"xhtml";s:4347:"Addie - <br /><br />I can tell that you have given this topic a lot of thought.  A few clarifying questions/comments:<br /><br />1. About the sequencing of your Aims.  You mention in Aim 2 that you will be focusing on the development of educational materials - will these serve as the basis of the training of your site champions (mentioned in Aim 1) and their training of site staff to perform PPD screens (mentioned in Aim 3)?  In other words, will you be unable to begin training your champions until these materials are created, or do you have plans/materials that will allow you to provide this training while educational materials are being developed?<br /><br />2. Have you considered implementation models to inform your approach in providing training and corrective feedback/fidelity assessment to site champions?  I am thinking about approaches such as implementation facilitation as approaches that could be a good fit to your project, both in terms of training your champions, then also allowing your champions to provide training to other staff at their sites.  If you are not planning on using facilitation, what models are you considering to inform your process of providing training, oversight, support, and monitoring for your champions?  The Perry study that you cite sounds like it may have used a modified facilitation approach to help with adoption of their PPD screening program - I am concerned that an education and monitoring approach alone may not be adequate to identify and discuss barriers to program implementation at your sites.<br /><br />3. To your question, studies can use multiple implementation strategies, if they address multiple unique program areas.  However, there has also been a movement to expand some strategies that focus on the main areas of concern for a project (e.g., training champions to properly implement the program and get others on board) that have been expanded to include additional supplemental techniques in the manner of a &quot;toolbox&quot; to help to ensure that a study approach is consistent and organized around the main area of concern.  Here is an example of a training manual for implementation facilitation that includes discussion of such a toolbox (I know it&#039;s a bit long: https://www.queri.research.va.gov/tools/implementation/Facilitation-Manual.pdf) - the authors of this manual have worked to develop a list of associated tools that could be used to cover many of the areas that you mention in your aims and allow for some consolidation of approach, including the use of virtual facilitation to support remote sites.<br /><br />4. I agree with the value of a stakeholder board to provide consultation, but also agree with your view that this project will ultimately depend on the engagement of quality program champions at your sites who will be the ones to support project implementation and ensure program fidelity.  It seems that this is the core element that will need to be targeted, with other project aspects (assessment of barriers and facilitators, modification of educational materials) being directly informed by these persons.<br /><br />5. To your question about the consumer&#039;s voice, I agree that it can be helpful to get their perspective about a project, although this is generally only essential when a program is not doing well.  As you mention, if the program is effectively implemented and functioning as expected, you can infer that the patients are viewing the screens as acceptable (they would otherwise be voting with their feet).  You might consider developing a patient questionnaire if you are getting feedback from your champions that patient buy-in as a barrier to effective project implementation, although I would focus on other stakeholders for your first implementation efforts.  Similarly, if you decide to assess patient preferences and experiences related to PPD screening, you might want to develop a broader evaluation of care perspective and preferences that might extend beyond this project, given the amount of effort needed to collect this information from patients - you may not want to devote your limited time and resources to developing such a questionnaire at this point in your project process.<br /><br />As always, please let me know if you would like to discuss further.<br /><br />Thanks,<br /><br />- Nick";s:6:"parent";s:32:"22305c431d38d3f762a47b051c03db0a";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"b4b8301bcf3394ebd4f42989e60bbfd4";}s:32:"44db5694c5f141b0ec4cdd472db49800";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"nbowersox";s:4:"name";s:13:"Nick Bowersox";s:4:"mail";s:24:"Nicholas.Bowersox@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540612961;}s:3:"raw";s:1452:"Hi, Ilana - 

Thank you for this additional information.  Here are my thoughts/questions:

While you mention many techniques associated with different implementation strategies, I am not seeing any discussion of implementation frameworks in your comments above.  Because of the focus on expert supervision, training, and practice monitoring, I recommend that you consider implementation facilitation as an implementation framework to help to organize your planned implementation spread activities.  Implementation facilitation is an approach that was initially developed to help with the spread of mental health interventions and feels very similar to clinical supervision (with some additional components of fidelity assessment and resource development).  There has been some recent work by the team that developed this approach to include a variety of other techniques that can be included within a facilitation "toolbox" which could support your IT-based information dissemination approaches and your planned interface with key stakeholders.  For more information, here is a link to an implementation facilitation training manual developed by some of my colleagues in VA:

https://www.queri.research.va.gov/tools/implementation/Facilitation-Manual.pdf

See if that seems like it could be a good fit to your project - if so, I would be happy to discuss this approach further with you or put you in touch with the authors of the manual.

Best,

- Nick";s:5:"xhtml";s:1522:"Hi, Ilana - <br /><br />Thank you for this additional information.  Here are my thoughts/questions:<br /><br />While you mention many techniques associated with different implementation strategies, I am not seeing any discussion of implementation frameworks in your comments above.  Because of the focus on expert supervision, training, and practice monitoring, I recommend that you consider implementation facilitation as an implementation framework to help to organize your planned implementation spread activities.  Implementation facilitation is an approach that was initially developed to help with the spread of mental health interventions and feels very similar to clinical supervision (with some additional components of fidelity assessment and resource development).  There has been some recent work by the team that developed this approach to include a variety of other techniques that can be included within a facilitation &quot;toolbox&quot; which could support your IT-based information dissemination approaches and your planned interface with key stakeholders.  For more information, here is a link to an implementation facilitation training manual developed by some of my colleagues in VA:<br /><br />https://www.queri.research.va.gov/tools/implementation/Facilitation-Manual.pdf<br /><br />See if that seems like it could be a good fit to your project - if so, I would be happy to discuss this approach further with you or put you in touch with the authors of the manual.<br /><br />Best,<br /><br />- Nick";s:6:"parent";s:32:"fb8c557aad9650006f0e75905593d245";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"44db5694c5f141b0ec4cdd472db49800";}s:32:"cd6f5b95031297b87a2c2427029378f9";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540682330;}s:3:"raw";s:4782:"Hi Brenna!

What I like about your assignment 5, is that you state your rationale. You state explicitly that prior research on clinician training indicates it is likely necessary, but not sufficient for changing provider behavior. 

That said, I would like to see “provider behavior” better operationalized and linked to either an effectiveness outcome or an implementation outcome. In other words, do you mean necessary but not sufficient for a measure of provider adoption of modified CBT? Or necessary but not sufficient for a measure of fidelity to the modified CBT protocol? Or…? As you can see below, I had some questions about this throughout.

Assignment 5 is focused on implementation strategies and is the last assignment before submitting your revised concept paper. Therefore, in my questions and review, I focused on synthesizing your prior answers. Below, I pull out some key components that could be better integrated so that each complements the other in the development of your proposal. Apologies, in advance if I missed something. 
 
Number 1-3 are some of the things that are clearer to me when threading together assignments 1-5. Still, I ask a key follow-up question for each:

1: You are focused on improving the delivery of modified CBT for adults with autism and co-occurring anxiety/depression) based on at least 5 studies showing efficacy, but not effectiveness (Is that right?). Question 1: What was modified in CBT for this population in this prior research, and how will your proposed implementation strategies address fidelity to the modified protocol necessary for clinical effectiveness? 

2: Based on F32 work you expect that the primary barriers to (or determinant of) delivery of modified CBT in urban community mental health settings is improving clinicians' lack of knowledge, skills and confidence with this intervention. Question(s) 2: Can you justify your focus on implementation outcomes for this intervention? Can you frame this as a determinants hypothesis with an implementation outcome? Make sure to locate the state of the science on the modified CBT protocol and connect it directly to your proposed implementation strategies (see also #7 below)

3: Your hypothesis is that the primary implementation strategies of training, consultation, and opinion leader engagement will address these key implementation targets. Question 3: How is this multicomponent implementation strategy going to be assessed in relationship to your implementation outcomes? For example, will you be assessing the relative contribution of each part of your strategy on key outcomes? What analyses will you use? (see #5 below).

Here are some things that are less clear to me. You mention these things in your prior assignments, and they  would benefit from being meaningfully linked to one another, and in support of your responses to some of my questions above.

4: You considered Proctor's conceptual framework and Aaron's mixed methods design typologies. Question (s) 4: How do you relate the Proctor concepts and Aarons typologies to your proposed study activities?  

5: You propose that clinician beliefs and attitudes are a determinant of implementation. Note 5: You need to specify your measures of this determinant and this outcome (have you done this)? (see #3 above).

6: You have had some discussion with TIDIRH facilitators about your overall design, which is either an RCT or cluster randomized trial. Note (s) 6: Proposing an RCT or CRT is quite different in terms of your research plan for recruitment, study design, measurement, and power. I’d recommend that this be clarified ASAP  for your concept paper (see also, #8 below).

7: You propose to randomly assign outpatient clinicians who serve adults with autism and co-occurring anxiety/depression to receive either implementation as usual or the multifaceted implementation strategy. Question (s) 7: What is “implementation as usual” for this modified CBT intervention? Is implementation as usual a completely different clinical intervention? (see also #2 above).

8: Regarding the discussion of the Curran et al. Type I, II, or III design: It is still unclear to me the balance between effectiveness outcomes and implementation outcomes, and your specific hypotheses. Question(s) 8: Does clinical effectiveness need to be established for the modified CBT and if so, for which clinical outcomes? 

As you can see, clarifying your operational definitions (concepts, methods, measures) of your effectiveness and/or implementation outcomes, and explaining how your design will test your manipulation of your implementation strategy in relationship to key determinants of those outcomes will really strengthen your concept paper (at least for this reader)!

Thanks Brenna!

Lindsey
";s:5:"xhtml";s:4956:"Hi Brenna!<br /><br />What I like about your assignment 5, is that you state your rationale. You state explicitly that prior research on clinician training indicates it is likely necessary, but not sufficient for changing provider behavior. <br /><br />That said, I would like to see “provider behavior” better operationalized and linked to either an effectiveness outcome or an implementation outcome. In other words, do you mean necessary but not sufficient for a measure of provider adoption of modified CBT? Or necessary but not sufficient for a measure of fidelity to the modified CBT protocol? Or…? As you can see below, I had some questions about this throughout.<br /><br />Assignment 5 is focused on implementation strategies and is the last assignment before submitting your revised concept paper. Therefore, in my questions and review, I focused on synthesizing your prior answers. Below, I pull out some key components that could be better integrated so that each complements the other in the development of your proposal. Apologies, in advance if I missed something. <br /> <br />Number 1-3 are some of the things that are clearer to me when threading together assignments 1-5. Still, I ask a key follow-up question for each:<br /><br />1: You are focused on improving the delivery of modified CBT for adults with autism and co-occurring anxiety/depression) based on at least 5 studies showing efficacy, but not effectiveness (Is that right?). Question 1: What was modified in CBT for this population in this prior research, and how will your proposed implementation strategies address fidelity to the modified protocol necessary for clinical effectiveness? <br /><br />2: Based on F32 work you expect that the primary barriers to (or determinant of) delivery of modified CBT in urban community mental health settings is improving clinicians&#039; lack of knowledge, skills and confidence with this intervention. Question(s) 2: Can you justify your focus on implementation outcomes for this intervention? Can you frame this as a determinants hypothesis with an implementation outcome? Make sure to locate the state of the science on the modified CBT protocol and connect it directly to your proposed implementation strategies (see also #7 below)<br /><br />3: Your hypothesis is that the primary implementation strategies of training, consultation, and opinion leader engagement will address these key implementation targets. Question 3: How is this multicomponent implementation strategy going to be assessed in relationship to your implementation outcomes? For example, will you be assessing the relative contribution of each part of your strategy on key outcomes? What analyses will you use? (see #5 below).<br /><br />Here are some things that are less clear to me. You mention these things in your prior assignments, and they  would benefit from being meaningfully linked to one another, and in support of your responses to some of my questions above.<br /><br />4: You considered Proctor&#039;s conceptual framework and Aaron&#039;s mixed methods design typologies. Question (s) 4: How do you relate the Proctor concepts and Aarons typologies to your proposed study activities?  <br /><br />5: You propose that clinician beliefs and attitudes are a determinant of implementation. Note 5: You need to specify your measures of this determinant and this outcome (have you done this)? (see #3 above).<br /><br />6: You have had some discussion with TIDIRH facilitators about your overall design, which is either an RCT or cluster randomized trial. Note (s) 6: Proposing an RCT or CRT is quite different in terms of your research plan for recruitment, study design, measurement, and power. I’d recommend that this be clarified ASAP  for your concept paper (see also, #8 below).<br /><br />7: You propose to randomly assign outpatient clinicians who serve adults with autism and co-occurring anxiety/depression to receive either implementation as usual or the multifaceted implementation strategy. Question (s) 7: What is “implementation as usual” for this modified CBT intervention? Is implementation as usual a completely different clinical intervention? (see also #2 above).<br /><br />8: Regarding the discussion of the Curran et al. Type I, II, or III design: It is still unclear to me the balance between effectiveness outcomes and implementation outcomes, and your specific hypotheses. Question(s) 8: Does clinical effectiveness need to be established for the modified CBT and if so, for which clinical outcomes? <br /><br />As you can see, clarifying your operational definitions (concepts, methods, measures) of your effectiveness and/or implementation outcomes, and explaining how your design will test your manipulation of your implementation strategy in relationship to key determinants of those outcomes will really strengthen your concept paper (at least for this reader)!<br /><br />Thanks Brenna!<br /><br />Lindsey";s:6:"parent";s:32:"8e7ed03cc182fe2a6799184e44a62848";s:7:"replies";a:1:{i:0;s:32:"57a172622924c618c3d70349077a8a90";}s:4:"show";b:1;s:3:"cid";s:32:"cd6f5b95031297b87a2c2427029378f9";}s:32:"5a594950d9a970110b99ca80d5b9cb59";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"lzimmerman";s:4:"name";s:17:"Lindsey Zimmerman";s:4:"mail";s:26:"lindseyzimmerman@gmail.com";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1540772448;s:8:"modified";i:1540860170;}s:3:"raw";s:4223:"Hi Ana!

I read your assignment 5 and I read back through your prior assignments to get caught up on how you've considered each piece of the TIDIRH lectures/course content. I thought this would be the best way to respond to your proposed implementation strategies. I relay some of my remaining questions as comments, which I hope will be helpful to you as you  make your final revisions to your concept paper. I apologize in advance if I missed any key details!

Here is a key idea that I'm tracking based on your prior posts:  You are particularly interested in process integration. This makes sense with your reverse integration primary care/mental health program model, and your conceptualization according to Singer et al's comprehensive theory of integration. On the one hand, in one assignment, you note that it is very difficult to measure normative and interpersonal process integration. And, on the other hand, you state that “simply linking data systems like EHRs does not overcome the enormous organizational, coordination, and financing barriers to combat care fragmentation.” 

Further, you mention five key outcomes (at least that I could track): adequate routine primary care, utilization signaling low quality, hospitalizations, cardiometabolic monitoring, and BMI. 

Comment 1: Despite pulling out these details and priorities, I am not sure why these are the key outcomes, and how your implementation strategies are addressing these factors. 

Comment 2: I would really like to amplify Denny’s encouragement to focus on parsimony. This is especially tricky, but important, when you are explicitly focused on resolving barriers to care integration. You mention Singer et al., and you mention CFIR, but both have many dimensions. You could really tighten up your scientific argument by clarifying a) which dimensions you think are critical to achieve key effectiveness/implementation outcomes, b) which dimensions your implementation strategies map on to specifically and why, and then c) articulate tests of these key determinants/pathways/mechanisms to your outcomes. 

Comment 3: Put a slightly different way, related to comments 1 and 2, you mention all of the following implementation activities without explaining a) what exactly you’re doing during these activities, b) why they are the key activities, and c) how they will ensure sustained care integration:
- mandating changes
- accessing new funding
- revising professional roles
- conducting education meetings
- training
- learning collaborative

Note, I’m not saying that they may not be key activities. Rather, you need further support for your rationale and explanation of how accomplishment of these key activities are directly linked to key effectiveness and implementation outcomes. For example, will these implementation activities always need to be in place? Or will there be transitions where some may no longer be necessary if integration has occurred? How exactly are a) these activities, and b) your key outcomes each measured and tested by your proposed study design?

Question 4: In one assignment you state that “degree of integration” is your primary implementation outcome, but I couldn’t find enough details regarding your proposed “Hybrid type II observational trial with quasi-experimental analysis for the quantitative data (using propensity-score matching generalized estimating equations)” to understand how some of the details really were going to be assessed, and whether you make any distinctions between integration as a process and integration as an outcome. Do you propose hypothesis tests? This needs more explanation.

Comment(s) 5: Despite having said that I think you should pick one primary theory (or model), I still want to mention May et al.’s normalization process theory, because there may be useful measures or constructs that fit what you are describing. However, I would still narrow down to one primary frame and then draft hypothesis tests consistent with this.

I hope these comments highlight areas that you could be even clearer about what you believe is needed to improve implementation and clinical effectiveness in this setting for your population.

Thanks Ana!

Lindsey


";s:5:"xhtml";s:4385:"Hi Ana!<br /><br />I read your assignment 5 and I read back through your prior assignments to get caught up on how you&#039;ve considered each piece of the TIDIRH lectures/course content. I thought this would be the best way to respond to your proposed implementation strategies. I relay some of my remaining questions as comments, which I hope will be helpful to you as you  make your final revisions to your concept paper. I apologize in advance if I missed any key details!<br /><br />Here is a key idea that I&#039;m tracking based on your prior posts:  You are particularly interested in process integration. This makes sense with your reverse integration primary care/mental health program model, and your conceptualization according to Singer et al&#039;s comprehensive theory of integration. On the one hand, in one assignment, you note that it is very difficult to measure normative and interpersonal process integration. And, on the other hand, you state that “simply linking data systems like EHRs does not overcome the enormous organizational, coordination, and financing barriers to combat care fragmentation.” <br /><br />Further, you mention five key outcomes (at least that I could track): adequate routine primary care, utilization signaling low quality, hospitalizations, cardiometabolic monitoring, and BMI. <br /><br />Comment 1: Despite pulling out these details and priorities, I am not sure why these are the key outcomes, and how your implementation strategies are addressing these factors. <br /><br />Comment 2: I would really like to amplify Denny’s encouragement to focus on parsimony. This is especially tricky, but important, when you are explicitly focused on resolving barriers to care integration. You mention Singer et al., and you mention CFIR, but both have many dimensions. You could really tighten up your scientific argument by clarifying a) which dimensions you think are critical to achieve key effectiveness/implementation outcomes, b) which dimensions your implementation strategies map on to specifically and why, and then c) articulate tests of these key determinants/pathways/mechanisms to your outcomes. <br /><br />Comment 3: Put a slightly different way, related to comments 1 and 2, you mention all of the following implementation activities without explaining a) what exactly you’re doing during these activities, b) why they are the key activities, and c) how they will ensure sustained care integration:<br />- mandating changes<br />- accessing new funding<br />- revising professional roles<br />- conducting education meetings<br />- training<br />- learning collaborative<br /><br />Note, I’m not saying that they may not be key activities. Rather, you need further support for your rationale and explanation of how accomplishment of these key activities are directly linked to key effectiveness and implementation outcomes. For example, will these implementation activities always need to be in place? Or will there be transitions where some may no longer be necessary if integration has occurred? How exactly are a) these activities, and b) your key outcomes each measured and tested by your proposed study design?<br /><br />Question 4: In one assignment you state that “degree of integration” is your primary implementation outcome, but I couldn’t find enough details regarding your proposed “Hybrid type II observational trial with quasi-experimental analysis for the quantitative data (using propensity-score matching generalized estimating equations)” to understand how some of the details really were going to be assessed, and whether you make any distinctions between integration as a process and integration as an outcome. Do you propose hypothesis tests? This needs more explanation.<br /><br />Comment(s) 5: Despite having said that I think you should pick one primary theory (or model), I still want to mention May et al.’s normalization process theory, because there may be useful measures or constructs that fit what you are describing. However, I would still narrow down to one primary frame and then draft hypothesis tests consistent with this.<br /><br />I hope these comments highlight areas that you could be even clearer about what you believe is needed to improve implementation and clinical effectiveness in this setting for your population.<br /><br />Thanks Ana!<br /><br />Lindsey";s:6:"parent";s:32:"cd4ffb9cb29177800213c1ca2b41f841";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"5a594950d9a970110b99ca80d5b9cb59";}s:32:"57a172622924c618c3d70349077a8a90";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bmaddox";s:4:"name";s:13:"Brenna Maddox";s:4:"mail";s:17:"maddoxb@upenn.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1541617354;}s:3:"raw";s:196:"Thank you, Lindsey! This feedback is very helpful as I finalize my concept note. I look forward to talking through some of these points in more detail with you, Denny, and Nick next month as well.";s:5:"xhtml";s:196:"Thank you, Lindsey! This feedback is very helpful as I finalize my concept note. I look forward to talking through some of these points in more detail with you, Denny, and Nick next month as well.";s:6:"parent";s:32:"cd6f5b95031297b87a2c2427029378f9";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"57a172622924c618c3d70349077a8a90";}}s:11:"subscribers";N;}