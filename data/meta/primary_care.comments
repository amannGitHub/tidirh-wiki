a:5:{s:5:"title";N;s:6:"status";i:1;s:6:"number";i:69;s:8:"comments";a:69:{s:32:"91234bc9e7eae54afe753ea585b203f1";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"rshah";s:4:"name";s:11:"Reshma Shah";s:4:"mail";s:16:"reshmamd@UIC.EDU";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1534520564;}s:3:"raw";s:266:"Hello,

I'm excited to work with everyone and start the training. I'll be on vacation and out of the country from 8/18-8/25 and will have limited access to email. I'll post my assignment now though and will make sure to check for any updates when I return. 

Reshma
";s:5:"xhtml";s:300:"Hello,<br /><br />I&#039;m excited to work with everyone and start the training. I&#039;ll be on vacation and out of the country from 8/18-8/25 and will have limited access to email. I&#039;ll post my assignment now though and will make sure to check for any updates when I return. <br /><br />Reshma";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"91234bc9e7eae54afe753ea585b203f1";}s:32:"9e8c03b0c2ec4c4d9375940324ecb94f";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"rshah";s:4:"name";s:11:"Reshma Shah";s:4:"mail";s:16:"reshmamd@UIC.EDU";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1534520820;}s:3:"raw";s:10264:"Shah, Reshma - Assignment #1
1. Specific Aims
My response to that is:

More than 45% of children under 3 years of age live in low-income families in the United States, defined as twice the federal poverty level (FPL), the minimum estimated income necessary to meet a child’s basic needs.1 Poverty can severely impact a child’s educational and subsequent life-course trajectory by significantly compromising early development. Children growing up in poverty demonstrate poorer language, cognitive, and social-emotional outcomes emerging prior to preschool entry.2–5 Because these early deficits are predictive of later academic and psychological well-being, these early disparities have significant educational and health implications.6–8 

Converging economic, developmental, and biological research highlight two aspects of parenting behaviors as key scaffolds central to children’s development and subsequent educational achievement: 1) early and frequent participation in cognitively stimulating activities (e.g., reading and play) and 2) the quality of parent-child interactions, which includes, but is not limited to, parent-child verbal communication and parental sensitivity.9–11 However, significant discrepancies between economically advantaged and disadvantaged parents in these behaviors lead to childhood disparities in developmental milestones that are evident as early as 18 months of age.12,13 Enriching parenting behaviors promotes child early childhood development and offers an evidence-based strategy to reduce future educational disparities.11,14–16 Consequently, in an effort to decrease poverty-related parenting disparities, a number of programs have been developed to enhance parenting behaviors during early infancy, many of which are delivered through home visits or center-based programs. Many of these intensive programs have positively impacted parenting behaviors and early childhood outcomes; however, financial, logistical, and staffing challenges have impacted widespread dissemination. The use of trained professionals (e.g., social workers and registered nurses) and the costs of these programs, which can range from $6500-$14,000 per family, financially limits delivery of these services to the millions of children living at or near poverty in the United States.17 

One potential approach to widespread dissemination is to utilize the pediatric primary care office, which offers the advantages of an established non-stigmatizing location, frequent and well-attended well-child visits, and the ability to access a large population. Indeed, our recently published meta-analysis demonstrates efficacy of primary care–based parenting interventions on parental sensitivity and participation in cognitively stimulating activities.18 Although encouraging, these exciting programs utilized developmental specialists, home visits, and/or additional appointments making cost, attrition, staffing, and sustainability concerns for widespread dissemination. Still, recognizing the potential for a low-cost, effective preventive strategy, scientific and public health communities have called for research on how to best implement primary care-based programs that promote positive parenting behaviors and reduce poverty-related developmental disparities.19,20

Our proposed research is significant because we address the critical need for an accessible and effective population-level strategy to promote evidence-based based parenting behaviors that foster early childhood development. The proposed research plan will be achieved in the context of a hybrid implementation-effectiveness design targeting parents who attend a primary care clinic serving predominantly low-income urban families. We are currently conducting a randomized controlled trial (NICHD K23HD086295) to evaluate the direct parental outcomes of Sit Down and Play (SDP), a brief, theoretically driven primary care-based program delivered during pediatric well-child visits. The research aims for this proposal integrate the application of RE-AIM and Social Cognitive Theory and provide context for a testable implementation strategy for SDP (Fig 1).21 If successful, this approach can be applied to the implementation of existing and the development of new interventions in the primary care setting that aim to encourage evidence-based parenting behaviors to improve early child developmental outcomes in low-income communities.

To provide public health officials, policy makers, and clinicians evidence on how to best implement parent-directed programs in primary care clinics serving predominantly low-income families, we will pursue the following research aims:
Aim 1: Examine organization, provider/staff, and intervention factors that promote the implementation of Sit Down and Play (SDP), a brief, theory-driven, primary care-based program to encourage positive parenting behaviors that promote early childhood development.
Aim 2: Investigate the relationship between barriers and facilitators to the implementation of SDP and explore strategies to overcome barriers to implementation.
Aim 3:  Develop a testable, data driven implementation strategy to promote implementation of SDP.

We will utilize a mixed methodology approach to 1) evaluate the reach, representativeness, adoption feasibility, and degree to which SDP was implemented as intended, 2) identify associated costs with implementing SDP, 3) elucidate facilitators and barriers that aided in staff, administrator, and parental participation in SDP and 4) explore causal pathways through which changes in parenting knowledge, self-regulation, observational learning, facilitators and self-efficacy influence key parenting behaviors. The results of this project will support an R01 submission and serve as a framework to implement accessible and sustainable primary care-based interventions that enhance early child outcomes in families from low-income urban populations.



References:

1. 	Jiang Y, Ekono M, Skinner C. Basic Facts about Low-Income Children: Children under 3 Years, 2015. New York: National Center for Children in Poverty, Columbia University Mailman School of Public Health; 2017. https://eric.ed.gov/?id=ED558527. Accessed July 7, 2017.
2. 	Johnson SB, Riis JL, Noble KG. State of the Art Review: Poverty and the Developing Brain. PEDIATRICS. 2016;137(4):e20153075-e20153075. doi:10.1542/peds.2015-3075
3. 	Black MM, Walker SP, Fernald LC, et al. Early childhood development coming of age: science through the life course. The Lancet. 2017;389(10064):77–90.
4. 	Duncan GJ, Magnuson K, Votruba-Drzal E. Moving Beyond Correlations in Assessing the Consequences of Poverty. Annu Rev Psychol. 2017;68(1):413-434. doi:10.1146/annurev-psych-010416-044224
5. 	Noble KG, Engelhardt LE, Brito NH, et al. Socioeconomic Disparities in Neurocognitive Development in the First Two Years of Life. Dev Psychobiol. 2015;57(5):535-551. doi:10.1002/dev.21303
6. 	Grantham-McGregor S, Cheung YB, Cueto S, et al. Developmental potential in the first 5 years for children in developing countries. The lancet. 2007;369(9555):60–70.
7. 	Duncan GJ, Brooks-Gunn J. Family poverty, welfare reform, and child development. Child Dev. 2000;71(1):188–196.
8. 	Jones DE, Greenberg M, Crowley M. Early Social-Emotional Functioning and Public Health: The Relationship Between Kindergarten Social Competence and Future Wellness. Am J Public Health. 2015;105(11):2283-2290. doi:10.2105/AJPH.2015.302630
9. 	Landry SH, Smith KE, Swank PR. Responsive parenting: Establishing early foundations for social, communication, and independent problem-solving skills. Dev Psychol. 2006;42(4):627-642. doi:10.1037/0012-1649.42.4.627
10. 	Tamis-LeMonda CS, Shannon JD, Cabrera NJ, Lamb ME. Fathers and mothers at play with their 2-and 3-year-olds: contributions to language and cognitive development. Child Dev. 2004;75(6):1806–1820.
11. 	Hackman DA, Farah MJ, Meaney MJ. Socioeconomic status and the brain: mechanistic insights from human and animal research. Nat Rev Neurosci. 2010;11(9):651-659. doi:10.1038/nrn2897
12. 	Fernald A, Marchman VA, Weisleder A. SES differences in language processing skill and vocabulary are evident at 18?months. Dev Sci. 2013;16(2):234-248. doi:10.1111/desc.12019
13. 	Shah R, Sobotka SA, Chen Y-F, Msall ME. Positive Parenting Practices, Health Disparities, and Developmental Progress. PEDIATRICS. 2015;136(2):318-326. doi:10.1542/peds.2014-3390
14. 	Morris AS, Robinson LR, Hays-Grudo J, Claussen AH, Hartwig SA, Treat AE. Targeting Parenting in Early Childhood: A Public Health Approach to Improve Outcomes for Children Living in Poverty. Child Dev. 2017;88(2):388-397. doi:10.1111/cdev.12743
15. 	Lugo-Gil J, Tamis-LeMonda CS. Family resources and parenting quality: Links to children’s cognitive development across the first 3 years. Child Dev. 2008;79(4):1065–1085.
16. 	Olds DL, Sadler L, Kitzman H. Programs for parents of infants and toddlers: recent evidence from randomized trials. J Child Psychol Psychiatry. 2007;48(3-4):355-391. doi:10.1111/j.1469-7610.2006.01702.x
17. 	Zaveri H, Burwick A, Maher E. Home Visiting: The Potential for Cost Savings from Home Visiting Due to Reductions in Child Maltreatment. Washington, DC: Casey Family Programs; 2014. http://www.chapinhall.org/sites/default/files/documents/EBHV%20Cost%20Savings%20Brief.pdf. Accessed January 31, 2018.
18. 	Shah R, Kennedy S, Clark MD, Bauer SC, Schwartz A. Primary care–based interventions to promote positive parenting behaviors: a meta-analysis. Pediatrics. 2016;137(5):e20153393.
19. 	Dworkin PH. 2003 C. Anderson Aldrich Award Lecture: Enhancing Developmental Services in Child Health Supervision--An Idea Whose Time Has Truly Arrived. PEDIATRICS. 2004;114(3):827-831. doi:10.1542/peds.2004-0416
20. 	NIMH » Priorities for Strategy 4.1. https://www.nimh.nih.gov/about/strategic-planning-reports/strategic-research-priorities/srp-objective-4/priorities-for-strategy-41.shtml. Accessed January 31, 2018.
21. 	Zoellner J, Chen Y, Davy B, et al. Talking Health, A pragmatic randomized-controlled health literacy trial targeting sugar-sweetened beverage consumption among adults: Rationale, design & methods. Contemp Clin Trials. 2014;37(1):43-57. doi:10.1016/j.cct.2013.11.003


";s:5:"xhtml";s:10480:"Shah, Reshma - Assignment #1<br />1. Specific Aims<br />My response to that is:<br /><br />More than 45% of children under 3 years of age live in low-income families in the United States, defined as twice the federal poverty level (FPL), the minimum estimated income necessary to meet a child’s basic needs.1 Poverty can severely impact a child’s educational and subsequent life-course trajectory by significantly compromising early development. Children growing up in poverty demonstrate poorer language, cognitive, and social-emotional outcomes emerging prior to preschool entry.2–5 Because these early deficits are predictive of later academic and psychological well-being, these early disparities have significant educational and health implications.6–8 <br /><br />Converging economic, developmental, and biological research highlight two aspects of parenting behaviors as key scaffolds central to children’s development and subsequent educational achievement: 1) early and frequent participation in cognitively stimulating activities (e.g., reading and play) and 2) the quality of parent-child interactions, which includes, but is not limited to, parent-child verbal communication and parental sensitivity.9–11 However, significant discrepancies between economically advantaged and disadvantaged parents in these behaviors lead to childhood disparities in developmental milestones that are evident as early as 18 months of age.12,13 Enriching parenting behaviors promotes child early childhood development and offers an evidence-based strategy to reduce future educational disparities.11,14–16 Consequently, in an effort to decrease poverty-related parenting disparities, a number of programs have been developed to enhance parenting behaviors during early infancy, many of which are delivered through home visits or center-based programs. Many of these intensive programs have positively impacted parenting behaviors and early childhood outcomes; however, financial, logistical, and staffing challenges have impacted widespread dissemination. The use of trained professionals (e.g., social workers and registered nurses) and the costs of these programs, which can range from $6500-$14,000 per family, financially limits delivery of these services to the millions of children living at or near poverty in the United States.17 <br /><br />One potential approach to widespread dissemination is to utilize the pediatric primary care office, which offers the advantages of an established non-stigmatizing location, frequent and well-attended well-child visits, and the ability to access a large population. Indeed, our recently published meta-analysis demonstrates efficacy of primary care–based parenting interventions on parental sensitivity and participation in cognitively stimulating activities.18 Although encouraging, these exciting programs utilized developmental specialists, home visits, and/or additional appointments making cost, attrition, staffing, and sustainability concerns for widespread dissemination. Still, recognizing the potential for a low-cost, effective preventive strategy, scientific and public health communities have called for research on how to best implement primary care-based programs that promote positive parenting behaviors and reduce poverty-related developmental disparities.19,20<br /><br />Our proposed research is significant because we address the critical need for an accessible and effective population-level strategy to promote evidence-based based parenting behaviors that foster early childhood development. The proposed research plan will be achieved in the context of a hybrid implementation-effectiveness design targeting parents who attend a primary care clinic serving predominantly low-income urban families. We are currently conducting a randomized controlled trial (NICHD K23HD086295) to evaluate the direct parental outcomes of Sit Down and Play (SDP), a brief, theoretically driven primary care-based program delivered during pediatric well-child visits. The research aims for this proposal integrate the application of RE-AIM and Social Cognitive Theory and provide context for a testable implementation strategy for SDP (Fig 1).21 If successful, this approach can be applied to the implementation of existing and the development of new interventions in the primary care setting that aim to encourage evidence-based parenting behaviors to improve early child developmental outcomes in low-income communities.<br /><br />To provide public health officials, policy makers, and clinicians evidence on how to best implement parent-directed programs in primary care clinics serving predominantly low-income families, we will pursue the following research aims:<br />Aim 1: Examine organization, provider/staff, and intervention factors that promote the implementation of Sit Down and Play (SDP), a brief, theory-driven, primary care-based program to encourage positive parenting behaviors that promote early childhood development.<br />Aim 2: Investigate the relationship between barriers and facilitators to the implementation of SDP and explore strategies to overcome barriers to implementation.<br />Aim 3:  Develop a testable, data driven implementation strategy to promote implementation of SDP.<br /><br />We will utilize a mixed methodology approach to 1) evaluate the reach, representativeness, adoption feasibility, and degree to which SDP was implemented as intended, 2) identify associated costs with implementing SDP, 3) elucidate facilitators and barriers that aided in staff, administrator, and parental participation in SDP and 4) explore causal pathways through which changes in parenting knowledge, self-regulation, observational learning, facilitators and self-efficacy influence key parenting behaviors. The results of this project will support an R01 submission and serve as a framework to implement accessible and sustainable primary care-based interventions that enhance early child outcomes in families from low-income urban populations.<br /><br /><br /><br />References:<br /><br />1. 	Jiang Y, Ekono M, Skinner C. Basic Facts about Low-Income Children: Children under 3 Years, 2015. New York: National Center for Children in Poverty, Columbia University Mailman School of Public Health; 2017. https://eric.ed.gov/?id=ED558527. Accessed July 7, 2017.<br />2. 	Johnson SB, Riis JL, Noble KG. State of the Art Review: Poverty and the Developing Brain. PEDIATRICS. 2016;137(4):e20153075-e20153075. doi:10.1542/peds.2015-3075<br />3. 	Black MM, Walker SP, Fernald LC, et al. Early childhood development coming of age: science through the life course. The Lancet. 2017;389(10064):77–90.<br />4. 	Duncan GJ, Magnuson K, Votruba-Drzal E. Moving Beyond Correlations in Assessing the Consequences of Poverty. Annu Rev Psychol. 2017;68(1):413-434. doi:10.1146/annurev-psych-010416-044224<br />5. 	Noble KG, Engelhardt LE, Brito NH, et al. Socioeconomic Disparities in Neurocognitive Development in the First Two Years of Life. Dev Psychobiol. 2015;57(5):535-551. doi:10.1002/dev.21303<br />6. 	Grantham-McGregor S, Cheung YB, Cueto S, et al. Developmental potential in the first 5 years for children in developing countries. The lancet. 2007;369(9555):60–70.<br />7. 	Duncan GJ, Brooks-Gunn J. Family poverty, welfare reform, and child development. Child Dev. 2000;71(1):188–196.<br />8. 	Jones DE, Greenberg M, Crowley M. Early Social-Emotional Functioning and Public Health: The Relationship Between Kindergarten Social Competence and Future Wellness. Am J Public Health. 2015;105(11):2283-2290. doi:10.2105/AJPH.2015.302630<br />9. 	Landry SH, Smith KE, Swank PR. Responsive parenting: Establishing early foundations for social, communication, and independent problem-solving skills. Dev Psychol. 2006;42(4):627-642. doi:10.1037/0012-1649.42.4.627<br />10. 	Tamis-LeMonda CS, Shannon JD, Cabrera NJ, Lamb ME. Fathers and mothers at play with their 2-and 3-year-olds: contributions to language and cognitive development. Child Dev. 2004;75(6):1806–1820.<br />11. 	Hackman DA, Farah MJ, Meaney MJ. Socioeconomic status and the brain: mechanistic insights from human and animal research. Nat Rev Neurosci. 2010;11(9):651-659. doi:10.1038/nrn2897<br />12. 	Fernald A, Marchman VA, Weisleder A. SES differences in language processing skill and vocabulary are evident at 18?months. Dev Sci. 2013;16(2):234-248. doi:10.1111/desc.12019<br />13. 	Shah R, Sobotka SA, Chen Y-F, Msall ME. Positive Parenting Practices, Health Disparities, and Developmental Progress. PEDIATRICS. 2015;136(2):318-326. doi:10.1542/peds.2014-3390<br />14. 	Morris AS, Robinson LR, Hays-Grudo J, Claussen AH, Hartwig SA, Treat AE. Targeting Parenting in Early Childhood: A Public Health Approach to Improve Outcomes for Children Living in Poverty. Child Dev. 2017;88(2):388-397. doi:10.1111/cdev.12743<br />15. 	Lugo-Gil J, Tamis-LeMonda CS. Family resources and parenting quality: Links to children’s cognitive development across the first 3 years. Child Dev. 2008;79(4):1065–1085.<br />16. 	Olds DL, Sadler L, Kitzman H. Programs for parents of infants and toddlers: recent evidence from randomized trials. J Child Psychol Psychiatry. 2007;48(3-4):355-391. doi:10.1111/j.1469-7610.2006.01702.x<br />17. 	Zaveri H, Burwick A, Maher E. Home Visiting: The Potential for Cost Savings from Home Visiting Due to Reductions in Child Maltreatment. Washington, DC: Casey Family Programs; 2014. http://www.chapinhall.org/sites/default/files/documents/EBHV%20Cost%20Savings%20Brief.pdf. Accessed January 31, 2018.<br />18. 	Shah R, Kennedy S, Clark MD, Bauer SC, Schwartz A. Primary care–based interventions to promote positive parenting behaviors: a meta-analysis. Pediatrics. 2016;137(5):e20153393.<br />19. 	Dworkin PH. 2003 C. Anderson Aldrich Award Lecture: Enhancing Developmental Services in Child Health Supervision--An Idea Whose Time Has Truly Arrived. PEDIATRICS. 2004;114(3):827-831. doi:10.1542/peds.2004-0416<br />20. 	NIMH » Priorities for Strategy 4.1. https://www.nimh.nih.gov/about/strategic-planning-reports/strategic-research-priorities/srp-objective-4/priorities-for-strategy-41.shtml. Accessed January 31, 2018.<br />21. 	Zoellner J, Chen Y, Davy B, et al. Talking Health, A pragmatic randomized-controlled health literacy trial targeting sugar-sweetened beverage consumption among adults: Rationale, design &amp; methods. Contemp Clin Trials. 2014;37(1):43-57. doi:10.1016/j.cct.2013.11.003";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"0bdfcbe7af4cc1d4d2a565b5e2ca35a1";}s:4:"show";b:1;s:3:"cid";s:32:"9e8c03b0c2ec4c4d9375940324ecb94f";}s:32:"e98084f78bd25f4ea7f91d5feb6f10ad";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"rshah";s:4:"name";s:11:"Reshma Shah";s:4:"mail";s:16:"reshmamd@UIC.EDU";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1534520903;}s:3:"raw";s:150:"I can't seem to figure out how to insert a figure or utilize bold/underline functions so that's missing from my above post. Sorry about that!

Reshma
";s:5:"xhtml";s:169:"I can&#039;t seem to figure out how to insert a figure or utilize bold/underline functions so that&#039;s missing from my above post. Sorry about that!<br /><br />Reshma";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"e98084f78bd25f4ea7f91d5feb6f10ad";}s:32:"a424c2a842052fdc97cf2e95e27e5e84";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"kgoldstein";s:4:"name";s:15:"Karen Goldstein";s:4:"mail";s:24:"karen.goldstein@duke.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1534955452;}s:3:"raw";s:10922:"Goldstein, Karen Assignment #1

Draft a specific aims page (2 pages maximum) of your proposed study. 

My response to that is:

The majority of middle-aged Veterans have at least one risk factor for cardiovascular disease (CVD) (1); these risk factors and subsequent events such as stroke and heart disease lead to significant personal and financial burden for patients and the healthcare system(2). Social support improves health outcomes among adults with acute(3) and chronic disease (4) including CVD (5). Social support can be derived from existing social networks or supplemented by individuals with a similar life experience (e.g., peers). Peer support is “the giving of assistance and encouragement by an individual considered equal” (6). As demonstrated in the context of diabetes (7), heart disease(8) and other chronic diseases (9), peer support leads to improved clinical and patient-centered outcomes. 

Veterans are accustomed to providing support and guidance to each other from their military service and have an important shared identity (10). Unfortunately, many Veterans are socially isolated. Peer support could be a valuable tool in promoting Veteran engagement in CVD risk reduction behaviors. Peer support has been found to be effective at promoting self-management for a range of chronic conditions including diabetes and asthma (7, 11). Moreover, the VA has already implemented a formal peer support specialist program in mental health clinics (12) which earns high levels of satisfaction from both patients and peer leaders (13, 14) and promotes patient empowerment (15) and health care engagement (12). Moreover, peer support is specifically mentioned in the recently passed VA Mission Act of 2018 as a method to improve VA Healthcare Delivery. However, peer support has not yet been implemented systematically within the VA to address health promotion and prevention of chronic disease, specifically CVD and most peer support trials within the VA have recruited predominantly or exclusively male Veterans.

Prior work has identified barriers to implementation of peer support interventions within the VA including role definitions and training, logistic challenges and reduced motivation and engagement, but that support from study staff, shared veteran identity, site preparation and external facilitation were key facilitators (10, 16). There are multiple gaps in our understanding of how to implement peer support within VA primary care to promote preventative health behaviors to reduce CVD risk. These gaps include: 1) how best to integrate a peer support program within existing patient aligned care teams (PACTs); 2) how best to adapt the structure of peer support to reach to less engaged, hardly reached patients; and 3) in what ways does a peer support program to decrease CVD risk need to be tailored by gender.

We are currently conducting a pilot study to assess the acceptability and feasibility of a novel hybrid peer coach-reciprocal peer support intervention promote engagement in CVD risk reduction behaviors for at-risk Veterans. This pilot intervention combines two types of peer support to allow for patient-centered, flexibility around accessing this support. Specifically the two types are reciprocal peer support which pairs individuals to both give and receive social support around a common health goal and peer coaching, which delivers unidirectional support from someone with advanced training and who has previously improved their health behaviors to an individual with the same disease state. Both models have been effective in improving health outcomes (7, 11). The rationale for combining these approaches is that they will allow for variation in support delivery based on naturally occurring differences in patients’ motivation levels and their ability to proactively engage with peers. We plan to recruit 50% women into this pilot study to support assessments about gender-based differences in acceptability and feasibility and preliminary evidence to support for gender-specific tailored if needed. We anticipate that our next step will be to conduct a larger two-site randomized clinical effectiveness Hybrid Type I study (17) of our combined peer coach/reciprocal peer support intervention in which we will test this clinical intervention and study the context of its implementation.

To test the effectiveness of this hybrid peer coach-reciprocal peer support intervention and to gather data on the implementation process, our aims are:

Aim1: Evaluate the effectiveness of a combined peer coach/reciprocal peer support intervention on increasing patient activation and engagement in CVD risk reduction behaviors (e.g. increased physical activity, dietary change) compared to usual care.

Aim 2: Assess potential barriers and facilitators to the uptake and adoption of a combined peer coach/reciprocal peer support intervention in the context of two geographically distinct VA primary care clinics.

We will enroll Veterans age 35-64 at risk for CVD (e.g. hypertension, hyperlipidemia, non-insulin dependent diabetes, or obesity) and who are physically inactive (<150 minutes per week). Enrolled Veterans will: 1) attend a series of interactive group sessions on CVD risk reduction (focused on increasing physical activity and improving diet), 2) be matched with another enrolled Veteran in their group session, and 3) receive brief training in peer communication skills and be encouraged to communicate by phone weekly with their matched peer partner. Trained peer coaches will provide extra support to participants who do not engage as planned (e.g. not making weekly calls or attending group sessions) or who request it. Interactive group sessions will provide structured content and peer partner activities. The intervention will take place over 6 months based on other peer interventions in the literature (7, 11, 18) with bi-monthly group sessions allowing for dose similar to other trials of peer support interventions. We will follow participants for up to 12 months. Randomization will occur at the patient level to the combined peer support intervention or an enhanced usual care control arm. Targets for recruitment of women Veterans will support analysis of both aims by gender. 

For Aim 1, we will measure patient activation as the primary outcome. Secondary outcomes will include measures of CVD risk reduction behavior engagement (e.g. minutes per week of physical activity and dietary intake), BMI and calculated CVD risk. In addition, a mixed-methods approach will address Aim 2, including pre-post qualitative interviews with patient, administrative and clinical stakeholders on barriers and facilitators of intervention activities. In addition, we will track logistic challenges and expeditors to the conduct of peer support activities, which may have implications for future implementation. The results of this project will support a future implementation trial for which we anticipate working with the VA Women’s Health Practice-Based Research Network’s (PBRN) to identify a collection of clinical sites with varied patient populations and resource composition to test implementation strategies in differently resourced settings. 

References
1.	Whitehead AM DM, Duvernoy C, Safdar B, Nkonde-Price C, Iqbal S,, Balasubramanian V FS, Friedman SA, Hayes, PM, Haskell, SG. The State of Cardiovascular Health in
Women Veterans. Volume 1: VA Outpatient Diagnoses and Procedures in Fiscal Year (FY) 2010. In: Affairs DoV, editor.: Veterans Health Administration; 2013.
2.	Yu W, Ravelo A, Wagner TH, Phibbs CS, Bhandari A, Chen S, et al. Prevalence and costs of chronic conditions in the VA health care system. Med Care Res Rev. 2003;60(3 Suppl):146s-67s.
3.	Cohen S, Doyle WJ, Skoner DP, Rabin BS, Gwaltney JM, Jr. Social ties and susceptibility to the common cold. Jama. 1997;277(24):1940-4.
4.	Gallant MP. The influence of social support on chronic illness self-management: a review and directions for research. Health education & behavior : the official publication of the Society for Public Health Education. 2003;30(2):170-95.
5.	Barth J, Schneider S, von Kanel R. Lack of social support in the etiology and the prognosis of coronary heart disease: a systematic review and meta-analysis. Psychosomatic medicine. 2010;72(3):229-38.
6.	Dennis CL. Peer support within a health care context: a concept analysis. International journal of nursing studies. 2003;40(3):321-32.
7.	Heisler M, Vijan S, Makki F, Piette JD. Diabetes control with reciprocal peer support versus nurse care management: a randomized trial. Annals of internal medicine. 2010;153(8):507-15.
8.	Parry M, Watt-Watson J. Peer support intervention trials for individuals with heart disease: a systematic review. European journal of cardiovascular nursing : journal of the Working Group on Cardiovascular Nursing of the European Society of Cardiology. 2010;9(1):57-67.
9.	Rhee H, McQuillan BE, Belyea MJ. Evaluation of a peer-led asthma self-management program and benefits of the program for adolescent peer leaders. Respir Care. 2012;57(12):2082-9.
10.	Matthias MS, Kukla M, McGuire AB, Damush TM, Gill N, Bair MJ. Facilitators and Barriers to Participation in a Peer Support Intervention for Veterans With Chronic Pain. The Clinical journal of pain. 2016;32(6):534-40.
11.	Long JA, Jahnle EC, Richardson DM, Loewenstein G, Volpp KG. Peer mentoring and financial incentives to improve glucose control in African American veterans: a randomized trial. Annals of internal medicine. 2012;156(6):416-24.
12.	Chinman M LA, Gresen R, Davis M, Losonczy M, Sussner B, Martone L. Early Experiences of Employing Consumer-Providers in the VA. Psychiatric Services. 2008;59(11):1315-21.
13.	Barber JA RR, Armstong M, Resnick SG. Monitoring the Dissemination of Peer Support in the VA Healthcare System. Community Mental Health Journal. 2008;44:433-41.
14.	Hamilton AB, Chinman M, Cohen AN, Oberman RS, Young AS. Implementation of Consumer Providers into Mental Health Intensive Case Management Teams. J Behav Health Serv Res. 2013.
15.	Resnick SG, Rosenheck RA. Integrating peer-provided services: a quasi-experimental study of recovery orientation, confidence, and empowerment. Psychiatric services (Washington, DC). 2008;59(11):1307-14.
16.	Hamilton AB, Chinman M, Cohen AN, Oberman RS, Young AS. Implementation of consumer providers into mental health intensive case management teams. The journal of behavioral health services & research. 2015;42(1):100-8.
17.	Curran GM BM, Mittman B, Pyne JM, Stetler C. Effectiveness-implementation Hybrid Designs: Combining Elements of Clinical Effectiveness and Implementation Research to Enhance Public Health Impact. Med Care. 2012;50(3):217-26.
18.	Leahey TM, Wing RR. A Randomized Controlled Pilot Study Testing Three Types of Health Coaches for Obesity Treatment: Professional, Peer, and Mentor. Obesity (Silver Spring). 2012.

";s:5:"xhtml";s:11146:"Goldstein, Karen Assignment #1<br /><br />Draft a specific aims page (2 pages maximum) of your proposed study. <br /><br />My response to that is:<br /><br />The majority of middle-aged Veterans have at least one risk factor for cardiovascular disease (CVD) (1); these risk factors and subsequent events such as stroke and heart disease lead to significant personal and financial burden for patients and the healthcare system(2). Social support improves health outcomes among adults with acute(3) and chronic disease (4) including CVD (5). Social support can be derived from existing social networks or supplemented by individuals with a similar life experience (e.g., peers). Peer support is “the giving of assistance and encouragement by an individual considered equal” (6). As demonstrated in the context of diabetes (7), heart disease(8) and other chronic diseases (9), peer support leads to improved clinical and patient-centered outcomes. <br /><br />Veterans are accustomed to providing support and guidance to each other from their military service and have an important shared identity (10). Unfortunately, many Veterans are socially isolated. Peer support could be a valuable tool in promoting Veteran engagement in CVD risk reduction behaviors. Peer support has been found to be effective at promoting self-management for a range of chronic conditions including diabetes and asthma (7, 11). Moreover, the VA has already implemented a formal peer support specialist program in mental health clinics (12) which earns high levels of satisfaction from both patients and peer leaders (13, 14) and promotes patient empowerment (15) and health care engagement (12). Moreover, peer support is specifically mentioned in the recently passed VA Mission Act of 2018 as a method to improve VA Healthcare Delivery. However, peer support has not yet been implemented systematically within the VA to address health promotion and prevention of chronic disease, specifically CVD and most peer support trials within the VA have recruited predominantly or exclusively male Veterans.<br /><br />Prior work has identified barriers to implementation of peer support interventions within the VA including role definitions and training, logistic challenges and reduced motivation and engagement, but that support from study staff, shared veteran identity, site preparation and external facilitation were key facilitators (10, 16). There are multiple gaps in our understanding of how to implement peer support within VA primary care to promote preventative health behaviors to reduce CVD risk. These gaps include: 1) how best to integrate a peer support program within existing patient aligned care teams (PACTs); 2) how best to adapt the structure of peer support to reach to less engaged, hardly reached patients; and 3) in what ways does a peer support program to decrease CVD risk need to be tailored by gender.<br /><br />We are currently conducting a pilot study to assess the acceptability and feasibility of a novel hybrid peer coach-reciprocal peer support intervention promote engagement in CVD risk reduction behaviors for at-risk Veterans. This pilot intervention combines two types of peer support to allow for patient-centered, flexibility around accessing this support. Specifically the two types are reciprocal peer support which pairs individuals to both give and receive social support around a common health goal and peer coaching, which delivers unidirectional support from someone with advanced training and who has previously improved their health behaviors to an individual with the same disease state. Both models have been effective in improving health outcomes (7, 11). The rationale for combining these approaches is that they will allow for variation in support delivery based on naturally occurring differences in patients’ motivation levels and their ability to proactively engage with peers. We plan to recruit 50% women into this pilot study to support assessments about gender-based differences in acceptability and feasibility and preliminary evidence to support for gender-specific tailored if needed. We anticipate that our next step will be to conduct a larger two-site randomized clinical effectiveness Hybrid Type I study (17) of our combined peer coach/reciprocal peer support intervention in which we will test this clinical intervention and study the context of its implementation.<br /><br />To test the effectiveness of this hybrid peer coach-reciprocal peer support intervention and to gather data on the implementation process, our aims are:<br /><br />Aim1: Evaluate the effectiveness of a combined peer coach/reciprocal peer support intervention on increasing patient activation and engagement in CVD risk reduction behaviors (e.g. increased physical activity, dietary change) compared to usual care.<br /><br />Aim 2: Assess potential barriers and facilitators to the uptake and adoption of a combined peer coach/reciprocal peer support intervention in the context of two geographically distinct VA primary care clinics.<br /><br />We will enroll Veterans age 35-64 at risk for CVD (e.g. hypertension, hyperlipidemia, non-insulin dependent diabetes, or obesity) and who are physically inactive (&lt;150 minutes per week). Enrolled Veterans will: 1) attend a series of interactive group sessions on CVD risk reduction (focused on increasing physical activity and improving diet), 2) be matched with another enrolled Veteran in their group session, and 3) receive brief training in peer communication skills and be encouraged to communicate by phone weekly with their matched peer partner. Trained peer coaches will provide extra support to participants who do not engage as planned (e.g. not making weekly calls or attending group sessions) or who request it. Interactive group sessions will provide structured content and peer partner activities. The intervention will take place over 6 months based on other peer interventions in the literature (7, 11, 18) with bi-monthly group sessions allowing for dose similar to other trials of peer support interventions. We will follow participants for up to 12 months. Randomization will occur at the patient level to the combined peer support intervention or an enhanced usual care control arm. Targets for recruitment of women Veterans will support analysis of both aims by gender. <br /><br />For Aim 1, we will measure patient activation as the primary outcome. Secondary outcomes will include measures of CVD risk reduction behavior engagement (e.g. minutes per week of physical activity and dietary intake), BMI and calculated CVD risk. In addition, a mixed-methods approach will address Aim 2, including pre-post qualitative interviews with patient, administrative and clinical stakeholders on barriers and facilitators of intervention activities. In addition, we will track logistic challenges and expeditors to the conduct of peer support activities, which may have implications for future implementation. The results of this project will support a future implementation trial for which we anticipate working with the VA Women’s Health Practice-Based Research Network’s (PBRN) to identify a collection of clinical sites with varied patient populations and resource composition to test implementation strategies in differently resourced settings. <br /><br />References<br />1.	Whitehead AM DM, Duvernoy C, Safdar B, Nkonde-Price C, Iqbal S,, Balasubramanian V FS, Friedman SA, Hayes, PM, Haskell, SG. The State of Cardiovascular Health in<br />Women Veterans. Volume 1: VA Outpatient Diagnoses and Procedures in Fiscal Year (FY) 2010. In: Affairs DoV, editor.: Veterans Health Administration; 2013.<br />2.	Yu W, Ravelo A, Wagner TH, Phibbs CS, Bhandari A, Chen S, et al. Prevalence and costs of chronic conditions in the VA health care system. Med Care Res Rev. 2003;60(3 Suppl):146s-67s.<br />3.	Cohen S, Doyle WJ, Skoner DP, Rabin BS, Gwaltney JM, Jr. Social ties and susceptibility to the common cold. Jama. 1997;277(24):1940-4.<br />4.	Gallant MP. The influence of social support on chronic illness self-management: a review and directions for research. Health education &amp; behavior : the official publication of the Society for Public Health Education. 2003;30(2):170-95.<br />5.	Barth J, Schneider S, von Kanel R. Lack of social support in the etiology and the prognosis of coronary heart disease: a systematic review and meta-analysis. Psychosomatic medicine. 2010;72(3):229-38.<br />6.	Dennis CL. Peer support within a health care context: a concept analysis. International journal of nursing studies. 2003;40(3):321-32.<br />7.	Heisler M, Vijan S, Makki F, Piette JD. Diabetes control with reciprocal peer support versus nurse care management: a randomized trial. Annals of internal medicine. 2010;153(8):507-15.<br />8.	Parry M, Watt-Watson J. Peer support intervention trials for individuals with heart disease: a systematic review. European journal of cardiovascular nursing : journal of the Working Group on Cardiovascular Nursing of the European Society of Cardiology. 2010;9(1):57-67.<br />9.	Rhee H, McQuillan BE, Belyea MJ. Evaluation of a peer-led asthma self-management program and benefits of the program for adolescent peer leaders. Respir Care. 2012;57(12):2082-9.<br />10.	Matthias MS, Kukla M, McGuire AB, Damush TM, Gill N, Bair MJ. Facilitators and Barriers to Participation in a Peer Support Intervention for Veterans With Chronic Pain. The Clinical journal of pain. 2016;32(6):534-40.<br />11.	Long JA, Jahnle EC, Richardson DM, Loewenstein G, Volpp KG. Peer mentoring and financial incentives to improve glucose control in African American veterans: a randomized trial. Annals of internal medicine. 2012;156(6):416-24.<br />12.	Chinman M LA, Gresen R, Davis M, Losonczy M, Sussner B, Martone L. Early Experiences of Employing Consumer-Providers in the VA. Psychiatric Services. 2008;59(11):1315-21.<br />13.	Barber JA RR, Armstong M, Resnick SG. Monitoring the Dissemination of Peer Support in the VA Healthcare System. Community Mental Health Journal. 2008;44:433-41.<br />14.	Hamilton AB, Chinman M, Cohen AN, Oberman RS, Young AS. Implementation of Consumer Providers into Mental Health Intensive Case Management Teams. J Behav Health Serv Res. 2013.<br />15.	Resnick SG, Rosenheck RA. Integrating peer-provided services: a quasi-experimental study of recovery orientation, confidence, and empowerment. Psychiatric services (Washington, DC). 2008;59(11):1307-14.<br />16.	Hamilton AB, Chinman M, Cohen AN, Oberman RS, Young AS. Implementation of consumer providers into mental health intensive case management teams. The journal of behavioral health services &amp; research. 2015;42(1):100-8.<br />17.	Curran GM BM, Mittman B, Pyne JM, Stetler C. Effectiveness-implementation Hybrid Designs: Combining Elements of Clinical Effectiveness and Implementation Research to Enhance Public Health Impact. Med Care. 2012;50(3):217-26.<br />18.	Leahey TM, Wing RR. A Randomized Controlled Pilot Study Testing Three Types of Health Coaches for Obesity Treatment: Professional, Peer, and Mentor. Obesity (Silver Spring). 2012.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"e80b5f59120f3dd6b851afeafc93497e";}s:4:"show";b:1;s:3:"cid";s:32:"a424c2a842052fdc97cf2e95e27e5e84";}s:32:"2bff6b7983ba0e72f95b8c24c4d81cc9";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"elattie";s:4:"name";s:12:"Emily Lattie";s:4:"mail";s:29:"emily.lattie@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1534962370;s:8:"modified";i:1534962393;}s:3:"raw";s:3672:"LATTIE  – Assignment #1a


1. Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&I-focused specific aims pages, it should generally address the following 5 elements (and questions). 
My response to this is…

Depression is common and is grossly undertreated [1]. Only 19.6% of those with mood disorders receive minimally adequate treatment [2] and the vast majority of patients with depression receive treatment in general primary care settings, where outcomes are poor [3, 4].  The collaborative care model of depression treatment, in which behavioral health professionals are embedded in primary care clinics, has demonstrated significantly better patient outcomes and reductions in health care costs when compared to care as usual [5]. 

In this model of care, embedded behavioral health professionals provide evidence-based pharmacotherapy and/or psychotherapy with support from regular psychiatric case consultation. While this model demonstrates an improvement beyond traditional primary care practice, it is not without limitations. An examination of collaborative care at one of Northwestern’s primary care clinics demonstrates that some patients decline referrals to collaborative care, similar to their declining of referrals to specialty mental health care, and a subset of those who do initially engage with treatment are lost to follow-up. 

While offering collaborative care for depression appears to help patients circumvent some of the barriers associated with traditional mental health treatments, barriers to engagement still exist and this treatment gap may be addressed through the use of technology-enabled programs, such as those delivered via apps and the web. These types of technology-enabled programs can be highly effective in treating depression [6-9]. However, the bulk of research on technology-enabled mental health programs has focused on efficacy trials that are not designed for collaborative care and implementation factors have not been considered during program design. Evidence is emerging that these interventions cannot be successfully implemented in primary care settings, as patients do not engage with the digital treatments, and care systems do not know how to implement them [10].

In this project, I aim to develop a feasible, acceptable and effective app-based mental health program to be used in collaborative care for depression. This program will be based on our previously developed and tested IntelliCare suite of apps [11, 12], which deliver components of evidence-based treatments (EBTs) in a mobile format that requires a relatively low time commitment for patients and providers. 
 
Specific Aim 1: Develop an app-based mental health program to be used in collaborative care for depression. A stakeholder advisory board consisting of primary care providers, behavioral health specialists, and patients will be formed. Stakeholders will provide feedback on the program as it is developed and piloted, and implementation strategies for program roll-out will be identified. 

Specific Aim 2: Conduct a stepped wedge trial of program implementation in 6 primary care clinics. I hypothesize that this program will demonstrate feasibility and acceptability, and, compared with collaborative care as usual, this program will produce a broader reach. I also hypothesize that clinical outcomes will be improved following implementation this program. 

This project will lead to a functional app-based mental health program to be used in collaborative care for depression, and will improve the ability of collaborative care clinicians to deliver components of EBTs. 
";s:5:"xhtml";s:3764:"LATTIE  – Assignment #1a<br /><br /><br />1. Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&amp;I-focused specific aims pages, it should generally address the following 5 elements (and questions). <br />My response to this is…<br /><br />Depression is common and is grossly undertreated [1]. Only 19.6% of those with mood disorders receive minimally adequate treatment [2] and the vast majority of patients with depression receive treatment in general primary care settings, where outcomes are poor [3, 4].  The collaborative care model of depression treatment, in which behavioral health professionals are embedded in primary care clinics, has demonstrated significantly better patient outcomes and reductions in health care costs when compared to care as usual [5]. <br /><br />In this model of care, embedded behavioral health professionals provide evidence-based pharmacotherapy and/or psychotherapy with support from regular psychiatric case consultation. While this model demonstrates an improvement beyond traditional primary care practice, it is not without limitations. An examination of collaborative care at one of Northwestern’s primary care clinics demonstrates that some patients decline referrals to collaborative care, similar to their declining of referrals to specialty mental health care, and a subset of those who do initially engage with treatment are lost to follow-up. <br /><br />While offering collaborative care for depression appears to help patients circumvent some of the barriers associated with traditional mental health treatments, barriers to engagement still exist and this treatment gap may be addressed through the use of technology-enabled programs, such as those delivered via apps and the web. These types of technology-enabled programs can be highly effective in treating depression [6-9]. However, the bulk of research on technology-enabled mental health programs has focused on efficacy trials that are not designed for collaborative care and implementation factors have not been considered during program design. Evidence is emerging that these interventions cannot be successfully implemented in primary care settings, as patients do not engage with the digital treatments, and care systems do not know how to implement them [10].<br /><br />In this project, I aim to develop a feasible, acceptable and effective app-based mental health program to be used in collaborative care for depression. This program will be based on our previously developed and tested IntelliCare suite of apps [11, 12], which deliver components of evidence-based treatments (EBTs) in a mobile format that requires a relatively low time commitment for patients and providers. <br /> <br />Specific Aim 1: Develop an app-based mental health program to be used in collaborative care for depression. A stakeholder advisory board consisting of primary care providers, behavioral health specialists, and patients will be formed. Stakeholders will provide feedback on the program as it is developed and piloted, and implementation strategies for program roll-out will be identified. <br /><br />Specific Aim 2: Conduct a stepped wedge trial of program implementation in 6 primary care clinics. I hypothesize that this program will demonstrate feasibility and acceptability, and, compared with collaborative care as usual, this program will produce a broader reach. I also hypothesize that clinical outcomes will be improved following implementation this program. <br /><br />This project will lead to a functional app-based mental health program to be used in collaborative care for depression, and will improve the ability of collaborative care clinicians to deliver components of EBTs.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"270368acb153e2d1045c861f5cf2bb7a";}s:4:"show";b:1;s:3:"cid";s:32:"2bff6b7983ba0e72f95b8c24c4d81cc9";}s:32:"6893a0ed4e8631ad051c69c0c85ab4b6";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"kpossemato";s:4:"name";s:14:"Kyle Possemato";s:4:"mail";s:21:"kyle.Possemato@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1535041068;s:8:"modified";i:1535042020;}s:3:"raw";s:6377:"Possemato- Assignment 1a

Draft a specific aims page

Post-traumatic Stress Disorder (PTSD), depression and, alcohol use disorder (AUD) serve as major risk factors for suicide attempts among military Veterans (Lee et al., 2018). Veterans with these disorders also have low rates of initiation and completion of behavior health treatments, which exacerbates suicide risk (Gonzalez, Williams, Noël, & Lee, 2005; Hearne et al 2013). Veterans within the primary care setting are particularly prone to underutilization of care compared to Veterans seen in specialty mental health settings, with large national studies finding that less than 10% of Veterans with mental health diagnoses receive adequate doses of care (e.g., Seal et al., 2010). Even when primary care providers make appropriate referrals, patients often do follow-through on referrals due to perceived mental health stigma, a desire to be self-reliant, and VA services being perceived as non-patient centered (Possemato et al., in press). Services provided in the Primary Care Mental Health Integration (PCMHI) services increase access to behavioral health treatment and therefore have the potential to decrease suicide risk (Brawer et al, 2010; Wray et al., 2012). However, PCMHI treatment is often not evidenced-based diminishing it’s impact (e.g., Possemato et al., 2011). This line of research seeks to implement an up-stream suicide prevention service that begins with evidenced-based PCMHI treatment and then uses evidence-based strategies to connect Veterans to additional evidence-based treatment services, as needed.  

My prior work developed, tested, and conducted a pilot implementation of the Coordinated and Accelerated Pathway to Engagement (CAPE) program (Possemato et al., 2018). CAPE combines several evidence-based components to engage primary care patients in behavioral health services. First, primary care staff are trained on VA/ DOD Clinical Practice Guidelines for managing PTSD, depression, and AUD in primary care. This training engages primary care staff to discuss barriers and facilitators for further assessing patients who screen positive, providing patients with information about behavioral health treatment, and connecting patients to the PCMHI provider within the their primary care clinic. Next, patients are contacted by a PCMHI provider by phone and engage in a one-session intervention called CBT for Treatment Seeking (CBT-TS; Stecker et al., 2014). CBT-TS teaches patients to identify and restructure negative treatment seeking beliefs that serve as barriers to treatment engagement (e.g. “Only weak people seek treatment”, “I will be forced to take medications”). PCMHI providers also work collaboratively with patients in a shared decision-making process to plan for further treatment engagement. CBT-TS has been found to increase treatment utilization among Veterans with substance use disorders, PTSD, and those at risk for suicide in multiple clinical trials (e.g., Stecker et al., 2014, Gallegos et al., 2016). Following this initial phone session the PCMHI provider regularly contacts the patient for phone-based referral management (i.e. to monitor symptoms and assess for any further barriers to following through on the treatment plan).  Contact continues until the patient has engaged in the appropriate level of care.  Phone-based referral management has been previously found to increase treatment engagement among veterans with behavioral health problems (Zanjani et al., 2008).  My earlier VA QUERI-funded project implemented CAPE in one VA primary care clinic for 6 months.  Patients in the clinic where CAPE was implemented were significantly more likely to engage in psychotherapy for PTSD than those from a similar primary care clinic where CAPE was not implemented (Possemato et al., 2018).  

The next step in this line of research is to propose a VA VISN-QUERI Partnered Evaluation that would implement CAPE at 4 VISN 2 sites and expand the target of CAPE beyond PTSD to primary care Veterans with depression and AUD, as these are also major risk factors for suicide. The implementation strategy will be guided by the Replicating Effective Programs (REP) framework. We propose to randomize sites to usual or facilitated implementation arms, to understand what activities are necessary for successful CAPE implementation. 

Our specific aims are to:
1. Assess and improve the feasibility of implementing CAPE in primary care settings.
   1A. Diagnose barriers and facilitators to implementing CAPE at each site with stakeholder interview guides by the Consolidated Framework for Implementation Research (CFIR).
   1B. Adapt/ package CAPE for the local context based on information gathered in 1A.
2. Evaluate impact of implementing CAPE within the RE-AIM domains of Reach, Efficacy/ Effectiveness, Adoption, Implementation and Maintenance
3. Evaluate whether implementation outcomes differ at sites randomized to usual or facilitated implementation. 

A stakeholder group will help guide implementation through all REP phases.  Members will include VISN and local leadership in primary care and mental health, frontline primary care and PCMHI providers, and veteran primary care patients. Stakeholders will work with research staff to adapt the CAPE materials in the pre-implementation phase, provide feedback on evaluation results in the implementation phase, and provide input on sustainability strategies in the maintenance and evaluation phase.  Frontline staff will receive training on CAPE at all sites.  Facilitated sites will also pilot test CAPE with two patients before formal kick-off, receive booster training sessions and twice month technical assistance phone calls. 

This research addresses two high priorities in VA: suicide prevention and access to care (VHA, 2018). Veterans who receive effective care for behavioral health concerns have significantly reduced suicide risk (Meerwijk et al., 2016). CAPE directly increases access to initial PCMHI care, but also facilitates continued access to care and completion of evidence-base treatment. This research is an excellent fit for a VISN-QUERI Partnered Evaluation as it addresses high priority areas in VISN 2 with a program that was successfully implemented at one facility. VISN 2 leadership have expressed excitement and support for this research in initial discussions.
";s:5:"xhtml";s:6475:"Possemato- Assignment 1a<br /><br />Draft a specific aims page<br /><br />Post-traumatic Stress Disorder (PTSD), depression and, alcohol use disorder (AUD) serve as major risk factors for suicide attempts among military Veterans (Lee et al., 2018). Veterans with these disorders also have low rates of initiation and completion of behavior health treatments, which exacerbates suicide risk (Gonzalez, Williams, Noël, &amp; Lee, 2005; Hearne et al 2013). Veterans within the primary care setting are particularly prone to underutilization of care compared to Veterans seen in specialty mental health settings, with large national studies finding that less than 10% of Veterans with mental health diagnoses receive adequate doses of care (e.g., Seal et al., 2010). Even when primary care providers make appropriate referrals, patients often do follow-through on referrals due to perceived mental health stigma, a desire to be self-reliant, and VA services being perceived as non-patient centered (Possemato et al., in press). Services provided in the Primary Care Mental Health Integration (PCMHI) services increase access to behavioral health treatment and therefore have the potential to decrease suicide risk (Brawer et al, 2010; Wray et al., 2012). However, PCMHI treatment is often not evidenced-based diminishing it’s impact (e.g., Possemato et al., 2011). This line of research seeks to implement an up-stream suicide prevention service that begins with evidenced-based PCMHI treatment and then uses evidence-based strategies to connect Veterans to additional evidence-based treatment services, as needed.  <br /><br />My prior work developed, tested, and conducted a pilot implementation of the Coordinated and Accelerated Pathway to Engagement (CAPE) program (Possemato et al., 2018). CAPE combines several evidence-based components to engage primary care patients in behavioral health services. First, primary care staff are trained on VA/ DOD Clinical Practice Guidelines for managing PTSD, depression, and AUD in primary care. This training engages primary care staff to discuss barriers and facilitators for further assessing patients who screen positive, providing patients with information about behavioral health treatment, and connecting patients to the PCMHI provider within the their primary care clinic. Next, patients are contacted by a PCMHI provider by phone and engage in a one-session intervention called CBT for Treatment Seeking (CBT-TS; Stecker et al., 2014). CBT-TS teaches patients to identify and restructure negative treatment seeking beliefs that serve as barriers to treatment engagement (e.g. “Only weak people seek treatment”, “I will be forced to take medications”). PCMHI providers also work collaboratively with patients in a shared decision-making process to plan for further treatment engagement. CBT-TS has been found to increase treatment utilization among Veterans with substance use disorders, PTSD, and those at risk for suicide in multiple clinical trials (e.g., Stecker et al., 2014, Gallegos et al., 2016). Following this initial phone session the PCMHI provider regularly contacts the patient for phone-based referral management (i.e. to monitor symptoms and assess for any further barriers to following through on the treatment plan).  Contact continues until the patient has engaged in the appropriate level of care.  Phone-based referral management has been previously found to increase treatment engagement among veterans with behavioral health problems (Zanjani et al., 2008).  My earlier VA QUERI-funded project implemented CAPE in one VA primary care clinic for 6 months.  Patients in the clinic where CAPE was implemented were significantly more likely to engage in psychotherapy for PTSD than those from a similar primary care clinic where CAPE was not implemented (Possemato et al., 2018).  <br /><br />The next step in this line of research is to propose a VA VISN-QUERI Partnered Evaluation that would implement CAPE at 4 VISN 2 sites and expand the target of CAPE beyond PTSD to primary care Veterans with depression and AUD, as these are also major risk factors for suicide. The implementation strategy will be guided by the Replicating Effective Programs (REP) framework. We propose to randomize sites to usual or facilitated implementation arms, to understand what activities are necessary for successful CAPE implementation. <br /><br />Our specific aims are to:<br />1. Assess and improve the feasibility of implementing CAPE in primary care settings.<br />   1A. Diagnose barriers and facilitators to implementing CAPE at each site with stakeholder interview guides by the Consolidated Framework for Implementation Research (CFIR).<br />   1B. Adapt/ package CAPE for the local context based on information gathered in 1A.<br />2. Evaluate impact of implementing CAPE within the RE-AIM domains of Reach, Efficacy/ Effectiveness, Adoption, Implementation and Maintenance<br />3. Evaluate whether implementation outcomes differ at sites randomized to usual or facilitated implementation. <br /><br />A stakeholder group will help guide implementation through all REP phases.  Members will include VISN and local leadership in primary care and mental health, frontline primary care and PCMHI providers, and veteran primary care patients. Stakeholders will work with research staff to adapt the CAPE materials in the pre-implementation phase, provide feedback on evaluation results in the implementation phase, and provide input on sustainability strategies in the maintenance and evaluation phase.  Frontline staff will receive training on CAPE at all sites.  Facilitated sites will also pilot test CAPE with two patients before formal kick-off, receive booster training sessions and twice month technical assistance phone calls. <br /><br />This research addresses two high priorities in VA: suicide prevention and access to care (VHA, 2018). Veterans who receive effective care for behavioral health concerns have significantly reduced suicide risk (Meerwijk et al., 2016). CAPE directly increases access to initial PCMHI care, but also facilitates continued access to care and completion of evidence-base treatment. This research is an excellent fit for a VISN-QUERI Partnered Evaluation as it addresses high priority areas in VISN 2 with a program that was successfully implemented at one facility. VISN 2 leadership have expressed excitement and support for this research in initial discussions.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"aecc5ed26139e806e1c11ff0250c7c94";}s:4:"show";b:1;s:3:"cid";s:32:"6893a0ed4e8631ad051c69c0c85ab4b6";}s:32:"dce05578a18bd2f3ed8f75e464a00580";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"acarvalho";s:4:"name";s:22:"Ana Bastos de Carvalho";s:4:"mail";s:14:"aba253@uky.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535148907;}s:3:"raw";s:6810:"BASTOS DE CARVALHO - Assignment #1a

1. Draft a specific aims page of your proposed study. 

My response to this is:

Diabetes or pre-diabetes affects over 100 million adults living in the United States. Diabetic retinopathy (DR), a very common ocular complication of diabetes, is the leading cause of blindness in working-age Americans and accounts for a significant drop in quality of life and occupational productivity. Regular DR screening allows early detection and treatment, which prevents severe complications and blindness in up to 90% of DR patients. Although DR screening is indicated yearly, many diabetic patients are not appropriately tested, with rates of screening as low as 25% in vulnerable populations. Most patients with diabetes are adherent to primary care provider visits but are often noncompliant with annual retinal exams. In rural settings, where access to eye care may be challenging, equipping primary care clinics with Telemedicine DR Screening (TDRS) – an evidence-based intervention (EBI) that is efficacious for detection of DR – may bridge this gap. 

The University of Kentucky Department of Ophthalmology initiated a TDRS network in 2013, equipping 14 primary care clinics (PCCs) in Kentucky with eye fundus cameras, and ensuring TDRS services for these sites through retina specialists. Our network focused on providing this telemedicine service to populations with limited access to eye care, namely those in rural remote areas, and in urban lower socio-economic clusters. TDRS has had very high demand by PCCs, with 30 new centers requesting to join the network in the past year. However, our numbers average only 13 exams per clinic per month, indicating that despite the high demand for these services from clinic stakeholders, it has been challenging for practitioners to integrate them into daily practice, and therefore optimizing TDRS implementation would be beneficial. 
Although TDRS use has grown exponentially and telemedicine is an efficacious technology for DR detection, very few studies address strategies for integration of TDRS. 

In collaboration with a subset of six clinics in our network – both urban and rural in Kentucky–, and supported by constructs from the Consolidated Framework for Implementation Research (CFIR), we have recently identified a set of barriers and facilitators for use of TDRS*. Based on these factors, and with feedback from our Community Advisory Board, we have developed an integration strategy package* to optimize uptake of TDRS, and have preliminarily tested this packet for feasibility and acceptability*, from both patient and provider perspectives.

We now propose to conduct a pilot trial in preparation for a hybrid type II effectiveness-implementation study. In this trial, we will implement our integration strategy in a group of four PCCs, and pilot the methods to assess the efficacy of our implementation strategy, and the methods to assess effectiveness of TDRS on patient-level outcomes.

To test the above, we propose the following interconnected but independent aims:

Specific Aim 1: Pilot methods to assess efficacy of TDRS integration in PCCs on implementation outcomes.  We will test methods to assess adoption, penetration, feasibility, and fidelity of the integration package, in our four pilot sites, at six and twelve months post-implementation.

Specific Aim 2: Pilot methods to assess effectiveness of TDRS with patient-level outcomes pre-and post-integration. We will test methods to measure time-to-screening, rate of follow-up with eye care provider, time-to-follow up with eye care provider, rate of patient recurrence in yearly exams after first exam, and visual acuity.

Specific Aim 3: Pilot the administration of measures of implementation factors, guided by CFIR.**

Together, these three aims will facilitate development and testing of implementation strategies designed to improve the quality and efficacy of TDRS delivery in PCCs, as well as the assessment of patient outcomes for TDRS in a real world setting. As the number of diabetic patients in the United States is projected to double in the next 15 years, accessible DR screening is a high priority in the public health agenda and in the field of eye care. Indeed, this proposal addresses several research priorities of the National Eye Institute Program for Collaborative Clinical Research, including 1) Testing new technologies and interventions for the diagnosis of eye diseases, and/or to improve vision-related quality of life, and 2) Conducting system-level health services research to improve vision health and/or reduce health disparities.

The purpose of type II hybrid trials is to simultaneously test interventions in the clinical and implementation spheres, so as to facilitate more rapid translation from research to practice. Results from this study will be used to seek funding through the R01 mechanism to conduct a large-scale type II hybrid trial, in which we will test efficacy of our implementation strategy and effectiveness of TDRS with patient-level outcomes, in our network of 44 sites across the state. If our interventions are found to be efficient and effective, their rapid rollout could significantly improve rates of diagnosis, follow-up and early treatment of diabetic retinopathy for underserved populations. 
 
This project is significant because its results will be used to expand uptake of DR screening via telemedicine in PCCs, thereby increasing access to diagnosis of a potentially blinding eye disease in rural and in urban poor populations, addressing underserved populations with known disparities in eye care. It is also significant as it simultaneously tests methods for implementation factors and for patient outcomes, to facilitate rapid translation of TDRS into practice. The project is innovative because it focuses on optimizing DR screening outside the eye care specialist office, and in a population with high need but few services. Implementation science and hybrid effectiveness-implementation designs are also innovative in the ophthalmology healthcare field. The investigators and environment are transdisciplinary with combined expertise, networks, and a collaborative focus that maximize the chances of successful completion of the proposed aims.


*These statements were written to best simulate the wording that will be used when this proposal is submitted, but the barriers and facilitators mentioned, as well as the integration strategy package, are currently object of study by this PI and have not been established yet.

** Tentative aim. May not be possible to execute in the amount of time permitted by the R34/K23 mechanism that will be the target of this proposal, given that collection of data for Aim 2 outcomes will likely require extensive time/effort
";s:5:"xhtml";s:6969:"BASTOS DE CARVALHO - Assignment #1a<br /><br />1. Draft a specific aims page of your proposed study. <br /><br />My response to this is:<br /><br />Diabetes or pre-diabetes affects over 100 million adults living in the United States. Diabetic retinopathy (DR), a very common ocular complication of diabetes, is the leading cause of blindness in working-age Americans and accounts for a significant drop in quality of life and occupational productivity. Regular DR screening allows early detection and treatment, which prevents severe complications and blindness in up to 90% of DR patients. Although DR screening is indicated yearly, many diabetic patients are not appropriately tested, with rates of screening as low as 25% in vulnerable populations. Most patients with diabetes are adherent to primary care provider visits but are often noncompliant with annual retinal exams. In rural settings, where access to eye care may be challenging, equipping primary care clinics with Telemedicine DR Screening (TDRS) – an evidence-based intervention (EBI) that is efficacious for detection of DR – may bridge this gap. <br /><br />The University of Kentucky Department of Ophthalmology initiated a TDRS network in 2013, equipping 14 primary care clinics (PCCs) in Kentucky with eye fundus cameras, and ensuring TDRS services for these sites through retina specialists. Our network focused on providing this telemedicine service to populations with limited access to eye care, namely those in rural remote areas, and in urban lower socio-economic clusters. TDRS has had very high demand by PCCs, with 30 new centers requesting to join the network in the past year. However, our numbers average only 13 exams per clinic per month, indicating that despite the high demand for these services from clinic stakeholders, it has been challenging for practitioners to integrate them into daily practice, and therefore optimizing TDRS implementation would be beneficial. <br />Although TDRS use has grown exponentially and telemedicine is an efficacious technology for DR detection, very few studies address strategies for integration of TDRS. <br /><br />In collaboration with a subset of six clinics in our network – both urban and rural in Kentucky–, and supported by constructs from the Consolidated Framework for Implementation Research (CFIR), we have recently identified a set of barriers and facilitators for use of TDRS*. Based on these factors, and with feedback from our Community Advisory Board, we have developed an integration strategy package* to optimize uptake of TDRS, and have preliminarily tested this packet for feasibility and acceptability*, from both patient and provider perspectives.<br /><br />We now propose to conduct a pilot trial in preparation for a hybrid type II effectiveness-implementation study. In this trial, we will implement our integration strategy in a group of four PCCs, and pilot the methods to assess the efficacy of our implementation strategy, and the methods to assess effectiveness of TDRS on patient-level outcomes.<br /><br />To test the above, we propose the following interconnected but independent aims:<br /><br />Specific Aim 1: Pilot methods to assess efficacy of TDRS integration in PCCs on implementation outcomes.  We will test methods to assess adoption, penetration, feasibility, and fidelity of the integration package, in our four pilot sites, at six and twelve months post-implementation.<br /><br />Specific Aim 2: Pilot methods to assess effectiveness of TDRS with patient-level outcomes pre-and post-integration. We will test methods to measure time-to-screening, rate of follow-up with eye care provider, time-to-follow up with eye care provider, rate of patient recurrence in yearly exams after first exam, and visual acuity.<br /><br />Specific Aim 3: Pilot the administration of measures of implementation factors, guided by CFIR.**<br /><br />Together, these three aims will facilitate development and testing of implementation strategies designed to improve the quality and efficacy of TDRS delivery in PCCs, as well as the assessment of patient outcomes for TDRS in a real world setting. As the number of diabetic patients in the United States is projected to double in the next 15 years, accessible DR screening is a high priority in the public health agenda and in the field of eye care. Indeed, this proposal addresses several research priorities of the National Eye Institute Program for Collaborative Clinical Research, including 1) Testing new technologies and interventions for the diagnosis of eye diseases, and/or to improve vision-related quality of life, and 2) Conducting system-level health services research to improve vision health and/or reduce health disparities.<br /><br />The purpose of type II hybrid trials is to simultaneously test interventions in the clinical and implementation spheres, so as to facilitate more rapid translation from research to practice. Results from this study will be used to seek funding through the R01 mechanism to conduct a large-scale type II hybrid trial, in which we will test efficacy of our implementation strategy and effectiveness of TDRS with patient-level outcomes, in our network of 44 sites across the state. If our interventions are found to be efficient and effective, their rapid rollout could significantly improve rates of diagnosis, follow-up and early treatment of diabetic retinopathy for underserved populations. <br /> <br />This project is significant because its results will be used to expand uptake of DR screening via telemedicine in PCCs, thereby increasing access to diagnosis of a potentially blinding eye disease in rural and in urban poor populations, addressing underserved populations with known disparities in eye care. It is also significant as it simultaneously tests methods for implementation factors and for patient outcomes, to facilitate rapid translation of TDRS into practice. The project is innovative because it focuses on optimizing DR screening outside the eye care specialist office, and in a population with high need but few services. Implementation science and hybrid effectiveness-implementation designs are also innovative in the ophthalmology healthcare field. The investigators and environment are transdisciplinary with combined expertise, networks, and a collaborative focus that maximize the chances of successful completion of the proposed aims.<br /><br /><br />*These statements were written to best simulate the wording that will be used when this proposal is submitted, but the barriers and facilitators mentioned, as well as the integration strategy package, are currently object of study by this PI and have not been established yet.<br /><br />** Tentative aim. May not be possible to execute in the amount of time permitted by the R34/K23 mechanism that will be the target of this proposal, given that collection of data for Aim 2 outcomes will likely require extensive time/effort";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"a29620e130b978f7be38f8dfe9ced895";}s:4:"show";b:1;s:3:"cid";s:32:"dce05578a18bd2f3ed8f75e464a00580";}s:32:"0267c2cab2bfa1166615a4a3cbe089a2";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"sdang";s:4:"name";s:10:"Stuti Dang";s:4:"mail";s:17:"Stuti.Dang@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535156375;}s:3:"raw";s:9088:"Stuti Dang - Assignment # 1a

1. Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&I-focused specific aims pages, it should generally address the following 5 elements (and questions). 

My response to this is…

The Department of Veteran Affairs (VA) Home-Based Primary Care (HBPC) program provides care to over 40,000 high need high risk, medically complex and costly patients in their homes annually using an interdisciplinary care team. Data show that HBPC improves healthcare outcomes while reducing healthcare utilization and cost. The VA Office of Geriatrics and Extended Care (GEC) has made estimates which suggest that an additional 400,000 “high-need high-risk” Veterans would benefit from enrollment in HBPC care. However, the VA HBPC program does not currently have the ability to expand its capacity for more Veterans. 

The VA’s Home Telehealth (HT) program HT has been shown to improve care and outcomes in management of patients with chronic diseases. The HT program uses telehealth technology to transmit biometric and symptom data from patients to HT care coordinators who triage the data and coordinate care with primary providers. HT can potentially increase the efficiency of the program by helping target the case management efforts, to thus enhance HBPC’s capacity to manage complex patients with chronic illnesses and extend HBPC’s service area. However, protocols for effective integration of HT in HBPC need to be better defined. Besides, the specific instances (who, what, when, and how long) wherein HT monitoring enhances the management or outcomes in the already closely case managed HBPC veterans have not been determined.

Furthermore, our preliminary assessment of HT use in VA HBPC programs showed that approximately 15% of the overall HBPC patients used HT, with a wide variation in HT use by HBPC site. A national survey and semi-structured interviews of HBPC program directors revealed mixed feedback regarding the added benefit of using HT in HBPC patients, lack of clarity regarding which HBPC patients to enroll in HT, and concerns regarding poor communication and role-clarity between HT and HBPC teams. Through an HSR and D funded study, using a mixed methods approach, we have surveyed HBPC providers nationally and conducted focus groups of about ten sites to understand HBPC perspective on HT use. These data are currently being analyzed. Yet, little is known about HT staff’s perspectives on HT use in HBPC, factors that lead to better integration of the two programs, and which HBPC patients would benefit most from HT use. 

Understanding what is contributing to the significant variation of HT use in HBPC and which HBPC patients would benefit is particularly important at this time, as the VA is challenged to optimize the efficiency and effectiveness of HBPC. Health care costs are rising and the demand for HBPC is increasing, both within the VA and outside, in Medicare’s multisite HBPC replication, the Independence at Home demonstration. 

The purpose of the proposed study to identify strategies to improve effective integration of HT in HBPC programs. This study will use a stakeholder-engaged approach to identify barriers and facilitators for HT use in HBPC using the Consolidated Framework for Implementation Research (CFIR) approach, identify successful HT and HBPC practices, and identify patients that are most likely to use and benefit from HT in HBPC.  

We will work with national VA Offices of Geriatrics and Extended Care and Telehealth, the national HBPC program, and Geriatrics and Extended Care Data Analysis Center (GECDAC), and survey all HBPCs and their corresponding HT programs nationally, to define and address the following specific aims and hypotheses:
 
Specific Aim 1: Identify barriers and facilitators for HT use in HBPC using the CFIR approach: The best starting point for redesigning health care systems is to understand the current system and practices in the context of their organizational structure and setting wherein care occurs. In this study, we will gather input from HT staff, HBPC staff (we have gathered this already), leadership of both programs nationally, and facility telehealth coordinators. We will also gather data on HBPC and HT program staffs’ understanding of the other program (HBPC or HT) and other factors that may affects HBPC’s use of HT to deliver high quality care for HBPC patients.

Specific Aim 2: Identify patient characteristics associated with HT use and reductions in healthcare utilization when using HT, with respect to the following: adherence to HT, hospital admissions and days, 30-day and 90-day rehospitalizations, and ED visits. Using national HBPC and HT data from the Geriatrics and Extended Care Data Analysis Center (GECDAC), we will retrospectively analyze VA and Medicare data to answer these questions regarding which HBPC patients use and benefit from HT.   

Specific Aim 3: Identify practices among HBPC and HT programs that use HT successfully, specifically focusing on effective inter-team processes among HBPC and HT teams using qualitative data from Aim 1, and HT use and patient and program level outcomes using quantitative data from Aim 2. Understanding factors associated with successful integration will contribute to the design of best practices for implementing and dissemination of guidance for HT use in HBPC.  

Specific Aim 4: Produce a validated set of process guidelines and models for scale up and dissemination of HT integration in HBPC programs: This guidance will improve workflow and productive information sharing to streamline the case-management function between the two programs to reduce redundancy between the HBPC and HT programs. Strategies and practices associated with improved outcomes will be disseminated by developing policies and implementing system redesign initiatives to ensure that use of HT in HBPC is effective, and more uniformly integrated across all HBPC programs.

Methods: This project will follow the following phases using mixed-methods, and use an established stakeholder-engaged approach for the development and dissemination of process guidelines with the following phases corresponding to the four specific aims:  

Phase 1 (corresponding to Specific Aim 1): This will have 2 steps: Step 1: HBPC and HT team member quantitative survey about HT use and referral, and perspectives regarding integration. Step 2: This will include qualitative data collected by conducting HBPC and HT program staff interviews to examine quantitative results to augment findings by using a broad sampling frame (programs with low, middle, and high use of HT). Semi structured interviews will be conducted, recorded, transcribed and uploaded into Atlas.ti, and coded for core constructs of the CFIR to inform systematic examination of patient-, provider-, site-, and national-level barriers and facilitators, implementation strategies used, and how the implementation was adapted or tailored to meet the needs of individual sites,

Phase 2 (corresponding to Specific Aim 2). This step will be used to identify patient characteristics for successful HT use and improved outcomes. This step will include secondary data from VA and Medicare Part A to construct program level metrics on HT use in HBPC, and patient level data for adherence with HT use, comorbidities, frailty indices, Care Assessment Needs score, and healthcare utilization, etc. Data analyses will identify patients most likely to use and benefit from HT use in HBPC, specifically with regards to improved patient outcomes and reduced healthcare utilization. 

Phase 3 (corresponding to Specific Aim 3): will have 2 steps: Step 1: Using data from phase 1 and 2, as both process and outcomes measures from the HBPC sites emerge regarding HT use, these data will be integrated to identify implementation practices impacting implementation success. Step 2: Utilize the resulting set of best practices to engage and get feedback from multi-level stakeholders, including program staff, and GEC and Office of Telehealth, Veterans, and facility telehealth coordinators, to assess feasibility, utility, and validity of identified best practices, refine as needed. 

Phase 4 (corresponding to Specific Aim 4): Produce a validated set of process guidelines and models for workflow and productive information sharing to streamline the case-management function between the two programs to reduce redundancy between the HBPC and HT programs.  An implementation toolkit including a HT Assessment Tool for HBPC patients to support rapid scale-up and adoption of HT in HBPC Veterans who are most likely to benefit from it. 

The findings from this study will help identify synergies that dual enrollment in HT and HBPC can achieve on efficiency, effectiveness, and cost of delivering VA HBPC care. This will enable HT use in HBPC in an evidence-based manner and promote prudent use of resources. This may be valuable information for Medicare’s the Independence at Home demonstration as well.
 
";s:5:"xhtml";s:9269:"Stuti Dang - Assignment # 1a<br /><br />1. Draft a specific aims page (2 pages maximum) of your proposed study. Consistent with most well-written D&amp;I-focused specific aims pages, it should generally address the following 5 elements (and questions). <br /><br />My response to this is…<br /><br />The Department of Veteran Affairs (VA) Home-Based Primary Care (HBPC) program provides care to over 40,000 high need high risk, medically complex and costly patients in their homes annually using an interdisciplinary care team. Data show that HBPC improves healthcare outcomes while reducing healthcare utilization and cost. The VA Office of Geriatrics and Extended Care (GEC) has made estimates which suggest that an additional 400,000 “high-need high-risk” Veterans would benefit from enrollment in HBPC care. However, the VA HBPC program does not currently have the ability to expand its capacity for more Veterans. <br /><br />The VA’s Home Telehealth (HT) program HT has been shown to improve care and outcomes in management of patients with chronic diseases. The HT program uses telehealth technology to transmit biometric and symptom data from patients to HT care coordinators who triage the data and coordinate care with primary providers. HT can potentially increase the efficiency of the program by helping target the case management efforts, to thus enhance HBPC’s capacity to manage complex patients with chronic illnesses and extend HBPC’s service area. However, protocols for effective integration of HT in HBPC need to be better defined. Besides, the specific instances (who, what, when, and how long) wherein HT monitoring enhances the management or outcomes in the already closely case managed HBPC veterans have not been determined.<br /><br />Furthermore, our preliminary assessment of HT use in VA HBPC programs showed that approximately 15% of the overall HBPC patients used HT, with a wide variation in HT use by HBPC site. A national survey and semi-structured interviews of HBPC program directors revealed mixed feedback regarding the added benefit of using HT in HBPC patients, lack of clarity regarding which HBPC patients to enroll in HT, and concerns regarding poor communication and role-clarity between HT and HBPC teams. Through an HSR and D funded study, using a mixed methods approach, we have surveyed HBPC providers nationally and conducted focus groups of about ten sites to understand HBPC perspective on HT use. These data are currently being analyzed. Yet, little is known about HT staff’s perspectives on HT use in HBPC, factors that lead to better integration of the two programs, and which HBPC patients would benefit most from HT use. <br /><br />Understanding what is contributing to the significant variation of HT use in HBPC and which HBPC patients would benefit is particularly important at this time, as the VA is challenged to optimize the efficiency and effectiveness of HBPC. Health care costs are rising and the demand for HBPC is increasing, both within the VA and outside, in Medicare’s multisite HBPC replication, the Independence at Home demonstration. <br /><br />The purpose of the proposed study to identify strategies to improve effective integration of HT in HBPC programs. This study will use a stakeholder-engaged approach to identify barriers and facilitators for HT use in HBPC using the Consolidated Framework for Implementation Research (CFIR) approach, identify successful HT and HBPC practices, and identify patients that are most likely to use and benefit from HT in HBPC.  <br /><br />We will work with national VA Offices of Geriatrics and Extended Care and Telehealth, the national HBPC program, and Geriatrics and Extended Care Data Analysis Center (GECDAC), and survey all HBPCs and their corresponding HT programs nationally, to define and address the following specific aims and hypotheses:<br /> <br />Specific Aim 1: Identify barriers and facilitators for HT use in HBPC using the CFIR approach: The best starting point for redesigning health care systems is to understand the current system and practices in the context of their organizational structure and setting wherein care occurs. In this study, we will gather input from HT staff, HBPC staff (we have gathered this already), leadership of both programs nationally, and facility telehealth coordinators. We will also gather data on HBPC and HT program staffs’ understanding of the other program (HBPC or HT) and other factors that may affects HBPC’s use of HT to deliver high quality care for HBPC patients.<br /><br />Specific Aim 2: Identify patient characteristics associated with HT use and reductions in healthcare utilization when using HT, with respect to the following: adherence to HT, hospital admissions and days, 30-day and 90-day rehospitalizations, and ED visits. Using national HBPC and HT data from the Geriatrics and Extended Care Data Analysis Center (GECDAC), we will retrospectively analyze VA and Medicare data to answer these questions regarding which HBPC patients use and benefit from HT.   <br /><br />Specific Aim 3: Identify practices among HBPC and HT programs that use HT successfully, specifically focusing on effective inter-team processes among HBPC and HT teams using qualitative data from Aim 1, and HT use and patient and program level outcomes using quantitative data from Aim 2. Understanding factors associated with successful integration will contribute to the design of best practices for implementing and dissemination of guidance for HT use in HBPC.  <br /><br />Specific Aim 4: Produce a validated set of process guidelines and models for scale up and dissemination of HT integration in HBPC programs: This guidance will improve workflow and productive information sharing to streamline the case-management function between the two programs to reduce redundancy between the HBPC and HT programs. Strategies and practices associated with improved outcomes will be disseminated by developing policies and implementing system redesign initiatives to ensure that use of HT in HBPC is effective, and more uniformly integrated across all HBPC programs.<br /><br />Methods: This project will follow the following phases using mixed-methods, and use an established stakeholder-engaged approach for the development and dissemination of process guidelines with the following phases corresponding to the four specific aims:  <br /><br />Phase 1 (corresponding to Specific Aim 1): This will have 2 steps: Step 1: HBPC and HT team member quantitative survey about HT use and referral, and perspectives regarding integration. Step 2: This will include qualitative data collected by conducting HBPC and HT program staff interviews to examine quantitative results to augment findings by using a broad sampling frame (programs with low, middle, and high use of HT). Semi structured interviews will be conducted, recorded, transcribed and uploaded into Atlas.ti, and coded for core constructs of the CFIR to inform systematic examination of patient-, provider-, site-, and national-level barriers and facilitators, implementation strategies used, and how the implementation was adapted or tailored to meet the needs of individual sites,<br /><br />Phase 2 (corresponding to Specific Aim 2). This step will be used to identify patient characteristics for successful HT use and improved outcomes. This step will include secondary data from VA and Medicare Part A to construct program level metrics on HT use in HBPC, and patient level data for adherence with HT use, comorbidities, frailty indices, Care Assessment Needs score, and healthcare utilization, etc. Data analyses will identify patients most likely to use and benefit from HT use in HBPC, specifically with regards to improved patient outcomes and reduced healthcare utilization. <br /><br />Phase 3 (corresponding to Specific Aim 3): will have 2 steps: Step 1: Using data from phase 1 and 2, as both process and outcomes measures from the HBPC sites emerge regarding HT use, these data will be integrated to identify implementation practices impacting implementation success. Step 2: Utilize the resulting set of best practices to engage and get feedback from multi-level stakeholders, including program staff, and GEC and Office of Telehealth, Veterans, and facility telehealth coordinators, to assess feasibility, utility, and validity of identified best practices, refine as needed. <br /><br />Phase 4 (corresponding to Specific Aim 4): Produce a validated set of process guidelines and models for workflow and productive information sharing to streamline the case-management function between the two programs to reduce redundancy between the HBPC and HT programs.  An implementation toolkit including a HT Assessment Tool for HBPC patients to support rapid scale-up and adoption of HT in HBPC Veterans who are most likely to benefit from it. <br /><br />The findings from this study will help identify synergies that dual enrollment in HT and HBPC can achieve on efficiency, effectiveness, and cost of delivering VA HBPC care. This will enable HT use in HBPC in an evidence-based manner and promote prudent use of resources. This may be valuable information for Medicare’s the Independence at Home demonstration as well.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"0267c2cab2bfa1166615a4a3cbe089a2";}s:32:"270368acb153e2d1045c861f5cf2bb7a";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bweiner";s:4:"name";s:12:"Bryan Weiner";s:4:"mail";s:15:"bjweiner@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535381047;}s:3:"raw";s:1213:"Hi Emily. This sounds like an interesting project. You've identified an important gap: patients decline referrals to collaborative care and some subset who do initially engage in treatment are lost to follow-up. Do you have any data on the magnitude/scale of the gap either in general or in the specific settings in which you will implement the app? Once you've documented the gap, the next step is to identify/discuss the barriers (determinants) contributing to that gap. Do you have any data (or ideas) about the reasons why patients decline referrals or get lost to follow-up? This is important because the implementation strategy that you deploy and evaluate (i.e., the app) should be designed to address those barriers/determinants. In other words, there should be a good fit between the identified problem and the proposed solution. It was hard to assess from the draft aims why you selected an app as a proposed solution and how the app would address the identified problems. Could you make this more explicit? 

A related question:  why a stepped wedge? This design seems to be popular, but it's really tricky to pull off correctly and there are real tradeoffs in using it versus a standard 2-group RCT.  ";s:5:"xhtml";s:1236:"Hi Emily. This sounds like an interesting project. You&#039;ve identified an important gap: patients decline referrals to collaborative care and some subset who do initially engage in treatment are lost to follow-up. Do you have any data on the magnitude/scale of the gap either in general or in the specific settings in which you will implement the app? Once you&#039;ve documented the gap, the next step is to identify/discuss the barriers (determinants) contributing to that gap. Do you have any data (or ideas) about the reasons why patients decline referrals or get lost to follow-up? This is important because the implementation strategy that you deploy and evaluate (i.e., the app) should be designed to address those barriers/determinants. In other words, there should be a good fit between the identified problem and the proposed solution. It was hard to assess from the draft aims why you selected an app as a proposed solution and how the app would address the identified problems. Could you make this more explicit? <br /><br />A related question:  why a stepped wedge? This design seems to be popular, but it&#039;s really tricky to pull off correctly and there are real tradeoffs in using it versus a standard 2-group RCT.";s:6:"parent";s:32:"2bff6b7983ba0e72f95b8c24c4d81cc9";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"270368acb153e2d1045c861f5cf2bb7a";}s:32:"aecc5ed26139e806e1c11ff0250c7c94";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bweiner";s:4:"name";s:12:"Bryan Weiner";s:4:"mail";s:15:"bjweiner@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535382475;}s:3:"raw";s:1518:"Kyle, this sounds like an interesting project. It sounds like you want to do two things: (1) expand the scope of CAPE to include depression and AUD, and (2) implement CAPE in four settings. You might want to consider doing this work in two phases, starting with implementing CAPE in four settings and, once it's up and running, expanding the scope to include depression and AUD. If you expand the scope, you probably want to check its effectiveness for depression and AUD since this goes beyond the effectiveness study in the one primary care clinic where you've deployed it. It's possible to do both (1) and (2) in the same study; it would probably be a hybrid implementation-effectiveness study. However, it might be easier (and less resource intensive) to phase this work as suggested above. You might also consider using one of the implementation strategy reporting standards to describe the CAPE (e.g., STARI or Enola Proctor's strategy description article). To adapt CAPE, you will want to identify the core components and the adaptable components. A clear strategy description is a good first step in this direction, followed by a "program theory" or "theory of change" that explains how and why the strategy works. This way you can protect against adaptations that undermine the efficacy/effectiveness of the strategy. I suspect that assessment of barriers and facilitators using CFIR will suggest the need for additional implementation strategies (in addition to, or perhaps instead of, adaptations to CAPE). ";s:5:"xhtml";s:1557:"Kyle, this sounds like an interesting project. It sounds like you want to do two things: (1) expand the scope of CAPE to include depression and AUD, and (2) implement CAPE in four settings. You might want to consider doing this work in two phases, starting with implementing CAPE in four settings and, once it&#039;s up and running, expanding the scope to include depression and AUD. If you expand the scope, you probably want to check its effectiveness for depression and AUD since this goes beyond the effectiveness study in the one primary care clinic where you&#039;ve deployed it. It&#039;s possible to do both (1) and (2) in the same study; it would probably be a hybrid implementation-effectiveness study. However, it might be easier (and less resource intensive) to phase this work as suggested above. You might also consider using one of the implementation strategy reporting standards to describe the CAPE (e.g., STARI or Enola Proctor&#039;s strategy description article). To adapt CAPE, you will want to identify the core components and the adaptable components. A clear strategy description is a good first step in this direction, followed by a &quot;program theory&quot; or &quot;theory of change&quot; that explains how and why the strategy works. This way you can protect against adaptations that undermine the efficacy/effectiveness of the strategy. I suspect that assessment of barriers and facilitators using CFIR will suggest the need for additional implementation strategies (in addition to, or perhaps instead of, adaptations to CAPE).";s:6:"parent";s:32:"6893a0ed4e8631ad051c69c0c85ab4b6";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"aecc5ed26139e806e1c11ff0250c7c94";}s:32:"0bdfcbe7af4cc1d4d2a565b5e2ca35a1";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"mengelgau";s:4:"name";s:13:"Mike Engelgau";s:4:"mail";s:24:"michael.engelgau@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535729934;}s:3:"raw";s:2019:"Thanks Reshma. Nice to me you. 
Very nice proposal and you are off to a great start. I really enjoyed reviewing it and this is obviously a very important human development/social determinant issue. Also, thanks for grounding this nicely with D&I frameworks that are appropriate for this research.  You have nicely described the problem and documented it well. 

My main comments are around:
•	the intervention venue (pediatric clinic office)  
•	targeting the high risk population – those with HH income of <200% the poverty level. 

What would be very helpful is more consideration about the use of the pediatric clinic during well child visits. You mention the social worker model which seems to be very expensive and it is assumed that this happens at some other venue – maybe a bit more on the reasons this approach is so expensive. Also, even with expense – did it work? 

The clinic approach, however, is very useful in the sense that it is an “opportunistic” encounter (one that is happening anyway for other reasons). This makes it very efficient in that you don’t have to find the at-risk population. However, my concern would be whether the high risk families are getting their well child visits. It seems the highest risk would not be attending clinic regularly or at all. 

Your work on the current intervention may inform this questions. The clinic, for this to be sustainable, would need some type of remuneration for staff time/space etc. It is not clear is they would have the space available. 

The use of mixed methods will be very important to understanding barriers and facilitators. You will also want to make sure that you get qualitative and quantitative information from all the key stakeholders: families, providers, payers, health system, the social workers, etc. who will be engaged and affected, even remotely, by the efforts. Very important stakeholders will be the health system and the current infrastructure working with this population – e.g. Medicaid and CHIP, etc. 
";s:5:"xhtml";s:2089:"Thanks Reshma. Nice to me you. <br />Very nice proposal and you are off to a great start. I really enjoyed reviewing it and this is obviously a very important human development/social determinant issue. Also, thanks for grounding this nicely with D&amp;I frameworks that are appropriate for this research.  You have nicely described the problem and documented it well. <br /><br />My main comments are around:<br />•	the intervention venue (pediatric clinic office)  <br />•	targeting the high risk population – those with HH income of &lt;200% the poverty level. <br /><br />What would be very helpful is more consideration about the use of the pediatric clinic during well child visits. You mention the social worker model which seems to be very expensive and it is assumed that this happens at some other venue – maybe a bit more on the reasons this approach is so expensive. Also, even with expense – did it work? <br /><br />The clinic approach, however, is very useful in the sense that it is an “opportunistic” encounter (one that is happening anyway for other reasons). This makes it very efficient in that you don’t have to find the at-risk population. However, my concern would be whether the high risk families are getting their well child visits. It seems the highest risk would not be attending clinic regularly or at all. <br /><br />Your work on the current intervention may inform this questions. The clinic, for this to be sustainable, would need some type of remuneration for staff time/space etc. It is not clear is they would have the space available. <br /><br />The use of mixed methods will be very important to understanding barriers and facilitators. You will also want to make sure that you get qualitative and quantitative information from all the key stakeholders: families, providers, payers, health system, the social workers, etc. who will be engaged and affected, even remotely, by the efforts. Very important stakeholders will be the health system and the current infrastructure working with this population – e.g. Medicaid and CHIP, etc.";s:6:"parent";s:32:"9e8c03b0c2ec4c4d9375940324ecb94f";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"0bdfcbe7af4cc1d4d2a565b5e2ca35a1";}s:32:"a29620e130b978f7be38f8dfe9ced895";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"mengelgau";s:4:"name";s:13:"Mike Engelgau";s:4:"mail";s:24:"michael.engelgau@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1535736364;}s:3:"raw";s:1596:"Thanks Bastos. Nice to meet you. 
Thanks for your nice proposal. Obviously, this is an important care element for people with diabetes and is highly effective to prevent DM related vision loss. One comment on a point in your rationale for this study – you make the point that there is “high demand” for this service. This not quite the point – there is “high need” and possibly not enough demand (because it is not being used). 

You have correctly selected a hybrid type 2 study design. This should work well for your study. 

It will be very important to understand all the stakeholders from which you should get your qualitative and qualitative data. This should include the health system, the payer, provider, patient (and maybe even families – if there are barriers like getting transport, etc.). Also there may be a photo reader than needs to be considered. Finally, must also consider the referral system for those who need photocoagulation/treatment for retinopathy. Far too often screening programs do not focus on referral treatment for those who are screen positive. 

In your pilot, I would suggest more frequent assessment than 6 and 12 months. This would allow you to get feedback on what is happening and potentially adapt the intervention if you have unforeseen barriers. Also, when you do your 6 and 12 month f/u – consider collecting more that process counts. Consider collecting some information from users and non-user and why they did or did not use the system. 

Finally, need to think  about this is integrating with over diabetes clinic and primary care. 
";s:5:"xhtml";s:1639:"Thanks Bastos. Nice to meet you. <br />Thanks for your nice proposal. Obviously, this is an important care element for people with diabetes and is highly effective to prevent DM related vision loss. One comment on a point in your rationale for this study – you make the point that there is “high demand” for this service. This not quite the point – there is “high need” and possibly not enough demand (because it is not being used). <br /><br />You have correctly selected a hybrid type 2 study design. This should work well for your study. <br /><br />It will be very important to understand all the stakeholders from which you should get your qualitative and qualitative data. This should include the health system, the payer, provider, patient (and maybe even families – if there are barriers like getting transport, etc.). Also there may be a photo reader than needs to be considered. Finally, must also consider the referral system for those who need photocoagulation/treatment for retinopathy. Far too often screening programs do not focus on referral treatment for those who are screen positive. <br /><br />In your pilot, I would suggest more frequent assessment than 6 and 12 months. This would allow you to get feedback on what is happening and potentially adapt the intervention if you have unforeseen barriers. Also, when you do your 6 and 12 month f/u – consider collecting more that process counts. Consider collecting some information from users and non-user and why they did or did not use the system. <br /><br />Finally, need to think  about this is integrating with over diabetes clinic and primary care.";s:6:"parent";s:32:"dce05578a18bd2f3ed8f75e464a00580";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"a29620e130b978f7be38f8dfe9ced895";}s:32:"c873b4b52b7c399dfad4132803c6b722";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"sdang";s:4:"name";s:10:"Stuti Dang";s:4:"mail";s:17:"Stuti.Dang@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536239625;}s:3:"raw";s:180:"Hi. I am looking forward to receiving feedback on my proposed project as well. I am not sure if it was overlooked by accident, or if we have just not gotten to it yet. Thank you.  ";s:5:"xhtml";s:178:"Hi. I am looking forward to receiving feedback on my proposed project as well. I am not sure if it was overlooked by accident, or if we have just not gotten to it yet. Thank you.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"078d809c5f0da1c09b7b15441bc7228e";}s:4:"show";b:1;s:3:"cid";s:32:"c873b4b52b7c399dfad4132803c6b722";}s:32:"ccd8933b530c956232f17936bcda758b";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"kgoldstein";s:4:"name";s:15:"Karen Goldstein";s:4:"mail";s:24:"karen.goldstein@duke.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536240596;}s:3:"raw";s:8232:"Goldstein - Assignment #2 
1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

Yes, we intend to monitor fidelity, but this is an area still under development. Some aspects of the intervention lend themselves to traditional fidelity assessment more easily (for example, in-person observation of group sessions), however other aspects are more challenging (for example, independent reciprocal peer interactions and peer coach to individual participant interactions). The theory supporting the role of peer support to promote behavior change and self-management hinges on the sharing of a lived experience similar to the person trying to make a change. In the authentic sharing of personal stories, suggestions for overcoming obstacles and general emotional support and encouragement, peers can bolster one another in ways that a provider giving instruction or advice cannot. However, while those conversations can be shaped and supported by general guidance, they are not scripted by design. We will provide a standardized, structured training for reciprocal peers around non-directive support and active listening that can be observed directly and communication patterns will be monitored during group sessions. However, while reciprocal peer pairs will be provided example language for their individual conversations, the communication is not proscribed by design to allow for authentic communication. Moreover, much of the reciprocal peer partner interactions will take place independently by phone or SMS/text or potentially in person and will not be monitored directly. 

We intend to collect self-report measures about the quality of those communications including the degree to which support was non-directive using an established 17-item peer process measure(1) and the Health Care Climate Questionnaire(2); these measures includes questions on availability of peer, assistance with goal setting, emotional support provided and perceptions of peer interaction. We will also use a measure that assesses to what extent support received was directive vs non-directive(3). We have also explored ways to record reciprocal peer partner conversations but have found this to be not feasible and have been concerned that the recording of these conversations would alter the intent and benefit of the peer interaction. Other possibilities for monitoring fidelity include capturing objective measures of the number and timing of communication between peer partners to see if they are completing the communications as designed. Technical and logistical difficulties have arisen that will make this challenging to do directly, but are still under exploration.

Similar issues have arisen around monitoring the fidelity of peer coach to individual study participant communication and will be approached in a similar way. In addition, coach interactions with participants will be monitored during group sessions and potentially during initial phone interactions with participants. Coaches will be selected based on being at risk for cardiovascular disease and having made a significant and positive change in their diet and/or exercise routine over the last 3-6 months. Because this later criteria is difficulty to objectively verify, we will rely on self-report. Consistency across coaches around the degree to which they meet this criteria will likely vary. Again, the natural variation and authenticity of those providing peer support is desirable and the essence of the value of peer support; however, it also raises challenges for fidelity assessment. We are still exploring how best to handle this issue.

There is some evidence supporting aspects of peer support though limited in direct connection to individual outcomes. Specifically, there is evidence supporting key aspects of the training for those providing peer support and the support needed for ongoing success of peer coaches (e.g. access to program leadership to ask questions, ongoing support with other coaches to hone skills). In addition, there is more general guidance around frequency of peer contact (e.g. weekly) though recognizing different people will have different needs.

2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

Certainly adaptations will need to be made. Again, because peer support hinges on the genuineness of each individual, there is built in variation. One particular adaptation that we are purposefully included is the addition of peer coaches to reciprocal peer support partners. Each of these approaches to peer support has an existing evidence base and inherent weaknesses. For example, in two of Michele Heisler’s prior RCTs of reciprocal peer support among previously hospitalized heart failure patients (4, 5) and adults with intractable depression (6), the peer support arm did no better than enhanced usual care. Because engagement was very low in both null trials, we concluded that for very ill individuals and for those with depression in which activation is low, models in which trained peers reach out to participants (7-10) would be more effective than mutual peer support models. Thus, we are testing the adaptation of combining the two models of peer support so that for those with low engagement, peer coaches will be able to reach out and encourage the less activated participants into the program. Prior to work with peer coaches supports it as an approach to connecting with the hardly reached(11). 

In addition, for a larger type 1 hybrid trial, we plan to use at least two sites. We anticipate variation in each site will require adaptation to location specific context. We will track adaptations as outlined by Stirman (2013) at that time.

1.	Progress Pf. Peers for Progress Program Development Guide: A comprehensive, flexible approach. In: Foundation ApotAAoFP, editor. 2015.
2.	Williams GC, Deci EL. Activating patients for smoking cessation through physician autonomy support. Medical care. 2001;39(8):813-23.
3.	Fisher EB, Earp JA, Maman S, Zolotor A. Cross-cultural and international adaptation of peer support for diabetes management. Family practice. 2010;27 Suppl 1:i6-16.
4.	Heisler M, Halasyamani L, Cowen ME, Davis MD, Resnicow K, Strawderman RL, et al. Randomized controlled effectiveness trial of reciprocal peer support in heart failure. Circulation Heart failure. 2013;6(2):246-53.
5.	Lockhart E, Foreman J, Mase R, Heisler M. Heart failure patients' experiences of a self-management peer support program: a qualitative study. Heart & lung : the journal of critical care. 2014;43(4):292-8.
6.	Valenstein M, Pfeiffer PN, Brandfon S, Walters H, Ganoczy D, Kim HM, et al. Augmenting Ongoing Depression Care With a Mutual Peer Support Intervention Versus Self-Help Materials Alone: A Randomized Trial. Psychiatric services (Washington, DC). 2016;67(2):236-9.
7.	Tang TS, Funnell MM, Gillard M, Nwankwo R, Heisler M. Training peers to provide ongoing diabetes self-management support (DSMS): results from a pilot study. Patient Educ Couns. 2011;85(2):160-8.
8.	Tang TS, Funnell MM, Gillard M, Nwankwo R, Heisler M. The development of a pilot training program for peer leaders in diabetes: process and content. The Diabetes educator. 2011;37(1):67-77.
9.	Tang TS, Funnell MM, Sinco B, Spencer MS, Heisler M. Peer-Led, Empowerment-Based Approach to Self-Management Efforts in Diabetes (PLEASED): A Randomized Controlled Trial in an African American Community. Annals of family medicine. 2015;13 Suppl 1:S27-35.
10.	Tang TS, Funnell M, Sinco B, Piatt G, Palmisano G, Spencer MS, et al. Comparative effectiveness of peer leaders and community health workers in diabetes self-management support: results of a randomized controlled trial. Diabetes care. 2014;37(6):1525-34.
11.	Sokol R, Fisher E. Peer Support for the Hardly Reached: A Systematic Review. Am J Public Health. 2016;106(7):1308.

";s:5:"xhtml";s:8374:"Goldstein - Assignment #2 <br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />Yes, we intend to monitor fidelity, but this is an area still under development. Some aspects of the intervention lend themselves to traditional fidelity assessment more easily (for example, in-person observation of group sessions), however other aspects are more challenging (for example, independent reciprocal peer interactions and peer coach to individual participant interactions). The theory supporting the role of peer support to promote behavior change and self-management hinges on the sharing of a lived experience similar to the person trying to make a change. In the authentic sharing of personal stories, suggestions for overcoming obstacles and general emotional support and encouragement, peers can bolster one another in ways that a provider giving instruction or advice cannot. However, while those conversations can be shaped and supported by general guidance, they are not scripted by design. We will provide a standardized, structured training for reciprocal peers around non-directive support and active listening that can be observed directly and communication patterns will be monitored during group sessions. However, while reciprocal peer pairs will be provided example language for their individual conversations, the communication is not proscribed by design to allow for authentic communication. Moreover, much of the reciprocal peer partner interactions will take place independently by phone or SMS/text or potentially in person and will not be monitored directly. <br /><br />We intend to collect self-report measures about the quality of those communications including the degree to which support was non-directive using an established 17-item peer process measure(1) and the Health Care Climate Questionnaire(2); these measures includes questions on availability of peer, assistance with goal setting, emotional support provided and perceptions of peer interaction. We will also use a measure that assesses to what extent support received was directive vs non-directive(3). We have also explored ways to record reciprocal peer partner conversations but have found this to be not feasible and have been concerned that the recording of these conversations would alter the intent and benefit of the peer interaction. Other possibilities for monitoring fidelity include capturing objective measures of the number and timing of communication between peer partners to see if they are completing the communications as designed. Technical and logistical difficulties have arisen that will make this challenging to do directly, but are still under exploration.<br /><br />Similar issues have arisen around monitoring the fidelity of peer coach to individual study participant communication and will be approached in a similar way. In addition, coach interactions with participants will be monitored during group sessions and potentially during initial phone interactions with participants. Coaches will be selected based on being at risk for cardiovascular disease and having made a significant and positive change in their diet and/or exercise routine over the last 3-6 months. Because this later criteria is difficulty to objectively verify, we will rely on self-report. Consistency across coaches around the degree to which they meet this criteria will likely vary. Again, the natural variation and authenticity of those providing peer support is desirable and the essence of the value of peer support; however, it also raises challenges for fidelity assessment. We are still exploring how best to handle this issue.<br /><br />There is some evidence supporting aspects of peer support though limited in direct connection to individual outcomes. Specifically, there is evidence supporting key aspects of the training for those providing peer support and the support needed for ongoing success of peer coaches (e.g. access to program leadership to ask questions, ongoing support with other coaches to hone skills). In addition, there is more general guidance around frequency of peer contact (e.g. weekly) though recognizing different people will have different needs.<br /><br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />Certainly adaptations will need to be made. Again, because peer support hinges on the genuineness of each individual, there is built in variation. One particular adaptation that we are purposefully included is the addition of peer coaches to reciprocal peer support partners. Each of these approaches to peer support has an existing evidence base and inherent weaknesses. For example, in two of Michele Heisler’s prior RCTs of reciprocal peer support among previously hospitalized heart failure patients (4, 5) and adults with intractable depression (6), the peer support arm did no better than enhanced usual care. Because engagement was very low in both null trials, we concluded that for very ill individuals and for those with depression in which activation is low, models in which trained peers reach out to participants (7-10) would be more effective than mutual peer support models. Thus, we are testing the adaptation of combining the two models of peer support so that for those with low engagement, peer coaches will be able to reach out and encourage the less activated participants into the program. Prior to work with peer coaches supports it as an approach to connecting with the hardly reached(11). <br /><br />In addition, for a larger type 1 hybrid trial, we plan to use at least two sites. We anticipate variation in each site will require adaptation to location specific context. We will track adaptations as outlined by Stirman (2013) at that time.<br /><br />1.	Progress Pf. Peers for Progress Program Development Guide: A comprehensive, flexible approach. In: Foundation ApotAAoFP, editor. 2015.<br />2.	Williams GC, Deci EL. Activating patients for smoking cessation through physician autonomy support. Medical care. 2001;39(8):813-23.<br />3.	Fisher EB, Earp JA, Maman S, Zolotor A. Cross-cultural and international adaptation of peer support for diabetes management. Family practice. 2010;27 Suppl 1:i6-16.<br />4.	Heisler M, Halasyamani L, Cowen ME, Davis MD, Resnicow K, Strawderman RL, et al. Randomized controlled effectiveness trial of reciprocal peer support in heart failure. Circulation Heart failure. 2013;6(2):246-53.<br />5.	Lockhart E, Foreman J, Mase R, Heisler M. Heart failure patients&#039; experiences of a self-management peer support program: a qualitative study. Heart &amp; lung : the journal of critical care. 2014;43(4):292-8.<br />6.	Valenstein M, Pfeiffer PN, Brandfon S, Walters H, Ganoczy D, Kim HM, et al. Augmenting Ongoing Depression Care With a Mutual Peer Support Intervention Versus Self-Help Materials Alone: A Randomized Trial. Psychiatric services (Washington, DC). 2016;67(2):236-9.<br />7.	Tang TS, Funnell MM, Gillard M, Nwankwo R, Heisler M. Training peers to provide ongoing diabetes self-management support (DSMS): results from a pilot study. Patient Educ Couns. 2011;85(2):160-8.<br />8.	Tang TS, Funnell MM, Gillard M, Nwankwo R, Heisler M. The development of a pilot training program for peer leaders in diabetes: process and content. The Diabetes educator. 2011;37(1):67-77.<br />9.	Tang TS, Funnell MM, Sinco B, Spencer MS, Heisler M. Peer-Led, Empowerment-Based Approach to Self-Management Efforts in Diabetes (PLEASED): A Randomized Controlled Trial in an African American Community. Annals of family medicine. 2015;13 Suppl 1:S27-35.<br />10.	Tang TS, Funnell M, Sinco B, Piatt G, Palmisano G, Spencer MS, et al. Comparative effectiveness of peer leaders and community health workers in diabetes self-management support: results of a randomized controlled trial. Diabetes care. 2014;37(6):1525-34.<br />11.	Sokol R, Fisher E. Peer Support for the Hardly Reached: A Systematic Review. Am J Public Health. 2016;106(7):1308.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"026114c7653c821aff5964a10a569de6";}s:4:"show";b:1;s:3:"cid";s:32:"ccd8933b530c956232f17936bcda758b";}s:32:"078d809c5f0da1c09b7b15441bc7228e";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"dbdemner";s:4:"name";s:20:"Dara Blachman-Demner";s:4:"mail";s:28:"dara.blachman-demner@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536255517;}s:3:"raw";s:1917:"Stuti,

I apologize for the delay in receiving feedback. I have reviewed your proposal and provide comments below. You should receive additional feedback from your facilitators here and/or your upcoming calls. 

Thank you for your thoughtful proposal on a topic of clear public health significance- I appreciate your use of a stakeholder engagement process and the relation back to the CFIR model (more on this will be explored in the models and frameworks lesson). A few comments/questions to consider at this early phase:

1) There may be some confusion of terms- in referring to "best practices" it seems that means implementation strategies that are associated with the patient outcomes you note. Is this correct? 
2) Relatedly, I wonder if you have any a priori ideas about what those strategies might be given your prior work? There seems to be an assumption that this integration will in fact lead to all of the positive outcomes you note, but its unclear to me if there is any patient data to support this assumption or if that is what you are attempting to do in this study. Related to that, the analytic strategy or approach for integrating Aim 1 and Aim 2 data is unclear. 
3) You provide a wonderful justification for the project and the potential public health impact, but I wonder if this might be a little ambitious to assume that from this study you will have fully validated guidelines that can be disseminated and scaled up. It also seems that would lead to yet another D&I study on how best to facilitate the dissemination of these best practices. This is not my area of expertise, so some of your faculty may be able to provide more realistic input here.
4) Unless I missed it, cost is noted as one of the key drivers but I didn't see it being explicitly measured here. 

I look forward to hearing from your facilitators how this develops- nice work and again, thank you for your patience.

Dara 
";s:5:"xhtml";s:1999:"Stuti,<br /><br />I apologize for the delay in receiving feedback. I have reviewed your proposal and provide comments below. You should receive additional feedback from your facilitators here and/or your upcoming calls. <br /><br />Thank you for your thoughtful proposal on a topic of clear public health significance- I appreciate your use of a stakeholder engagement process and the relation back to the CFIR model (more on this will be explored in the models and frameworks lesson). A few comments/questions to consider at this early phase:<br /><br />1) There may be some confusion of terms- in referring to &quot;best practices&quot; it seems that means implementation strategies that are associated with the patient outcomes you note. Is this correct? <br />2) Relatedly, I wonder if you have any a priori ideas about what those strategies might be given your prior work? There seems to be an assumption that this integration will in fact lead to all of the positive outcomes you note, but its unclear to me if there is any patient data to support this assumption or if that is what you are attempting to do in this study. Related to that, the analytic strategy or approach for integrating Aim 1 and Aim 2 data is unclear. <br />3) You provide a wonderful justification for the project and the potential public health impact, but I wonder if this might be a little ambitious to assume that from this study you will have fully validated guidelines that can be disseminated and scaled up. It also seems that would lead to yet another D&amp;I study on how best to facilitate the dissemination of these best practices. This is not my area of expertise, so some of your faculty may be able to provide more realistic input here.<br />4) Unless I missed it, cost is noted as one of the key drivers but I didn&#039;t see it being explicitly measured here. <br /><br />I look forward to hearing from your facilitators how this develops- nice work and again, thank you for your patience.<br /><br />Dara";s:6:"parent";s:32:"c873b4b52b7c399dfad4132803c6b722";s:7:"replies";a:1:{i:0;s:32:"f179ecec81f00284681ce0070b5d6953";}s:4:"show";b:1;s:3:"cid";s:32:"078d809c5f0da1c09b7b15441bc7228e";}s:32:"e80b5f59120f3dd6b851afeafc93497e";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"dbdemner";s:4:"name";s:20:"Dara Blachman-Demner";s:4:"mail";s:28:"dara.blachman-demner@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536256497;}s:3:"raw";s:1678:"Karen, 

Thank you for this concept on an interesting topic. I will provide some initial feedback below and you should hear from your facilitators shortly either here and/or on the call with additional feedback. Thank you for your patience.

I thought this was a well-written initial concept with clearly laid out rationale, identification of the EBP, the D&I gap and the proposed study. I especially thought the focus on gender differences is innovative and perhaps uniquely relevant given the focus of your intervention on peer support. A few things to consider:
1) Do you have any additional information on the geographic differences between the two sites and how you expect that might impact the identification of specific barriers and/or facilitators? Do you have any reason to expect that gender may vary by site and if so how will you incorporate that? 
2) You note the final step after this would be a full implementation trial, which seems a logical next step but it was unclear if the vision for that was to focus only on women? 
3) More detail would be helpful on the types of measures and methods you are considering for use in Aim 2 (this can come under that module's assignment!). Is there a specific D&I framework or model you are considering to guide the development of the interview? 
4) A minor question, but curious about the "trained coaches." who trains them and in what? is that part of your study or have they been trained prior?
5) If you have more information about what has been learned thus far from the pilot (even if preliminary) that may guide the hybrid trial that will be helpful.

A great start- look forward to hearing how it progresses!

Dara ";s:5:"xhtml";s:1765:"Karen, <br /><br />Thank you for this concept on an interesting topic. I will provide some initial feedback below and you should hear from your facilitators shortly either here and/or on the call with additional feedback. Thank you for your patience.<br /><br />I thought this was a well-written initial concept with clearly laid out rationale, identification of the EBP, the D&amp;I gap and the proposed study. I especially thought the focus on gender differences is innovative and perhaps uniquely relevant given the focus of your intervention on peer support. A few things to consider:<br />1) Do you have any additional information on the geographic differences between the two sites and how you expect that might impact the identification of specific barriers and/or facilitators? Do you have any reason to expect that gender may vary by site and if so how will you incorporate that? <br />2) You note the final step after this would be a full implementation trial, which seems a logical next step but it was unclear if the vision for that was to focus only on women? <br />3) More detail would be helpful on the types of measures and methods you are considering for use in Aim 2 (this can come under that module&#039;s assignment!). Is there a specific D&amp;I framework or model you are considering to guide the development of the interview? <br />4) A minor question, but curious about the &quot;trained coaches.&quot; who trains them and in what? is that part of your study or have they been trained prior?<br />5) If you have more information about what has been learned thus far from the pilot (even if preliminary) that may guide the hybrid trial that will be helpful.<br /><br />A great start- look forward to hearing how it progresses!<br /><br />Dara";s:6:"parent";s:32:"a424c2a842052fdc97cf2e95e27e5e84";s:7:"replies";a:1:{i:0;s:32:"3c667965f1f89f1d7fee180c179184c6";}s:4:"show";b:1;s:3:"cid";s:32:"e80b5f59120f3dd6b851afeafc93497e";}s:32:"36aa5c5877832ef019f6130b2c8236b8";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"elattie";s:4:"name";s:12:"Emily Lattie";s:4:"mail";s:29:"emily.lattie@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536258685;}s:3:"raw";s:1661:"Lattie - Assignment #2

1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?
While there is not current concrete evidence linking level of fidelity with individual level outcomes for the proposed intervention, fidelity will be measured through a combination of clinician self-report and logs of app usage and clinician dashboard use. These logs provide a form of remote observation that does not require additional effort from the patients or clinicians. 

2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?
Yes, adaptations will need to be made and this is a main focus of the proposed study. Aspects that might need to be adapted include content changes (that is. to the mental health app to make it more appropriate for the patients seen in primary care) and contextual changes, such that the care managers have responsibilities to the patients (and to the healthcare system) beyond the delivery of an app-based mental health treatment program. To guide these adaptations, we will engage in user centered design activities and consult with the relevant stakeholders. Because technologies are evolving rapidly, and care settings are also often evolving rapidly, we will record adaptions that are made throughout the delivery of the intervention. 
";s:5:"xhtml";s:1689:"Lattie - Assignment #2<br /><br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br />While there is not current concrete evidence linking level of fidelity with individual level outcomes for the proposed intervention, fidelity will be measured through a combination of clinician self-report and logs of app usage and clinician dashboard use. These logs provide a form of remote observation that does not require additional effort from the patients or clinicians. <br /><br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br />Yes, adaptations will need to be made and this is a main focus of the proposed study. Aspects that might need to be adapted include content changes (that is. to the mental health app to make it more appropriate for the patients seen in primary care) and contextual changes, such that the care managers have responsibilities to the patients (and to the healthcare system) beyond the delivery of an app-based mental health treatment program. To guide these adaptations, we will engage in user centered design activities and consult with the relevant stakeholders. Because technologies are evolving rapidly, and care settings are also often evolving rapidly, we will record adaptions that are made throughout the delivery of the intervention.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"775a4b46441ae274a1fe78cc95aec718";}s:4:"show";b:1;s:3:"cid";s:32:"36aa5c5877832ef019f6130b2c8236b8";}s:32:"f179ecec81f00284681ce0070b5d6953";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"sdang";s:4:"name";s:10:"Stuti Dang";s:4:"mail";s:17:"Stuti.Dang@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536259274;}s:3:"raw";s:219:"No worries about the delay. I really appreciate your input. I kinda thought all others were done, and perhaps mine might have been overlooked. I know I have a lot of fixing to do, and feedback will be really valuable. 
";s:5:"xhtml";s:217:"No worries about the delay. I really appreciate your input. I kinda thought all others were done, and perhaps mine might have been overlooked. I know I have a lot of fixing to do, and feedback will be really valuable.";s:6:"parent";s:32:"078d809c5f0da1c09b7b15441bc7228e";s:7:"replies";a:1:{i:0;s:32:"becbe50f137b957a144ee8063cd58572";}s:4:"show";b:1;s:3:"cid";s:32:"f179ecec81f00284681ce0070b5d6953";}s:32:"3b26db557da1274a97c75e143cbacac2";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"rshah";s:4:"name";s:11:"Reshma Shah";s:4:"mail";s:16:"reshmamd@UIC.EDU";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536265983;}s:3:"raw";s:4070:"Shah- Assignment 2
1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?
2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

My response to this is:
We will be measuring fidelity to Sit Down and Play (SDP), a brief, theory-based program delivered during pediatric well-child visits that aims to increase key evidence-based parenting behaviors that support early childhood development. Administrators of SDP receive training in a group format and we utilize basic principles of active learning (e.g., demonstrations followed by discussions and role-playing) to provide content information regarding the importance of play and parent-child verbal communication for child development. Skills are also taught in the following key domains: 1) clear communication techniques (e.g., describing the importance of daily play interactions on a child’s development with minimal jargon, use of toys to facilitate parent-child interactions); 2) modeling of targeted behaviors; and 3) providing feedback with positive reinforcement. An SDP Fidelity Checklist was adapted from the Chicago Parent Program Fidelity Checklist,  a previously published measure that assessed administrator adherence and competence delivering a parenting program serving low-income families in Chicago (Breitenstein et al., 2010). The SDP Fidelity Checklist assesses SDP administrator competencies in each of the three key domains and a threshold score of 85% is required to verify each SDP administrator’s acquisition of skills prior to delivery in a clinical setting. After passing this initial fidelity checklist, each of the administrators of SDP subsequently audio record themselves delivering the program at scheduled intervals (every 4 weeks). The SDP Fidelity Checklist is completed by the following three individuals after each of the recordings: 1) the administrator who was recorded 2) another trained administrator of SDP and 3) the creator of the program. Individuals who do not score 85% are asked to review the content of the program and undergo repeat fidelity training prior to further administration. As this is a Type 1 hybrid-implementation effectiveness trial the data collected will provide evidence regarding the level of fidelity to the program and individual level parenting behavior outcomes.

Sit Down and Play will almost certainly need to be adapted. Indeed, one aspect that is currently being adapted is in the context of the program, specifically as it relates to personnel. The program was initially delivered by hired research assistants with graduate-level training in psychology. However, to promote sustainability, we are also recruiting volunteer undergraduate and graduate-level students who will undergo the same training as described above. We will be able to measure this adaptation by tracking key parenting behavioral outcomes and assessing whether changes in parenting behavior varied depending administrator characteristics (e.g., education, volunteer versus paid). Other factors that may need to be modified include the content of the material as well as the population that it serves. Currently, the program is brief and intended to be universally delivered. It may be that the program may need to be adjusted to be delivered to a more select group of parents (e.g., parents of infants with prematurity versus only healthy term infants). These adaptations will be considered based upon whether there was a differential uptake/change in parenting behavior based upon sociodemographic factors. As noted, the program is brief and depending upon evaluation of outcomes, it may be that SDP may also need to be further revised to be more intensive. 


";s:5:"xhtml";s:4101:"Shah- Assignment 2<br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />My response to this is:<br />We will be measuring fidelity to Sit Down and Play (SDP), a brief, theory-based program delivered during pediatric well-child visits that aims to increase key evidence-based parenting behaviors that support early childhood development. Administrators of SDP receive training in a group format and we utilize basic principles of active learning (e.g., demonstrations followed by discussions and role-playing) to provide content information regarding the importance of play and parent-child verbal communication for child development. Skills are also taught in the following key domains: 1) clear communication techniques (e.g., describing the importance of daily play interactions on a child’s development with minimal jargon, use of toys to facilitate parent-child interactions); 2) modeling of targeted behaviors; and 3) providing feedback with positive reinforcement. An SDP Fidelity Checklist was adapted from the Chicago Parent Program Fidelity Checklist,  a previously published measure that assessed administrator adherence and competence delivering a parenting program serving low-income families in Chicago (Breitenstein et al., 2010). The SDP Fidelity Checklist assesses SDP administrator competencies in each of the three key domains and a threshold score of 85% is required to verify each SDP administrator’s acquisition of skills prior to delivery in a clinical setting. After passing this initial fidelity checklist, each of the administrators of SDP subsequently audio record themselves delivering the program at scheduled intervals (every 4 weeks). The SDP Fidelity Checklist is completed by the following three individuals after each of the recordings: 1) the administrator who was recorded 2) another trained administrator of SDP and 3) the creator of the program. Individuals who do not score 85% are asked to review the content of the program and undergo repeat fidelity training prior to further administration. As this is a Type 1 hybrid-implementation effectiveness trial the data collected will provide evidence regarding the level of fidelity to the program and individual level parenting behavior outcomes.<br /><br />Sit Down and Play will almost certainly need to be adapted. Indeed, one aspect that is currently being adapted is in the context of the program, specifically as it relates to personnel. The program was initially delivered by hired research assistants with graduate-level training in psychology. However, to promote sustainability, we are also recruiting volunteer undergraduate and graduate-level students who will undergo the same training as described above. We will be able to measure this adaptation by tracking key parenting behavioral outcomes and assessing whether changes in parenting behavior varied depending administrator characteristics (e.g., education, volunteer versus paid). Other factors that may need to be modified include the content of the material as well as the population that it serves. Currently, the program is brief and intended to be universally delivered. It may be that the program may need to be adjusted to be delivered to a more select group of parents (e.g., parents of infants with prematurity versus only healthy term infants). These adaptations will be considered based upon whether there was a differential uptake/change in parenting behavior based upon sociodemographic factors. As noted, the program is brief and depending upon evaluation of outcomes, it may be that SDP may also need to be further revised to be more intensive.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"f5664b06f60406584be34c9971dedbec";}s:4:"show";b:1;s:3:"cid";s:32:"3b26db557da1274a97c75e143cbacac2";}s:32:"a2d677a3927da8b841640f9cd3e7b8f5";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"rshah";s:4:"name";s:11:"Reshma Shah";s:4:"mail";s:16:"reshmamd@UIC.EDU";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536266020;}s:3:"raw";s:77:"HI All,

Are we supposed to submit the power point somewhere?

Thanks!
Reshma";s:5:"xhtml";s:102:"HI All,<br /><br />Are we supposed to submit the power point somewhere?<br /><br />Thanks!<br />Reshma";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"e0fa39ada2aeed68dfb20ab6e15cfca6";}s:4:"show";b:1;s:3:"cid";s:32:"a2d677a3927da8b841640f9cd3e7b8f5";}s:32:"becbe50f137b957a144ee8063cd58572";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"chelfrich";s:4:"name";s:18:"Christian Helfrich";s:4:"mail";s:15:"helfrich@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536280043;}s:3:"raw";s:1678:"Hi Stuti, it's a pleasure to meet you and I'm so sorry for the delay in getting you feedback! Apparently when I posted it Friday I didn't save it. Here are my initial thoughts:

This is a very interesting topic. Can you say more about the need protocols for effective integration of home telehealth in HBPC? Have other health care systems used home telehealth? And if so, do we know anything about integration of home telehealth in those systems?
In terms of the question of who, what, when, and how long home telehealth monitoring enhances care management or outcomes, how do you mean? I’m trying to reconcile that with the statement that the home telehealth program has been shown to improve care and outcomes. It would be helpful to know more about this gap in our understanding of the effectiveness of home telehealth. And similarly, it would be helpful to elaborate on the observation that 15% of home-based primary care patients receive home telehealth. What percentage should be getting it? Or is that what you’re trying to understand with aim 2?
Finally, do you have thoughts about how using CFIR will help you contribute to implementation science? Obviously you’re using it to code for implementation factors in Aim 1. But what does using CFIR do over and beyond just coding inductively for implementation barriers & facilitators? I think this will be a useful point of broader discussion when we talk next week. I think we often (all of us) use conceptual models without thinking about why we’re using them and what purpose they’re serving in our research (other than mollifying reviewers who are going to demand that you have a conceptual model).
Christian
";s:5:"xhtml";s:1721:"Hi Stuti, it&#039;s a pleasure to meet you and I&#039;m so sorry for the delay in getting you feedback! Apparently when I posted it Friday I didn&#039;t save it. Here are my initial thoughts:<br /><br />This is a very interesting topic. Can you say more about the need protocols for effective integration of home telehealth in HBPC? Have other health care systems used home telehealth? And if so, do we know anything about integration of home telehealth in those systems?<br />In terms of the question of who, what, when, and how long home telehealth monitoring enhances care management or outcomes, how do you mean? I’m trying to reconcile that with the statement that the home telehealth program has been shown to improve care and outcomes. It would be helpful to know more about this gap in our understanding of the effectiveness of home telehealth. And similarly, it would be helpful to elaborate on the observation that 15% of home-based primary care patients receive home telehealth. What percentage should be getting it? Or is that what you’re trying to understand with aim 2?<br />Finally, do you have thoughts about how using CFIR will help you contribute to implementation science? Obviously you’re using it to code for implementation factors in Aim 1. But what does using CFIR do over and beyond just coding inductively for implementation barriers &amp; facilitators? I think this will be a useful point of broader discussion when we talk next week. I think we often (all of us) use conceptual models without thinking about why we’re using them and what purpose they’re serving in our research (other than mollifying reviewers who are going to demand that you have a conceptual model).<br />Christian";s:6:"parent";s:32:"f179ecec81f00284681ce0070b5d6953";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"becbe50f137b957a144ee8063cd58572";}s:32:"3c667965f1f89f1d7fee180c179184c6";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"chelfrich";s:4:"name";s:18:"Christian Helfrich";s:4:"mail";s:15:"helfrich@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536280133;}s:3:"raw";s:948:"Hi Karen, my apologies for not getting you feedback earlier. 

Thanks for sharing this work. It would be helpful to hear more about both what we know about the use of peer support programs and the gaps in what we know. For example, what sort of integration patient aligned care teams is needed? Do primary care providers refer their patients for peer support? Are there examples of peer support programs that have been successfully operated over a number of year?  
It’s helpful to think about how your study can contribute to the broader implementation science knowledge base. Understanding the barriers and facilitators of will be important for developing the implementation trial, but isn't really a contribution to our broader understanding of implementation. Ideally, your findings help develop, test and/or refine some underlying concepts and their relationships. Have you thought about what implementation strategies you'll use?
Christian
";s:5:"xhtml";s:977:"Hi Karen, my apologies for not getting you feedback earlier. <br /><br />Thanks for sharing this work. It would be helpful to hear more about both what we know about the use of peer support programs and the gaps in what we know. For example, what sort of integration patient aligned care teams is needed? Do primary care providers refer their patients for peer support? Are there examples of peer support programs that have been successfully operated over a number of year?  <br />It’s helpful to think about how your study can contribute to the broader implementation science knowledge base. Understanding the barriers and facilitators of will be important for developing the implementation trial, but isn&#039;t really a contribution to our broader understanding of implementation. Ideally, your findings help develop, test and/or refine some underlying concepts and their relationships. Have you thought about what implementation strategies you&#039;ll use?<br />Christian";s:6:"parent";s:32:"e80b5f59120f3dd6b851afeafc93497e";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"3c667965f1f89f1d7fee180c179184c6";}s:32:"e0fa39ada2aeed68dfb20ab6e15cfca6";a:8:{s:4:"user";a:5:{s:2:"id";s:8:"dbdemner";s:4:"name";s:20:"Dara Blachman-Demner";s:4:"mail";s:28:"dara.blachman-demner@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536280613;}s:3:"raw";s:264:"Reshma,

Sorry for the confusion- we are realizing that this wasn't clear. There isn't a place to upload them so you can either email to your group or just plan to present them on the call. It's up to your faculty if they want in advance. 

Thanks and sorry,
Dara ";s:5:"xhtml";s:303:"Reshma,<br /><br />Sorry for the confusion- we are realizing that this wasn&#039;t clear. There isn&#039;t a place to upload them so you can either email to your group or just plan to present them on the call. It&#039;s up to your faculty if they want in advance. <br /><br />Thanks and sorry,<br />Dara";s:6:"parent";s:32:"a2d677a3927da8b841640f9cd3e7b8f5";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"e0fa39ada2aeed68dfb20ab6e15cfca6";}s:32:"d564ecfb22947660e8592dee59d86304";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"chelfrich";s:4:"name";s:18:"Christian Helfrich";s:4:"mail";s:15:"helfrich@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536354307;}s:3:"raw";s:1132:"Hi all, just a few thoughts as we're having the phone discussion:

1) Here's the piece I mentioned by Elliott Fisher, Steve Shortell & Lucy Savitz about about the different types of innovations we try to implement, and how the stakeholders might differ://jamanetwork.com/journals/jama/fullarticle/2484347. I think it also helps because sometimes we start getting confused about the evidence-based practice we're implementing.

2) One of the challenges we as researchers face is the curse of knowledge. As we become more and more steeped in implementation science, we come to understand what we observe in conceptual terms and we tend to present our ideas primarily or exclusively in conceptual terms, e.g., hypothesizing that leadership support & resource availability create a strong implementation climate via specific implementation policies and practices. While we know what that means, our reviewers & readers often don't, and even those that understand intellectually may struggle to understand intuitively. Starting with the concrete example, and then going to the concepts helps. BTW, Made to Stick & Switch are really good.";s:5:"xhtml";s:1188:"Hi all, just a few thoughts as we&#039;re having the phone discussion:<br /><br />1) Here&#039;s the piece I mentioned by Elliott Fisher, Steve Shortell &amp; Lucy Savitz about about the different types of innovations we try to implement, and how the stakeholders might differ://jamanetwork.com/journals/jama/fullarticle/2484347. I think it also helps because sometimes we start getting confused about the evidence-based practice we&#039;re implementing.<br /><br />2) One of the challenges we as researchers face is the curse of knowledge. As we become more and more steeped in implementation science, we come to understand what we observe in conceptual terms and we tend to present our ideas primarily or exclusively in conceptual terms, e.g., hypothesizing that leadership support &amp; resource availability create a strong implementation climate via specific implementation policies and practices. While we know what that means, our reviewers &amp; readers often don&#039;t, and even those that understand intellectually may struggle to understand intuitively. Starting with the concrete example, and then going to the concepts helps. BTW, Made to Stick &amp; Switch are really good.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"d564ecfb22947660e8592dee59d86304";}s:32:"cae125e7a6dd4a0e9b41aed4d975f8fc";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"acarvalho";s:4:"name";s:22:"Ana Bastos de Carvalho";s:4:"mail";s:14:"aba253@uky.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536427526;}s:3:"raw";s:4312:"Bastos de Carvalho- Assignment #2
My response is:

1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?

We will measure fidelity at two levels: a) the level of the integration strategies, and b) at the level of the EBI.

a) Our integration strategy consists of three main interventions: implementation of a reliable EMR reminder system, identification of a screening champion with assigned tasks/goals, and reorganization of workflow so that the screening exam is performed by staff other than the provider team (e.g., by the radiology department/team, or at the clinic's lab)*. We propose to measure fidelity of these strategies through self-reporting (online trimestral survey directed at providers, staff, clinic director and patient coordinators), as well as limited in vivo observation (semestral). Fidelity of the EBI will be assessed focusing on the elements of 1) adherence and dose, and 2) participant responsiveness. This frequent fidelity assessment will allow timely detection of emerging barriers and subsequent implementation of adaptations.

b) The EBI consists of a retina picture taken with a fundus camera, without pupillary dilation. Although the process for execution of the exam is very simple and quick, there are some aspects that are core intervention elements and are fundamental for the picture quality, such as the exam being executed in dim light, allowing for pupillary adaptation to dim light for at least 2 minutes, and providing clear instructions to the patient regarding eye opening and head stability during the exam. Fidelity of the EBI will be assessed on the elements of 1) quality of delivery, through in vivo process observation, and 2) participant responsiveness, through short satisfaction questionnaires. We have also developed a fidelity score (three core elements included: >2 minutes dark adaptation, exam performed in dark room, clear and repetitive instructions for head positioning, fixation and eye opening, given during exam) that we will use to analyze fidelity.


2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

Here too, similarly to fidelity, we need to consider adaptations at two levels: a) the level of the integration strategies, and b) the level of the EBI.

a) Due to the nature of the integration strategies and the heterogeneity of our sites, we expect that we'll need to initially taylor the strategies to each site, and possibly re-adapt them during the study period. In order to allow for these and other adaptations, and simultaneously ensure that core components of the strategy are kept, we will do a Planned Adaptation, tailored to each study site, and in collaboration with each site's stakeholders. As an example, while in some sites reorganization of workflow will lead to the exam being performed by a dedicated photographer, whereas in other sites it may become an additional task for the lab technician.

b) We expect that adaptations to the EBI may be needed, as our study sites are heterogeneous in terms of population served, physical space, staff structure, and location of the screening tool. Previously, we have done in vivo observation of the process and noted adaptations made by examiners, such as covering the device's screen light with a sheet of paper to decrease light in the room, and increase exam quality. Other adaptations of this nature are expected in sites where the screening tool is not located in a dedicated room, where patients wait time is prolonged, etc. In order to allow for these and other adaptations, and simultaneously ensure that core components of the EBI are kept, similarly to the integration strategies, we will do a Planned Adaptation of the EBI, tailored to each study site, and in collaboration with each site's stakeholders.  

*Preliminary data. We will assume these integrations strategies for the purpose of this assignment, but the strategies to be used are still in study.";s:5:"xhtml";s:4440:"Bastos de Carvalho- Assignment #2<br />My response is:<br /><br />1. Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br /><br />We will measure fidelity at two levels: a) the level of the integration strategies, and b) at the level of the EBI.<br /><br />a) Our integration strategy consists of three main interventions: implementation of a reliable EMR reminder system, identification of a screening champion with assigned tasks/goals, and reorganization of workflow so that the screening exam is performed by staff other than the provider team (e.g., by the radiology department/team, or at the clinic&#039;s lab)*. We propose to measure fidelity of these strategies through self-reporting (online trimestral survey directed at providers, staff, clinic director and patient coordinators), as well as limited in vivo observation (semestral). Fidelity of the EBI will be assessed focusing on the elements of 1) adherence and dose, and 2) participant responsiveness. This frequent fidelity assessment will allow timely detection of emerging barriers and subsequent implementation of adaptations.<br /><br />b) The EBI consists of a retina picture taken with a fundus camera, without pupillary dilation. Although the process for execution of the exam is very simple and quick, there are some aspects that are core intervention elements and are fundamental for the picture quality, such as the exam being executed in dim light, allowing for pupillary adaptation to dim light for at least 2 minutes, and providing clear instructions to the patient regarding eye opening and head stability during the exam. Fidelity of the EBI will be assessed on the elements of 1) quality of delivery, through in vivo process observation, and 2) participant responsiveness, through short satisfaction questionnaires. We have also developed a fidelity score (three core elements included: &gt;2 minutes dark adaptation, exam performed in dark room, clear and repetitive instructions for head positioning, fixation and eye opening, given during exam) that we will use to analyze fidelity.<br /><br /><br />2. Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />Here too, similarly to fidelity, we need to consider adaptations at two levels: a) the level of the integration strategies, and b) the level of the EBI.<br /><br />a) Due to the nature of the integration strategies and the heterogeneity of our sites, we expect that we&#039;ll need to initially taylor the strategies to each site, and possibly re-adapt them during the study period. In order to allow for these and other adaptations, and simultaneously ensure that core components of the strategy are kept, we will do a Planned Adaptation, tailored to each study site, and in collaboration with each site&#039;s stakeholders. As an example, while in some sites reorganization of workflow will lead to the exam being performed by a dedicated photographer, whereas in other sites it may become an additional task for the lab technician.<br /><br />b) We expect that adaptations to the EBI may be needed, as our study sites are heterogeneous in terms of population served, physical space, staff structure, and location of the screening tool. Previously, we have done in vivo observation of the process and noted adaptations made by examiners, such as covering the device&#039;s screen light with a sheet of paper to decrease light in the room, and increase exam quality. Other adaptations of this nature are expected in sites where the screening tool is not located in a dedicated room, where patients wait time is prolonged, etc. In order to allow for these and other adaptations, and simultaneously ensure that core components of the EBI are kept, similarly to the integration strategies, we will do a Planned Adaptation of the EBI, tailored to each study site, and in collaboration with each site&#039;s stakeholders.  <br /><br />*Preliminary data. We will assume these integrations strategies for the purpose of this assignment, but the strategies to be used are still in study.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"967a21dda4e086a94a775390deb27ec7";}s:4:"show";b:1;s:3:"cid";s:32:"cae125e7a6dd4a0e9b41aed4d975f8fc";}s:32:"6a0b6a46db6e54c0d267e0bfc5b5c93b";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"sdang";s:4:"name";s:10:"Stuti Dang";s:4:"mail";s:17:"Stuti.Dang@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536784960;}s:3:"raw";s:4343:"Stuti Dang, Assignment #2

My response is:

I would like to preface this response with my intent to revise my aims section, based on the feedback received and the last call. Thank you. I feel that an effectiveness-implementation hybrid trial, type 1, would be more suitable. I will revise my aims ASAP. 

Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not?

While HT is considered to be a tool that can improve health outcomes and increase efficiency and reduce cost for patients with chronic conditions such as heart failure, diabetes, and HTN, the reports have not all been consistent. Moreover, evidence supporting its use in HBPC population is absent. 
There is also no current concrete evidence linking level of HT use with individual level outcomes for the HT intervention. However, like most HT interventions, the current VA HT protocol requires every patient to use the HT technology daily. There is talk about changing that but it has not happened yet to my knowledge. As such, the evidence-based intervention is not currently not fully defined which makes it challenging to consider the issue of fidelity measurement and monitoring. The VA intervention will be our prototype HT intervention; however, we would like to understand which patients it works for, and how this needs to be adapted to the HBPC setting through stakeholder feedback. 

I am not sure if I can measure fidelity from a provider perspective since the HT implementation protocols are standardized for PACT patients, but no specific protocols are available for implementing HT in HBPC patients per se. The HT in this case is implemented in a different context, both setting and population. HBPC patients are frail and often have ADL/IADL and cognitive deficits, leading to challenges regarding usability and adherence to technology. This group has access to a provider team who can come and visit them in their home. This offers opportunities. 

We will assess implementation fidelity from the patient perspective in two ways: self-report data from the HBPC patients and caregivers who are receiving HT, to evaluate whether they thought the HT use was of value and how they were taught to use and adhere to HT technology, and adherence to HT intervention by gathering data from the HT vendors on patient response rate (number of days patient responded as a fraction of all possible days they could respond) and the duration they stay in the program. 


2. To what degree is there evidence that associates level of fidelity with individual level outcomes?
Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?

One of the goals of this project is to implement and adapt HT for HBPC across multiple HBPC programs in the VA that demonstrate variation in HT use. We notice that different programs have adapted in different ways to use HT. Understanding adaptations that promote implementation is a major focus of the proposed study. Aspects that might need to be adapted include several contextual modifications: e.g., communication strategies between the two teams since HBPC staff are out in the field, so emails and electronic record sharing may not be most efficient; identifying and targeting the right HBPC patients for the program, and also perhaps making some adaptations so that the presence of the HBPC team members in the patients’ homes can be used as an opportunity to teach the patients and caregivers about using the HT technology, but this requires that the HBPC staff are well trained in HT technology as well. 

To understand these adaptations, we will engage in stakeholder feedback using semi-structured interviews and focus groups – both patients and providers, at 10 high and 10 low using programs. We will interview them about their practice of identifying patients, implementing the technology, and processes for communicating and sharing data between the two teams, since these have emerged as areas of concern in our previous work. we will also ask about adaptations that the providers had to make for this specific group of patients.";s:5:"xhtml";s:4448:"Stuti Dang, Assignment #2<br /><br />My response is:<br /><br />I would like to preface this response with my intent to revise my aims section, based on the feedback received and the last call. Thank you. I feel that an effectiveness-implementation hybrid trial, type 1, would be more suitable. I will revise my aims ASAP. <br /><br />Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not?<br /><br />While HT is considered to be a tool that can improve health outcomes and increase efficiency and reduce cost for patients with chronic conditions such as heart failure, diabetes, and HTN, the reports have not all been consistent. Moreover, evidence supporting its use in HBPC population is absent. <br />There is also no current concrete evidence linking level of HT use with individual level outcomes for the HT intervention. However, like most HT interventions, the current VA HT protocol requires every patient to use the HT technology daily. There is talk about changing that but it has not happened yet to my knowledge. As such, the evidence-based intervention is not currently not fully defined which makes it challenging to consider the issue of fidelity measurement and monitoring. The VA intervention will be our prototype HT intervention; however, we would like to understand which patients it works for, and how this needs to be adapted to the HBPC setting through stakeholder feedback. <br /><br />I am not sure if I can measure fidelity from a provider perspective since the HT implementation protocols are standardized for PACT patients, but no specific protocols are available for implementing HT in HBPC patients per se. The HT in this case is implemented in a different context, both setting and population. HBPC patients are frail and often have ADL/IADL and cognitive deficits, leading to challenges regarding usability and adherence to technology. This group has access to a provider team who can come and visit them in their home. This offers opportunities. <br /><br />We will assess implementation fidelity from the patient perspective in two ways: self-report data from the HBPC patients and caregivers who are receiving HT, to evaluate whether they thought the HT use was of value and how they were taught to use and adhere to HT technology, and adherence to HT intervention by gathering data from the HT vendors on patient response rate (number of days patient responded as a fraction of all possible days they could respond) and the duration they stay in the program. <br /><br /><br />2. To what degree is there evidence that associates level of fidelity with individual level outcomes?<br />Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br /><br />One of the goals of this project is to implement and adapt HT for HBPC across multiple HBPC programs in the VA that demonstrate variation in HT use. We notice that different programs have adapted in different ways to use HT. Understanding adaptations that promote implementation is a major focus of the proposed study. Aspects that might need to be adapted include several contextual modifications: e.g., communication strategies between the two teams since HBPC staff are out in the field, so emails and electronic record sharing may not be most efficient; identifying and targeting the right HBPC patients for the program, and also perhaps making some adaptations so that the presence of the HBPC team members in the patients’ homes can be used as an opportunity to teach the patients and caregivers about using the HT technology, but this requires that the HBPC staff are well trained in HT technology as well. <br /><br />To understand these adaptations, we will engage in stakeholder feedback using semi-structured interviews and focus groups – both patients and providers, at 10 high and 10 low using programs. We will interview them about their practice of identifying patients, implementing the technology, and processes for communicating and sharing data between the two teams, since these have emerged as areas of concern in our previous work. we will also ask about adaptations that the providers had to make for this specific group of patients.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"df7bb6b7746283500176053ae5fec81c";}s:4:"show";b:1;s:3:"cid";s:32:"6a0b6a46db6e54c0d267e0bfc5b5c93b";}s:32:"902b76f9c3f83b6f196d0431fb17fdb4";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"kpossemato";s:4:"name";s:14:"Kyle Possemato";s:4:"mail";s:21:"kyle.Possemato@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536854344;}s:3:"raw";s:6559:"Possemato- Assignment #2:
1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?
There are three primary components of the CAPE evidenced-based intervention and fidelity to each component will be measured. The first component is training PACT staff on VA/ DOD Clinical Practice Guidelines for managing mental health concerns in primary care and how to use CAPE to meet these guidelines. Primary care providers fidelity in enacting these guidelines will be measured by a simple behavior checklist emailed to primary care providers asking them to check what behaviors they engaged in since the training.  Behaviors are specific actions and should be easy for primary care providers to recall such as “directly recommended a patient receive mental health treatment” or “corrected patient misconceptions about mental health treatment (e.g. treatment doesn’t work, it will make me worse, it’s not confidential).”  
The second component is the one session of CBT for Treatment Seeking (CBT-TS) that will be delivered by Primary Care Mental Health Integration (PCMHI) providers. PCMHI providers will be asked to use am electronic note template that includes check boxes for the core features of the intervention. The core elements were derived from the more intensive fidelity monitoring that was used in the earlier randomized controlled trials investigating the efficacy of CBT-TS. Core elements are described in specific behavioral terms so providers can determine if they have been enacted.  Examples include, “described the difference between constructive and destructive thoughts, “elicited specific thoughts and emotions about treatment seeking” and “made a specific plan with the patient about further treatment seeking.” Placement of the check boxes in the note template serve as a measure of provider-reported fidelity and also cue providers to incorporate the core elements within each session. 
The third evidenced-based component of CAPE is referral management.  Fidelity to this component will also be assessed with a check boxes for core elements with a note template.  Examples of core elements include “discussion of previously elicited treatment seeking beliefs” and “discussion of current mental healthcare utilization.”  
All three methods of fidelity measurement proposed here were used in our earlier one-clinic pilot implementation of CAPE.  Providers completed these at high rates: Eighty-four percent of primary care providers returned the email survey and 100% of note templates for CBT-TS and referral management were completed. Currently we have some preliminary evidence associating level of fidelity to treatment outcomes.  Primary care providers who returned the email survey and checked seven out of ten of the behaviors had more patients receive CAPE services then providers who did not return the survey or checked less than seven boxes.  Among PCMHI providers, fidelity to the core elements of CBT-TS and referral management were universally high, however when the box for “made a specific plan with the patient about further treatment seeking” was not checked patients were much less likely to receive additional mental health treatment.  Provider notes indicate that the most common reason for specific treatment seeking plans not being developed was that the patient declined further treatment. The larger proposed study will have a greater ability to investigate how elements of fidelity are related to treatment utilization outcomes.
2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?
I anticipate that adaptions to CAPE will need to be made to increase fit with the local primary care clinics.  As described above, we have documented the core elements of each components of CAPE.  We do not expect these core elements to be altered but we do want to allow clinics to adapt peripheral elements of CAPE to ease implementation and possibly increase effectiveness.  Anticipated adaptions to ease implementation include making CAPE fit into current clinic flow procedures. While CAPE was originally designed to fit typical VA primary care settings, primary care clinics operate in unique ways regarding how patients flow through the clinics and which staff they see when. For instance, some may have greater availability of warm hand-offs to PCMHI providers in clinic and others may rely more on scheduled appointments or telephone-based PCMHI services.  CAPE can incorporate all of these methods and the implementation strategy relies on local clinical staff stakeholders to determine what patient flow procedures work best in their clinic.  The goal is to create simple and easy to follow CAPE procedures that allow that core elements to be enacted while fitting into the local clinic flow and culture. Our implementation plan includes a pre-implementation phase where we will identify barriers and facilitators to implementation by interviewing key clinic staff.  Interviews will be guided by the Consolidated Framework for Implementation Research (CFIR).  Next, we will work with local PCMHI providers to refine CAPE package materials for each local clinic, as needed.  We plan to systematically document what adaptions are made to CAPE at each clinic using the methods described by Rabin et al., 2018. Our research staff will use real-time tracking to document adaptions and we will interview PCMHI providers post-implementation to assess for adaptions that may have went unnoticed by research staff.  We will assess for content modification, which is not expected to occur, and context modifications (e.g., is the intervention typically delivered by phone or in-person), which is expected to occur. Overall, modifications are expected to be minor and characterized as “tailoring or tweaking”. We hope that adaptions will represent positive drift as PACT staff have expertise on how best to implement new procedures in their clinic and on the needs of their patients. More specifically, adaptions could increase the reach of CAPE to more patients, thereby increasing the overall effectiveness of the program by boosting treatment utilization for patients at-risk for suicide.

";s:5:"xhtml";s:6592:"Possemato- Assignment #2:<br />1.	Will you be measuring and monitoring the fidelity with which the evidence-based intervention is delivered? If so, how? If not, why not? To what degree is there evidence that associates level of fidelity with individual level outcomes?<br />There are three primary components of the CAPE evidenced-based intervention and fidelity to each component will be measured. The first component is training PACT staff on VA/ DOD Clinical Practice Guidelines for managing mental health concerns in primary care and how to use CAPE to meet these guidelines. Primary care providers fidelity in enacting these guidelines will be measured by a simple behavior checklist emailed to primary care providers asking them to check what behaviors they engaged in since the training.  Behaviors are specific actions and should be easy for primary care providers to recall such as “directly recommended a patient receive mental health treatment” or “corrected patient misconceptions about mental health treatment (e.g. treatment doesn’t work, it will make me worse, it’s not confidential).”  <br />The second component is the one session of CBT for Treatment Seeking (CBT-TS) that will be delivered by Primary Care Mental Health Integration (PCMHI) providers. PCMHI providers will be asked to use am electronic note template that includes check boxes for the core features of the intervention. The core elements were derived from the more intensive fidelity monitoring that was used in the earlier randomized controlled trials investigating the efficacy of CBT-TS. Core elements are described in specific behavioral terms so providers can determine if they have been enacted.  Examples include, “described the difference between constructive and destructive thoughts, “elicited specific thoughts and emotions about treatment seeking” and “made a specific plan with the patient about further treatment seeking.” Placement of the check boxes in the note template serve as a measure of provider-reported fidelity and also cue providers to incorporate the core elements within each session. <br />The third evidenced-based component of CAPE is referral management.  Fidelity to this component will also be assessed with a check boxes for core elements with a note template.  Examples of core elements include “discussion of previously elicited treatment seeking beliefs” and “discussion of current mental healthcare utilization.”  <br />All three methods of fidelity measurement proposed here were used in our earlier one-clinic pilot implementation of CAPE.  Providers completed these at high rates: Eighty-four percent of primary care providers returned the email survey and 100% of note templates for CBT-TS and referral management were completed. Currently we have some preliminary evidence associating level of fidelity to treatment outcomes.  Primary care providers who returned the email survey and checked seven out of ten of the behaviors had more patients receive CAPE services then providers who did not return the survey or checked less than seven boxes.  Among PCMHI providers, fidelity to the core elements of CBT-TS and referral management were universally high, however when the box for “made a specific plan with the patient about further treatment seeking” was not checked patients were much less likely to receive additional mental health treatment.  Provider notes indicate that the most common reason for specific treatment seeking plans not being developed was that the patient declined further treatment. The larger proposed study will have a greater ability to investigate how elements of fidelity are related to treatment utilization outcomes.<br />2.	Will adaptations need to be made to your evidence-based intervention? If so, what aspects might need to be adapted? If applicable, what process would you use to guide those adaptations? Will you be considering how the intervention is likely to be adapted as it is delivered?<br />I anticipate that adaptions to CAPE will need to be made to increase fit with the local primary care clinics.  As described above, we have documented the core elements of each components of CAPE.  We do not expect these core elements to be altered but we do want to allow clinics to adapt peripheral elements of CAPE to ease implementation and possibly increase effectiveness.  Anticipated adaptions to ease implementation include making CAPE fit into current clinic flow procedures. While CAPE was originally designed to fit typical VA primary care settings, primary care clinics operate in unique ways regarding how patients flow through the clinics and which staff they see when. For instance, some may have greater availability of warm hand-offs to PCMHI providers in clinic and others may rely more on scheduled appointments or telephone-based PCMHI services.  CAPE can incorporate all of these methods and the implementation strategy relies on local clinical staff stakeholders to determine what patient flow procedures work best in their clinic.  The goal is to create simple and easy to follow CAPE procedures that allow that core elements to be enacted while fitting into the local clinic flow and culture. Our implementation plan includes a pre-implementation phase where we will identify barriers and facilitators to implementation by interviewing key clinic staff.  Interviews will be guided by the Consolidated Framework for Implementation Research (CFIR).  Next, we will work with local PCMHI providers to refine CAPE package materials for each local clinic, as needed.  We plan to systematically document what adaptions are made to CAPE at each clinic using the methods described by Rabin et al., 2018. Our research staff will use real-time tracking to document adaptions and we will interview PCMHI providers post-implementation to assess for adaptions that may have went unnoticed by research staff.  We will assess for content modification, which is not expected to occur, and context modifications (e.g., is the intervention typically delivered by phone or in-person), which is expected to occur. Overall, modifications are expected to be minor and characterized as “tailoring or tweaking”. We hope that adaptions will represent positive drift as PACT staff have expertise on how best to implement new procedures in their clinic and on the needs of their patients. More specifically, adaptions could increase the reach of CAPE to more patients, thereby increasing the overall effectiveness of the program by boosting treatment utilization for patients at-risk for suicide.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"902b76f9c3f83b6f196d0431fb17fdb4";}s:32:"775a4b46441ae274a1fe78cc95aec718";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bweiner";s:4:"name";s:12:"Bryan Weiner";s:4:"mail";s:15:"bjweiner@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536870650;}s:3:"raw";s:1775:"Hi Emily. You noted in your concept paper that apps for treating depression have been tested for efficacy. Do you know whether patients enrolled in those studies used these apps with fidelity? Although fidelity is not a primary implementation outcome in your proposed study, it would be important to know whether patients are using the app with fidelity. Their treatment outcomes would presumably depend on the "dose" of treatment they receive in using the app. I'm guessing the app logs would tell you something about the frequency and duration of app use and maybe even the coverage/content of use if there are embedded modules, lessons, or exercises. I'm not sure that clinician self-report or clinician dashboards (?) would tell you much about EBI fidelity, although both might tell you something about service penetration (i.e., the percentage of eligible patients receiving the EBI). I like that you could collect treatment fidelity data directly from the app without additional data collection/burden on patients or providers. 

Indeed, Aim 1 does focus on adapting the EBI (which is the app). You mention potential adaptations to the content of the EBI. There might be other adaptations needed/recommended for this patient population/setting. These might include changes to the look and feel of the app, changes in the frequency or duration of prescribed use of the app (relative to what's been done in other studies), etc. The contextual changes that you mention have less to do with the EBI and they do with the setting. As such, these changes probably reflect the implementation strategies you might employ rather than the EBI itself.

The user-centered design approach is a great way to make adaptations to EBIs (and obtain input on implementation strategies). 
 ";s:5:"xhtml";s:1817:"Hi Emily. You noted in your concept paper that apps for treating depression have been tested for efficacy. Do you know whether patients enrolled in those studies used these apps with fidelity? Although fidelity is not a primary implementation outcome in your proposed study, it would be important to know whether patients are using the app with fidelity. Their treatment outcomes would presumably depend on the &quot;dose&quot; of treatment they receive in using the app. I&#039;m guessing the app logs would tell you something about the frequency and duration of app use and maybe even the coverage/content of use if there are embedded modules, lessons, or exercises. I&#039;m not sure that clinician self-report or clinician dashboards (?) would tell you much about EBI fidelity, although both might tell you something about service penetration (i.e., the percentage of eligible patients receiving the EBI). I like that you could collect treatment fidelity data directly from the app without additional data collection/burden on patients or providers. <br /><br />Indeed, Aim 1 does focus on adapting the EBI (which is the app). You mention potential adaptations to the content of the EBI. There might be other adaptations needed/recommended for this patient population/setting. These might include changes to the look and feel of the app, changes in the frequency or duration of prescribed use of the app (relative to what&#039;s been done in other studies), etc. The contextual changes that you mention have less to do with the EBI and they do with the setting. As such, these changes probably reflect the implementation strategies you might employ rather than the EBI itself.<br /><br />The user-centered design approach is a great way to make adaptations to EBIs (and obtain input on implementation strategies).";s:6:"parent";s:32:"36aa5c5877832ef019f6130b2c8236b8";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"775a4b46441ae274a1fe78cc95aec718";}s:32:"c378aace9f930151d02c1eaaed530608";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bweiner";s:4:"name";s:12:"Bryan Weiner";s:4:"mail";s:15:"bjweiner@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1536871019;}s:3:"raw";s:464:"Nicely done! You have thought carefully about and planned for assessing fidelity of program delivery. Ditto for adapting program delivery methods. The only downside that I see is reliance on self-report of fidelity. This is definitely an efficient way to go. And it's the most feasible in real-world settings. But, as you can expect, self-reported fidelity data are subject to bias and incomplete reporting. Still, this seems like a tradeoff that is worth making. ";s:5:"xhtml";s:468:"Nicely done! You have thought carefully about and planned for assessing fidelity of program delivery. Ditto for adapting program delivery methods. The only downside that I see is reliance on self-report of fidelity. This is definitely an efficient way to go. And it&#039;s the most feasible in real-world settings. But, as you can expect, self-reported fidelity data are subject to bias and incomplete reporting. Still, this seems like a tradeoff that is worth making.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"c378aace9f930151d02c1eaaed530608";}s:32:"a067bb50f64a2aaae44ea53358a48b42";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"kgoldstein";s:4:"name";s:15:"Karen Goldstein";s:4:"mail";s:24:"karen.goldstein@duke.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537014752;}s:3:"raw";s:5203:"(posting early as am out of the country all next week)

Goldstein 
Assignment #3a - Models:
1.	Which model or combination of models is most applicable to your proposed study and why? 
My next step is to conduct a Type 1 Hybrid randomized trial to test the effectiveness of a new peer support intervention while collecting information about the intervention delivery and potential barrier and facilitators for the implementation process. During this step, I will be primarily exploring the determinants of implementation.  Specifically, this will be formative evaluation work occurring as part of a pre-implementation assessment. To this end, CFIR outlines multiple constructs that are particularly important and appropriate for peer support work within VA primary care. In particular for the intervention of peer support, constructs such as intervention source and relative advantage can support conceptualization of this project given that this is an intervention designed and piloted within the VA context, a care delivery setting that has experience with and is heavily invested in peer support (e.g. peer support specialist employees). Similarly, patient needs and resources (outer setting) are highly prioritized by the VA and there is extensive existing knowledge about the barrier and facilitators to those needs. The inner setting and the characteristics of individuals will cover the key concepts needing exploration, including capability with providers and patient aligned health care teams and tension for change, as well as veterans’ self-efficacy around providing peer support, their knowledge and beliefs about peer support and their stage of change regarding use of peer support. 

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 
We will likely use a mixed methods approach to evaluation. We would use CFIR constructs to develop the qualitative interview guide that we would use when interviewing peer support participants and primary care team members to assess barriers and facilitators to implementation. This information would provide key context needed to develop a robust future implementation plan. CFIR constructs would also be used to guide and organize measures chosen to evaluate the intervention (e.g. peer support process measures). 

Assignment #3b - Measures & Evaluations:
1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?
At the clinical level, we plan to measure outcomes related to changes in behavior such as physical activity and dietary change, cardiovascular and health outcomes such as weight, blood pressure, lipid levels and cardiovascular risk. These outcomes are important to patients and providers alike and have well-established association with long-term important outcomes such as mortality. We also plan to process outcomes such as the quantity and quality of peer support interactions as assessed by the recipient of the support. This can help us to understand to what extent the different peer support relationships (i.e. reciprocal vs peer coach) are perceived differently by participants and with which do they have the most frequent contact. Moreover, this will allow for exploration of the relationship between perceived quality of peer support received and clinical outcomes. This information will inform future intervention adaptation.

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?
One process that we will be measuring, as noted above, is the interaction between the participant and their peer support partner and their peer coach (if applicable). There are established peer support measures available developed by Peers4progress, an organization supporting peer support work, and we will also use the Health Specific Social Support measure. In addition, we will assess both the frequency and mode of communication (e.g. phone call, text, email) used for peer support to identify those methods preferred by participants. We will also explore experiences of primary care teams with this peer support intervention to identify potential barriers and facilitators of future implementation. The later will be primary obtained through qualitative methods after the intervention is complete and through field notes collected during study activities.

3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? 
One co-benefit that we intend to assess is engagement with primary care through utilization and patient-reported satisfaction. As one aspect of the group education of this intervention and peer coach training is engagement with primary care around cardiovascular health, it is possible that participant attendance at primary care visits and overall primary care experience satisfaction will be increased. We also plan to assess current depression symptoms and current smoking as potential co-benefits.
";s:5:"xhtml";s:5296:"(posting early as am out of the country all next week)<br /><br />Goldstein <br />Assignment #3a - Models:<br />1.	Which model or combination of models is most applicable to your proposed study and why? <br />My next step is to conduct a Type 1 Hybrid randomized trial to test the effectiveness of a new peer support intervention while collecting information about the intervention delivery and potential barrier and facilitators for the implementation process. During this step, I will be primarily exploring the determinants of implementation.  Specifically, this will be formative evaluation work occurring as part of a pre-implementation assessment. To this end, CFIR outlines multiple constructs that are particularly important and appropriate for peer support work within VA primary care. In particular for the intervention of peer support, constructs such as intervention source and relative advantage can support conceptualization of this project given that this is an intervention designed and piloted within the VA context, a care delivery setting that has experience with and is heavily invested in peer support (e.g. peer support specialist employees). Similarly, patient needs and resources (outer setting) are highly prioritized by the VA and there is extensive existing knowledge about the barrier and facilitators to those needs. The inner setting and the characteristics of individuals will cover the key concepts needing exploration, including capability with providers and patient aligned health care teams and tension for change, as well as veterans’ self-efficacy around providing peer support, their knowledge and beliefs about peer support and their stage of change regarding use of peer support. <br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br />We will likely use a mixed methods approach to evaluation. We would use CFIR constructs to develop the qualitative interview guide that we would use when interviewing peer support participants and primary care team members to assess barriers and facilitators to implementation. This information would provide key context needed to develop a robust future implementation plan. CFIR constructs would also be used to guide and organize measures chosen to evaluate the intervention (e.g. peer support process measures). <br /><br />Assignment #3b - Measures &amp; Evaluations:<br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br />At the clinical level, we plan to measure outcomes related to changes in behavior such as physical activity and dietary change, cardiovascular and health outcomes such as weight, blood pressure, lipid levels and cardiovascular risk. These outcomes are important to patients and providers alike and have well-established association with long-term important outcomes such as mortality. We also plan to process outcomes such as the quantity and quality of peer support interactions as assessed by the recipient of the support. This can help us to understand to what extent the different peer support relationships (i.e. reciprocal vs peer coach) are perceived differently by participants and with which do they have the most frequent contact. Moreover, this will allow for exploration of the relationship between perceived quality of peer support received and clinical outcomes. This information will inform future intervention adaptation.<br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br />One process that we will be measuring, as noted above, is the interaction between the participant and their peer support partner and their peer coach (if applicable). There are established peer support measures available developed by Peers4progress, an organization supporting peer support work, and we will also use the Health Specific Social Support measure. In addition, we will assess both the frequency and mode of communication (e.g. phone call, text, email) used for peer support to identify those methods preferred by participants. We will also explore experiences of primary care teams with this peer support intervention to identify potential barriers and facilitators of future implementation. The later will be primary obtained through qualitative methods after the intervention is complete and through field notes collected during study activities.<br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? <br />One co-benefit that we intend to assess is engagement with primary care through utilization and patient-reported satisfaction. As one aspect of the group education of this intervention and peer coach training is engagement with primary care around cardiovascular health, it is possible that participant attendance at primary care visits and overall primary care experience satisfaction will be increased. We also plan to assess current depression symptoms and current smoking as potential co-benefits.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"cf57606b5e26f60e9b45d1e3755c0492";}s:4:"show";b:1;s:3:"cid";s:32:"a067bb50f64a2aaae44ea53358a48b42";}s:32:"df7bb6b7746283500176053ae5fec81c";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"chelfrich";s:4:"name";s:18:"Christian Helfrich";s:4:"mail";s:15:"helfrich@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537115742;}s:3:"raw";s:1834:"What you might do is outline, up-front, some of the variables in HT delivery (e.g., frequency of contacts; duration; type of content that could be covered) that you think will need to be considered when defining fidelity, and as part of your work you might be able to see what those look like for the high HT vs. low HT-using patients. It sounds like the association of potential fidelity measures with individual-level outcomes is something you could potentially explore in this work. One of the ways I think about fidelity and hard core of an intervention vs. the soft-periphery is if we left out a particular piece, would I still call this thing the intervention we had planned. For example, we have a coaching intervention study with cardiology cath lab teams where we're trying to promote greater use of coronary procedures via the radial artery in the wrist rather than the femoral artery in the groin (radial is safer, more comfortable, cheaper, but initially is more technically challenging and takes inexperienced operators longer), and in the pilot we included time on a radial simulator, but were having trouble obtaining one for the study. For us, we asked ourselves what specific skills or knowledge the simulator was providing and whether that content was (a) important for operators to implement the radial approach (which is definitely subjective) & (b) was non-redundant with other parts of the coaching intervention. I think for you, it may simply be a question of asking if you'd call it delivery of HT under different assumptions of frequency, duration & content of contacts.

I also think it's particularly helpful to think about the underlying mechanisms that you think are how HT helps close gaps in optimal care. What does HT do for both patients and providers that improves quality and/or outcomes of care?
  ";s:5:"xhtml";s:1864:"What you might do is outline, up-front, some of the variables in HT delivery (e.g., frequency of contacts; duration; type of content that could be covered) that you think will need to be considered when defining fidelity, and as part of your work you might be able to see what those look like for the high HT vs. low HT-using patients. It sounds like the association of potential fidelity measures with individual-level outcomes is something you could potentially explore in this work. One of the ways I think about fidelity and hard core of an intervention vs. the soft-periphery is if we left out a particular piece, would I still call this thing the intervention we had planned. For example, we have a coaching intervention study with cardiology cath lab teams where we&#039;re trying to promote greater use of coronary procedures via the radial artery in the wrist rather than the femoral artery in the groin (radial is safer, more comfortable, cheaper, but initially is more technically challenging and takes inexperienced operators longer), and in the pilot we included time on a radial simulator, but were having trouble obtaining one for the study. For us, we asked ourselves what specific skills or knowledge the simulator was providing and whether that content was (a) important for operators to implement the radial approach (which is definitely subjective) &amp; (b) was non-redundant with other parts of the coaching intervention. I think for you, it may simply be a question of asking if you&#039;d call it delivery of HT under different assumptions of frequency, duration &amp; content of contacts.<br /><br />I also think it&#039;s particularly helpful to think about the underlying mechanisms that you think are how HT helps close gaps in optimal care. What does HT do for both patients and providers that improves quality and/or outcomes of care?";s:6:"parent";s:32:"6a0b6a46db6e54c0d267e0bfc5b5c93b";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"df7bb6b7746283500176053ae5fec81c";}s:32:"026114c7653c821aff5964a10a569de6";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"chelfrich";s:4:"name";s:18:"Christian Helfrich";s:4:"mail";s:15:"helfrich@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537130673;}s:3:"raw";s:941:"I think self-report fidelity is often OK. I think sometimes it's more important to be clear with the participants what you think they need to do to main fidelity to the program. 
Regarding genuineness, do you have thoughts about how to assess that? Bryan previously mentioned the importance of spelling out mechanisms in order to both understand why something has worked when it does and when it doesn't work, why it hasn't. It sounds as though in the null trials of peer support engagement was low you're surmising that had peer outreach been higher (e.g., by trained peers) then it would be more effective. Will you be able to assess engagement on the part of both the peer coach and the patient? I wonder how much engagement depends on some minimal capacity (e.g., emotional, cognitive/attention, time) on the parts of both the peer coach and the patient being coached. In any event, an interesting study to assess fidelity & adaptation. ";s:5:"xhtml";s:969:"I think self-report fidelity is often OK. I think sometimes it&#039;s more important to be clear with the participants what you think they need to do to main fidelity to the program. <br />Regarding genuineness, do you have thoughts about how to assess that? Bryan previously mentioned the importance of spelling out mechanisms in order to both understand why something has worked when it does and when it doesn&#039;t work, why it hasn&#039;t. It sounds as though in the null trials of peer support engagement was low you&#039;re surmising that had peer outreach been higher (e.g., by trained peers) then it would be more effective. Will you be able to assess engagement on the part of both the peer coach and the patient? I wonder how much engagement depends on some minimal capacity (e.g., emotional, cognitive/attention, time) on the parts of both the peer coach and the patient being coached. In any event, an interesting study to assess fidelity &amp; adaptation.";s:6:"parent";s:32:"ccd8933b530c956232f17936bcda758b";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"026114c7653c821aff5964a10a569de6";}s:32:"f5664b06f60406584be34c9971dedbec";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"mengelgau";s:4:"name";s:13:"Mike Engelgau";s:4:"mail";s:24:"michael.engelgau@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537308261;}s:3:"raw";s:1493:"Hi Reshma, 
Thanks for your responses to these questions. They are very thoughtful and complete. You report having a very well developed and established assessment for fidelity and also an established threshold for assessing what you consider acceptable fidelity. The one challenge you may have is that during the monthly recorded session, administrators may behave different that when they are not being recorded. You may want to consider in a sample of the administrator to do a more random assessment. This would really strengthen your fidelity assessment and you would potentially learn much about its delivery. In addition, as you adopt different type of people with different levels of training and education, it will be very important to track fidelity - and possibly even be able to assess it across the different types of administrators that you end using. 

In terms of adaptation, you are correct it that it will happen and can be driven by the administrator (e.g. level of education and training), cultural norms, season patterns, and other things. What will be critical and where fidelity and adaptation interplay, is to make sure the key element of your intervention are still being delivered. You type 1 study design is important in that you will want to key an eye of the outcome of your known proven effective intervention. You might want to think about how you will address it if you end of losing some of the impact you are expecting. 
Again, nice work with your responses. ";s:5:"xhtml";s:1512:"Hi Reshma, <br />Thanks for your responses to these questions. They are very thoughtful and complete. You report having a very well developed and established assessment for fidelity and also an established threshold for assessing what you consider acceptable fidelity. The one challenge you may have is that during the monthly recorded session, administrators may behave different that when they are not being recorded. You may want to consider in a sample of the administrator to do a more random assessment. This would really strengthen your fidelity assessment and you would potentially learn much about its delivery. In addition, as you adopt different type of people with different levels of training and education, it will be very important to track fidelity - and possibly even be able to assess it across the different types of administrators that you end using. <br /><br />In terms of adaptation, you are correct it that it will happen and can be driven by the administrator (e.g. level of education and training), cultural norms, season patterns, and other things. What will be critical and where fidelity and adaptation interplay, is to make sure the key element of your intervention are still being delivered. You type 1 study design is important in that you will want to key an eye of the outcome of your known proven effective intervention. You might want to think about how you will address it if you end of losing some of the impact you are expecting. <br />Again, nice work with your responses.";s:6:"parent";s:32:"3b26db557da1274a97c75e143cbacac2";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"f5664b06f60406584be34c9971dedbec";}s:32:"967a21dda4e086a94a775390deb27ec7";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"mengelgau";s:4:"name";s:13:"Mike Engelgau";s:4:"mail";s:24:"michael.engelgau@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537309078;}s:3:"raw";s:1072:"Thanks Ana for your very nice response. You have thought carefully about both fidelity and adaptation and note that they are much overlapping is some ways. It is good that you have an observational component (objective) for your fidelity in addition to you self report. Both of these will provide you with much information and insights on how best this can be measured. I really think you have done a nice job also thinking of this in the 2 levels - which indeed are separate but both much be done with fidelity for the delivery strategy to work. 
In terms of adaptation, indeed it will occur. The key element is that the essential elements to maintain fidelity remain. You will learn much from the way adaptations are done. I am glad you are considering that the venue may be quite variable across the various clinic and possibly within a give clinic on certain day/time, etc. Again here you direct observations will be very important - to learn about how the adaptations are done. This may be useful in scale up and sustainability later on. 
Thanks again and nice work. ";s:5:"xhtml";s:1081:"Thanks Ana for your very nice response. You have thought carefully about both fidelity and adaptation and note that they are much overlapping is some ways. It is good that you have an observational component (objective) for your fidelity in addition to you self report. Both of these will provide you with much information and insights on how best this can be measured. I really think you have done a nice job also thinking of this in the 2 levels - which indeed are separate but both much be done with fidelity for the delivery strategy to work. <br />In terms of adaptation, indeed it will occur. The key element is that the essential elements to maintain fidelity remain. You will learn much from the way adaptations are done. I am glad you are considering that the venue may be quite variable across the various clinic and possibly within a give clinic on certain day/time, etc. Again here you direct observations will be very important - to learn about how the adaptations are done. This may be useful in scale up and sustainability later on. <br />Thanks again and nice work.";s:6:"parent";s:32:"cae125e7a6dd4a0e9b41aed4d975f8fc";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"967a21dda4e086a94a775390deb27ec7";}s:32:"af7277050a861ecd7bdcb3781434eb47";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"sdang";s:4:"name";s:10:"Stuti Dang";s:4:"mail";s:17:"Stuti.Dang@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537466111;}s:3:"raw";s:7639:"Dang 
Assignment #3a - Models:

1. Which model or combination of models is most applicable to your proposed study and why? 

     I had started with the Consolidated framework of Implementation Research (CFIR), but based on your feedback, at this time, I thought I would switch to using the Conceptual Model of Implementation Research by Procter et al, 2009. Of course, there are so many options to choose from which makes it quite confusing. 

     However, I would like to now use the Consolidated framework of Implementation Research (CFIR), since I feel that it offers many advantages. The CFIR is not a model but a framework, it seems to make the most logical sense to use this to understand determinants that influence HT implementation and outcomes in HBPC. CFIR can be used as an evaluation framework explaining Implementation outcomes. In addition, it provides me a model to assess a national VA program, similar to the evaluation of the VA MOVE program nationally. Evaluation of a large-scale weight management program using the consolidated framework for implementation research (CFIR) by Laura J Damschroder and Julie C Lowery.  I feel this would be especially fitting, since the CFIR provides a comprehensive taxonomy of constructs and moderators that are likely to influence implementation of complex programs such as HT in HBPC. I think that this framework also makes sense for this study, given that it recognizes multiple levels of influence and recognizes the importance of evaluating the system as a whole. Another advantage is that the CFIR allows designing of CFIR guided semi-structured interviews. www.CFIRguide.org. We will compare and contrast findings across sites that implement more and less HT in HBPC. We feel that based on our preliminary work, and based on the MOVE results, “network and communication” are often the distinguishing factors between high and low implementing sites. We feel that this might be the best approach to allow us to explain barriers and facilitators and moderators of HT implementation. 

    I would also like to assess the impact of the implementation on program and patient level outcomes.   For that the Conceptual Model of Implementation Research by Procter et al, 2009, may be more suited. This model has the advantage that it focuses on outcomes at the service, program, and patient level, which is helpful to me to organize the data and outcomes. I would like to compare outcomes among HBPC programs that use HT more compared to those that use it sparingly, and also compare patient outcomes among HBPC patients that use HT compared to patients that do not. 

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? 

      I feel the CFIR would help me organize the data I need into barriers and facilitators, focusing on the constructs that we suspect are important apriori. CFIR has some constructs which make sense when applied to this project, e.g., characteristics of the (HBPC and HT) program (e.g., quality, complexity); the outer setting (e.g., patient needs and ability to use technology); inner setting (e.g., compatibility of HBPC with the HT program, leadership engagement); and the process used to implement the program (e.g., planning, engagement of key stakeholders), and characteristics of individuals involved (e.g., knowledge and attitudes, comfort with technology and HT). It might help us to better understand the variance in HT intervention in HBPC, and the why the implementation worked or did not in HBPC, and the moderators between them. Understanding this will allow us to design successful implementation strategies for HT use in HBPC.

    The Conceptual Model of Implementation Research focuses on outcomes at the service, program, and patient level, which is helpful to me to organize the data and outcomes at patient and program levels. 


Assignment #3b - Measures and Evaluations:

1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

     For Aim 1, I will be focusing on the following implementation outcomes based on the CFIR framework.  We will evaluate implementation outcomes including acceptability, appropriateness, feasibility, fidelity, adoption, and penetration. I will use qualitative interviews of five high use and five low use sites. I am planning to use focus groups and semi-structured interviews for all this. 

Aim 2 would be to assess the impact of the implementation on program and patient level outcomes such as number of patients served per staff, efficiency, effectiveness, pt centeredness, etc. For this we will use data form the VA data warehouse with the help of the GECDAC.   

Outcomes:
Feasibility or Proportion of HBPC patients who are eligible for HT 
How: Asking the HBPC providers 
Why: To determine if HT is suited for this dependent population 

Penetration 
How many patients are referred for HT

Fidelity
How: asking the HT staff regarding the steps of the program and how they were able to follow them 
Why: To assess the extent to which the HT intervention has been implemented per guideline

Acceptability 
Do the HBPC staff see a benefit to using HT? and vice-versa. What are the benefits they see?

Uptake 
How many patients who were given HT used it - HT Adherence
How: Through getting data from HT vendors on what the percent response was for the foirst three months 

Barriers to the implementation of HT  
How: Individual interviews and focus group discussions of all involved stakeholders
Why: To identify specific factors that need to be addressed in order to integrate HT use in HBPC 

Facilitators to the implementation of HT  
How: Individual interviews and focus group discussions of all involved stakeholders
Why: To identify what prompted the providers to using HT 

Program level outcomes:

patients served per staff, efficiency, effectiveness, pt centeredness, etc. 

patient level outcomes – which patients benefit from the intervention 


2. What processes are you measuring in your study, how are you measuring them, and why are you measuring them?
     I will assess the process of HT patient identification and what percentage of cases accepted the HT. We will measure this through EMR extraction of the consults placed, and patients that used the program.  I will also try to understand the process of adaptation of the intervention for HBPC patients. This might be critical to the use of HT in patients that may not be able to use it (do they allow caregivers to use it, do they do ok with fewer responses), and needs more exploration. This will also in a way link back to fidelity, but adaptation may be important for this use of HT in this population. I will also specifically focus on the communication processes between the HT and HBPC team. That has come up as a barrier/facilitator in our pilot work.  If the communication is good, then more patients are shared between the two programs. I will try to understand how often and how this communication happens. We will use tools from the CFIR website to come up with appropriate questions to ask to measure process constructs.


3. Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?  

     I will question about that in the interviews. Sometimes HT data raises (false) alarms and leads to additional work, telephone calls, and visits to the patient. ";s:5:"xhtml";s:7968:"Dang <br />Assignment #3a - Models:<br /><br />1. Which model or combination of models is most applicable to your proposed study and why? <br /><br />     I had started with the Consolidated framework of Implementation Research (CFIR), but based on your feedback, at this time, I thought I would switch to using the Conceptual Model of Implementation Research by Procter et al, 2009. Of course, there are so many options to choose from which makes it quite confusing. <br /><br />     However, I would like to now use the Consolidated framework of Implementation Research (CFIR), since I feel that it offers many advantages. The CFIR is not a model but a framework, it seems to make the most logical sense to use this to understand determinants that influence HT implementation and outcomes in HBPC. CFIR can be used as an evaluation framework explaining Implementation outcomes. In addition, it provides me a model to assess a national VA program, similar to the evaluation of the VA MOVE program nationally. Evaluation of a large-scale weight management program using the consolidated framework for implementation research (CFIR) by Laura J Damschroder and Julie C Lowery.  I feel this would be especially fitting, since the CFIR provides a comprehensive taxonomy of constructs and moderators that are likely to influence implementation of complex programs such as HT in HBPC. I think that this framework also makes sense for this study, given that it recognizes multiple levels of influence and recognizes the importance of evaluating the system as a whole. Another advantage is that the CFIR allows designing of CFIR guided semi-structured interviews. www.CFIRguide.org. We will compare and contrast findings across sites that implement more and less HT in HBPC. We feel that based on our preliminary work, and based on the MOVE results, “network and communication” are often the distinguishing factors between high and low implementing sites. We feel that this might be the best approach to allow us to explain barriers and facilitators and moderators of HT implementation. <br /><br />    I would also like to assess the impact of the implementation on program and patient level outcomes.   For that the Conceptual Model of Implementation Research by Procter et al, 2009, may be more suited. This model has the advantage that it focuses on outcomes at the service, program, and patient level, which is helpful to me to organize the data and outcomes. I would like to compare outcomes among HBPC programs that use HT more compared to those that use it sparingly, and also compare patient outcomes among HBPC patients that use HT compared to patients that do not. <br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)? <br /><br />      I feel the CFIR would help me organize the data I need into barriers and facilitators, focusing on the constructs that we suspect are important apriori. CFIR has some constructs which make sense when applied to this project, e.g., characteristics of the (HBPC and HT) program (e.g., quality, complexity); the outer setting (e.g., patient needs and ability to use technology); inner setting (e.g., compatibility of HBPC with the HT program, leadership engagement); and the process used to implement the program (e.g., planning, engagement of key stakeholders), and characteristics of individuals involved (e.g., knowledge and attitudes, comfort with technology and HT). It might help us to better understand the variance in HT intervention in HBPC, and the why the implementation worked or did not in HBPC, and the moderators between them. Understanding this will allow us to design successful implementation strategies for HT use in HBPC.<br /><br />    The Conceptual Model of Implementation Research focuses on outcomes at the service, program, and patient level, which is helpful to me to organize the data and outcomes at patient and program levels. <br /><br /><br />Assignment #3b - Measures and Evaluations:<br /><br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />     For Aim 1, I will be focusing on the following implementation outcomes based on the CFIR framework.  We will evaluate implementation outcomes including acceptability, appropriateness, feasibility, fidelity, adoption, and penetration. I will use qualitative interviews of five high use and five low use sites. I am planning to use focus groups and semi-structured interviews for all this. <br /><br />Aim 2 would be to assess the impact of the implementation on program and patient level outcomes such as number of patients served per staff, efficiency, effectiveness, pt centeredness, etc. For this we will use data form the VA data warehouse with the help of the GECDAC.   <br /><br />Outcomes:<br />Feasibility or Proportion of HBPC patients who are eligible for HT <br />How: Asking the HBPC providers <br />Why: To determine if HT is suited for this dependent population <br /><br />Penetration <br />How many patients are referred for HT<br /><br />Fidelity<br />How: asking the HT staff regarding the steps of the program and how they were able to follow them <br />Why: To assess the extent to which the HT intervention has been implemented per guideline<br /><br />Acceptability <br />Do the HBPC staff see a benefit to using HT? and vice-versa. What are the benefits they see?<br /><br />Uptake <br />How many patients who were given HT used it - HT Adherence<br />How: Through getting data from HT vendors on what the percent response was for the foirst three months <br /><br />Barriers to the implementation of HT  <br />How: Individual interviews and focus group discussions of all involved stakeholders<br />Why: To identify specific factors that need to be addressed in order to integrate HT use in HBPC <br /><br />Facilitators to the implementation of HT  <br />How: Individual interviews and focus group discussions of all involved stakeholders<br />Why: To identify what prompted the providers to using HT <br /><br />Program level outcomes:<br /><br />patients served per staff, efficiency, effectiveness, pt centeredness, etc. <br /><br />patient level outcomes – which patients benefit from the intervention <br /><br /><br />2. What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br />     I will assess the process of HT patient identification and what percentage of cases accepted the HT. We will measure this through EMR extraction of the consults placed, and patients that used the program.  I will also try to understand the process of adaptation of the intervention for HBPC patients. This might be critical to the use of HT in patients that may not be able to use it (do they allow caregivers to use it, do they do ok with fewer responses), and needs more exploration. This will also in a way link back to fidelity, but adaptation may be important for this use of HT in this population. I will also specifically focus on the communication processes between the HT and HBPC team. That has come up as a barrier/facilitator in our pilot work.  If the communication is good, then more patients are shared between the two programs. I will try to understand how often and how this communication happens. We will use tools from the CFIR website to come up with appropriate questions to ask to measure process constructs.<br /><br /><br />3. Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?  <br /><br />     I will question about that in the interviews. Sometimes HT data raises (false) alarms and leads to additional work, telephone calls, and visits to the patient.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"6d8b7db81725ece7062e1950178578f9";}s:4:"show";b:1;s:3:"cid";s:32:"af7277050a861ecd7bdcb3781434eb47";}s:32:"b303bbe88554e309bd4bd3d842b39b40";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"elattie";s:4:"name";s:12:"Emily Lattie";s:4:"mail";s:29:"emily.lattie@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537538811;}s:3:"raw";s:3336:"Lattie
Assignment #3a - Models

1.	Which model of combination of models is most applicable to your proposed study and why?

My proposed study focuses on adapting an app-based program for collaborative care that aims to facilitate communication and provide patients with easily accessible components of evidence-based treatments for depression. The EPIS framework, which provides guidance in examining the inner and outer context of the setting, feels applicable and much of the work within the exploration and preparation phases have been completed. 

2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selections of strategies, etc.)? 

Following the EPIS framework will inform the measures used in the study and helped me to remember the importance of assessing the fiscal viability of the intervention, and of tracking other concurrent initiatives that may be happening in primary care for depression. The EPIS framework also helped inform the way I’m thinking about the process of adapting the intervention to the setting and provided guidance on continuously revisiting the administrator/leadership stakeholders within the healthcare system to work toward sustainment of the intervention.

Assignment #3b – Measures and Evaluations

1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

Outcomes to be measured include feasibility, acceptability, fidelity and reach (for implementation) and depressive symptoms (for clinical). 
Feasibility and acceptability will be measured through interviews with patients and providers and are both being measured to determine 1) any additional changes that may need to be made to the intervention and 2) the likelihood of continued interest for the intervention. 
Fidelity will be measured through logs of app usage and clinician dashboard use which provide a remote form of observation. Fidelity will be measured to help determine the confidence we can have in the clinical results of the intervention. 
Depressive symptoms will be measured using the PHQ-9, which is routinely administered at weekly contacts and serves as a valuable marker of program outcome for individual patients. 

2.	What processes are you measuring in your study, how are you measuring them and why are you measuring them? 
I will assess the process of communication between the care coordinator and patient, and this will be observed through contact logs. I will be measuring this process to identify communication strategies that are beneficial and/or problematic to consider for further training on the adapted intervention. 
3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? 

I will assess the impact on primary care physician time, and will measure this through interviews with the PCPs. One concern that arose during our initial interviews with PCPs was that patients having additional contact with collaborative care could result in additional requests being directed to the PCPs outside of actual appointments which creates an additional time strain on the PCPs. 
";s:5:"xhtml";s:3454:"Lattie<br />Assignment #3a - Models<br /><br />1.	Which model of combination of models is most applicable to your proposed study and why?<br /><br />My proposed study focuses on adapting an app-based program for collaborative care that aims to facilitate communication and provide patients with easily accessible components of evidence-based treatments for depression. The EPIS framework, which provides guidance in examining the inner and outer context of the setting, feels applicable and much of the work within the exploration and preparation phases have been completed. <br /><br />2.	How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selections of strategies, etc.)? <br /><br />Following the EPIS framework will inform the measures used in the study and helped me to remember the importance of assessing the fiscal viability of the intervention, and of tracking other concurrent initiatives that may be happening in primary care for depression. The EPIS framework also helped inform the way I’m thinking about the process of adapting the intervention to the setting and provided guidance on continuously revisiting the administrator/leadership stakeholders within the healthcare system to work toward sustainment of the intervention.<br /><br />Assignment #3b – Measures and Evaluations<br /><br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />Outcomes to be measured include feasibility, acceptability, fidelity and reach (for implementation) and depressive symptoms (for clinical). <br />Feasibility and acceptability will be measured through interviews with patients and providers and are both being measured to determine 1) any additional changes that may need to be made to the intervention and 2) the likelihood of continued interest for the intervention. <br />Fidelity will be measured through logs of app usage and clinician dashboard use which provide a remote form of observation. Fidelity will be measured to help determine the confidence we can have in the clinical results of the intervention. <br />Depressive symptoms will be measured using the PHQ-9, which is routinely administered at weekly contacts and serves as a valuable marker of program outcome for individual patients. <br /><br />2.	What processes are you measuring in your study, how are you measuring them and why are you measuring them? <br />I will assess the process of communication between the care coordinator and patient, and this will be observed through contact logs. I will be measuring this process to identify communication strategies that are beneficial and/or problematic to consider for further training on the adapted intervention. <br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? <br /><br />I will assess the impact on primary care physician time, and will measure this through interviews with the PCPs. One concern that arose during our initial interviews with PCPs was that patients having additional contact with collaborative care could result in additional requests being directed to the PCPs outside of actual appointments which creates an additional time strain on the PCPs.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"6f0cc65028e84f33cf02fc547f1cefba";}s:4:"show";b:1;s:3:"cid";s:32:"b303bbe88554e309bd4bd3d842b39b40";}s:32:"d05b4bb94c98f4b1d87725a93fd09a6a";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"rshah";s:4:"name";s:11:"Reshma Shah";s:4:"mail";s:16:"reshmamd@UIC.EDU";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537548626;}s:3:"raw";s:5143:"Shah
Assignment 3A
Assignment #3a - Models:
1.Which model or combination of models is most applicable to your proposed study and why?

We are proposing to execute the research aims in the context of a Type 1 hybrid implementation-effectiveness trial targeting parents who attend a primary care clinic serving predominantly low-income urban families. We are seeking to a implement primary care-based program (i.e., Sit Down and Play; SDP) that encourages evidence-based parenting behaviors to improve early child developmental outcomes in low-resource settings. We are currently conducting a randomized controlled trial to evaluate the direct outcomes of SDP on parenting outcomes (e.g., parenting behavior, self-efficacy, knowledge) among predominantly low-income parents. The research aims for this proposal will align with application of RE-AIM and Social Cognitive Theory and are intended to provide context for a testable implementation strategy for SDP and other primary care-based programs that aim to reduce poverty-related educational and health disparities.

2.How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

The RE-AIM Framework will guide which measures and outcomes to collect. Specifically, we will 1) evaluate the reach, representativeness, adoption feasibility, and degree to which SDP was implemented as intended, 2) associated costs with implementing SDP, and 3) explore causal pathways through which changes in parenting knowledge, self-regulation, observational learning, facilitators and self-efficacy influence key parenting behaviors. It will also inform the methodology in data collection in that we will apply quantitative and qualitative methods to evaluate various outcomes (e.g., acceptability, fidelity).

Assignment #3b - Measures & Evaluations:
1.What outcomes (both clinical/system/public health and dissemination /implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

On a clinic level, we will be measuring parenting knowledge, and parenting self-efficacy and key parenting behaviors that promote early childhood development. We will be measuring parenting knowledge and parenting self-efficacy using validated self-report measures. We will be measuring parenting behaviors through observational assessments. Parenting knowledge and parenting self-efficacy will be measured because they are key constructs in Social Cognitive Theory and our primary care-based program works to promote behavioral change by targeting these key constructs. We will be measuring parenting behavior as that is a key influencer in promoting early childhood development outcomes. We would also like to measure child outcomes, but would like to focus on the more proximal behavior of parenting behaviors to determine whether modifications to the program or implementation strategy need to be made should no parenting behavioral changes occur.

For dissemination and implementation outcomes we will be evaluating the reach, representativeness, adoption feasibility, and degree to which SDP was implemented as intended and 2) associated costs with implementing SDP. We will collect and analyze data looking at the characteristics of those who enrolled in the program versus those who did not and reasons for attrition and refusals. Information on refusals will be used to inform future recruitment methods. For adoption, we will assess that parents actually received the program through SDP record logs and parental questionnaires. We will assess fidelity to the components based upon audio recorded fidelity checklists completed by at least two members of the team.

2.What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

As discussed above, we will explore causal pathways through which changes in parenting knowledge and self-efficacy influence key parenting behaviors that promote early childhood development. We will also look to see if the number of exposures to the program impacted outcomes (through SDP log data). Lastly, we assess parental and administrator attitudes regarding the program to determine the presence of facilitators or barriers that impacted uptake of the program (qualitative and Likert scale measures). We will be measuring these attitudes to inform potential adaptations of program and implementation strategies.

3.Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

Yes, we will be assessing the co-benefit of health utilization markers. Specifically, we will be looking to see the following: 1) if more children were likely to receive their well-child visit on time, 2) if more children were likely to receive vaccinations on time, 3) improved continuity of care, 4) satisfaction with primary care provider and 5) retention at clinic. We will be measuring the above outcomes through a combination of medical chart review and satisfaction surveys.

";s:5:"xhtml";s:5265:"Shah<br />Assignment 3A<br />Assignment #3a - Models:<br />1.Which model or combination of models is most applicable to your proposed study and why?<br /><br />We are proposing to execute the research aims in the context of a Type 1 hybrid implementation-effectiveness trial targeting parents who attend a primary care clinic serving predominantly low-income urban families. We are seeking to a implement primary care-based program (i.e., Sit Down and Play; SDP) that encourages evidence-based parenting behaviors to improve early child developmental outcomes in low-resource settings. We are currently conducting a randomized controlled trial to evaluate the direct outcomes of SDP on parenting outcomes (e.g., parenting behavior, self-efficacy, knowledge) among predominantly low-income parents. The research aims for this proposal will align with application of RE-AIM and Social Cognitive Theory and are intended to provide context for a testable implementation strategy for SDP and other primary care-based programs that aim to reduce poverty-related educational and health disparities.<br /><br />2.How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />The RE-AIM Framework will guide which measures and outcomes to collect. Specifically, we will 1) evaluate the reach, representativeness, adoption feasibility, and degree to which SDP was implemented as intended, 2) associated costs with implementing SDP, and 3) explore causal pathways through which changes in parenting knowledge, self-regulation, observational learning, facilitators and self-efficacy influence key parenting behaviors. It will also inform the methodology in data collection in that we will apply quantitative and qualitative methods to evaluate various outcomes (e.g., acceptability, fidelity).<br /><br />Assignment #3b - Measures &amp; Evaluations:<br />1.What outcomes (both clinical/system/public health and dissemination /implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />On a clinic level, we will be measuring parenting knowledge, and parenting self-efficacy and key parenting behaviors that promote early childhood development. We will be measuring parenting knowledge and parenting self-efficacy using validated self-report measures. We will be measuring parenting behaviors through observational assessments. Parenting knowledge and parenting self-efficacy will be measured because they are key constructs in Social Cognitive Theory and our primary care-based program works to promote behavioral change by targeting these key constructs. We will be measuring parenting behavior as that is a key influencer in promoting early childhood development outcomes. We would also like to measure child outcomes, but would like to focus on the more proximal behavior of parenting behaviors to determine whether modifications to the program or implementation strategy need to be made should no parenting behavioral changes occur.<br /><br />For dissemination and implementation outcomes we will be evaluating the reach, representativeness, adoption feasibility, and degree to which SDP was implemented as intended and 2) associated costs with implementing SDP. We will collect and analyze data looking at the characteristics of those who enrolled in the program versus those who did not and reasons for attrition and refusals. Information on refusals will be used to inform future recruitment methods. For adoption, we will assess that parents actually received the program through SDP record logs and parental questionnaires. We will assess fidelity to the components based upon audio recorded fidelity checklists completed by at least two members of the team.<br /><br />2.What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />As discussed above, we will explore causal pathways through which changes in parenting knowledge and self-efficacy influence key parenting behaviors that promote early childhood development. We will also look to see if the number of exposures to the program impacted outcomes (through SDP log data). Lastly, we assess parental and administrator attitudes regarding the program to determine the presence of facilitators or barriers that impacted uptake of the program (qualitative and Likert scale measures). We will be measuring these attitudes to inform potential adaptations of program and implementation strategies.<br /><br />3.Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />Yes, we will be assessing the co-benefit of health utilization markers. Specifically, we will be looking to see the following: 1) if more children were likely to receive their well-child visit on time, 2) if more children were likely to receive vaccinations on time, 3) improved continuity of care, 4) satisfaction with primary care provider and 5) retention at clinic. We will be measuring the above outcomes through a combination of medical chart review and satisfaction surveys.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"aa350baeeeda679f7677763513f4b5b4";}s:4:"show";b:1;s:3:"cid";s:32:"d05b4bb94c98f4b1d87725a93fd09a6a";}s:32:"106bdb43641cff581fb7baf949a7ec49";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"kpossemato";s:4:"name";s:14:"Kyle Possemato";s:4:"mail";s:21:"kyle.Possemato@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537566312;}s:3:"raw";s:7123:"Kyle Possemato

Assignment #3a - Models:
Which model or combination of models is most applicable to your proposed study and why? How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

I have selected three models to guide my proposed study.  

1. The Consolidated Framework for Implementation Research (CFIR) is guiding the identification of barriers and facilitators to implementing CAPE at each site. Pre- and post- implementation, key stakeholders (e.g., primary care staff, primary care mental health providers) will be interviewed to assess the relevance of selected CFIR constructs to the implementation of CAPE.  Based on our previous research with CAPE the key constructs selected for inclusion in the interview include intervention characteristics (e.g., evidence strength and quality, relative advantage over usual care, adaptability, complexity), outer setting characteristics (e.g., patient needs and resources), inner setting characteristics (e.g., communication, tension for change, relative priority, leadership engagement), and process characteristics (champions, external change agents, reflecting evaluating). 

2. The implementation strategy will be guided by the Replicating Effective Programs (REP) framework.  Specific implementation activities will take place in each of the four REP phases.  In the Pre-Condition phase barriers and facilitators to CAPE implementation will be identified (as described above).  Based on the findings, the research team will draft CAPE package materials. In the Pre-Implementation phase the research team will work with a stakeholder group consisting of VISN and local leadership in primary care and mental health, frontline primary care and PCMHI providers, and veteran primary care patients with past or current mental health concerns to refine and make small local adaptions to the CAPE package materials. We propose to randomize sites to usual or facilitated implementation arms to understand what activities are necessary for successful CAPE implementation. Facilitated sites will pilot test CAPE implementation with two patients in the Pre-Implementation phase. All sites will have kick-off meetings where the final CAPE implementation package is disseminated. During the Implementation phase, frontline staff at all sites will be trained in CAPE procedures. Facilitated sites will also receive booster training sessions every six months and twice monthly technical assistance phone calls. The research staff will also be evaluating implementation outcomes and vetting results with the stakeholder group in the Implementation phase. In the Maintenance and Evolution phase the stakeholder group will advise the research team on sustainability strategies and we will plan for wider dissemination activities. 

3. RE-AIM will guide our outcome selection and how we will measurement our outcomes. We will assess the Reach of CAPE by using medical record data to indicate what percentage of primary care patients who screen positive for PTSD, depression, or alcohol use disorder engage in CAPE activities. Effectiveness will be measured by medical record data on how many patients receiving CAPE engage in mental health services following CAPE.  Mental health utilization data including psychotropic medication or psychotherapy will be categorized as evidence-based or non-evidenced-based. Adoption will be measured using medical record data to indicate the percentage of primary care providers who actively refer patients to the PCMHI provider and the percentage of PCMHI providers who complete CAPE note templates. Implementation will be measured by CAPE fidelity indicators as described in assignment number two. Maintenance will be measured by evaluating all the RE-AI measures two years post CAPE kick off. Follow-up studies will include additional metrics of maintenance. 

Assignment #3b - Measures & Evaluations:

1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?
As described in assignment 3a, RE-AIM is guiding our outcome measurement.  RE-AIM includes the implementation outcomes of reach, adoption, implementation (fidelity), and maintenance, as well as the service outcome of effectiveness (defined at mental health utilization in the proposed study).  I propose to supplement these outcomes by adding measuring of the additional implementation outcomes of acceptability, appropriateness, and feasibility using the 4-item measures validated by Weiner et al., 2017) at both pre and post-implementation. The selection of these outcomes are driven by our aims to evaluate the impact of a system to increase treatment utilization of primary care veterans at risk for suicide and by the CFIR and REP frameworks described above.

2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?  
Within the field of implementation science the distinction between outcomes and process is not clear to me, Traditionally, many constructs considered implementation outcomes like intervention fidelity and acceptability are considered process outcomes.  Therefore, all the processes I’m measuring are described in my answer to 1.  Please let me know if I’m misinterpreting what is mean by processes here.

3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? 
One consequence that will be assessed is how CAPE effects wait-time within the specialty mental health clinics.  The pilot implementation of CAPE dramatically increased the rate of referral of veterans with PTSD from primary care to the specialty PTSD clinic and this increased wait-time for PTSD clinic intakes. When this consequence was discussed with mental health leadership they welcomed the increased referrals because it provided data that justified the need for additional mental health staffing.  However, increased wait-times can undermine the effectiveness of CAPE in that motivation for treatment will wane if patients need to wait too long to engage in care.  CAPE anticipates this challenge by engaging patients in phone-based referral management and support until they are fully engaged in mental health services. 
One co-benefit I would like to assess is whether CAPE increases the efficiency of PCMHI services and therefore increases open access to PCMHI.  Consistent with high-fidelity PCMHI services, CAPE facilitates brief PCMHI services and using referral management to facilitate transfer to specialty mental health for patients who would benefit from full-length evidence-based treatment.  Keeping PCMHI services time-limited, allows PCMHI providers to be available more often for warm-handoffs allowing new patients to be seen on demand.  VA has national metrics of open-access PCMHI and these can be evaluated at the clinic level pre and post implementation.  
";s:5:"xhtml";s:7239:"Kyle Possemato<br /><br />Assignment #3a - Models:<br />Which model or combination of models is most applicable to your proposed study and why? How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />I have selected three models to guide my proposed study.  <br /><br />1. The Consolidated Framework for Implementation Research (CFIR) is guiding the identification of barriers and facilitators to implementing CAPE at each site. Pre- and post- implementation, key stakeholders (e.g., primary care staff, primary care mental health providers) will be interviewed to assess the relevance of selected CFIR constructs to the implementation of CAPE.  Based on our previous research with CAPE the key constructs selected for inclusion in the interview include intervention characteristics (e.g., evidence strength and quality, relative advantage over usual care, adaptability, complexity), outer setting characteristics (e.g., patient needs and resources), inner setting characteristics (e.g., communication, tension for change, relative priority, leadership engagement), and process characteristics (champions, external change agents, reflecting evaluating). <br /><br />2. The implementation strategy will be guided by the Replicating Effective Programs (REP) framework.  Specific implementation activities will take place in each of the four REP phases.  In the Pre-Condition phase barriers and facilitators to CAPE implementation will be identified (as described above).  Based on the findings, the research team will draft CAPE package materials. In the Pre-Implementation phase the research team will work with a stakeholder group consisting of VISN and local leadership in primary care and mental health, frontline primary care and PCMHI providers, and veteran primary care patients with past or current mental health concerns to refine and make small local adaptions to the CAPE package materials. We propose to randomize sites to usual or facilitated implementation arms to understand what activities are necessary for successful CAPE implementation. Facilitated sites will pilot test CAPE implementation with two patients in the Pre-Implementation phase. All sites will have kick-off meetings where the final CAPE implementation package is disseminated. During the Implementation phase, frontline staff at all sites will be trained in CAPE procedures. Facilitated sites will also receive booster training sessions every six months and twice monthly technical assistance phone calls. The research staff will also be evaluating implementation outcomes and vetting results with the stakeholder group in the Implementation phase. In the Maintenance and Evolution phase the stakeholder group will advise the research team on sustainability strategies and we will plan for wider dissemination activities. <br /><br />3. RE-AIM will guide our outcome selection and how we will measurement our outcomes. We will assess the Reach of CAPE by using medical record data to indicate what percentage of primary care patients who screen positive for PTSD, depression, or alcohol use disorder engage in CAPE activities. Effectiveness will be measured by medical record data on how many patients receiving CAPE engage in mental health services following CAPE.  Mental health utilization data including psychotropic medication or psychotherapy will be categorized as evidence-based or non-evidenced-based. Adoption will be measured using medical record data to indicate the percentage of primary care providers who actively refer patients to the PCMHI provider and the percentage of PCMHI providers who complete CAPE note templates. Implementation will be measured by CAPE fidelity indicators as described in assignment number two. Maintenance will be measured by evaluating all the RE-AI measures two years post CAPE kick off. Follow-up studies will include additional metrics of maintenance. <br /><br />Assignment #3b - Measures &amp; Evaluations:<br /><br />1.	What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br />As described in assignment 3a, RE-AIM is guiding our outcome measurement.  RE-AIM includes the implementation outcomes of reach, adoption, implementation (fidelity), and maintenance, as well as the service outcome of effectiveness (defined at mental health utilization in the proposed study).  I propose to supplement these outcomes by adding measuring of the additional implementation outcomes of acceptability, appropriateness, and feasibility using the 4-item measures validated by Weiner et al., 2017) at both pre and post-implementation. The selection of these outcomes are driven by our aims to evaluate the impact of a system to increase treatment utilization of primary care veterans at risk for suicide and by the CFIR and REP frameworks described above.<br /><br />2.	What processes are you measuring in your study, how are you measuring them, and why are you measuring them?  <br />Within the field of implementation science the distinction between outcomes and process is not clear to me, Traditionally, many constructs considered implementation outcomes like intervention fidelity and acceptability are considered process outcomes.  Therefore, all the processes I’m measuring are described in my answer to 1.  Please let me know if I’m misinterpreting what is mean by processes here.<br /><br />3.	Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not? <br />One consequence that will be assessed is how CAPE effects wait-time within the specialty mental health clinics.  The pilot implementation of CAPE dramatically increased the rate of referral of veterans with PTSD from primary care to the specialty PTSD clinic and this increased wait-time for PTSD clinic intakes. When this consequence was discussed with mental health leadership they welcomed the increased referrals because it provided data that justified the need for additional mental health staffing.  However, increased wait-times can undermine the effectiveness of CAPE in that motivation for treatment will wane if patients need to wait too long to engage in care.  CAPE anticipates this challenge by engaging patients in phone-based referral management and support until they are fully engaged in mental health services. <br />One co-benefit I would like to assess is whether CAPE increases the efficiency of PCMHI services and therefore increases open access to PCMHI.  Consistent with high-fidelity PCMHI services, CAPE facilitates brief PCMHI services and using referral management to facilitate transfer to specialty mental health for patients who would benefit from full-length evidence-based treatment.  Keeping PCMHI services time-limited, allows PCMHI providers to be available more often for warm-handoffs allowing new patients to be seen on demand.  VA has national metrics of open-access PCMHI and these can be evaluated at the clinic level pre and post implementation.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"f82c52efad55b2f8a63aed76c0941189";}s:4:"show";b:1;s:3:"cid";s:32:"106bdb43641cff581fb7baf949a7ec49";}s:32:"f64b74aafe18e5ba5c8d0c2099abb2f9";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"acarvalho";s:4:"name";s:22:"Ana Bastos de Carvalho";s:4:"mail";s:14:"aba253@uky.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1537743397;}s:3:"raw";s:5599:"Assignment #3a - Models:

Acronym list:
TDRS: telemedicine diabetic retinopathy screening
CFIR: Consolidated Framework for Implementation Research
RE-AIM: Reach, Efficacy/Effectiveness, Adoption, Implementation, Maintenance

1. Which model or combination of models is most applicable to your proposed study and why? How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?

I am proposing a pilot evaluation study (pilot to a Hybrid type 2), where I will measure Implementation outcomes (Aim 1) and Patient-level outcomes (Aim 2), and I will measure the determinants that affect my implementation strategy outcomes (Aim 3). To address these aims, I have elected to use three frameworks: the RE-AIM, the Proctor et al, and the CFIR.

For aims 1 and 2, an evaluation framework will work optimally to achieve my goals. From these, I have elected to apply the RE-AIM and the Proctor et al frameworks to measure our outcomes of interest, as they complement each other and encompass all my measures of interest (we will measure Reach, Adoption, Implementation, Efficacy/Effectiveness, and Maintenance/Sustainability).
These models will guide what outcomes I'll be assessing, as well as the tools I use to assess them (please see 3b for details on outcomes measures). 

As for Aim 3, in order to study the determinants influencing my implementation strategy, a determinant framework is the most appropriate. I chose the CFIR framework for its well defined structure and easy access to its tools (interview guide, observation guide). This framework will allow me to study the five major domains associated with implementation success: Individual, Inner setting, Outer setting, Intervention characteristics, and Process. For this aim, we will use a mixed methods approach, and the CFIR framework will inform our qualitative measures (the structure of our interviews).


Assignment #3b - Measures & Evaluations:

1. What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?

I will measure Implementation outcomes in Aim 1, and Patient-level outcomes in Aim 2 (I will not measure outcomes in Aim 3).
Regarding implementation outcomes, I will assess Reach, Adoption, Implementation (Fidelity, Adaptation, Acceptability), and Maintenance/Sustainability. I will assess Reach of the implementation strategy by querying our TDRS reports database to determine percentage of providers requesting the exam. I will measure Efficacy by assessing rates of diabetic patients screened with TDRS and comparing them with pre-intervention rates. Adoption will be measured. Adoption will be measured through structured surveys to the medical directors of institutions, to assess whether the strategy has been adopted in each site. Implementation (Fidelity, Adaptation, Acceptability) will be measured as described in assignment #2, and acceptability will be measured through structured surveys. Finally, Maintenance/Sustainability will be measured by testing Reach, Adoption and Implementation (Fidelity, Adaptation, Acceptability) at 1-year and 3-years post-implementation of our integration strategies.

As for patient-level outcomes, I will study Patient Satisfaction (through a paper survey), and Effectiveness of our strategies through the following clinical measures: 
a) time-to-screening: I will use medical record data to determine the interval between two consecutive retinal exams for patients.

b) rate of follow-up with eye care provider: I will use a combination of medical record data and TDRS reports to determine how many patients receiving a positive screening in TDRS engage in eye care services following TDRS.

c) time to follow-up with eye care provider: I will use medical record data and phone interviews to determine the time lapsed between a positive screening and follow-up with provider. 

d) rate of patient recurrence in yearly exams: I will query our TDRS reports database to determine how many patients receive repeated yearly TDRS exams

The measures above are clinically relevant for patients and providers, as they have been shown to be correlated to early detection and treatment of diabetic retinopathy and visual prognosis. 


2. What processes are you measuring in your study, how are you measuring them, and why are you measuring them?

I will measure/assess the process of delivery of TDRS to the patients, since the workflow used for the exam is an important aspect of implementation outcomes and patient-level outcomes. I will assess this process via direct observation, as well as through semi-structured interviews to the professionals involved in it.


3. Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?

I believe that our integration strategies may increase awareness of providers and staff regarding the need for yearly retinal exams in diabetics. Additionally, being an object of these exams may lead to increased awareness in patients regarding the need for yearly retinal exams, and may be associated with better control of Diabetes mellitus in general. To assess these potential co-benefits I will administer structured surveys to personnel and patients to test awareness and knowledge. I will also use medical records data to identify values of hemoglobin A1c, creatinine, microalbuminuria, and foot exam parameters.";s:5:"xhtml";s:5823:"Assignment #3a - Models:<br /><br />Acronym list:<br />TDRS: telemedicine diabetic retinopathy screening<br />CFIR: Consolidated Framework for Implementation Research<br />RE-AIM: Reach, Efficacy/Effectiveness, Adoption, Implementation, Maintenance<br /><br />1. Which model or combination of models is most applicable to your proposed study and why? How might your selected model(s) guide or inform other aspects of your study (e.g., hypotheses, measures, outcomes, processes, selection of strategies, etc.)?<br /><br />I am proposing a pilot evaluation study (pilot to a Hybrid type 2), where I will measure Implementation outcomes (Aim 1) and Patient-level outcomes (Aim 2), and I will measure the determinants that affect my implementation strategy outcomes (Aim 3). To address these aims, I have elected to use three frameworks: the RE-AIM, the Proctor et al, and the CFIR.<br /><br />For aims 1 and 2, an evaluation framework will work optimally to achieve my goals. From these, I have elected to apply the RE-AIM and the Proctor et al frameworks to measure our outcomes of interest, as they complement each other and encompass all my measures of interest (we will measure Reach, Adoption, Implementation, Efficacy/Effectiveness, and Maintenance/Sustainability).<br />These models will guide what outcomes I&#039;ll be assessing, as well as the tools I use to assess them (please see 3b for details on outcomes measures). <br /><br />As for Aim 3, in order to study the determinants influencing my implementation strategy, a determinant framework is the most appropriate. I chose the CFIR framework for its well defined structure and easy access to its tools (interview guide, observation guide). This framework will allow me to study the five major domains associated with implementation success: Individual, Inner setting, Outer setting, Intervention characteristics, and Process. For this aim, we will use a mixed methods approach, and the CFIR framework will inform our qualitative measures (the structure of our interviews).<br /><br /><br />Assignment #3b - Measures &amp; Evaluations:<br /><br />1. What outcomes (both clinical/system/public health and dissemination/implementation) are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />I will measure Implementation outcomes in Aim 1, and Patient-level outcomes in Aim 2 (I will not measure outcomes in Aim 3).<br />Regarding implementation outcomes, I will assess Reach, Adoption, Implementation (Fidelity, Adaptation, Acceptability), and Maintenance/Sustainability. I will assess Reach of the implementation strategy by querying our TDRS reports database to determine percentage of providers requesting the exam. I will measure Efficacy by assessing rates of diabetic patients screened with TDRS and comparing them with pre-intervention rates. Adoption will be measured. Adoption will be measured through structured surveys to the medical directors of institutions, to assess whether the strategy has been adopted in each site. Implementation (Fidelity, Adaptation, Acceptability) will be measured as described in assignment #2, and acceptability will be measured through structured surveys. Finally, Maintenance/Sustainability will be measured by testing Reach, Adoption and Implementation (Fidelity, Adaptation, Acceptability) at 1-year and 3-years post-implementation of our integration strategies.<br /><br />As for patient-level outcomes, I will study Patient Satisfaction (through a paper survey), and Effectiveness of our strategies through the following clinical measures: <br />a) time-to-screening: I will use medical record data to determine the interval between two consecutive retinal exams for patients.<br /><br />b) rate of follow-up with eye care provider: I will use a combination of medical record data and TDRS reports to determine how many patients receiving a positive screening in TDRS engage in eye care services following TDRS.<br /><br />c) time to follow-up with eye care provider: I will use medical record data and phone interviews to determine the time lapsed between a positive screening and follow-up with provider. <br /><br />d) rate of patient recurrence in yearly exams: I will query our TDRS reports database to determine how many patients receive repeated yearly TDRS exams<br /><br />The measures above are clinically relevant for patients and providers, as they have been shown to be correlated to early detection and treatment of diabetic retinopathy and visual prognosis. <br /><br /><br />2. What processes are you measuring in your study, how are you measuring them, and why are you measuring them?<br /><br />I will measure/assess the process of delivery of TDRS to the patients, since the workflow used for the exam is an important aspect of implementation outcomes and patient-level outcomes. I will assess this process via direct observation, as well as through semi-structured interviews to the professionals involved in it.<br /><br /><br />3. Will you be assessing any potential co-benefits or unintended consequences of the implementation? If so, what outcomes will you assess and how will you measure this? If not, why not?<br /><br />I believe that our integration strategies may increase awareness of providers and staff regarding the need for yearly retinal exams in diabetics. Additionally, being an object of these exams may lead to increased awareness in patients regarding the need for yearly retinal exams, and may be associated with better control of Diabetes mellitus in general. To assess these potential co-benefits I will administer structured surveys to personnel and patients to test awareness and knowledge. I will also use medical records data to identify values of hemoglobin A1c, creatinine, microalbuminuria, and foot exam parameters.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"f64b74aafe18e5ba5c8d0c2099abb2f9";}s:32:"69af68e2536b2a27afab1112689ff134";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"mengelgau";s:4:"name";s:13:"Mike Engelgau";s:4:"mail";s:24:"michael.engelgau@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538071495;}s:3:"raw";s:2143:"3a. Thanks Ann. This is a very good response and good justification. This issue as actually come up earlier in out discussion. You have selected as suitable methodology. What you may want to also do is to consider your reading and think about what might be other methodologies that you might consider and think about reasons of what would be the advantages/disadvantages with that approach. While not crucial to your study, this will make you explicitly think about how different methods are selected for other studies. As it turn out, you are using 2 types of methods (one for Aim 1 and 2 and another for Aim 3). You are finding out that each aim, in some cases, may take different method approaches. 

3.b. 1. I suggest to add that you are measuring the implementation outcomes at the systems level (this seems to be implied) but could be made more explicit. For Aim 1, keep in mind whether you will have a suitable denominator to make the calculation (ie % of providers who order the test). Also, you may want to think patients that demand the exam because of their increase awareness. Not sure you would get this from the TDRS record but it may be something to explore at some point. Also, a bit more clarification on how you will measure sustainability would be useful. Measures at 1 year would be more of the initial uptake/adoption. Year 3 to see if the annual exam was done after the first one would be very important to consider. I appears you are thinking along these lines. Will you be able to know whether those with positive findings will be referred AND treated? This is a very important finding and may be a small number. 
2.  You may want to think more explicitly about which process elements you will want to measure. Also you will want to focus most on the key process elements that are in the logic model leading to the end desired result. 
3. I agree with you co-benefits, especially for the patient awareness issue. Also, the medical record data on other diabetes monitored tests (e.g. A1C) will potentially add to you understanding of whether this exam has the added benefit of improving glycemic control in some cases. 
";s:5:"xhtml";s:2161:"3a. Thanks Ann. This is a very good response and good justification. This issue as actually come up earlier in out discussion. You have selected as suitable methodology. What you may want to also do is to consider your reading and think about what might be other methodologies that you might consider and think about reasons of what would be the advantages/disadvantages with that approach. While not crucial to your study, this will make you explicitly think about how different methods are selected for other studies. As it turn out, you are using 2 types of methods (one for Aim 1 and 2 and another for Aim 3). You are finding out that each aim, in some cases, may take different method approaches. <br /><br />3.b. 1. I suggest to add that you are measuring the implementation outcomes at the systems level (this seems to be implied) but could be made more explicit. For Aim 1, keep in mind whether you will have a suitable denominator to make the calculation (ie % of providers who order the test). Also, you may want to think patients that demand the exam because of their increase awareness. Not sure you would get this from the TDRS record but it may be something to explore at some point. Also, a bit more clarification on how you will measure sustainability would be useful. Measures at 1 year would be more of the initial uptake/adoption. Year 3 to see if the annual exam was done after the first one would be very important to consider. I appears you are thinking along these lines. Will you be able to know whether those with positive findings will be referred AND treated? This is a very important finding and may be a small number. <br />2.  You may want to think more explicitly about which process elements you will want to measure. Also you will want to focus most on the key process elements that are in the logic model leading to the end desired result. <br />3. I agree with you co-benefits, especially for the patient awareness issue. Also, the medical record data on other diabetes monitored tests (e.g. A1C) will potentially add to you understanding of whether this exam has the added benefit of improving glycemic control in some cases.";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"69af68e2536b2a27afab1112689ff134";}s:32:"aa350baeeeda679f7677763513f4b5b4";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"mengelgau";s:4:"name";s:13:"Mike Engelgau";s:4:"mail";s:24:"michael.engelgau@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1538072571;s:8:"modified";i:1538072736;}s:3:"raw";s:1438:"Thanks Reshma.
#3A 1. This seems like an adequate response. You may also want to consider other methods and consider why you are not selecting that approach – and what might be the pros and cons of the different approaches. 
2.You may want to think in more detail about how this approach will impact other parts of your study. 

3B 1. You have a good plan to get outcomes from the parents. However, you may want to consider more elements from the health system. While you mention costs, there many be other barriers at the system level that might be key not only for adoption but for sustainability. My concern is that we should understand the system behaviors in the context of implementation. 
2.You could consider going a bit deeper on why you are measuring different elements and what you may expect to find. This thinking up front may influence how you design your data collection instruments so you don’t miss anything. Keep in mind that you may be quite a range of responses. Finally, you will have some that will not want to participate. You may want to consider how to understand this element since there may be some key barriers (including cultural ones) that we should understand. 
3. This is a good response and we may see several co-benefits. These benefits are a mixture of ones to the families and to the health system. You may want to think in these two domains and see if there may be potentially addition benefits. 
";s:5:"xhtml";s:1466:"Thanks Reshma.<br />#3A 1. This seems like an adequate response. You may also want to consider other methods and consider why you are not selecting that approach – and what might be the pros and cons of the different approaches. <br />2.You may want to think in more detail about how this approach will impact other parts of your study. <br /><br />3B 1. You have a good plan to get outcomes from the parents. However, you may want to consider more elements from the health system. While you mention costs, there many be other barriers at the system level that might be key not only for adoption but for sustainability. My concern is that we should understand the system behaviors in the context of implementation. <br />2.You could consider going a bit deeper on why you are measuring different elements and what you may expect to find. This thinking up front may influence how you design your data collection instruments so you don’t miss anything. Keep in mind that you may be quite a range of responses. Finally, you will have some that will not want to participate. You may want to consider how to understand this element since there may be some key barriers (including cultural ones) that we should understand. <br />3. This is a good response and we may see several co-benefits. These benefits are a mixture of ones to the families and to the health system. You may want to think in these two domains and see if there may be potentially addition benefits.";s:6:"parent";s:32:"d05b4bb94c98f4b1d87725a93fd09a6a";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"aa350baeeeda679f7677763513f4b5b4";}s:32:"f82c52efad55b2f8a63aed76c0941189";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bweiner";s:4:"name";s:12:"Bryan Weiner";s:4:"mail";s:15:"bjweiner@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538152145;}s:3:"raw";s:1121:"Hi Kyle. This looks like a sound plan. A few minor comments. (1) You're proposing to examine a fairly large number of CFIR constructs through interviews. Investigating all of these factors would make for a long interview. You might consider narrowing the list of factors in order to reduce interview participant burden. A good way to do so is to think about the hypothesis or logic for each construct and eliminate those for whom the hypothesis or logic is tenuous or speculative. (2) Implementation process indicates include the timing of initiation, the speed/duration of implementation, the smoothness of implementation (# problems encountered), or the completeness of implementation (all the elements installed). These might or might not be important to you in this project. But you can see how these differ from implementation outcomes. (3) I think I mentioned earlier that RE-AIM can be difficult to apply in the context of pilot studies or controlled trials where several of the RE-AIM dimensions are fixed or constrained by the study design (e.g., reach or adoption might be constrained or fixed by sample size). ";s:5:"xhtml";s:1125:"Hi Kyle. This looks like a sound plan. A few minor comments. (1) You&#039;re proposing to examine a fairly large number of CFIR constructs through interviews. Investigating all of these factors would make for a long interview. You might consider narrowing the list of factors in order to reduce interview participant burden. A good way to do so is to think about the hypothesis or logic for each construct and eliminate those for whom the hypothesis or logic is tenuous or speculative. (2) Implementation process indicates include the timing of initiation, the speed/duration of implementation, the smoothness of implementation (# problems encountered), or the completeness of implementation (all the elements installed). These might or might not be important to you in this project. But you can see how these differ from implementation outcomes. (3) I think I mentioned earlier that RE-AIM can be difficult to apply in the context of pilot studies or controlled trials where several of the RE-AIM dimensions are fixed or constrained by the study design (e.g., reach or adoption might be constrained or fixed by sample size).";s:6:"parent";s:32:"106bdb43641cff581fb7baf949a7ec49";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"f82c52efad55b2f8a63aed76c0941189";}s:32:"6f0cc65028e84f33cf02fc547f1cefba";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bweiner";s:4:"name";s:12:"Bryan Weiner";s:4:"mail";s:15:"bjweiner@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538154814;}s:3:"raw";s:1388:"It sounds like you're proposing a hybrid study since you're interest in both implementation (acceptability, feasibility, fidelity, and you mentioned reach in your concept note) and effectiveness (patient outcomes). If this is the case, you will have to decide at some point whether this is Hybrid 1, 2, or 3 since this decision will affect how you power your study and allocate your resources (time, effort, budget, etc.). One thing that's not clear to me is how you plan to implement this in primary care. What strategies will you use to get this app implemented/integrated into the clinical setting (workflow, work roles, communication processes, information systems, etc.).

I'm having trouble seeing how you will use EPIS in your study. Do you plan to use it as a process model to guide the implementation effort? If you're thinking of it as a determinants framework, what specific EPIS factors will you assess, how will you assess them, and what hypotheses will you explore? More detail here would be helpful. 

We discussed fidelity in earlier posts. Fidelity means as intended or as designed. Is there a protocol for app use that defines "as intended" or "as designed"? App usage might give you some indication of engagement but it won't give you an indication of fidelity without a protocol or standard against which to assess fidelity (e.g., content, duration, frequency, etc.). ";s:5:"xhtml";s:1457:"It sounds like you&#039;re proposing a hybrid study since you&#039;re interest in both implementation (acceptability, feasibility, fidelity, and you mentioned reach in your concept note) and effectiveness (patient outcomes). If this is the case, you will have to decide at some point whether this is Hybrid 1, 2, or 3 since this decision will affect how you power your study and allocate your resources (time, effort, budget, etc.). One thing that&#039;s not clear to me is how you plan to implement this in primary care. What strategies will you use to get this app implemented/integrated into the clinical setting (workflow, work roles, communication processes, information systems, etc.).<br /><br />I&#039;m having trouble seeing how you will use EPIS in your study. Do you plan to use it as a process model to guide the implementation effort? If you&#039;re thinking of it as a determinants framework, what specific EPIS factors will you assess, how will you assess them, and what hypotheses will you explore? More detail here would be helpful. <br /><br />We discussed fidelity in earlier posts. Fidelity means as intended or as designed. Is there a protocol for app use that defines &quot;as intended&quot; or &quot;as designed&quot;? App usage might give you some indication of engagement but it won&#039;t give you an indication of fidelity without a protocol or standard against which to assess fidelity (e.g., content, duration, frequency, etc.).";s:6:"parent";s:32:"b303bbe88554e309bd4bd3d842b39b40";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"6f0cc65028e84f33cf02fc547f1cefba";}s:32:"3e7d7a2339ef69b128f4bf13cd26b28c";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"elattie";s:4:"name";s:12:"Emily Lattie";s:4:"mail";s:29:"emily.lattie@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538492648;}s:3:"raw";s:1881:"Lattie, Module #4

1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?
I’m proposing a type 2 hybrid trial that will be structured as a stepped wedge cluster RCT design. This is the best design to answer my research questions because the clinical intervention and implementation intervention are viewed as equally as important as one another. I’m proposing a stepped wedge cluster RCT design which allows us to stagger the implementation of the program into each of the clinics (which is of interest to the healthcare system), while preserving the tenet of randomization and increasing the sample size of the study.

2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?)
Yes, I have planned to incorporate a mixed methods design into my study. 
For adapting the program, qualitative data from interviews with key stakeholders will be used in an exploratory/development manner. 
For the hybrid trial: For the clinical outcome of depression, the assessment plan is quantitative data (PHQ-9 scores) but implementation outcomes will be assessed through a mixed methods approach. This approach includes examination of system logs of app use (quantitative), communication logs between patient and provider (which is provides quantitative count data of messages exchanged, and  the content of those messages can be analyzed qualitatively), and interviews with patients and providers (which provide qualitative data). I’m proposing a convergence design of analyzing this data as the quantitative and qualitative data are likely to bring equally valuable insights into the process and will thus be collected mostly concurrently. 
";s:5:"xhtml";s:1919:"Lattie, Module #4<br /><br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br />I’m proposing a type 2 hybrid trial that will be structured as a stepped wedge cluster RCT design. This is the best design to answer my research questions because the clinical intervention and implementation intervention are viewed as equally as important as one another. I’m proposing a stepped wedge cluster RCT design which allows us to stagger the implementation of the program into each of the clinics (which is of interest to the healthcare system), while preserving the tenet of randomization and increasing the sample size of the study.<br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?)<br />Yes, I have planned to incorporate a mixed methods design into my study. <br />For adapting the program, qualitative data from interviews with key stakeholders will be used in an exploratory/development manner. <br />For the hybrid trial: For the clinical outcome of depression, the assessment plan is quantitative data (PHQ-9 scores) but implementation outcomes will be assessed through a mixed methods approach. This approach includes examination of system logs of app use (quantitative), communication logs between patient and provider (which is provides quantitative count data of messages exchanged, and  the content of those messages can be analyzed qualitatively), and interviews with patients and providers (which provide qualitative data). I’m proposing a convergence design of analyzing this data as the quantitative and qualitative data are likely to bring equally valuable insights into the process and will thus be collected mostly concurrently.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"31c59a18f53db10ee2ae2b9ee96f9c35";}s:4:"show";b:1;s:3:"cid";s:32:"3e7d7a2339ef69b128f4bf13cd26b28c";}s:32:"d06014874848a12c88b2f35e15ffcd8d";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"kgoldstein";s:4:"name";s:15:"Karen Goldstein";s:4:"mail";s:24:"karen.goldstein@duke.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538570292;}s:3:"raw";s:3402:"Goldstein - Assignment # 4

1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

This study will be testing the novel combination of two established types of peer support (i.e. reciprocal peer support and peer coaches) to promote cardiovascular risk reduction behaviors (i.e. diet and/or exercise). Our research questions are 1) does reciprocal peer support supplemented by peer coaching improve engagement in healthy behaviors to reduce cardiovascular risk?, 2) what are the barriers and facilitators to implementing a combined peer coaching/reciprocal peer support program? Because supplemental social support has been shown to be effective in the context of behavior change and in the Veteran population, a reasonable next step is an clinical effectiveness trial to test the combination of these two methods of delivering social support together. In addition to clinical effectiveness and to address our second question, we propose to include a process evaluation to elucidate the context for implementation and to identify barriers and facilitators to implementation from patient, coach, and staff. Because of the dual aims of this study, it is well suited to be designed as a Type 1 Hybrid effectiveness-implementation study.

2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? 

This study will incorporate a mixed methods approach specifically for the second aim addressing identifying the potential barriers and facilitators to the implementation of a combined peer support intervention. We will use a sequential approach with the use of quantitative methods for the primary effectiveness aim and qualitative methods as a complementary approach to better understand the quantitative findings (e.g. what were the challenges of receiving support from both a coach and a reciprocal peer partner?; for those who did not adequately engage with their peer partner, what were the barriers to doing so?) and to clarify the likely barriers and facilitators for future implementation (e.g. what resources would be required for a clinic or facility to recruit and train peer coaches?; what existing programs could support the identification of potential coaches?). The quantitative and qualitative data will be combined during the data interpretation phase to inform the understanding of the quantitative aim findings. In addition, these results will also be considered in conjunction to inform the planning for a subsequent implementation trial and inform the specific implementation approaches studied. 

We plan to conduct individual interviews (for participants and clinical staff) and focus groups (for peer coaches) at the conclusion of study activities. We will use constructs from the CFIR model to create preliminary interview and focus group guides covering key constructs related to determinants of implementation outcomes. During the course of the study, we will keep field notes and track important decisions and process complications. We will use these field notes to further tailor our preliminary interview guides to hone the data collection specifically to the intervention context and increase the yield of the qualitative work.  
";s:5:"xhtml";s:3449:"Goldstein - Assignment # 4<br /><br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />This study will be testing the novel combination of two established types of peer support (i.e. reciprocal peer support and peer coaches) to promote cardiovascular risk reduction behaviors (i.e. diet and/or exercise). Our research questions are 1) does reciprocal peer support supplemented by peer coaching improve engagement in healthy behaviors to reduce cardiovascular risk?, 2) what are the barriers and facilitators to implementing a combined peer coaching/reciprocal peer support program? Because supplemental social support has been shown to be effective in the context of behavior change and in the Veteran population, a reasonable next step is an clinical effectiveness trial to test the combination of these two methods of delivering social support together. In addition to clinical effectiveness and to address our second question, we propose to include a process evaluation to elucidate the context for implementation and to identify barriers and facilitators to implementation from patient, coach, and staff. Because of the dual aims of this study, it is well suited to be designed as a Type 1 Hybrid effectiveness-implementation study.<br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? <br /><br />This study will incorporate a mixed methods approach specifically for the second aim addressing identifying the potential barriers and facilitators to the implementation of a combined peer support intervention. We will use a sequential approach with the use of quantitative methods for the primary effectiveness aim and qualitative methods as a complementary approach to better understand the quantitative findings (e.g. what were the challenges of receiving support from both a coach and a reciprocal peer partner?; for those who did not adequately engage with their peer partner, what were the barriers to doing so?) and to clarify the likely barriers and facilitators for future implementation (e.g. what resources would be required for a clinic or facility to recruit and train peer coaches?; what existing programs could support the identification of potential coaches?). The quantitative and qualitative data will be combined during the data interpretation phase to inform the understanding of the quantitative aim findings. In addition, these results will also be considered in conjunction to inform the planning for a subsequent implementation trial and inform the specific implementation approaches studied. <br /><br />We plan to conduct individual interviews (for participants and clinical staff) and focus groups (for peer coaches) at the conclusion of study activities. We will use constructs from the CFIR model to create preliminary interview and focus group guides covering key constructs related to determinants of implementation outcomes. During the course of the study, we will keep field notes and track important decisions and process complications. We will use these field notes to further tailor our preliminary interview guides to hone the data collection specifically to the intervention context and increase the yield of the qualitative work.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"5fdf52b4a55c6319dda18f405999693a";}s:4:"show";b:1;s:3:"cid";s:32:"d06014874848a12c88b2f35e15ffcd8d";}s:32:"be07e2162f61afb0869e752183aebcc8";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"acarvalho";s:4:"name";s:22:"Ana Bastos de Carvalho";s:4:"mail";s:14:"aba253@uky.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538581021;}s:3:"raw";s:5795:"Bastos de Carvalho, Ana     **Assignment #4: Design**

Acronym list:

TDRS: telemedicine diabetic retinopathy screening

RCT: Randomized controlled trial

RE-AIM: Reach, Efficacy, Adoption, Implementation, Maintenance framework

1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

Regarding my aims 1 and 2, I am equally interested in studying and documenting the effectiveness of my integration strategies for TDRS, as well as assessing the patient-level outcomes of such intervention. Therefore, I propose to conduct a Hybrid Type 2 effectiveness-implementation trial. I will have 20 participating sites (primary care clinics), which will be included in the trial following a stepped wedge cluster RCT design. This design is more feasible for a large number of sites, as it allows starting sites at different timepoints. It is also more acceptable from an ethical perspective, as all clinics will receive the intervention, which is in principle assumed to be beneficial. This design is also optimal to test sustainability of my integration strategies (because some of my sites-the ones randomized to the intervention earlier on - will have a very long follow-up time). Given that telemedicine EBIs are notorious for their lack of sustainability, this is one of the most important outcomes in my study, and again, the stepped wedge design will provide optimal conditions to test it.

Regarding the design for the patient-level outcomes (time-to-screening, rate of follow-up with eye care provider, time to follow-up with eye care provider and rate of patient recurrence in yearly exams), there isn't an automated ubiquitous system that collects this information in patients' charts. If we were to rely exclusively on patients' charts, the availability of this information would be dependent on providers' documentation quality and thoroughness. To ensure high quality data, we have opted to randomly select 5% of the patients who are screened during the first 2 years of the study (expected to be around 1,000 individuals) and collect information through two channels: analysis of medical records and patient phone questionnaire.

As for my Aim 3 (Measure the factors influencing implementation outcomes, guided by CFIR), it will follow an observational design, since I will be observing and measuring factors (inner setting, outer setting, individual, intervention characteristics, process) influencing the integration strategies. I will measure factors before and after implementation of my strategies. I will also assess  how sustainability of TDRS is influenced by these factors.


2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? 

Yes, I will use a mixed methods design to measure my implementation outcomes based on the RE-AIM framework. Mixed methods are an optimal way to address several of my questions, as the qualitative and quantitative data complement each other. I will conduct concurrent quantitative and qualitative data collection, with the function of these two sets of data being complementarity (using quantitative data to evaluate the breadth of outcomes, and qualitative data to evaluate the process).

- Reach will be measured quantitatively by querying our TDRS reports database to determine percentage of providers requesting the exam. I will also assess these data qualitatively by studying the characteristics of the participants vs non-participants, and will interview non-participants and participants to determine barriers and facilitators to participation.

- I will measure Efficiency by assessing rates of diabetic patients screened with TDRS and comparing them with pre-intervention rates. We will also look at demographic data and disease characteristics of the screened vs non-screened patients, to identify patient factors that may be related to lower efficiency of the implementations strategies.

- Adoption will be measured through structured surveys to the medical directors of institutions, to assess whether the strategy has been adopted in each site. These surveys will also provide us with insights from the directors regarding factors influencing adoption.

- Fidelity will be measured through self-reporting and in vivo observation. These strategies will provide me with qualitative data (e.g., are the strategies being employed as pre-determined, are there any adaptations) and quantitative data (e.g. how many providers are using the implementations with fidelity).

With this approach to the RE-AIM factors, I aim to determine quantitative outcomes of the implementation strategy, and to simultaneously unveil qualitative determinants for each of these outcomes, which will guide modifications, in case they are needed. 

As for patient-level outcomes, I will take a similar approach to study Effectiveness of our strategies through time-to-screening, rate of follow-up with eye care provider, time to follow-up with eye care provider and rate of patient recurrence in yearly exams. For all these measures, we will collect quantitative information, as well as qualitative through our programed phone interviews to 5% of the screened patients (e.g., barriers and facilitators to following-up with eye care provider or getting a repeated yearly TDRS screening).

Finally, in aim 3, I will also use mixed methods. I will use a connecting data mixed methods approach to firstly identify the factors influencing implementation outcomes (qualitative, via key informant interviews), and secondly design structured surveys that will measure importance of each of these factors (quantitative).";s:5:"xhtml";s:5990:"Bastos de Carvalho, Ana     **Assignment #4: Design**<br /><br />Acronym list:<br /><br />TDRS: telemedicine diabetic retinopathy screening<br /><br />RCT: Randomized controlled trial<br /><br />RE-AIM: Reach, Efficacy, Adoption, Implementation, Maintenance framework<br /><br />1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />Regarding my aims 1 and 2, I am equally interested in studying and documenting the effectiveness of my integration strategies for TDRS, as well as assessing the patient-level outcomes of such intervention. Therefore, I propose to conduct a Hybrid Type 2 effectiveness-implementation trial. I will have 20 participating sites (primary care clinics), which will be included in the trial following a stepped wedge cluster RCT design. This design is more feasible for a large number of sites, as it allows starting sites at different timepoints. It is also more acceptable from an ethical perspective, as all clinics will receive the intervention, which is in principle assumed to be beneficial. This design is also optimal to test sustainability of my integration strategies (because some of my sites-the ones randomized to the intervention earlier on - will have a very long follow-up time). Given that telemedicine EBIs are notorious for their lack of sustainability, this is one of the most important outcomes in my study, and again, the stepped wedge design will provide optimal conditions to test it.<br /><br />Regarding the design for the patient-level outcomes (time-to-screening, rate of follow-up with eye care provider, time to follow-up with eye care provider and rate of patient recurrence in yearly exams), there isn&#039;t an automated ubiquitous system that collects this information in patients&#039; charts. If we were to rely exclusively on patients&#039; charts, the availability of this information would be dependent on providers&#039; documentation quality and thoroughness. To ensure high quality data, we have opted to randomly select 5% of the patients who are screened during the first 2 years of the study (expected to be around 1,000 individuals) and collect information through two channels: analysis of medical records and patient phone questionnaire.<br /><br />As for my Aim 3 (Measure the factors influencing implementation outcomes, guided by CFIR), it will follow an observational design, since I will be observing and measuring factors (inner setting, outer setting, individual, intervention characteristics, process) influencing the integration strategies. I will measure factors before and after implementation of my strategies. I will also assess  how sustainability of TDRS is influenced by these factors.<br /><br /><br />2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? <br /><br />Yes, I will use a mixed methods design to measure my implementation outcomes based on the RE-AIM framework. Mixed methods are an optimal way to address several of my questions, as the qualitative and quantitative data complement each other. I will conduct concurrent quantitative and qualitative data collection, with the function of these two sets of data being complementarity (using quantitative data to evaluate the breadth of outcomes, and qualitative data to evaluate the process).<br /><br />- Reach will be measured quantitatively by querying our TDRS reports database to determine percentage of providers requesting the exam. I will also assess these data qualitatively by studying the characteristics of the participants vs non-participants, and will interview non-participants and participants to determine barriers and facilitators to participation.<br /><br />- I will measure Efficiency by assessing rates of diabetic patients screened with TDRS and comparing them with pre-intervention rates. We will also look at demographic data and disease characteristics of the screened vs non-screened patients, to identify patient factors that may be related to lower efficiency of the implementations strategies.<br /><br />- Adoption will be measured through structured surveys to the medical directors of institutions, to assess whether the strategy has been adopted in each site. These surveys will also provide us with insights from the directors regarding factors influencing adoption.<br /><br />- Fidelity will be measured through self-reporting and in vivo observation. These strategies will provide me with qualitative data (e.g., are the strategies being employed as pre-determined, are there any adaptations) and quantitative data (e.g. how many providers are using the implementations with fidelity).<br /><br />With this approach to the RE-AIM factors, I aim to determine quantitative outcomes of the implementation strategy, and to simultaneously unveil qualitative determinants for each of these outcomes, which will guide modifications, in case they are needed. <br /><br />As for patient-level outcomes, I will take a similar approach to study Effectiveness of our strategies through time-to-screening, rate of follow-up with eye care provider, time to follow-up with eye care provider and rate of patient recurrence in yearly exams. For all these measures, we will collect quantitative information, as well as qualitative through our programed phone interviews to 5% of the screened patients (e.g., barriers and facilitators to following-up with eye care provider or getting a repeated yearly TDRS screening).<br /><br />Finally, in aim 3, I will also use mixed methods. I will use a connecting data mixed methods approach to firstly identify the factors influencing implementation outcomes (qualitative, via key informant interviews), and secondly design structured surveys that will measure importance of each of these factors (quantitative).";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"715a666bfcb673632cb00eec602a686e";}s:4:"show";b:1;s:3:"cid";s:32:"be07e2162f61afb0869e752183aebcc8";}s:32:"6d8b7db81725ece7062e1950178578f9";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"chelfrich";s:4:"name";s:18:"Christian Helfrich";s:4:"mail";s:15:"helfrich@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538596791;}s:3:"raw";s:3291:"Hi Stuti, thanks for this response and your thoughts on the the conceptual model, and so sorry for my tardy response.

I think you’re quite right about the large number of models and how confusing it can be. I think Per Nielsen’s paper (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4406164) is really important for that reason. The 3 main types of models that he describes, which we most often encounter & use in implementation science, each have different characteristics and purposes: 
1) process models: what sequence of events leads to implementation. Process models really encompass 2 types of models:
1a) descriptive process models: what sequence of events/activities/conditions leading to implementation do we observe in the world? 
1b) prescriptive models: here’s the sequence of events/activities/conditions that we think optimally create effective implementation (irrespective whether we can actually observe the optimal model in the real world)
2) classical models: what are the groups of factors that influence implementation process and outcomes.
3) evaluation models: what are the outcomes and factors we need to assess in order to evaluate the effect of an implementation initiative.

I think it’s important to consider what you want the model to help you do. CFIR is a classical model, and I think is very helpful for a few things. First and foremost it’s a common taxonomy and organizing principle. If I define and label things “implementation climate” the same way (more or less) that you do, we can more easily find each other’s work; corroborate findings; identify existing measures; & potentially identify & adapt implementation approaches used in other settings. CFIR can also help you simply look through and consider whether there are key constructs that are likely to come into play that you haven’t considered. 
But CFIR also has limitations (like every model). It doesn’t spell out hypothesized relationships (other than the factors in the model are all expected to influence implementation). CFIR doesn’t tell you which factors (combination of factors; sequence of factors) are likely most important for your project. Finally, because CFIR covers the waterfront, and it can tempt you to measure too much. Those aren’t arguments against CFIR; they’re things to consider when deciding how you’ll use it. I think it could be a good model for this purpose, but I recommend thinking about what you’ll specifically do with the model or do differently because of the model (if anything) beyond simply using it as a way of labeling your findings when you report them. For example, given you think that networks & communication might be particularly important, are there specific networks / communication related to HT integration that you’ll measure? And do you have ideas about why communication increases HT integration? E.g., is it a trust thing? Or informational--without a sufficient level of communication HBPC doesn’t know how to use HT? Or motivation--w/out regular communication HT drops off HPPC’s radar?  

That’s great that you’re thinking about unintended consequences. I think it’s often a failing in studies and can really undermine our credibility, and in some cases it can also help explain why programs falter.
";s:5:"xhtml";s:3366:"Hi Stuti, thanks for this response and your thoughts on the the conceptual model, and so sorry for my tardy response.<br /><br />I think you’re quite right about the large number of models and how confusing it can be. I think Per Nielsen’s paper (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4406164) is really important for that reason. The 3 main types of models that he describes, which we most often encounter &amp; use in implementation science, each have different characteristics and purposes: <br />1) process models: what sequence of events leads to implementation. Process models really encompass 2 types of models:<br />1a) descriptive process models: what sequence of events/activities/conditions leading to implementation do we observe in the world? <br />1b) prescriptive models: here’s the sequence of events/activities/conditions that we think optimally create effective implementation (irrespective whether we can actually observe the optimal model in the real world)<br />2) classical models: what are the groups of factors that influence implementation process and outcomes.<br />3) evaluation models: what are the outcomes and factors we need to assess in order to evaluate the effect of an implementation initiative.<br /><br />I think it’s important to consider what you want the model to help you do. CFIR is a classical model, and I think is very helpful for a few things. First and foremost it’s a common taxonomy and organizing principle. If I define and label things “implementation climate” the same way (more or less) that you do, we can more easily find each other’s work; corroborate findings; identify existing measures; &amp; potentially identify &amp; adapt implementation approaches used in other settings. CFIR can also help you simply look through and consider whether there are key constructs that are likely to come into play that you haven’t considered. <br />But CFIR also has limitations (like every model). It doesn’t spell out hypothesized relationships (other than the factors in the model are all expected to influence implementation). CFIR doesn’t tell you which factors (combination of factors; sequence of factors) are likely most important for your project. Finally, because CFIR covers the waterfront, and it can tempt you to measure too much. Those aren’t arguments against CFIR; they’re things to consider when deciding how you’ll use it. I think it could be a good model for this purpose, but I recommend thinking about what you’ll specifically do with the model or do differently because of the model (if anything) beyond simply using it as a way of labeling your findings when you report them. For example, given you think that networks &amp; communication might be particularly important, are there specific networks / communication related to HT integration that you’ll measure? And do you have ideas about why communication increases HT integration? E.g., is it a trust thing? Or informational--without a sufficient level of communication HBPC doesn’t know how to use HT? Or motivation--w/out regular communication HT drops off HPPC’s radar?  <br /><br />That’s great that you’re thinking about unintended consequences. I think it’s often a failing in studies and can really undermine our credibility, and in some cases it can also help explain why programs falter.";s:6:"parent";s:32:"af7277050a861ecd7bdcb3781434eb47";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"6d8b7db81725ece7062e1950178578f9";}s:32:"cf57606b5e26f60e9b45d1e3755c0492";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"chelfrich";s:4:"name";s:18:"Christian Helfrich";s:4:"mail";s:15:"helfrich@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538602592;}s:3:"raw";s:3886:"Hi Karen, thank you for the thoughtful response, and apologies for my tardy response.

What you say about the CFIR elements certainly makes sense. I have a couple of additional thoughts. First, I think it’s really helpful to think about the mechanisms by which you think your intervention will have an effect, and then think about how you can measure those mechanisms and what factors are likely to serve as barriers and facilitators. For example, you mentioned relative advantage of the peer support program, you might consider relative advantage for whom, and whether that perceived relative advantage is hypothetically maleable. Do you think you’ll be able to improve the perceived benefits of the program to the Veteran? And will that be a potential mediator of the effectiveness of your implementation strategy? 

Regarding CFIR, my personal view is that CFIR is very helpful for categorizing constructs and reporting findings; using CFIR makes it much easier for researchers working on the same ideas (e.g., the role of implementation climate in influencing implementation effectiveness) but in different domains (e.g., IT implementation in a hospital and team-based primary care implementation in CBOCs) to find each others’ findings & build on each other. But there are a lot of things CFIR doesn’t really tell us (nor is meant to tell us): which constructs will be important under what conditions; what sequence of events or activities lead to implementation; or conditional associations, i.e., this factor or combination of factors causes/nullifies/amplifies this other factor or set of factors. It’s worth thinking about whether your pilot work can provide the basis for making some hypotheses about those types of relationships, or use your qualitative data collection to try to understand some of those potential relationships. 

Where possible it’s also helpful (& interesting) from a research standpoint, to thinking about competing hypotheses. For example, for an implementation trial we’re conducting right to increase the use of an evidence-based alternative method of performing coronary interventions, our hypothesis is that the primary barrier is the interventional cardiology teams’ proficiency with the new procedure relative to the existing procedure (which we’re operationalizing as procedure duration & ability to complete the procedure). But it’s also possible that the cardiology teams are uncertain if the new procedure is actually better than the old one in terms of the outcomes it’s supposed to improve (in this case, lower bleeding complication rates, patient satisfaction, and lower cost), so we’re also measuring participants’ perceptions of relative advantage on those factors, over time, to see if they change and potentially mediate the relationship between our intervention (a team-based coaching intervention) and the outcome (% change in use of the new procedure). 

Regarding unintended consequences, it might be worth assessing how Veterans’ use of the health care system, and specifically urgent care & emergency care changes. I had a colleague, Vince Fan, who did a trial of a selfcare behavioral intervention for patients with COPD, and the trial was stopped early for higher mortality in the selfcare group. They couldn’t be certain the reason, but one hypothesis was that Veterans in the selfcare group got overly confident in their ability to manage breathing exacerbations and delayed seeking appropriate treatment. Incidentally, Vince is now doing a study to try to understand how Veterans with COPD determine when to seek care for exacerbations; what factors might lead to them to delay seeking appropriate care for exacerbations; and how they migth be able to help these Veterans seek care more rapidly when they need it. 

You have an interesting project with a wealth of things to explore.

Christian
";s:5:"xhtml";s:3961:"Hi Karen, thank you for the thoughtful response, and apologies for my tardy response.<br /><br />What you say about the CFIR elements certainly makes sense. I have a couple of additional thoughts. First, I think it’s really helpful to think about the mechanisms by which you think your intervention will have an effect, and then think about how you can measure those mechanisms and what factors are likely to serve as barriers and facilitators. For example, you mentioned relative advantage of the peer support program, you might consider relative advantage for whom, and whether that perceived relative advantage is hypothetically maleable. Do you think you’ll be able to improve the perceived benefits of the program to the Veteran? And will that be a potential mediator of the effectiveness of your implementation strategy? <br /><br />Regarding CFIR, my personal view is that CFIR is very helpful for categorizing constructs and reporting findings; using CFIR makes it much easier for researchers working on the same ideas (e.g., the role of implementation climate in influencing implementation effectiveness) but in different domains (e.g., IT implementation in a hospital and team-based primary care implementation in CBOCs) to find each others’ findings &amp; build on each other. But there are a lot of things CFIR doesn’t really tell us (nor is meant to tell us): which constructs will be important under what conditions; what sequence of events or activities lead to implementation; or conditional associations, i.e., this factor or combination of factors causes/nullifies/amplifies this other factor or set of factors. It’s worth thinking about whether your pilot work can provide the basis for making some hypotheses about those types of relationships, or use your qualitative data collection to try to understand some of those potential relationships. <br /><br />Where possible it’s also helpful (&amp; interesting) from a research standpoint, to thinking about competing hypotheses. For example, for an implementation trial we’re conducting right to increase the use of an evidence-based alternative method of performing coronary interventions, our hypothesis is that the primary barrier is the interventional cardiology teams’ proficiency with the new procedure relative to the existing procedure (which we’re operationalizing as procedure duration &amp; ability to complete the procedure). But it’s also possible that the cardiology teams are uncertain if the new procedure is actually better than the old one in terms of the outcomes it’s supposed to improve (in this case, lower bleeding complication rates, patient satisfaction, and lower cost), so we’re also measuring participants’ perceptions of relative advantage on those factors, over time, to see if they change and potentially mediate the relationship between our intervention (a team-based coaching intervention) and the outcome (% change in use of the new procedure). <br /><br />Regarding unintended consequences, it might be worth assessing how Veterans’ use of the health care system, and specifically urgent care &amp; emergency care changes. I had a colleague, Vince Fan, who did a trial of a selfcare behavioral intervention for patients with COPD, and the trial was stopped early for higher mortality in the selfcare group. They couldn’t be certain the reason, but one hypothesis was that Veterans in the selfcare group got overly confident in their ability to manage breathing exacerbations and delayed seeking appropriate treatment. Incidentally, Vince is now doing a study to try to understand how Veterans with COPD determine when to seek care for exacerbations; what factors might lead to them to delay seeking appropriate care for exacerbations; and how they migth be able to help these Veterans seek care more rapidly when they need it. <br /><br />You have an interesting project with a wealth of things to explore.<br /><br />Christian";s:6:"parent";s:32:"a067bb50f64a2aaae44ea53358a48b42";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"cf57606b5e26f60e9b45d1e3755c0492";}s:32:"74f8ce165655ad83ed8413b5aed7371a";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"rshah";s:4:"name";s:11:"Reshma Shah";s:4:"mail";s:16:"reshmamd@UIC.EDU";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538762166;}s:3:"raw";s:2914:"Reshma Shah #4
1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

The proposed study design is a type 1 hybrid implementation-effectiveness randomized control. The study works towards addressing the critical need for an accessible and effective population-level strategy to promote evidence-based based parenting behaviors that promote early childhood development. This study seeks to evaluate Sit Down and Play (SDP), a brief, theory-based program delivered during pediatric well-child visits as a promising opportunity with a universal approach to enhance parenting behaviors that support early childhood development. Our research questions are: 1) Does SDP improve key parenting behaviors that promote early childhood development? 2) What are the facilitators and barriers to delivering SDP? 3) How could SDP be changed to improve adoption and sustainability? The aims of the study are both to assess clinical outcomes as well as to gather information for future potential modifications to maximize its implementation in primary care setting that serve low-income communities. Consequently, as we are gathering information regarding both implementation and effectiveness outcomes, a type1 hybrid implementation-effectiveness trial seems like the best choice.

2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?

Yes, we will be incorporating a mixed-methods design into the study. The study will utilize a convergent design to: 1) evaluate the reach, representativeness, adoption feasibility, and degree to which SDP was implemented as intended, and 2) explore causal pathways through which changes in parenting knowledge, self-regulation, observational learning, facilitators and self-efficacy influence key parenting behaviors. We will apply quantitative and qualitative methods to evaluate what extent the various intervention components were delivered as intended in the protocol, especially when conducted by different non-research staff members. Additionally, we will utilize a mixed methodology approach to elucidate facilitators and barriers that aided in staff, administrator, and parental participation in SDP. We will obtain both observational and self-report measures to assess the impact of SDP on key parenting behaviors. Further, we will conduct: 1) semi-structured interviews witha subgroup of participants from the intervention group to elicit positive or negative reactions to receiving the SDP and 2) in-person debriefing interviews with administrators of SDP and clinical staff separately to discuss their experiences with SDP. These interviews will be used to further refine the intervention as well as the implementation strategy.
";s:5:"xhtml";s:2948:"Reshma Shah #4<br />1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />The proposed study design is a type 1 hybrid implementation-effectiveness randomized control. The study works towards addressing the critical need for an accessible and effective population-level strategy to promote evidence-based based parenting behaviors that promote early childhood development. This study seeks to evaluate Sit Down and Play (SDP), a brief, theory-based program delivered during pediatric well-child visits as a promising opportunity with a universal approach to enhance parenting behaviors that support early childhood development. Our research questions are: 1) Does SDP improve key parenting behaviors that promote early childhood development? 2) What are the facilitators and barriers to delivering SDP? 3) How could SDP be changed to improve adoption and sustainability? The aims of the study are both to assess clinical outcomes as well as to gather information for future potential modifications to maximize its implementation in primary care setting that serve low-income communities. Consequently, as we are gathering information regarding both implementation and effectiveness outcomes, a type1 hybrid implementation-effectiveness trial seems like the best choice.<br /><br />2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why?<br /><br />Yes, we will be incorporating a mixed-methods design into the study. The study will utilize a convergent design to: 1) evaluate the reach, representativeness, adoption feasibility, and degree to which SDP was implemented as intended, and 2) explore causal pathways through which changes in parenting knowledge, self-regulation, observational learning, facilitators and self-efficacy influence key parenting behaviors. We will apply quantitative and qualitative methods to evaluate what extent the various intervention components were delivered as intended in the protocol, especially when conducted by different non-research staff members. Additionally, we will utilize a mixed methodology approach to elucidate facilitators and barriers that aided in staff, administrator, and parental participation in SDP. We will obtain both observational and self-report measures to assess the impact of SDP on key parenting behaviors. Further, we will conduct: 1) semi-structured interviews witha subgroup of participants from the intervention group to elicit positive or negative reactions to receiving the SDP and 2) in-person debriefing interviews with administrators of SDP and clinical staff separately to discuss their experiences with SDP. These interviews will be used to further refine the intervention as well as the implementation strategy.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"aa35a4523c56c585af6fe2a66f99368c";}s:4:"show";b:1;s:3:"cid";s:32:"74f8ce165655ad83ed8413b5aed7371a";}s:32:"201dd9ed95a5c21c9b986143d6b69d75";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"kpossemato";s:4:"name";s:14:"Kyle Possemato";s:4:"mail";s:21:"kyle.Possemato@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538771908;}s:3:"raw";s:4121:"Kyle Possemato
Assignment #4:

1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

After familiarizing myself with all the materials for module 4, I believe that a stepped-wedge design is the best fit for my research question.  I want to implement CAPE, a package of evidence-based practices to increase mental healthcare engagement among veterans at risk for suicide, at primary care sites in VISN 2. I will propose to implement CAPE is five primary care sites. Each site will receive a baseline assessment of how many patients who screen positive for PTSD, depression, and alcohol use disorder are engaged in mental health services (this is the primary measure of effectiveness of CAPE).  CAPE will then be implemented in each site sequentially with a new site receiving implementation every six months.  Sites will be randomized to the order they receive implementation.  Data is collected every six months (at the point a new implementation occurs) for four years. 

This design fits well with the research questions for several reasons.  First, it allows baseline data to be collected at each site so changes associated with implementation can be assessed.  Second, a stepped-wedge design also enables sites where implementation has not yet taken place to act as controls. Third, we will be able to measure factors associated with the maintenance of CAPE in the sites randomized to early implementation as CAPE will be implemented in the first two sites for 3-3.5 years. A stepped -wedge design also fits the grant mechanism I will apply for.  The QUERI/ VISN Partnered Evaluation mechanism has strong input for VISN leadership who want CAPE to be implemented in multiple sites with facilitation at each site. VISN leadership was not supportive of some sites receiving less implementation facilitation than other sites. Also, the order in which sites receive CAPE is random, so leadership will not have to determine which sites should get CAPE first. The funding mechanism starts with one-year start-up funds.  This will allow baseline assessment at each site and the first site to receive implementation for 6 months.  The start-up funds can be followed by 3 years of additional of funding to complete the project.  The stepped wedge design also allows the study to have one facilitation team that can provide facilitation as each site sequentially instead of having multiple teams, increasing the feasibility of the project. Measure fatigue and reactivity from repeated measures is a common concern associated with stepped-wedge designs.  This concern will be minimized on the proposed study because the majority of the outcomes are collected through the electronic medical record and therefore do not burden staff or patients. However, statistical analyses will need to account for temporal effects over a three-year implementation cycle. 

2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? 

I am proposing to collect both quantitative and qualitative data.  The primary outcome will be collected quantitatively via indicators from the electronic medical record guide measuring RE-AIM metrics. Qualitative data will be collected through interviewing key stakeholders in the pre- and post-implementation phases on selected CFIR constructs to understand barriers and facilitators of implementation.  Therefore I plan on using a sequential data collection method where qualitative data is following by quantitative data, which is following by qualitative data. However, the qualitative and quantitative data are not answering the same study questions but instead answering different questions in different phases of the study, so I’m not sure if this is a true mixed method design.  The pre-implementation qualitative data will inform the implementation strategy, but will not inform the quantitative aims or data collection in the implementation phase. 
";s:5:"xhtml";s:4174:"Kyle Possemato<br />Assignment #4:<br /><br />1.	What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />After familiarizing myself with all the materials for module 4, I believe that a stepped-wedge design is the best fit for my research question.  I want to implement CAPE, a package of evidence-based practices to increase mental healthcare engagement among veterans at risk for suicide, at primary care sites in VISN 2. I will propose to implement CAPE is five primary care sites. Each site will receive a baseline assessment of how many patients who screen positive for PTSD, depression, and alcohol use disorder are engaged in mental health services (this is the primary measure of effectiveness of CAPE).  CAPE will then be implemented in each site sequentially with a new site receiving implementation every six months.  Sites will be randomized to the order they receive implementation.  Data is collected every six months (at the point a new implementation occurs) for four years. <br /><br />This design fits well with the research questions for several reasons.  First, it allows baseline data to be collected at each site so changes associated with implementation can be assessed.  Second, a stepped-wedge design also enables sites where implementation has not yet taken place to act as controls. Third, we will be able to measure factors associated with the maintenance of CAPE in the sites randomized to early implementation as CAPE will be implemented in the first two sites for 3-3.5 years. A stepped -wedge design also fits the grant mechanism I will apply for.  The QUERI/ VISN Partnered Evaluation mechanism has strong input for VISN leadership who want CAPE to be implemented in multiple sites with facilitation at each site. VISN leadership was not supportive of some sites receiving less implementation facilitation than other sites. Also, the order in which sites receive CAPE is random, so leadership will not have to determine which sites should get CAPE first. The funding mechanism starts with one-year start-up funds.  This will allow baseline assessment at each site and the first site to receive implementation for 6 months.  The start-up funds can be followed by 3 years of additional of funding to complete the project.  The stepped wedge design also allows the study to have one facilitation team that can provide facilitation as each site sequentially instead of having multiple teams, increasing the feasibility of the project. Measure fatigue and reactivity from repeated measures is a common concern associated with stepped-wedge designs.  This concern will be minimized on the proposed study because the majority of the outcomes are collected through the electronic medical record and therefore do not burden staff or patients. However, statistical analyses will need to account for temporal effects over a three-year implementation cycle. <br /><br />2.	Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? <br /><br />I am proposing to collect both quantitative and qualitative data.  The primary outcome will be collected quantitatively via indicators from the electronic medical record guide measuring RE-AIM metrics. Qualitative data will be collected through interviewing key stakeholders in the pre- and post-implementation phases on selected CFIR constructs to understand barriers and facilitators of implementation.  Therefore I plan on using a sequential data collection method where qualitative data is following by quantitative data, which is following by qualitative data. However, the qualitative and quantitative data are not answering the same study questions but instead answering different questions in different phases of the study, so I’m not sure if this is a true mixed method design.  The pre-implementation qualitative data will inform the implementation strategy, but will not inform the quantitative aims or data collection in the implementation phase.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"7ab4d55408bc8927d335777b732e2645";}s:4:"show";b:1;s:3:"cid";s:32:"201dd9ed95a5c21c9b986143d6b69d75";}s:32:"4960c3ad6f772f7ae405ff3d7018f4f2";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"sdang";s:4:"name";s:10:"Stuti Dang";s:4:"mail";s:17:"Stuti.Dang@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1538953467;}s:3:"raw";s:3317:"Dang, Assignment #4 

1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?

My research program entails the implementation and evaluation of a revamped intervention for promoting use of HT in HBPC in VA. 

Following intervention development, we will first be conducting a hybrid type 2 effectiveness-implementation trial, using either a cluster RCT or a stepped wedge cluster RCT.  The clusters maybe assigned by VISN, using 2 VISNs for initial pilot of 8 identified VISNs, and then adding two more every year. Outcome data will be collected at all sites every year. The goals of this RCT are to evaluate the effectiveness of the HT intervention, compared to usual care, and collect preliminary data on implementation barriers and facilitators that may affect future real-world uptake. 

We will conduct a mixed methods process evaluation using the RE-AIM framework and assess intervention adoption, fidelity, and sustainability. Reach will be assessed by calculating the percent of patients approached to participate who choose to participate, how representative of the target population they are, and the numbers excluded and for what reasons. We will interview non-participants and participants for their insights regarding the value of the program and barriers/incentives to participation. Our purpose would be to understand the impact of the implementation strategy and gather information to modify it if needed. This framework, although targeting the implementation, will also inform our findings regarding the comparative effectiveness of the programs. Effectiveness data will also include healthcare utilization, quality of care, and satisfaction. 

We are conducting an effectiveness trial because we are adapting existing evidence-based techniques shown to be effective based on our ongoing work into a new intervention package  A hybrid 2 design I believe would be suited since the intervention as a whole is being proposed for the first time. While we assess effectiveness, it would be important and timely to collect data on implementation barriers and facilitators to inform next steps. 


2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? 

We are currently conducting a mixed methods study that is examining sources of variation in HT use in HBPC across VA HBPCs that integrates a qualitative component (semi-structured interviews with HBPC staff at 10 HBPCs) with a quantitative component (data on teams and outcomes collected from administrative data and from a national survey) to understand contextual features at VA facilities that demonstrate variation in HT use. This preliminary work will serve as the exploratory phase of the proposed implementation study.

In the proposed study as well, I plan to use mixed methods in the proposed trial. For the implementation evaluation especially, I envision gathering both quantitative and qualitative data. This will allow us to explore the characteristics HBPC sites that appear to be associated with implementation success.

(I really apologize for the delay - I was not clear about the design)";s:5:"xhtml";s:3412:"Dang, Assignment #4 <br /><br />1. What is your proposed study design? Why is that the best design to answer your research questions or hypotheses?<br /><br />My research program entails the implementation and evaluation of a revamped intervention for promoting use of HT in HBPC in VA. <br /><br />Following intervention development, we will first be conducting a hybrid type 2 effectiveness-implementation trial, using either a cluster RCT or a stepped wedge cluster RCT.  The clusters maybe assigned by VISN, using 2 VISNs for initial pilot of 8 identified VISNs, and then adding two more every year. Outcome data will be collected at all sites every year. The goals of this RCT are to evaluate the effectiveness of the HT intervention, compared to usual care, and collect preliminary data on implementation barriers and facilitators that may affect future real-world uptake. <br /><br />We will conduct a mixed methods process evaluation using the RE-AIM framework and assess intervention adoption, fidelity, and sustainability. Reach will be assessed by calculating the percent of patients approached to participate who choose to participate, how representative of the target population they are, and the numbers excluded and for what reasons. We will interview non-participants and participants for their insights regarding the value of the program and barriers/incentives to participation. Our purpose would be to understand the impact of the implementation strategy and gather information to modify it if needed. This framework, although targeting the implementation, will also inform our findings regarding the comparative effectiveness of the programs. Effectiveness data will also include healthcare utilization, quality of care, and satisfaction. <br /><br />We are conducting an effectiveness trial because we are adapting existing evidence-based techniques shown to be effective based on our ongoing work into a new intervention package  A hybrid 2 design I believe would be suited since the intervention as a whole is being proposed for the first time. While we assess effectiveness, it would be important and timely to collect data on implementation barriers and facilitators to inform next steps. <br /><br /><br />2. Will you be incorporating a mixed methods design into your study? If so, what approach will you use to incorporate the qualitative data into the study (e.g., integrate the quantitative and qualitative data to inform your aims? If not, why? <br /><br />We are currently conducting a mixed methods study that is examining sources of variation in HT use in HBPC across VA HBPCs that integrates a qualitative component (semi-structured interviews with HBPC staff at 10 HBPCs) with a quantitative component (data on teams and outcomes collected from administrative data and from a national survey) to understand contextual features at VA facilities that demonstrate variation in HT use. This preliminary work will serve as the exploratory phase of the proposed implementation study.<br /><br />In the proposed study as well, I plan to use mixed methods in the proposed trial. For the implementation evaluation especially, I envision gathering both quantitative and qualitative data. This will allow us to explore the characteristics HBPC sites that appear to be associated with implementation success.<br /><br />(I really apologize for the delay - I was not clear about the design)";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"4f2a5ca8e3630fc2dfa568c016fe3981";}s:4:"show";b:1;s:3:"cid";s:32:"4960c3ad6f772f7ae405ff3d7018f4f2";}s:32:"4948e506f6e45b5554e7ca60dafe3a38";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"kgoldstein";s:4:"name";s:15:"Karen Goldstein";s:4:"mail";s:24:"karen.goldstein@duke.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539713370;}s:3:"raw";s:3615:"Goldstein, Assignment #5

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

During the next study, we aim to conduct a type 1 hybrid study. At this time, we will conduct formative evaluation to identify barriers and facilitators to implementation of a combined reciprocal peer support/peer coach program. During this evaluation, we will also explore provider preferences around relay of clinically important information from the peer coaches to the providers. If there is a clear preference for peer coaches to relay information back to providers and/or PACTs, then this strategy may be included during subsequent implementation efforts. 
Because peer support programs hinge on the initiative and participation of patients with each other, identifying ways to recruit and obtain Veteran endorsement and buy-in through involvement of patients will be critical. Preliminary efforts to obtain Veteran input through qualitative interviews with patients and presentations with local Veteran engagement panel have already been informative and shaped the development of the intervention structure to date. 
We will also use strategies designed to promote clinical and administrative staff buy-in specifically 1) building a coalition, 2) identifying program champions and 3) conducting education outreach visits to clinic staff.  Findings from previous peer support work and qualitative interviews with VA clinical staff by author have found one of the most significant concerns about peer support programs among staff is the potential for peer supporters to act outside their intended scope. While this does not appear to be a common problem in practice, it will be important to identify program champions within the organization during coalition building. Education outreach visits is a familiar approach to primary care PACTs and, thus, having peer support program members attend clinic meetings to educate PACT members about a new peer support program will be an easily integrated strategy. In addition, we will promote adaptability to work with relevant staff to promote alignment and programmatic support.
Additional strategies will be considered based on the findings from the initial type 1 hybrid study designed to identify determinants of successful implementation.

2.	How might you link specific implementation strategies to the context in which your work is set?

The chosen strategies will be tailored depending on the concurrent clinical and quality improvement (QI) activities of the clinical setting at the time of implementation. For example, if the ambulatory care service (ACS) line at the given site where the peer support program will be initiated is working on a patient satisfaction QI project, then additional outcomes might be included in evaluation plans to assess patient satisfaction in a manner congruent to the ACS QI team. At initial meetings with the local clinical staff, the peer support project leads will discuss with local partners about current priorities and needs in an effort to identify mutually beneficial opportunities. Similarly, patient oriented strategies could also be tailored to concurrent patient-identified needs and/or interests.

3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?  NA
";s:5:"xhtml";s:3679:"Goldstein, Assignment #5<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />During the next study, we aim to conduct a type 1 hybrid study. At this time, we will conduct formative evaluation to identify barriers and facilitators to implementation of a combined reciprocal peer support/peer coach program. During this evaluation, we will also explore provider preferences around relay of clinically important information from the peer coaches to the providers. If there is a clear preference for peer coaches to relay information back to providers and/or PACTs, then this strategy may be included during subsequent implementation efforts. <br />Because peer support programs hinge on the initiative and participation of patients with each other, identifying ways to recruit and obtain Veteran endorsement and buy-in through involvement of patients will be critical. Preliminary efforts to obtain Veteran input through qualitative interviews with patients and presentations with local Veteran engagement panel have already been informative and shaped the development of the intervention structure to date. <br />We will also use strategies designed to promote clinical and administrative staff buy-in specifically 1) building a coalition, 2) identifying program champions and 3) conducting education outreach visits to clinic staff.  Findings from previous peer support work and qualitative interviews with VA clinical staff by author have found one of the most significant concerns about peer support programs among staff is the potential for peer supporters to act outside their intended scope. While this does not appear to be a common problem in practice, it will be important to identify program champions within the organization during coalition building. Education outreach visits is a familiar approach to primary care PACTs and, thus, having peer support program members attend clinic meetings to educate PACT members about a new peer support program will be an easily integrated strategy. In addition, we will promote adaptability to work with relevant staff to promote alignment and programmatic support.<br />Additional strategies will be considered based on the findings from the initial type 1 hybrid study designed to identify determinants of successful implementation.<br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />The chosen strategies will be tailored depending on the concurrent clinical and quality improvement (QI) activities of the clinical setting at the time of implementation. For example, if the ambulatory care service (ACS) line at the given site where the peer support program will be initiated is working on a patient satisfaction QI project, then additional outcomes might be included in evaluation plans to assess patient satisfaction in a manner congruent to the ACS QI team. At initial meetings with the local clinical staff, the peer support project leads will discuss with local partners about current priorities and needs in an effort to identify mutually beneficial opportunities. Similarly, patient oriented strategies could also be tailored to concurrent patient-identified needs and/or interests.<br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?  NA";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"070cd6514d30045b3f6fcfbf67903634";}s:4:"show";b:1;s:3:"cid";s:32:"4948e506f6e45b5554e7ca60dafe3a38";}s:32:"31c59a18f53db10ee2ae2b9ee96f9c35";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bweiner";s:4:"name";s:12:"Bryan Weiner";s:4:"mail";s:15:"bjweiner@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539721376;}s:3:"raw";s:821:"Sounds like a Hybrid 2 is a good choice. Note that the choice of hybrid design has implications for powering the study and allocating resources for collecting and analyzing data. As I noted in response to the first assignment, stepped wedge designs are increasingly common as investigators are attracted the phased rollout of this design and the fact that all sites get the intervention/implementation strategy. However, stepped wedge designs can be challenging to pull off and they lose statistical power if the length of the steps is not properly calibrated and the outcome data are not collected at each step (i.e., if the design is incomplete). Before committing yourself to this design, consider carefully the pros and cons. 

Kudos for specifying the "mixing" procedure you will employ in your mixed methods design.";s:5:"xhtml";s:841:"Sounds like a Hybrid 2 is a good choice. Note that the choice of hybrid design has implications for powering the study and allocating resources for collecting and analyzing data. As I noted in response to the first assignment, stepped wedge designs are increasingly common as investigators are attracted the phased rollout of this design and the fact that all sites get the intervention/implementation strategy. However, stepped wedge designs can be challenging to pull off and they lose statistical power if the length of the steps is not properly calibrated and the outcome data are not collected at each step (i.e., if the design is incomplete). Before committing yourself to this design, consider carefully the pros and cons. <br /><br />Kudos for specifying the &quot;mixing&quot; procedure you will employ in your mixed methods design.";s:6:"parent";s:32:"3e7d7a2339ef69b128f4bf13cd26b28c";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"31c59a18f53db10ee2ae2b9ee96f9c35";}s:32:"7ab4d55408bc8927d335777b732e2645";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bweiner";s:4:"name";s:12:"Bryan Weiner";s:4:"mail";s:15:"bjweiner@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539721625;}s:3:"raw";s:159:"Very thoughtful rationale for employing a stepped wedge design. And kudos for specifying the "mixing" procedure for the mixed-methods component of your study. ";s:5:"xhtml";s:168:"Very thoughtful rationale for employing a stepped wedge design. And kudos for specifying the &quot;mixing&quot; procedure for the mixed-methods component of your study.";s:6:"parent";s:32:"201dd9ed95a5c21c9b986143d6b69d75";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"7ab4d55408bc8927d335777b732e2645";}s:32:"aa35a4523c56c585af6fe2a66f99368c";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"mengelgau";s:4:"name";s:13:"Mike Engelgau";s:4:"mail";s:24:"michael.engelgau@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539789080;}s:3:"raw";s:1091:"Thanks Reshma for your thoughtful response and my regrets for being a bit late with my reply. You have selected a good study design for this effort. I appreciate your mentioning assessing causality and this is always what we are striving for. Also, what may be helpful is to think about logic models that can map out the various inputs and expected outputs along the way that can help flesh out each step and what was accomplished/achieved. In a sense you can thinks of these as micro steps in the over all causality. Your mixed methods assessments than can understand more about why/why not this step did not occur/was not acceptable/not sustainable, etc. It will also be important to keep you outcome focus on the parenting behaviors which in a sense is an intermediate outcome (ie a step in the logic model) that is necessary to achieve improved early childhood development. The mixed methods assessments should allow you to see at which micro step in the model was problematic and allow much knowledge gained about the intervention, even if it is not successful. Keep up the good work! 
";s:5:"xhtml";s:1089:"Thanks Reshma for your thoughtful response and my regrets for being a bit late with my reply. You have selected a good study design for this effort. I appreciate your mentioning assessing causality and this is always what we are striving for. Also, what may be helpful is to think about logic models that can map out the various inputs and expected outputs along the way that can help flesh out each step and what was accomplished/achieved. In a sense you can thinks of these as micro steps in the over all causality. Your mixed methods assessments than can understand more about why/why not this step did not occur/was not acceptable/not sustainable, etc. It will also be important to keep you outcome focus on the parenting behaviors which in a sense is an intermediate outcome (ie a step in the logic model) that is necessary to achieve improved early childhood development. The mixed methods assessments should allow you to see at which micro step in the model was problematic and allow much knowledge gained about the intervention, even if it is not successful. Keep up the good work!";s:6:"parent";s:32:"74f8ce165655ad83ed8413b5aed7371a";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"aa35a4523c56c585af6fe2a66f99368c";}s:32:"715a666bfcb673632cb00eec602a686e";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"mengelgau";s:4:"name";s:13:"Mike Engelgau";s:4:"mail";s:24:"michael.engelgau@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539790096;}s:3:"raw";s:1299:"Thanks Ana for your thoughtful responses to the questions. Am sorry for being a bit late in my responses.A couple items to consider. Would try to capture who those with positive screens who actually get referred and treated. You may be including that but I would like to reinforce it here. Far too many screening programs do not consider the end of the process where treatment is given and long term outcomes are improved. When you don't have easily retrieved data, I agree with your approach to do a sample. In addition, if you sample is not serving the purpose of capturing the needed data, you can expand it some to make sure if will answer you queries. In terms of you reach assessment, you are proposing basically an interrupted time series that will compare rates of screening before and after the intervention. You will need to consider how far back you will want to collect baseline data before the intervention. If the rates are unstable you will want to collect a longer time period. Finally, keep in mind your stepped wedge design will have different durations of exposures to the intervention and duration of the intervention may prove very important in terms of the site learning curve and adaptation. You should make sure you are able to capture this in the mixed methods assessments. ";s:5:"xhtml";s:1303:"Thanks Ana for your thoughtful responses to the questions. Am sorry for being a bit late in my responses.A couple items to consider. Would try to capture who those with positive screens who actually get referred and treated. You may be including that but I would like to reinforce it here. Far too many screening programs do not consider the end of the process where treatment is given and long term outcomes are improved. When you don&#039;t have easily retrieved data, I agree with your approach to do a sample. In addition, if you sample is not serving the purpose of capturing the needed data, you can expand it some to make sure if will answer you queries. In terms of you reach assessment, you are proposing basically an interrupted time series that will compare rates of screening before and after the intervention. You will need to consider how far back you will want to collect baseline data before the intervention. If the rates are unstable you will want to collect a longer time period. Finally, keep in mind your stepped wedge design will have different durations of exposures to the intervention and duration of the intervention may prove very important in terms of the site learning curve and adaptation. You should make sure you are able to capture this in the mixed methods assessments.";s:6:"parent";s:32:"be07e2162f61afb0869e752183aebcc8";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"715a666bfcb673632cb00eec602a686e";}s:32:"c2f97288583a7b55a71a2fae65139bb8";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"elattie";s:4:"name";s:12:"Emily Lattie";s:4:"mail";s:29:"emily.lattie@northwestern.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539883825;}s:3:"raw";s:1621:"Lattie, Assignment #5

1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?

Specific implementation strategies for the proposed study will include interactive assistance strategies, including the provision of technical assistance and clinical supervision, educational strategies, including the development of training materials and resources, and strategies aimed at supporting clinicians, such as programming user-friendly reminders into the computerized system. Prior to the onset of the proposed study, there has been significant work put into developing stakeholder relationships and these efforts will continue throughout the study. The specific implementation strategies have been selected based on known barriers to the implementation of similar programs. 

2.	How might you link specific implementation strategies to the context in which your work is set?

Because there will likely be other ongoing innovations occurring within the primary care practices of study, these strategies will be examined in conjunction with knowledge of concurrent efforts and will be modified as needed to minimize burden on clinical and administrative staff. 

3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? 

Not applicable to my proposed study. 
";s:5:"xhtml";s:1679:"Lattie, Assignment #5<br /><br />1.	If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br /><br />Specific implementation strategies for the proposed study will include interactive assistance strategies, including the provision of technical assistance and clinical supervision, educational strategies, including the development of training materials and resources, and strategies aimed at supporting clinicians, such as programming user-friendly reminders into the computerized system. Prior to the onset of the proposed study, there has been significant work put into developing stakeholder relationships and these efforts will continue throughout the study. The specific implementation strategies have been selected based on known barriers to the implementation of similar programs. <br /><br />2.	How might you link specific implementation strategies to the context in which your work is set?<br /><br />Because there will likely be other ongoing innovations occurring within the primary care practices of study, these strategies will be examined in conjunction with knowledge of concurrent efforts and will be modified as needed to minimize burden on clinical and administrative staff. <br /><br />3.	If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? <br /><br />Not applicable to my proposed study.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"00096acaa38abe565b82d02b287fa351";}s:4:"show";b:1;s:3:"cid";s:32:"c2f97288583a7b55a71a2fae65139bb8";}s:32:"f291e4dc1d6fad5b9b9bf63ff1c0680e";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"rshah";s:4:"name";s:11:"Reshma Shah";s:4:"mail";s:16:"reshmamd@UIC.EDU";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:2:{s:7:"created";i:1539884586;s:8:"modified";i:1539884633;}s:3:"raw";s:4543:"Shah, Assignment #5

The proposed study design is a type 1 hybrid implementation-effectiveness randomized control. The study works towards addressing the critical need for an accessible and effective population-level strategy to promote evidence-based based parenting behaviors that promote early childhood development. This study will seek to evaluate Sit Down and Play (SDP), a brief, theory-based program delivered during pediatric well-child visits as a promising opportunity with a universal approach to enhance parenting behaviors that support early childhood development. We will concurrently work to address the challenges of “lag time” between intervention development and translation into a usable and sustainable clinical practice through the following implementation strategies:

1.  Evaluation and Iterative Strategies
a. Identify barriers and facilitators- Key informant interviews will be conducted with clinicians, clinical staff (e.g., clinic administrators), and payer stakeholders to identify factors that may facilitate or impede the acceptability and sustainability of the intervention from the perspective of a primary care operational standpoint and identify strategies to maximize its acceptability and sustainability. We will also conduct focus groups and/or semi-structured with parents who received the program to identify barriers and facilitators to uptake of the program, usability, and acceptability. Lastly, we will conduct semi-structured interviews with administrators of the program to assess barriers and facilitators to training, level of preparedness, and reaction to fidelity checklists. We will conduct focus groups with the clinic where we are currently evaluating Sit Down and Play (i.e., UI Health) but will also conduct focus groups with clinicians, administrators and parents from local Federally Qualified Health Centers (FQHCs) where the goal will be to also implement Sit Down and Play and whose infrastructure may be very different from a university-based, medical school affiliated clinic.

2. Adapting and Tailoring to Context
a. Tailor Strategies- As suggested by Powel et al (2015),  we will tailor our implementation strategy based upon the strengths, barriers, and facilitators identified in the above data collection.  
b. Promote adaptability- Will identify ways SDP can be tailored to meet FQHCs’  needs while preserving content fidelity
c. Work with educational institutions- Consider the use of volunteer undergraduates for delivery of the program

3. Train and Educate Stakeholders
a. Develop educational materials- A training manual that will include fidelity checklists will be developed for administrators of SDP and supervisors at UI Health and FQHCs. 
b. Conduct educational meetings- We will conduct meeting with clinic administrators, physicians, and SDP administrators to discuss the program. Recognizing the objectives will be different for each group, these meetings will be held separately for each group.


4. Engage Consumers
a. Intervene with patients/consumers to enhance uptake and adherence. Based upon results from our evaluation, we will develop strategies with families to encourage and problem solve around ways to make the message of SDP more effective (e.g., text message reminders, social media usage)

5. Use Financial Strategies
a. Access new funding- Work with organizational fundraising programs (e.g., hospital or University-based fundraising programs) to provide toys for the program
 

2. How might you link specific implementation strategies to the context in which your work is set?
Our goal is to implement SDP at primary care clinics that serve largely low-income communities. We have conducted feasibility studies of delivering SDP at UI Health, a hospital-based clinic that serves primarily low-income families. Consequently, the implementation strategies were selected to address common facilitators (e.g., university affiliation, undergraduates) and barriers that encompass not only UI Health, but other FQHCs and university-based primary care clinics, which are settings that we aim to deliver a usable, sustainable, and effective primary care-based program to promote key parenting behaviors.

 
3. If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? 

N/A

";s:5:"xhtml";s:4691:"Shah, Assignment #5<br /><br />The proposed study design is a type 1 hybrid implementation-effectiveness randomized control. The study works towards addressing the critical need for an accessible and effective population-level strategy to promote evidence-based based parenting behaviors that promote early childhood development. This study will seek to evaluate Sit Down and Play (SDP), a brief, theory-based program delivered during pediatric well-child visits as a promising opportunity with a universal approach to enhance parenting behaviors that support early childhood development. We will concurrently work to address the challenges of “lag time” between intervention development and translation into a usable and sustainable clinical practice through the following implementation strategies:<br /><br />1.  Evaluation and Iterative Strategies<br />a. Identify barriers and facilitators- Key informant interviews will be conducted with clinicians, clinical staff (e.g., clinic administrators), and payer stakeholders to identify factors that may facilitate or impede the acceptability and sustainability of the intervention from the perspective of a primary care operational standpoint and identify strategies to maximize its acceptability and sustainability. We will also conduct focus groups and/or semi-structured with parents who received the program to identify barriers and facilitators to uptake of the program, usability, and acceptability. Lastly, we will conduct semi-structured interviews with administrators of the program to assess barriers and facilitators to training, level of preparedness, and reaction to fidelity checklists. We will conduct focus groups with the clinic where we are currently evaluating Sit Down and Play (i.e., UI Health) but will also conduct focus groups with clinicians, administrators and parents from local Federally Qualified Health Centers (FQHCs) where the goal will be to also implement Sit Down and Play and whose infrastructure may be very different from a university-based, medical school affiliated clinic.<br /><br />2. Adapting and Tailoring to Context<br />a. Tailor Strategies- As suggested by Powel et al (2015),  we will tailor our implementation strategy based upon the strengths, barriers, and facilitators identified in the above data collection.  <br />b. Promote adaptability- Will identify ways SDP can be tailored to meet FQHCs’  needs while preserving content fidelity<br />c. Work with educational institutions- Consider the use of volunteer undergraduates for delivery of the program<br /><br />3. Train and Educate Stakeholders<br />a. Develop educational materials- A training manual that will include fidelity checklists will be developed for administrators of SDP and supervisors at UI Health and FQHCs. <br />b. Conduct educational meetings- We will conduct meeting with clinic administrators, physicians, and SDP administrators to discuss the program. Recognizing the objectives will be different for each group, these meetings will be held separately for each group.<br /><br /><br />4. Engage Consumers<br />a. Intervene with patients/consumers to enhance uptake and adherence. Based upon results from our evaluation, we will develop strategies with families to encourage and problem solve around ways to make the message of SDP more effective (e.g., text message reminders, social media usage)<br /><br />5. Use Financial Strategies<br />a. Access new funding- Work with organizational fundraising programs (e.g., hospital or University-based fundraising programs) to provide toys for the program<br /> <br /><br />2. How might you link specific implementation strategies to the context in which your work is set?<br />Our goal is to implement SDP at primary care clinics that serve largely low-income communities. We have conducted feasibility studies of delivering SDP at UI Health, a hospital-based clinic that serves primarily low-income families. Consequently, the implementation strategies were selected to address common facilitators (e.g., university affiliation, undergraduates) and barriers that encompass not only UI Health, but other FQHCs and university-based primary care clinics, which are settings that we aim to deliver a usable, sustainable, and effective primary care-based program to promote key parenting behaviors.<br /><br /> <br />3. If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? <br /><br />N/A";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"2dd7cec8da052dd8981c4de074a1d6dd";}s:4:"show";b:1;s:3:"cid";s:32:"f291e4dc1d6fad5b9b9bf63ff1c0680e";}s:32:"d0092ee21c581059a2c1eda4174b8c22";a:8:{s:4:"user";a:5:{s:2:"id";s:5:"sdang";s:4:"name";s:10:"Stuti Dang";s:4:"mail";s:17:"Stuti.Dang@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539956518;}s:3:"raw";s:4039:"Assignment #5

1.If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?
During the initial part of this study, we will conduct formative evaluation to identify barriers and facilitators to implementation of HT in HBPC. During this evaluation, we will also explore HBPC and HT provider and HBPC patient preferences about what they think would encourage them to use HT, and how information exchange should occur between the two programs. If there is a clear option that is preferred, then this strategy may be helpful for subsequent implementation efforts.

We will also use strategies designed to promote HBPC and HT staff buy-in, specifically 1) educational strategies for both programs to learn more about the other, 2) identifying a program champion in each HBPC and HT program, 3) gathering data to impress the added value of HT in HBPC, 4) identify strategies for timely efficient information exchange between both programs, and 5). Provide a pilot screening tool to identify appropriate patients for HT.  Education for HBPC and HT members about both programs including the development of training materials and resources via TMS, and setting up agreements about information sharing will be key. In addition, setting up a specific day annually where they can get to know each other might be a worthwhile strategy. In addition, we will promote having a HBPC champion at each site who leads the effort to work with the HT program. This person will be the communication liaison between programs, and will be expected to attend the HBPC team meetings when common patients are being discussed. 

The specific implementation strategies have been selected based on known barriers to the implementation from previous work and qualitative interviews with HBPC clinical staff by author. We have found one of the most significant concerns about HT use among HBPC staff is the lack of timely information exchange and communication between the two programs, usability concerns about the patients’ ability to use HT, and perceived lack of utility for HT in an already closely case managed HBPC program. It will be important to identify program champions and provide education to both programs about their complementary roles, set up good practices for information sharing, and gather evidence to support HT use in HBPC patients. We have identified that the HBPC staff do not feel knowledgeable about HT devices which they find challenging because they are the ones who have to troubleshoot any equipment issues since they are in the veterans’ homes.  

Additional strategies will be considered based on the initial findings from the hybrid study designed to identify key drivers and barriers of successful implementation.

2.How might you link specific implementation strategies to the context in which your work is set?

The implementation strategies will be tailored depending on the needs and environment of each HBPC and their relationship with the HT program. For example, based on our previously gathered data, if both programs are under the same service line, then they tend to use HT more. We will have to make accommodations for that at different locations. At initial meetings with the local HBPC and HT staff, we will encourage discussion of current priorities and needs in order to identify mutually beneficial opportunities. As an example, recruiting and retaining patients is a performance measure for HT programs, and reducing healthcare utilization is a performance measure for both programs. Similarly, strategies to identify appropriate patients could also be tailored to concurrent program needs.

3. If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? NA
";s:5:"xhtml";s:4113:"Assignment #5<br /><br />1.If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them?<br />During the initial part of this study, we will conduct formative evaluation to identify barriers and facilitators to implementation of HT in HBPC. During this evaluation, we will also explore HBPC and HT provider and HBPC patient preferences about what they think would encourage them to use HT, and how information exchange should occur between the two programs. If there is a clear option that is preferred, then this strategy may be helpful for subsequent implementation efforts.<br /><br />We will also use strategies designed to promote HBPC and HT staff buy-in, specifically 1) educational strategies for both programs to learn more about the other, 2) identifying a program champion in each HBPC and HT program, 3) gathering data to impress the added value of HT in HBPC, 4) identify strategies for timely efficient information exchange between both programs, and 5). Provide a pilot screening tool to identify appropriate patients for HT.  Education for HBPC and HT members about both programs including the development of training materials and resources via TMS, and setting up agreements about information sharing will be key. In addition, setting up a specific day annually where they can get to know each other might be a worthwhile strategy. In addition, we will promote having a HBPC champion at each site who leads the effort to work with the HT program. This person will be the communication liaison between programs, and will be expected to attend the HBPC team meetings when common patients are being discussed. <br /><br />The specific implementation strategies have been selected based on known barriers to the implementation from previous work and qualitative interviews with HBPC clinical staff by author. We have found one of the most significant concerns about HT use among HBPC staff is the lack of timely information exchange and communication between the two programs, usability concerns about the patients’ ability to use HT, and perceived lack of utility for HT in an already closely case managed HBPC program. It will be important to identify program champions and provide education to both programs about their complementary roles, set up good practices for information sharing, and gather evidence to support HT use in HBPC patients. We have identified that the HBPC staff do not feel knowledgeable about HT devices which they find challenging because they are the ones who have to troubleshoot any equipment issues since they are in the veterans’ homes.  <br /><br />Additional strategies will be considered based on the initial findings from the hybrid study designed to identify key drivers and barriers of successful implementation.<br /><br />2.How might you link specific implementation strategies to the context in which your work is set?<br /><br />The implementation strategies will be tailored depending on the needs and environment of each HBPC and their relationship with the HT program. For example, based on our previously gathered data, if both programs are under the same service line, then they tend to use HT more. We will have to make accommodations for that at different locations. At initial meetings with the local HBPC and HT staff, we will encourage discussion of current priorities and needs in order to identify mutually beneficial opportunities. As an example, recruiting and retaining patients is a performance measure for HT programs, and reducing healthcare utilization is a performance measure for both programs. Similarly, strategies to identify appropriate patients could also be tailored to concurrent program needs.<br /><br />3. If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content? NA";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"6c9864ee2320a0f60d0cf8e7296811e1";}s:4:"show";b:1;s:3:"cid";s:32:"d0092ee21c581059a2c1eda4174b8c22";}s:32:"bf3d6c99cd9b98b8e3e42245351d7a66";a:8:{s:4:"user";a:5:{s:2:"id";s:10:"kpossemato";s:4:"name";s:14:"Kyle Possemato";s:4:"mail";s:21:"kyle.Possemato@va.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1539984000;}s:3:"raw";s:3311:"Possemato Assignment #5:
If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them? How might you link specific implementation strategies to the context in which your work is set?

I plan to implement CAPE using the Replicating Effective Programs (REP) framework. REP is an empirically-validated framework for implementing interventions into clinical practice through a combination of intervention packaging, training, technical assistance, and other strategies (Kilbourne et al., 2007).  REP includes four phases.  Due to the use of a stepped-wedge design, the four REP phases will be happening at different times at each site. In the pre-condition phase we will identify site-specific barriers and facilitators to implementation (guided by CFIR) and draft CAPE package materials based on these findings. In the pre-implementation phase we work with a local stakeholder group to refine and locally adapt CAPE package materials.  Stakeholder group members will include local leadership in primary care and mental health, frontline primary care and PCMHI providers, and veteran primary care patients. In the Implementation phase we will train frontline primary care and mental health providers in CAPE and provide twice monthly technical assistance (TA) phones calls.  TA calls will be conducted by trained TA experts who will document implementation progress at the site and advise clinicians on how to maintain fidelity. The primary outcomes of CAPE (e.g., rates of engagement in mental health care) will also be vetted with stakeholder group in the Implementation phase. In the final phase of maintenance and evolution, the stakeholder groups will advise on sustainability strategies. 

I choose REP for several reasons.   First, REP emphasizes achieving a balance between intervention fidelity and flexibility at the local level to maximize the likelihood of successful implementation. Second, REP is explained in the literature in detail making it possible to apply to new interventions and settings. Third, the components of REP provide a framework to address the anticipated challenges to implementing CAPE, such as the need for provider education and continued support (TA) at each site and creating stakeholders buy-in through the use of stakeholder groups. Finally, REP is evidenced-based in that it has been effectively used to implement a variety of health services programs.  REP incorporates most of the nine categories of implementation strategies described by Waltz et al. (2105) including evaluation strategies, interactive assistance, adapting to context, developing stakeholder relationships, training stakeholders, and supporting clinicians. 

The components of REP are linked to the context of local primary care clinics in several ways.  Individual site barriers and facilitators will be assessed, then addressed in the tailored intervention package. Also, a local stakeholder group will help guide implementation through all REP phases.  Stakeholders will work with research staff to adapt the CAPE materials in the pre-implementation phase, provide feedback on evaluation results in the implementation phase, and provide input on sustainability strategies in the maintenance and evaluation phase.  
";s:5:"xhtml";s:3343:"Possemato Assignment #5:<br />If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them? How might you link specific implementation strategies to the context in which your work is set?<br /><br />I plan to implement CAPE using the Replicating Effective Programs (REP) framework. REP is an empirically-validated framework for implementing interventions into clinical practice through a combination of intervention packaging, training, technical assistance, and other strategies (Kilbourne et al., 2007).  REP includes four phases.  Due to the use of a stepped-wedge design, the four REP phases will be happening at different times at each site. In the pre-condition phase we will identify site-specific barriers and facilitators to implementation (guided by CFIR) and draft CAPE package materials based on these findings. In the pre-implementation phase we work with a local stakeholder group to refine and locally adapt CAPE package materials.  Stakeholder group members will include local leadership in primary care and mental health, frontline primary care and PCMHI providers, and veteran primary care patients. In the Implementation phase we will train frontline primary care and mental health providers in CAPE and provide twice monthly technical assistance (TA) phones calls.  TA calls will be conducted by trained TA experts who will document implementation progress at the site and advise clinicians on how to maintain fidelity. The primary outcomes of CAPE (e.g., rates of engagement in mental health care) will also be vetted with stakeholder group in the Implementation phase. In the final phase of maintenance and evolution, the stakeholder groups will advise on sustainability strategies. <br /><br />I choose REP for several reasons.   First, REP emphasizes achieving a balance between intervention fidelity and flexibility at the local level to maximize the likelihood of successful implementation. Second, REP is explained in the literature in detail making it possible to apply to new interventions and settings. Third, the components of REP provide a framework to address the anticipated challenges to implementing CAPE, such as the need for provider education and continued support (TA) at each site and creating stakeholders buy-in through the use of stakeholder groups. Finally, REP is evidenced-based in that it has been effectively used to implement a variety of health services programs.  REP incorporates most of the nine categories of implementation strategies described by Waltz et al. (2105) including evaluation strategies, interactive assistance, adapting to context, developing stakeholder relationships, training stakeholders, and supporting clinicians. <br /><br />The components of REP are linked to the context of local primary care clinics in several ways.  Individual site barriers and facilitators will be assessed, then addressed in the tailored intervention package. Also, a local stakeholder group will help guide implementation through all REP phases.  Stakeholders will work with research staff to adapt the CAPE materials in the pre-implementation phase, provide feedback on evaluation results in the implementation phase, and provide input on sustainability strategies in the maintenance and evaluation phase.";s:6:"parent";N;s:7:"replies";a:1:{i:0;s:32:"32d63c39ffde504454c20d9d3b237f88";}s:4:"show";b:1;s:3:"cid";s:32:"bf3d6c99cd9b98b8e3e42245351d7a66";}s:32:"59c42dafcb49f8341700674f9109ae87";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"acarvalho";s:4:"name";s:22:"Ana Bastos de Carvalho";s:4:"mail";s:14:"aba253@uky.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540034868;}s:3:"raw";s:6005:"Bastos de Carvalho, Ana
Assignment #5

Acronyms list:
TDRS -Telemedicine diabetic retinopathy screening
PCCs - Primary Care Clinics

1. If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them? 

In our current project, we are working on identifying barriers and facilitators to TDRS integration in the PCCs that already have this instrument but are underperforming in terms of patients screened (screening is not offered to most patients). We are using process observation, key informant interviews, and structured surveys to assess these barriers/facilitators. Once these have been identified, we will define a specific set of integration strategies to use, ensuring there is no mismatch between the barriers/facilitators identified and the strategies chosen.
For the purpose of this module, below I will use our preliminary data to define the implementation strategies. 

Based on our previous data we have identified several determinants to the use of TDRS:
- For Staff: the barriers are 1) lack of education, and 2) negative pressure from Providers; and the facilitators are 1) positive pressure from Leaders (Staff director and clinic director).
- For Providers: the barriers are 1) time constraints, and 2) lack of motivation; and the facilitators are 1) feedback on performance, and 2) positive pressure from Leaders (Clinic director).
- We also found that positive pressure from leaders could be furthered if billing for TDRS was possible and easy (Currently, billing for TDRS in Kentucky is very complex and, due to this, most of our centers opted to not bill for it. This means there is no financial incentive for the execution of these exams.).

Taking this in consideration, and after meeting and discussing with the study's Community Advisory Board, we developed a multi-component implementation strategy, by selecting strategies from the ERIC project (Powell et al, 2015) to address these determinants. The components of the implementation strategy are below.

Implementation Strategy components names and definitions:
- Conduct ongoing training via educational meetings (Train/educate stakeholders) - through training sessions designed specifically for staff who order/execute the TDRS exam. These sessions will provide empowerment and education in the importance of diabetic retinopathy screening, diagnosis and treatment, and the social, health and economic impact of blindness from diabetes.
- Distribute educational materials (Train/educate stakeholders)- through online videos and printed instructional material designed for the staff.
- Involve CEOs, CMOs, Clinic Directors, and Staff Directors (Obtaining executive support) - through engagement with these professionals via one-on-one meetings with the Research team.
- Audit and provide feedback (Evaluation/Iterative strategies) - performance audits (number of exams performed per staff or clinician) will be conducted. Written feedback reports regarding professionals' performance compared to institutional goals and diabetes quality measures will be provided to both clinicians/staff and clinic executives.
- Develop and implement effective billing procedures for TDRS - we will work with the state's Medicare agency (the main barrier agency) to identify procedures used in other states and apply these same procedures in Kentucky.

Implementation strategy operationalization:
- Actors: Researchers who developed the integration strategies, and clinic directors.
- Actions: Provide educational materials and resources, audit and feedback, identify and set up billing method for TDRS.
- Target(s) of the action: Staff, Providers, Clinic executives
- Temporality: Clinic executives contacted 1 month prior to offering program. Staff and Providers contacted 2 weeks prior to offering the program.
- Dose:  Training sessions will be held once at the beginning of the program, and educational materials will be distributed quarterly. Audit and feedback will be done once per trimester. Executives will be engaged with 1-2 meetings/year. The targets can reach out to research team liaison as needed.
- Implementation outcomes affected: Reach (how many staff/clinicians participate), Efficacy (rates of screening pre- and post-implementation strategy), Adoption (assess whether the strategy has been adopted in each site, perceptions on feasibility and quality of the program), Implementation (fidelity of the EBI), and Maintenance/Sustainability (is the EBI still offered (and with what reach, efficacy and adoption) at 1- and 3-years post-implementation).
- Justification: previous assessment of barriers and facilitators


2. How might you link specific implementation strategies to the context in which your work is set?

This multi-component implementation strategy will be used in primary care clinics equipped with TDRS cameras. Further, it will be geared towards the professionals involved in requesting/performing/following up on this exam, namely the providers and the clinical staff. 

Each of the strategy components will be designed specifically to fit into the daily practice of primary care clinics and to target the professional category it's meant to impact (ie, some components will target staff, others will target clinicians). We expect to establish a good link between the integration strategy and the context because our strategy design is based on determinants identified in this context (by the targets of the action), and because we have engaged a community advisory board - composed of stakeholders from these clinics -, who have contributed to this design with significant input.


3. If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?  N/A";s:5:"xhtml";s:6240:"Bastos de Carvalho, Ana<br />Assignment #5<br /><br />Acronyms list:<br />TDRS -Telemedicine diabetic retinopathy screening<br />PCCs - Primary Care Clinics<br /><br />1. If relevant, what are the specific implementation strategies that you will be focusing on in your proposed research and how have you selected them? <br /><br />In our current project, we are working on identifying barriers and facilitators to TDRS integration in the PCCs that already have this instrument but are underperforming in terms of patients screened (screening is not offered to most patients). We are using process observation, key informant interviews, and structured surveys to assess these barriers/facilitators. Once these have been identified, we will define a specific set of integration strategies to use, ensuring there is no mismatch between the barriers/facilitators identified and the strategies chosen.<br />For the purpose of this module, below I will use our preliminary data to define the implementation strategies. <br /><br />Based on our previous data we have identified several determinants to the use of TDRS:<br />- For Staff: the barriers are 1) lack of education, and 2) negative pressure from Providers; and the facilitators are 1) positive pressure from Leaders (Staff director and clinic director).<br />- For Providers: the barriers are 1) time constraints, and 2) lack of motivation; and the facilitators are 1) feedback on performance, and 2) positive pressure from Leaders (Clinic director).<br />- We also found that positive pressure from leaders could be furthered if billing for TDRS was possible and easy (Currently, billing for TDRS in Kentucky is very complex and, due to this, most of our centers opted to not bill for it. This means there is no financial incentive for the execution of these exams.).<br /><br />Taking this in consideration, and after meeting and discussing with the study&#039;s Community Advisory Board, we developed a multi-component implementation strategy, by selecting strategies from the ERIC project (Powell et al, 2015) to address these determinants. The components of the implementation strategy are below.<br /><br />Implementation Strategy components names and definitions:<br />- Conduct ongoing training via educational meetings (Train/educate stakeholders) - through training sessions designed specifically for staff who order/execute the TDRS exam. These sessions will provide empowerment and education in the importance of diabetic retinopathy screening, diagnosis and treatment, and the social, health and economic impact of blindness from diabetes.<br />- Distribute educational materials (Train/educate stakeholders)- through online videos and printed instructional material designed for the staff.<br />- Involve CEOs, CMOs, Clinic Directors, and Staff Directors (Obtaining executive support) - through engagement with these professionals via one-on-one meetings with the Research team.<br />- Audit and provide feedback (Evaluation/Iterative strategies) - performance audits (number of exams performed per staff or clinician) will be conducted. Written feedback reports regarding professionals&#039; performance compared to institutional goals and diabetes quality measures will be provided to both clinicians/staff and clinic executives.<br />- Develop and implement effective billing procedures for TDRS - we will work with the state&#039;s Medicare agency (the main barrier agency) to identify procedures used in other states and apply these same procedures in Kentucky.<br /><br />Implementation strategy operationalization:<br />- Actors: Researchers who developed the integration strategies, and clinic directors.<br />- Actions: Provide educational materials and resources, audit and feedback, identify and set up billing method for TDRS.<br />- Target(s) of the action: Staff, Providers, Clinic executives<br />- Temporality: Clinic executives contacted 1 month prior to offering program. Staff and Providers contacted 2 weeks prior to offering the program.<br />- Dose:  Training sessions will be held once at the beginning of the program, and educational materials will be distributed quarterly. Audit and feedback will be done once per trimester. Executives will be engaged with 1-2 meetings/year. The targets can reach out to research team liaison as needed.<br />- Implementation outcomes affected: Reach (how many staff/clinicians participate), Efficacy (rates of screening pre- and post-implementation strategy), Adoption (assess whether the strategy has been adopted in each site, perceptions on feasibility and quality of the program), Implementation (fidelity of the EBI), and Maintenance/Sustainability (is the EBI still offered (and with what reach, efficacy and adoption) at 1- and 3-years post-implementation).<br />- Justification: previous assessment of barriers and facilitators<br /><br /><br />2. How might you link specific implementation strategies to the context in which your work is set?<br /><br />This multi-component implementation strategy will be used in primary care clinics equipped with TDRS cameras. Further, it will be geared towards the professionals involved in requesting/performing/following up on this exam, namely the providers and the clinical staff. <br /><br />Each of the strategy components will be designed specifically to fit into the daily practice of primary care clinics and to target the professional category it&#039;s meant to impact (ie, some components will target staff, others will target clinicians). We expect to establish a good link between the integration strategy and the context because our strategy design is based on determinants identified in this context (by the targets of the action), and because we have engaged a community advisory board - composed of stakeholders from these clinics -, who have contributed to this design with significant input.<br /><br /><br />3. If your proposed study does not involve selecting implementational strategies but you are evaluating the outcomes of exposure to a program or policy implemented by others (e.g., natural experiment), how will you measure key differences in implementation variation such as by time, population, setting/location, dose or, content?  N/A";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"59c42dafcb49f8341700674f9109ae87";}s:32:"d8922d94729a6c63bacfa9bb06a0b45a";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"mengelgau";s:4:"name";s:13:"Mike Engelgau";s:4:"mail";s:24:"michael.engelgau@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540502910;}s:3:"raw";s:1657:"A critical bottle neck for your intervention is the billing and the challenge that providers have with it. There are several layers that you would want to understand. Why is it difficult and to whom – if could be providers, clinic staff, billing staff, etc. You might also want to understand the challenges for the collecting agency – and understand the barriers on their side too. Low incentive in the clinic would affect all expect for the patient. 
You may also want to consider other barriers and facilitators. Among the ones you have listed for staff their might be addition challenges to consider than what you have listed – is may be difficult to connect to the telemedicine system and there may be limitations when it is available, do other compete to use it all during the same time frame, is there only one or limited number of sites. On way to understand the “flow” would be to directly observe the procedure from start to finish (when the patient arrives to clinic until they leave). 
Providers you may want to also think about more about. Time constraints is a big one that you have noted. However, there may be others, including examine rooms, their role in the procedure, distance from their exam room to the eye exam room, limitations in the availability to the service, duration until they get the results and how they connect the results with a patient, etc. Again watching the process would be very helpful and/or interview providers that get a lot or exams (the high performers) and the those who do not get many/any exams done (low performers). This information may really drive you intervention approach. 
Hope this is useful";s:5:"xhtml";s:1672:"A critical bottle neck for your intervention is the billing and the challenge that providers have with it. There are several layers that you would want to understand. Why is it difficult and to whom – if could be providers, clinic staff, billing staff, etc. You might also want to understand the challenges for the collecting agency – and understand the barriers on their side too. Low incentive in the clinic would affect all expect for the patient. <br />You may also want to consider other barriers and facilitators. Among the ones you have listed for staff their might be addition challenges to consider than what you have listed – is may be difficult to connect to the telemedicine system and there may be limitations when it is available, do other compete to use it all during the same time frame, is there only one or limited number of sites. On way to understand the “flow” would be to directly observe the procedure from start to finish (when the patient arrives to clinic until they leave). <br />Providers you may want to also think about more about. Time constraints is a big one that you have noted. However, there may be others, including examine rooms, their role in the procedure, distance from their exam room to the eye exam room, limitations in the availability to the service, duration until they get the results and how they connect the results with a patient, etc. Again watching the process would be very helpful and/or interview providers that get a lot or exams (the high performers) and the those who do not get many/any exams done (low performers). This information may really drive you intervention approach. <br />Hope this is useful";s:6:"parent";N;s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"d8922d94729a6c63bacfa9bb06a0b45a";}s:32:"2dd7cec8da052dd8981c4de074a1d6dd";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"mengelgau";s:4:"name";s:13:"Mike Engelgau";s:4:"mail";s:24:"michael.engelgau@nih.gov";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1540503834;}s:3:"raw";s:1112:"Thanks for your very thoughtful responses. In terms of determining the barriers and the facilitators, you are a bit challenged because the intervention is not being done currently such that you could observe. You could try to reconstruct what was done during the feasibility studies. You may want to consider finding another intervention that is being delivered in the clinic currently that would be similar to you SDP intervention use it as a proxy to assess what might be some of the clinic flow issues that will arise. Also, it would be useful to think about what would be a list of possible barriers for each of those involved – child, parent(s), nursing staff, provider staff, managing staff etc. so that you can probe during some of your focus groups and structured interviews. 
Another big challenge is the consumers – that is, engaging parents to want/demand this intervention. Focus groups might be helpful here. You may want to have at least 2 groups – one group that really wants/demands the service and a group that is not interested. You could then get the challenges from both perspectives. 
";s:5:"xhtml";s:1115:"Thanks for your very thoughtful responses. In terms of determining the barriers and the facilitators, you are a bit challenged because the intervention is not being done currently such that you could observe. You could try to reconstruct what was done during the feasibility studies. You may want to consider finding another intervention that is being delivered in the clinic currently that would be similar to you SDP intervention use it as a proxy to assess what might be some of the clinic flow issues that will arise. Also, it would be useful to think about what would be a list of possible barriers for each of those involved – child, parent(s), nursing staff, provider staff, managing staff etc. so that you can probe during some of your focus groups and structured interviews. <br />Another big challenge is the consumers – that is, engaging parents to want/demand this intervention. Focus groups might be helpful here. You may want to have at least 2 groups – one group that really wants/demands the service and a group that is not interested. You could then get the challenges from both perspectives.";s:6:"parent";s:32:"f291e4dc1d6fad5b9b9bf63ff1c0680e";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"2dd7cec8da052dd8981c4de074a1d6dd";}s:32:"00096acaa38abe565b82d02b287fa351";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bweiner";s:4:"name";s:12:"Bryan Weiner";s:4:"mail";s:15:"bjweiner@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1541446909;}s:3:"raw";s:301:"Emily, these look like plausible strategies. You note that you have selected them based on known barriers to the implementation of similar programs. What are those known barriers? A table showing the linkage between barriers and strategies can help improve the fit or match of problems and solutions. ";s:5:"xhtml";s:300:"Emily, these look like plausible strategies. You note that you have selected them based on known barriers to the implementation of similar programs. What are those known barriers? A table showing the linkage between barriers and strategies can help improve the fit or match of problems and solutions.";s:6:"parent";s:32:"c2f97288583a7b55a71a2fae65139bb8";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"00096acaa38abe565b82d02b287fa351";}s:32:"32d63c39ffde504454c20d9d3b237f88";a:8:{s:4:"user";a:5:{s:2:"id";s:7:"bweiner";s:4:"name";s:12:"Bryan Weiner";s:4:"mail";s:15:"bjweiner@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1541447183;}s:3:"raw";s:158:"REP makes a lot of sense as a framework or approach for implementing programs. Its a good process model or planning model for guiding implementation efforts. ";s:5:"xhtml";s:157:"REP makes a lot of sense as a framework or approach for implementing programs. Its a good process model or planning model for guiding implementation efforts.";s:6:"parent";s:32:"bf3d6c99cd9b98b8e3e42245351d7a66";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"32d63c39ffde504454c20d9d3b237f88";}s:32:"5fdf52b4a55c6319dda18f405999693a";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"chelfrich";s:4:"name";s:18:"Christian Helfrich";s:4:"mail";s:15:"helfrich@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1541657909;}s:3:"raw";s:1749:"Hi Karen, again, profuse apologies for being late with my reply.
What quantitative measures of effectiveness are you measuring? What you describe in terms of quantitative & qualitative methods sounds more like what I would call multi-methods--with the quantitative methods serving the purpose of testing effectiveness and the qualitative methods serving the purpose of assessing barriers & facilitators. I usually think of mixed-methods as an approach where quantitative and qualitative methods are integrated in some way to answer a common question. For example, in one of our recent de-implementation grants we heard from PCPs in baseline interviews that if another provider wrote the prescription for an inhaled corticosteroid (for a patient with mild COPD) they were unlikely to take the patient off the prescription even if they wouldn't have put the patient on the ICS in the first place (we hear things like "why rock the boat?" and "if they're having no adverse events...I might leave it"). So we revised our baseline survey to add an item about this in order to see if the qualitative finding generalized in our study population; we found about 2 out of 5 PCP respondents agreed they were unlikely to take a patient w/ COPD off an inherited ICS. What you're currently proposing is totally appropriate, and I just bring up the distinction in case (a) you can add language to keep your reviewers from getting tripped up and making it a critique; and (b) you might add a more integrated quant/qual component, e.g., using follow-up interviews to understand why reciprocal peer support supplemented by peer coaching is effective (if it is in your study) e.g., creating external accountability vs. problem solving of specific barriers.
Christian ";s:5:"xhtml";s:1801:"Hi Karen, again, profuse apologies for being late with my reply.<br />What quantitative measures of effectiveness are you measuring? What you describe in terms of quantitative &amp; qualitative methods sounds more like what I would call multi-methods--with the quantitative methods serving the purpose of testing effectiveness and the qualitative methods serving the purpose of assessing barriers &amp; facilitators. I usually think of mixed-methods as an approach where quantitative and qualitative methods are integrated in some way to answer a common question. For example, in one of our recent de-implementation grants we heard from PCPs in baseline interviews that if another provider wrote the prescription for an inhaled corticosteroid (for a patient with mild COPD) they were unlikely to take the patient off the prescription even if they wouldn&#039;t have put the patient on the ICS in the first place (we hear things like &quot;why rock the boat?&quot; and &quot;if they&#039;re having no adverse events...I might leave it&quot;). So we revised our baseline survey to add an item about this in order to see if the qualitative finding generalized in our study population; we found about 2 out of 5 PCP respondents agreed they were unlikely to take a patient w/ COPD off an inherited ICS. What you&#039;re currently proposing is totally appropriate, and I just bring up the distinction in case (a) you can add language to keep your reviewers from getting tripped up and making it a critique; and (b) you might add a more integrated quant/qual component, e.g., using follow-up interviews to understand why reciprocal peer support supplemented by peer coaching is effective (if it is in your study) e.g., creating external accountability vs. problem solving of specific barriers.<br />Christian";s:6:"parent";s:32:"d06014874848a12c88b2f35e15ffcd8d";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"5fdf52b4a55c6319dda18f405999693a";}s:32:"070cd6514d30045b3f6fcfbf67903634";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"chelfrich";s:4:"name";s:18:"Christian Helfrich";s:4:"mail";s:15:"helfrich@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1541658182;}s:3:"raw";s:168:"The PCP concern over peer-coaches acting outside there specified scope is interesting. How are peer-coaches trained? And how is there practice scope assessed?
Christian";s:5:"xhtml";s:173:"The PCP concern over peer-coaches acting outside there specified scope is interesting. How are peer-coaches trained? And how is there practice scope assessed?<br />Christian";s:6:"parent";s:32:"4948e506f6e45b5554e7ca60dafe3a38";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"070cd6514d30045b3f6fcfbf67903634";}s:32:"4f2a5ca8e3630fc2dfa568c016fe3981";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"chelfrich";s:4:"name";s:18:"Christian Helfrich";s:4:"mail";s:15:"helfrich@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1541659560;}s:3:"raw";s:2495:"Hi Stuti, my profuse apologies for the delay in responding. And thanks for this explanation. In terms of conducting a hybrid II design, what's the implementation strategy or approach you'll be using? Usually, if we're trying to understand barriers and facilitators to using home telehealth while testing its effectiveness, we'd probably call that a hybrid 1. The sequential roll out over 8 VISNs might offer a great opportunity to develop an initial implementation strategy (e.g., an implementation guide with some remote facilitation) and then tweak it sequentially: you evaluate how it works in the first 2 VISNS and keep what works and revise what doesn't and take suggestions from the VISNs on things that would be good to add. You then test that revised package of implementation tools/strategies in the next group. That might be what you're proposing. 

Regarding contextual factors I'd encourage you to anticipate and distinguish between immutable (or mostly immutable) contextual factors that you just have to take account of and mutable contextual factors that you can try to change. For example, in one of our implementation trials we're testing a coaching intervention to promote use of an alternative way of performing coronary catheterizations via the radial artery in the wrist rather than the femoral artery in the groin (trans-radial caths are safer, more comfortable for patients and cheaper per episode of care, but technically more challenging and entail a sometimes frustrating learning curve for cardiologists proficient at trans-femoral caths). From previous work, we know that sometimes the cath lab director or a senior cardiologist don't like the idea, and in those settings encouraging a junior cardiologist to push back is unlikely to be effective and highly likely to create further resistance (it's low win situation). Conversely, we often encounter sites where a key obstacle is that when patients come out of the cath lab they're sent to the floor and the trans-radial patients will have an inflatable closure device that the floor nurses aren't familiar with and don't want to deal with; we've found multiple successful strategies for dealing with that from training (the closure device isn't as scary or complicated as they think) to changing policy to keep the patient in the cath lab observation space longer. Both are contextual challenges, but they're totally different animals in terms of how we recommend participants respond to them. Does that make sense?";s:5:"xhtml";s:2585:"Hi Stuti, my profuse apologies for the delay in responding. And thanks for this explanation. In terms of conducting a hybrid II design, what&#039;s the implementation strategy or approach you&#039;ll be using? Usually, if we&#039;re trying to understand barriers and facilitators to using home telehealth while testing its effectiveness, we&#039;d probably call that a hybrid 1. The sequential roll out over 8 VISNs might offer a great opportunity to develop an initial implementation strategy (e.g., an implementation guide with some remote facilitation) and then tweak it sequentially: you evaluate how it works in the first 2 VISNS and keep what works and revise what doesn&#039;t and take suggestions from the VISNs on things that would be good to add. You then test that revised package of implementation tools/strategies in the next group. That might be what you&#039;re proposing. <br /><br />Regarding contextual factors I&#039;d encourage you to anticipate and distinguish between immutable (or mostly immutable) contextual factors that you just have to take account of and mutable contextual factors that you can try to change. For example, in one of our implementation trials we&#039;re testing a coaching intervention to promote use of an alternative way of performing coronary catheterizations via the radial artery in the wrist rather than the femoral artery in the groin (trans-radial caths are safer, more comfortable for patients and cheaper per episode of care, but technically more challenging and entail a sometimes frustrating learning curve for cardiologists proficient at trans-femoral caths). From previous work, we know that sometimes the cath lab director or a senior cardiologist don&#039;t like the idea, and in those settings encouraging a junior cardiologist to push back is unlikely to be effective and highly likely to create further resistance (it&#039;s low win situation). Conversely, we often encounter sites where a key obstacle is that when patients come out of the cath lab they&#039;re sent to the floor and the trans-radial patients will have an inflatable closure device that the floor nurses aren&#039;t familiar with and don&#039;t want to deal with; we&#039;ve found multiple successful strategies for dealing with that from training (the closure device isn&#039;t as scary or complicated as they think) to changing policy to keep the patient in the cath lab observation space longer. Both are contextual challenges, but they&#039;re totally different animals in terms of how we recommend participants respond to them. Does that make sense?";s:6:"parent";s:32:"4960c3ad6f772f7ae405ff3d7018f4f2";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"4f2a5ca8e3630fc2dfa568c016fe3981";}s:32:"6c9864ee2320a0f60d0cf8e7296811e1";a:8:{s:4:"user";a:5:{s:2:"id";s:9:"chelfrich";s:4:"name";s:18:"Christian Helfrich";s:4:"mail";s:15:"helfrich@uw.edu";s:7:"address";s:0:"";s:3:"url";s:0:"";}s:4:"date";a:1:{s:7:"created";i:1541660867;}s:3:"raw";s:1345:"As part of your formative work, you might want to determine what appears to be the minimum level of use of HT that's effective (by whatever metrics you're assessing effectiveness). Our tendency is to establish high expectations about the level of engagement we expect and want, but sometimes we might be better served thinking about what's the minimum that delivers a benefit so we know what to work up from.
The barriers you have identified are really helpful to see and understand. Regarding the HBPC concern over having to trouble-shoot the technology, I wonder if there are other models of the introduction of new technology that staff have to manage, and lessons about how to best do that? E.g., I wonder about programs like SCAN-ECHO where they had to set-up tele-video contacts at CBOCs where there wasn't tech support--that was a problem initially. I wonder if there are applicable lessons from that experience?
Regarding the observation about how things are different when HBPC & HT are under the same service line vs. different service lines--that's a great opportunity for an organizational behavior/systems dynamics aim: what is it about being within the same service line vs. different service lines that makes a difference in HT use? Are there downsides to when HT is in the same service line as HBPC? 
Interesting stuff!
Christian";s:5:"xhtml";s:1394:"As part of your formative work, you might want to determine what appears to be the minimum level of use of HT that&#039;s effective (by whatever metrics you&#039;re assessing effectiveness). Our tendency is to establish high expectations about the level of engagement we expect and want, but sometimes we might be better served thinking about what&#039;s the minimum that delivers a benefit so we know what to work up from.<br />The barriers you have identified are really helpful to see and understand. Regarding the HBPC concern over having to trouble-shoot the technology, I wonder if there are other models of the introduction of new technology that staff have to manage, and lessons about how to best do that? E.g., I wonder about programs like SCAN-ECHO where they had to set-up tele-video contacts at CBOCs where there wasn&#039;t tech support--that was a problem initially. I wonder if there are applicable lessons from that experience?<br />Regarding the observation about how things are different when HBPC &amp; HT are under the same service line vs. different service lines--that&#039;s a great opportunity for an organizational behavior/systems dynamics aim: what is it about being within the same service line vs. different service lines that makes a difference in HT use? Are there downsides to when HT is in the same service line as HBPC? <br />Interesting stuff!<br />Christian";s:6:"parent";s:32:"d0092ee21c581059a2c1eda4174b8c22";s:7:"replies";a:0:{}s:4:"show";b:1;s:3:"cid";s:32:"6c9864ee2320a0f60d0cf8e7296811e1";}}s:11:"subscribers";N;}